{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607ef78c-bad3-4306-9a66-401f2a0b2cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid, Flickr\n",
    "from torch_geometric.loader import GraphSAINTRandomWalkSampler, GraphSAINTNodeSampler\n",
    "\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "from graph_gpt_classification import Graph_GPT_Classification\n",
    "from baselines import GCN_Classification, GAT_Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4996c4d-9977-4175-afbe-24fde1f2c648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[89250, 500], edge_index=[2, 899756], y=[89250], train_mask=[89250], val_mask=[89250], test_mask=[89250])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = Planetoid(root = '/tmp/Cora', name = 'Cora')\n",
    "dataset = Flickr(root = './tmp/Flickr')\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f447c49f-1950-4a70-850e-c289e08edd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GraphSAINTNodeSampler(\n",
    "    dataset[0],\n",
    "    batch_size = 1280, \n",
    "    num_steps = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd71918b-9fc5-4053-85da-561417a624ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5e15cd4-c193-4210-873a-cfabf9c73c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subgraph in loader:\n",
    "    subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f44a7bb4-aa93-4f34-b4b9-6808da3012a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=711, edge_index=[2, 1816], x=[711, 500], y=[711], train_mask=[711], val_mask=[711], test_mask=[711])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf5d2a5-a8ea-4c2a-ba23-5221b3ec67d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5276d20-db71-4976-a743-0bb666b4c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0].to(device)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc35cd-4679-4a19-8c11-788ae77163d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_model = GAT_Classification(dataset.num_node_features, \n",
    "                               100,\n",
    "                               100,\n",
    "                               dataset.num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(gat_model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "gat_model.train()\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    out = gat_model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        t_loss = F.nll_loss(out[data.test_mask], data.y[data.test_mask])\n",
    "        print(f'epoch: {epoch+1}, training loss: {loss.item()}, testing loss: {t_loss.item()}')\n",
    "        \n",
    "        pred = gat_model(data).argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "        acc = int(correct) / int(data.test_mask.sum())\n",
    "        print(f'Accuracy: {acc:.4f}')\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc24d4a-1165-49cb-8841-6e68935ca3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilize model\n",
    "gcn_model = GCN_Classification(dataset.num_node_features, \n",
    "                               100,\n",
    "                               100,\n",
    "                               dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c83875c5-fce4-4ea7-834d-a5caee2fdb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, training loss: 1.4964473247528076, testing loss: 1.537003517150879\n",
      "Accuracy: 0.4413\n",
      "epoch: 10, training loss: 1.4905471801757812, testing loss: 1.603054165840149\n",
      "Accuracy: 0.4327\n",
      "epoch: 10, training loss: 1.5389080047607422, testing loss: 1.5880107879638672\n",
      "Accuracy: 0.4224\n",
      "epoch: 10, training loss: 1.4966007471084595, testing loss: 1.5964055061340332\n",
      "Accuracy: 0.4255\n",
      "epoch: 10, training loss: 1.5071686506271362, testing loss: 1.5903626680374146\n",
      "Accuracy: 0.4241\n",
      "epoch: 10, training loss: 1.5266331434249878, testing loss: 1.583277940750122\n",
      "Accuracy: 0.4204\n",
      "epoch: 10, training loss: 1.5424693822860718, testing loss: 1.6965174674987793\n",
      "Accuracy: 0.3949\n",
      "epoch: 10, training loss: 1.4544286727905273, testing loss: 1.5270622968673706\n",
      "Accuracy: 0.4519\n",
      "epoch: 10, training loss: 1.531178593635559, testing loss: 1.6760027408599854\n",
      "Accuracy: 0.4362\n",
      "epoch: 10, training loss: 1.476992130279541, testing loss: 1.5219155550003052\n",
      "Accuracy: 0.4774\n",
      "epoch: 10, training loss: 1.5462515354156494, testing loss: 1.5686216354370117\n",
      "Accuracy: 0.4148\n",
      "epoch: 10, training loss: 1.4920421838760376, testing loss: 1.6056370735168457\n",
      "Accuracy: 0.4536\n",
      "epoch: 10, training loss: 1.5915035009384155, testing loss: 1.51649010181427\n",
      "Accuracy: 0.4531\n",
      "epoch: 10, training loss: 1.5515698194503784, testing loss: 1.5407235622406006\n",
      "Accuracy: 0.4412\n",
      "epoch: 10, training loss: 1.5445605516433716, testing loss: 1.5401490926742554\n",
      "Accuracy: 0.4428\n",
      "epoch: 10, training loss: 1.5842422246932983, testing loss: 1.5573655366897583\n",
      "Accuracy: 0.4597\n",
      "epoch: 10, training loss: 1.5812432765960693, testing loss: 1.5287179946899414\n",
      "Accuracy: 0.4578\n",
      "epoch: 10, training loss: 1.5151489973068237, testing loss: 1.6352778673171997\n",
      "Accuracy: 0.4309\n",
      "epoch: 10, training loss: 1.4604911804199219, testing loss: 1.5680127143859863\n",
      "Accuracy: 0.4613\n",
      "epoch: 10, training loss: 1.5545588731765747, testing loss: 1.5860910415649414\n",
      "Accuracy: 0.4371\n",
      "epoch: 10, training loss: 1.5551795959472656, testing loss: 1.5657000541687012\n",
      "Accuracy: 0.4318\n",
      "epoch: 10, training loss: 1.5248029232025146, testing loss: 1.6629278659820557\n",
      "Accuracy: 0.3841\n",
      "epoch: 10, training loss: 1.5777291059494019, testing loss: 1.4461638927459717\n",
      "Accuracy: 0.4741\n",
      "epoch: 10, training loss: 1.5223019123077393, testing loss: 1.5968959331512451\n",
      "Accuracy: 0.4568\n",
      "epoch: 10, training loss: 1.539665699005127, testing loss: 1.6216559410095215\n",
      "Accuracy: 0.4448\n",
      "epoch: 10, training loss: 1.5060526132583618, testing loss: 1.5577348470687866\n",
      "Accuracy: 0.4606\n",
      "epoch: 10, training loss: 1.5705217123031616, testing loss: 1.62293541431427\n",
      "Accuracy: 0.4603\n",
      "epoch: 10, training loss: 1.5034236907958984, testing loss: 1.5340614318847656\n",
      "Accuracy: 0.4391\n",
      "epoch: 10, training loss: 1.4719293117523193, testing loss: 1.536219835281372\n",
      "Accuracy: 0.4644\n",
      "epoch: 10, training loss: 1.5124766826629639, testing loss: 1.4958056211471558\n",
      "Accuracy: 0.4839\n",
      "epoch: 10, training loss: 1.5476088523864746, testing loss: 1.670912742614746\n",
      "Accuracy: 0.4058\n",
      "epoch: 10, training loss: 1.4766426086425781, testing loss: 1.5970895290374756\n",
      "Accuracy: 0.4033\n",
      "epoch: 10, training loss: 1.5093327760696411, testing loss: 1.577443242073059\n",
      "Accuracy: 0.4214\n",
      "epoch: 10, training loss: 1.4865736961364746, testing loss: 1.6137598752975464\n",
      "Accuracy: 0.4335\n",
      "epoch: 10, training loss: 1.4983007907867432, testing loss: 1.5507320165634155\n",
      "Accuracy: 0.4529\n",
      "epoch: 10, training loss: 1.4990100860595703, testing loss: 1.5612661838531494\n",
      "Accuracy: 0.4769\n",
      "epoch: 10, training loss: 1.4929382801055908, testing loss: 1.4779351949691772\n",
      "Accuracy: 0.4730\n",
      "epoch: 10, training loss: 1.5334476232528687, testing loss: 1.5777003765106201\n",
      "Accuracy: 0.4899\n",
      "epoch: 10, training loss: 1.5578588247299194, testing loss: 1.6125575304031372\n",
      "Accuracy: 0.4379\n",
      "epoch: 10, training loss: 1.472123622894287, testing loss: 1.5434966087341309\n",
      "Accuracy: 0.4424\n",
      "epoch: 10, training loss: 1.5063583850860596, testing loss: 1.5980032682418823\n",
      "Accuracy: 0.3988\n",
      "epoch: 10, training loss: 1.4987428188323975, testing loss: 1.6470693349838257\n",
      "Accuracy: 0.4167\n",
      "epoch: 10, training loss: 1.4810271263122559, testing loss: 1.5550413131713867\n",
      "Accuracy: 0.4098\n",
      "epoch: 10, training loss: 1.556089997291565, testing loss: 1.5047107934951782\n",
      "Accuracy: 0.4797\n",
      "epoch: 10, training loss: 1.4864087104797363, testing loss: 1.4904320240020752\n",
      "Accuracy: 0.4713\n",
      "epoch: 10, training loss: 1.5039864778518677, testing loss: 1.5585052967071533\n",
      "Accuracy: 0.4643\n",
      "epoch: 10, training loss: 1.5521610975265503, testing loss: 1.6523103713989258\n",
      "Accuracy: 0.4381\n",
      "epoch: 10, training loss: 1.5339293479919434, testing loss: 1.5106933116912842\n",
      "Accuracy: 0.4693\n",
      "epoch: 10, training loss: 1.5307716131210327, testing loss: 1.5491124391555786\n",
      "Accuracy: 0.4455\n",
      "epoch: 10, training loss: 1.5904557704925537, testing loss: 1.5217833518981934\n",
      "Accuracy: 0.4303\n",
      "epoch: 10, training loss: 1.521405577659607, testing loss: 1.587062120437622\n",
      "Accuracy: 0.4290\n",
      "epoch: 10, training loss: 1.5137791633605957, testing loss: 1.5411183834075928\n",
      "Accuracy: 0.4643\n",
      "epoch: 10, training loss: 1.5392451286315918, testing loss: 1.5005360841751099\n",
      "Accuracy: 0.4985\n",
      "epoch: 10, training loss: 1.5032392740249634, testing loss: 1.6500866413116455\n",
      "Accuracy: 0.3956\n",
      "epoch: 10, training loss: 1.5206748247146606, testing loss: 1.5277642011642456\n",
      "Accuracy: 0.4545\n",
      "epoch: 10, training loss: 1.5506720542907715, testing loss: 1.6132868528366089\n",
      "Accuracy: 0.4351\n",
      "epoch: 10, training loss: 1.5396236181259155, testing loss: 1.5217047929763794\n",
      "Accuracy: 0.4400\n",
      "epoch: 10, training loss: 1.5186694860458374, testing loss: 1.6149367094039917\n",
      "Accuracy: 0.4389\n",
      "epoch: 10, training loss: 1.5298175811767578, testing loss: 1.5587486028671265\n",
      "Accuracy: 0.4386\n",
      "epoch: 10, training loss: 1.5339001417160034, testing loss: 1.507586121559143\n",
      "Accuracy: 0.4233\n",
      "epoch: 10, training loss: 1.502917766571045, testing loss: 1.5644125938415527\n",
      "Accuracy: 0.4371\n",
      "epoch: 10, training loss: 1.5256757736206055, testing loss: 1.6081393957138062\n",
      "Accuracy: 0.4251\n",
      "epoch: 10, training loss: 1.5148210525512695, testing loss: 1.5326050519943237\n",
      "Accuracy: 0.4277\n",
      "epoch: 10, training loss: 1.5395557880401611, testing loss: 1.4992402791976929\n",
      "Accuracy: 0.4613\n",
      "epoch: 10, training loss: 1.4556907415390015, testing loss: 1.523391842842102\n",
      "Accuracy: 0.4564\n",
      "epoch: 10, training loss: 1.5251225233078003, testing loss: 1.5956203937530518\n",
      "Accuracy: 0.4150\n",
      "epoch: 10, training loss: 1.5192434787750244, testing loss: 1.5945863723754883\n",
      "Accuracy: 0.4266\n",
      "epoch: 10, training loss: 1.5296788215637207, testing loss: 1.6368669271469116\n",
      "Accuracy: 0.4235\n",
      "epoch: 10, training loss: 1.4939029216766357, testing loss: 1.624746322631836\n",
      "Accuracy: 0.4328\n",
      "epoch: 10, training loss: 1.5153917074203491, testing loss: 1.5731278657913208\n",
      "Accuracy: 0.4545\n",
      "epoch: 10, training loss: 1.501241683959961, testing loss: 1.5875658988952637\n",
      "Accuracy: 0.4618\n",
      "epoch: 10, training loss: 1.539292812347412, testing loss: 1.5653773546218872\n",
      "Accuracy: 0.4685\n",
      "epoch: 10, training loss: 1.4569015502929688, testing loss: 1.547838568687439\n",
      "Accuracy: 0.4512\n",
      "epoch: 10, training loss: 1.5163488388061523, testing loss: 1.5581685304641724\n",
      "Accuracy: 0.4816\n",
      "epoch: 10, training loss: 1.5010826587677002, testing loss: 1.540647029876709\n",
      "Accuracy: 0.4633\n",
      "epoch: 10, training loss: 1.485434889793396, testing loss: 1.5396974086761475\n",
      "Accuracy: 0.4904\n",
      "epoch: 10, training loss: 1.5283403396606445, testing loss: 1.5926882028579712\n",
      "Accuracy: 0.4414\n",
      "epoch: 10, training loss: 1.500644326210022, testing loss: 1.5309792757034302\n",
      "Accuracy: 0.4469\n",
      "epoch: 10, training loss: 1.497201919555664, testing loss: 1.5737993717193604\n",
      "Accuracy: 0.4262\n",
      "epoch: 10, training loss: 1.5115300416946411, testing loss: 1.4459728002548218\n",
      "Accuracy: 0.4797\n",
      "epoch: 10, training loss: 1.5081567764282227, testing loss: 1.5458987951278687\n",
      "Accuracy: 0.4545\n",
      "epoch: 10, training loss: 1.5047907829284668, testing loss: 1.619674801826477\n",
      "Accuracy: 0.3926\n",
      "epoch: 10, training loss: 1.4757887125015259, testing loss: 1.622510313987732\n",
      "Accuracy: 0.3988\n",
      "epoch: 10, training loss: 1.5143781900405884, testing loss: 1.571925163269043\n",
      "Accuracy: 0.4314\n",
      "epoch: 10, training loss: 1.4970433712005615, testing loss: 1.6188889741897583\n",
      "Accuracy: 0.4528\n",
      "epoch: 10, training loss: 1.487181544303894, testing loss: 1.553311824798584\n",
      "Accuracy: 0.4701\n",
      "epoch: 10, training loss: 1.5307691097259521, testing loss: 1.5978833436965942\n",
      "Accuracy: 0.4633\n",
      "epoch: 10, training loss: 1.519113540649414, testing loss: 1.5907715559005737\n",
      "Accuracy: 0.4389\n",
      "epoch: 10, training loss: 1.5116093158721924, testing loss: 1.6156518459320068\n",
      "Accuracy: 0.4271\n",
      "epoch: 10, training loss: 1.5493470430374146, testing loss: 1.7118932008743286\n",
      "Accuracy: 0.4375\n",
      "epoch: 10, training loss: 1.5048558712005615, testing loss: 1.5653873682022095\n",
      "Accuracy: 0.4197\n",
      "epoch: 10, training loss: 1.48822820186615, testing loss: 1.5305861234664917\n",
      "Accuracy: 0.4295\n",
      "epoch: 10, training loss: 1.4855506420135498, testing loss: 1.5744985342025757\n",
      "Accuracy: 0.4456\n",
      "epoch: 10, training loss: 1.4512505531311035, testing loss: 1.6041254997253418\n",
      "Accuracy: 0.4082\n",
      "epoch: 10, training loss: 1.5475608110427856, testing loss: 1.572230339050293\n",
      "Accuracy: 0.4426\n",
      "epoch: 10, training loss: 1.5249627828598022, testing loss: 1.4731968641281128\n",
      "Accuracy: 0.4675\n",
      "epoch: 10, training loss: 1.4842195510864258, testing loss: 1.5766140222549438\n",
      "Accuracy: 0.4430\n",
      "epoch: 10, training loss: 1.4942255020141602, testing loss: 1.6275750398635864\n",
      "Accuracy: 0.4233\n",
      "epoch: 10, training loss: 1.5048786401748657, testing loss: 1.6009598970413208\n",
      "Accuracy: 0.4423\n",
      "epoch: 10, training loss: 1.5239343643188477, testing loss: 1.6190211772918701\n",
      "Accuracy: 0.4109\n",
      "epoch: 20, training loss: 1.488372564315796, testing loss: 1.4913337230682373\n",
      "Accuracy: 0.4784\n",
      "epoch: 20, training loss: 1.4627766609191895, testing loss: 1.6226729154586792\n",
      "Accuracy: 0.4624\n",
      "epoch: 20, training loss: 1.5377229452133179, testing loss: 1.5816881656646729\n",
      "Accuracy: 0.4318\n",
      "epoch: 20, training loss: 1.4731765985488892, testing loss: 1.516097068786621\n",
      "Accuracy: 0.4679\n",
      "epoch: 20, training loss: 1.4393320083618164, testing loss: 1.520853042602539\n",
      "Accuracy: 0.4190\n",
      "epoch: 20, training loss: 1.46338951587677, testing loss: 1.490642786026001\n",
      "Accuracy: 0.4745\n",
      "epoch: 20, training loss: 1.484534740447998, testing loss: 1.5826350450515747\n",
      "Accuracy: 0.4290\n",
      "epoch: 20, training loss: 1.4987541437149048, testing loss: 1.5742623805999756\n",
      "Accuracy: 0.4383\n",
      "epoch: 20, training loss: 1.4802166223526, testing loss: 1.5860751867294312\n",
      "Accuracy: 0.4379\n",
      "epoch: 20, training loss: 1.476303219795227, testing loss: 1.545266032218933\n",
      "Accuracy: 0.4441\n",
      "epoch: 20, training loss: 1.499157428741455, testing loss: 1.522177815437317\n",
      "Accuracy: 0.4589\n",
      "epoch: 20, training loss: 1.4965338706970215, testing loss: 1.5267280340194702\n",
      "Accuracy: 0.4671\n",
      "epoch: 20, training loss: 1.4599539041519165, testing loss: 1.518212914466858\n",
      "Accuracy: 0.4286\n",
      "epoch: 20, training loss: 1.4808188676834106, testing loss: 1.6016874313354492\n",
      "Accuracy: 0.3937\n",
      "epoch: 20, training loss: 1.4747109413146973, testing loss: 1.5798797607421875\n",
      "Accuracy: 0.4158\n",
      "epoch: 20, training loss: 1.5241118669509888, testing loss: 1.525985836982727\n",
      "Accuracy: 0.4461\n",
      "epoch: 20, training loss: 1.3755003213882446, testing loss: 1.5376107692718506\n",
      "Accuracy: 0.4314\n",
      "epoch: 20, training loss: 1.4775874614715576, testing loss: 1.5614237785339355\n",
      "Accuracy: 0.4225\n",
      "epoch: 20, training loss: 1.4108325242996216, testing loss: 1.509769082069397\n",
      "Accuracy: 0.4332\n",
      "epoch: 20, training loss: 1.49004328250885, testing loss: 1.6220768690109253\n",
      "Accuracy: 0.4613\n",
      "epoch: 20, training loss: 1.4828691482543945, testing loss: 1.5906811952590942\n",
      "Accuracy: 0.3941\n",
      "epoch: 20, training loss: 1.5065492391586304, testing loss: 1.5876867771148682\n",
      "Accuracy: 0.4387\n",
      "epoch: 20, training loss: 1.4796595573425293, testing loss: 1.5885032415390015\n",
      "Accuracy: 0.4047\n",
      "epoch: 20, training loss: 1.4204990863800049, testing loss: 1.4799965620040894\n",
      "Accuracy: 0.4870\n",
      "epoch: 20, training loss: 1.444007158279419, testing loss: 1.5491183996200562\n",
      "Accuracy: 0.4535\n",
      "epoch: 20, training loss: 1.4777517318725586, testing loss: 1.6196049451828003\n",
      "Accuracy: 0.4228\n",
      "epoch: 20, training loss: 1.4886901378631592, testing loss: 1.5373479127883911\n",
      "Accuracy: 0.4680\n",
      "epoch: 20, training loss: 1.4734543561935425, testing loss: 1.5091239213943481\n",
      "Accuracy: 0.4580\n",
      "epoch: 20, training loss: 1.409528136253357, testing loss: 1.7227230072021484\n",
      "Accuracy: 0.3574\n",
      "epoch: 20, training loss: 1.5271902084350586, testing loss: 1.6411398649215698\n",
      "Accuracy: 0.3738\n",
      "epoch: 20, training loss: 1.450890302658081, testing loss: 1.5615638494491577\n",
      "Accuracy: 0.4377\n",
      "epoch: 20, training loss: 1.494586706161499, testing loss: 1.5674070119857788\n",
      "Accuracy: 0.4381\n",
      "epoch: 20, training loss: 1.4697023630142212, testing loss: 1.5337048768997192\n",
      "Accuracy: 0.4631\n",
      "epoch: 20, training loss: 1.5022952556610107, testing loss: 1.5642528533935547\n",
      "Accuracy: 0.4646\n",
      "epoch: 20, training loss: 1.4968613386154175, testing loss: 1.542301893234253\n",
      "Accuracy: 0.4565\n",
      "epoch: 20, training loss: 1.4595555067062378, testing loss: 1.6333314180374146\n",
      "Accuracy: 0.4338\n",
      "epoch: 20, training loss: 1.5263373851776123, testing loss: 1.5742897987365723\n",
      "Accuracy: 0.4048\n",
      "epoch: 20, training loss: 1.4514656066894531, testing loss: 1.5558487176895142\n",
      "Accuracy: 0.4801\n",
      "epoch: 20, training loss: 1.4731848239898682, testing loss: 1.5903401374816895\n",
      "Accuracy: 0.4246\n",
      "epoch: 20, training loss: 1.4203243255615234, testing loss: 1.5216251611709595\n",
      "Accuracy: 0.4307\n",
      "epoch: 20, training loss: 1.461654782295227, testing loss: 1.5537058115005493\n",
      "Accuracy: 0.4569\n",
      "epoch: 20, training loss: 1.4534040689468384, testing loss: 1.6017415523529053\n",
      "Accuracy: 0.4103\n",
      "epoch: 20, training loss: 1.4309799671173096, testing loss: 1.5212645530700684\n",
      "Accuracy: 0.4764\n",
      "epoch: 20, training loss: 1.4265564680099487, testing loss: 1.6014924049377441\n",
      "Accuracy: 0.4669\n",
      "epoch: 20, training loss: 1.4883610010147095, testing loss: 1.572256326675415\n",
      "Accuracy: 0.4331\n",
      "epoch: 20, training loss: 1.4905085563659668, testing loss: 1.490264654159546\n",
      "Accuracy: 0.4752\n",
      "epoch: 20, training loss: 1.4860281944274902, testing loss: 1.5875489711761475\n",
      "Accuracy: 0.4184\n",
      "epoch: 20, training loss: 1.4369213581085205, testing loss: 1.6990941762924194\n",
      "Accuracy: 0.3839\n",
      "epoch: 20, training loss: 1.4887667894363403, testing loss: 1.5321232080459595\n",
      "Accuracy: 0.4502\n",
      "epoch: 20, training loss: 1.4648151397705078, testing loss: 1.6051933765411377\n",
      "Accuracy: 0.3925\n",
      "epoch: 20, training loss: 1.4571508169174194, testing loss: 1.6079118251800537\n",
      "Accuracy: 0.4232\n",
      "epoch: 20, training loss: 1.4843775033950806, testing loss: 1.5067535638809204\n",
      "Accuracy: 0.4295\n",
      "epoch: 20, training loss: 1.448606014251709, testing loss: 1.5728354454040527\n",
      "Accuracy: 0.4228\n",
      "epoch: 20, training loss: 1.4934138059616089, testing loss: 1.459707260131836\n",
      "Accuracy: 0.4490\n",
      "epoch: 20, training loss: 1.4428720474243164, testing loss: 1.509782075881958\n",
      "Accuracy: 0.4596\n",
      "epoch: 20, training loss: 1.4530134201049805, testing loss: 1.530259132385254\n",
      "Accuracy: 0.4377\n",
      "epoch: 20, training loss: 1.4788779020309448, testing loss: 1.5469142198562622\n",
      "Accuracy: 0.4309\n",
      "epoch: 20, training loss: 1.4898930788040161, testing loss: 1.6032819747924805\n",
      "Accuracy: 0.4156\n",
      "epoch: 20, training loss: 1.4956837892532349, testing loss: 1.5242913961410522\n",
      "Accuracy: 0.4805\n",
      "epoch: 20, training loss: 1.4819377660751343, testing loss: 1.6096208095550537\n",
      "Accuracy: 0.4326\n",
      "epoch: 20, training loss: 1.470201849937439, testing loss: 1.5719929933547974\n",
      "Accuracy: 0.4525\n",
      "epoch: 20, training loss: 1.5248143672943115, testing loss: 1.5519218444824219\n",
      "Accuracy: 0.4334\n",
      "epoch: 20, training loss: 1.4921672344207764, testing loss: 1.638211727142334\n",
      "Accuracy: 0.4424\n",
      "epoch: 20, training loss: 1.5168503522872925, testing loss: 1.5401611328125\n",
      "Accuracy: 0.4522\n",
      "epoch: 20, training loss: 1.4733935594558716, testing loss: 1.5852864980697632\n",
      "Accuracy: 0.4465\n",
      "epoch: 20, training loss: 1.465079426765442, testing loss: 1.5683332681655884\n",
      "Accuracy: 0.4286\n",
      "epoch: 20, training loss: 1.5080652236938477, testing loss: 1.4848347902297974\n",
      "Accuracy: 0.4899\n",
      "epoch: 20, training loss: 1.4496400356292725, testing loss: 1.6008086204528809\n",
      "Accuracy: 0.3994\n",
      "epoch: 20, training loss: 1.4445809125900269, testing loss: 1.5494580268859863\n",
      "Accuracy: 0.4915\n",
      "epoch: 20, training loss: 1.4690154790878296, testing loss: 1.5019440650939941\n",
      "Accuracy: 0.4430\n",
      "epoch: 20, training loss: 1.4357085227966309, testing loss: 1.643553376197815\n",
      "Accuracy: 0.4330\n",
      "epoch: 20, training loss: 1.4670883417129517, testing loss: 1.5493183135986328\n",
      "Accuracy: 0.4543\n",
      "epoch: 20, training loss: 1.5024276971817017, testing loss: 1.5952205657958984\n",
      "Accuracy: 0.4323\n",
      "epoch: 20, training loss: 1.4720866680145264, testing loss: 1.5763607025146484\n",
      "Accuracy: 0.4305\n",
      "epoch: 20, training loss: 1.51058030128479, testing loss: 1.5861990451812744\n",
      "Accuracy: 0.4456\n",
      "epoch: 20, training loss: 1.4433996677398682, testing loss: 1.5058400630950928\n",
      "Accuracy: 0.4906\n",
      "epoch: 20, training loss: 1.4628968238830566, testing loss: 1.6624505519866943\n",
      "Accuracy: 0.3878\n",
      "epoch: 20, training loss: 1.5114086866378784, testing loss: 1.6200981140136719\n",
      "Accuracy: 0.4317\n",
      "epoch: 20, training loss: 1.558017373085022, testing loss: 1.5648928880691528\n",
      "Accuracy: 0.4039\n",
      "epoch: 20, training loss: 1.4847760200500488, testing loss: 1.5831438302993774\n",
      "Accuracy: 0.4456\n",
      "epoch: 20, training loss: 1.461696982383728, testing loss: 1.433802604675293\n",
      "Accuracy: 0.5112\n",
      "epoch: 20, training loss: 1.5448367595672607, testing loss: 1.5221301317214966\n",
      "Accuracy: 0.4756\n",
      "epoch: 20, training loss: 1.4587533473968506, testing loss: 1.5487391948699951\n",
      "Accuracy: 0.4367\n",
      "epoch: 20, training loss: 1.4378012418746948, testing loss: 1.5101808309555054\n",
      "Accuracy: 0.4691\n",
      "epoch: 20, training loss: 1.4442147016525269, testing loss: 1.5785105228424072\n",
      "Accuracy: 0.4574\n",
      "epoch: 20, training loss: 1.4426348209381104, testing loss: 1.5075217485427856\n",
      "Accuracy: 0.4801\n",
      "epoch: 20, training loss: 1.4696557521820068, testing loss: 1.5433481931686401\n",
      "Accuracy: 0.4533\n",
      "epoch: 20, training loss: 1.4525424242019653, testing loss: 1.6315419673919678\n",
      "Accuracy: 0.4112\n",
      "epoch: 20, training loss: 1.5049943923950195, testing loss: 1.5106768608093262\n",
      "Accuracy: 0.4777\n",
      "epoch: 20, training loss: 1.486136555671692, testing loss: 1.498245358467102\n",
      "Accuracy: 0.4702\n",
      "epoch: 20, training loss: 1.522344708442688, testing loss: 1.5034675598144531\n",
      "Accuracy: 0.4826\n",
      "epoch: 20, training loss: 1.3989734649658203, testing loss: 1.5236010551452637\n",
      "Accuracy: 0.4456\n",
      "epoch: 20, training loss: 1.454084873199463, testing loss: 1.5111186504364014\n",
      "Accuracy: 0.4792\n",
      "epoch: 20, training loss: 1.4874036312103271, testing loss: 1.6733535528182983\n",
      "Accuracy: 0.4271\n",
      "epoch: 20, training loss: 1.4194331169128418, testing loss: 1.547181248664856\n",
      "Accuracy: 0.4458\n",
      "epoch: 20, training loss: 1.5289020538330078, testing loss: 1.6260371208190918\n",
      "Accuracy: 0.4211\n",
      "epoch: 20, training loss: 1.4814876317977905, testing loss: 1.5514373779296875\n",
      "Accuracy: 0.4393\n",
      "epoch: 20, training loss: 1.4510596990585327, testing loss: 1.5746241807937622\n",
      "Accuracy: 0.4409\n",
      "epoch: 20, training loss: 1.451149821281433, testing loss: 1.5579986572265625\n",
      "Accuracy: 0.4031\n",
      "epoch: 20, training loss: 1.5352779626846313, testing loss: 1.5508416891098022\n",
      "Accuracy: 0.4198\n",
      "epoch: 30, training loss: 1.4590595960617065, testing loss: 1.5869131088256836\n",
      "Accuracy: 0.4105\n",
      "epoch: 30, training loss: 1.388097882270813, testing loss: 1.5558576583862305\n",
      "Accuracy: 0.4803\n",
      "epoch: 30, training loss: 1.4882547855377197, testing loss: 1.5038797855377197\n",
      "Accuracy: 0.4572\n",
      "epoch: 30, training loss: 1.4535020589828491, testing loss: 1.4920287132263184\n",
      "Accuracy: 0.4664\n",
      "epoch: 30, training loss: 1.4533244371414185, testing loss: 1.5952342748641968\n",
      "Accuracy: 0.4202\n",
      "epoch: 30, training loss: 1.4449303150177002, testing loss: 1.6395670175552368\n",
      "Accuracy: 0.4051\n",
      "epoch: 30, training loss: 1.4818259477615356, testing loss: 1.5142978429794312\n",
      "Accuracy: 0.4771\n",
      "epoch: 30, training loss: 1.4350210428237915, testing loss: 1.5311890840530396\n",
      "Accuracy: 0.4305\n",
      "epoch: 30, training loss: 1.4907341003417969, testing loss: 1.5122389793395996\n",
      "Accuracy: 0.4548\n",
      "epoch: 30, training loss: 1.4611140489578247, testing loss: 1.5610253810882568\n",
      "Accuracy: 0.4217\n",
      "epoch: 30, training loss: 1.4017448425292969, testing loss: 1.5553511381149292\n",
      "Accuracy: 0.4398\n",
      "epoch: 30, training loss: 1.3546714782714844, testing loss: 1.5194878578186035\n",
      "Accuracy: 0.4407\n",
      "epoch: 30, training loss: 1.5030993223190308, testing loss: 1.4353678226470947\n",
      "Accuracy: 0.4712\n",
      "epoch: 30, training loss: 1.451966643333435, testing loss: 1.6241682767868042\n",
      "Accuracy: 0.4169\n",
      "epoch: 30, training loss: 1.422300934791565, testing loss: 1.6491680145263672\n",
      "Accuracy: 0.4063\n",
      "epoch: 30, training loss: 1.3790113925933838, testing loss: 1.5493096113204956\n",
      "Accuracy: 0.4108\n",
      "epoch: 30, training loss: 1.4332767724990845, testing loss: 1.531711220741272\n",
      "Accuracy: 0.4426\n",
      "epoch: 30, training loss: 1.4552357196807861, testing loss: 1.51433265209198\n",
      "Accuracy: 0.4532\n",
      "epoch: 30, training loss: 1.4203698635101318, testing loss: 1.549637794494629\n",
      "Accuracy: 0.4396\n",
      "epoch: 30, training loss: 1.4981906414031982, testing loss: 1.488655924797058\n",
      "Accuracy: 0.4448\n",
      "epoch: 30, training loss: 1.4347875118255615, testing loss: 1.6225825548171997\n",
      "Accuracy: 0.4530\n",
      "epoch: 30, training loss: 1.4405237436294556, testing loss: 1.4733853340148926\n",
      "Accuracy: 0.4983\n",
      "epoch: 30, training loss: 1.4367977380752563, testing loss: 1.5069553852081299\n",
      "Accuracy: 0.5017\n",
      "epoch: 30, training loss: 1.4626010656356812, testing loss: 1.6627013683319092\n",
      "Accuracy: 0.4006\n",
      "epoch: 30, training loss: 1.405215859413147, testing loss: 1.5894792079925537\n",
      "Accuracy: 0.4000\n",
      "epoch: 30, training loss: 1.3799816370010376, testing loss: 1.590226411819458\n",
      "Accuracy: 0.4430\n",
      "epoch: 30, training loss: 1.4628417491912842, testing loss: 1.554969310760498\n",
      "Accuracy: 0.4393\n",
      "epoch: 30, training loss: 1.4574253559112549, testing loss: 1.4967259168624878\n",
      "Accuracy: 0.4514\n",
      "epoch: 30, training loss: 1.4418095350265503, testing loss: 1.5359097719192505\n",
      "Accuracy: 0.4423\n",
      "epoch: 30, training loss: 1.415372371673584, testing loss: 1.536604881286621\n",
      "Accuracy: 0.4575\n",
      "epoch: 30, training loss: 1.439436674118042, testing loss: 1.6022429466247559\n",
      "Accuracy: 0.3892\n",
      "epoch: 30, training loss: 1.4658933877944946, testing loss: 1.5730855464935303\n",
      "Accuracy: 0.4598\n",
      "epoch: 30, training loss: 1.4260233640670776, testing loss: 1.5737632513046265\n",
      "Accuracy: 0.4503\n",
      "epoch: 30, training loss: 1.4362094402313232, testing loss: 1.604681372642517\n",
      "Accuracy: 0.4077\n",
      "epoch: 30, training loss: 1.411165475845337, testing loss: 1.4932397603988647\n",
      "Accuracy: 0.4663\n",
      "epoch: 30, training loss: 1.468007206916809, testing loss: 1.6421302556991577\n",
      "Accuracy: 0.3907\n",
      "epoch: 30, training loss: 1.4322400093078613, testing loss: 1.5312844514846802\n",
      "Accuracy: 0.4625\n",
      "epoch: 30, training loss: 1.4228630065917969, testing loss: 1.5448248386383057\n",
      "Accuracy: 0.4058\n",
      "epoch: 30, training loss: 1.4523305892944336, testing loss: 1.4832127094268799\n",
      "Accuracy: 0.4236\n",
      "epoch: 30, training loss: 1.4541531801223755, testing loss: 1.5577609539031982\n",
      "Accuracy: 0.4074\n",
      "epoch: 30, training loss: 1.3661699295043945, testing loss: 1.6461924314498901\n",
      "Accuracy: 0.4224\n",
      "epoch: 30, training loss: 1.4227715730667114, testing loss: 1.6727373600006104\n",
      "Accuracy: 0.4038\n",
      "epoch: 30, training loss: 1.4538931846618652, testing loss: 1.5696609020233154\n",
      "Accuracy: 0.4357\n",
      "epoch: 30, training loss: 1.4308944940567017, testing loss: 1.5335891246795654\n",
      "Accuracy: 0.4471\n",
      "epoch: 30, training loss: 1.465119481086731, testing loss: 1.4236475229263306\n",
      "Accuracy: 0.4903\n",
      "epoch: 30, training loss: 1.4409414529800415, testing loss: 1.5707828998565674\n",
      "Accuracy: 0.4658\n",
      "epoch: 30, training loss: 1.4626251459121704, testing loss: 1.591262936592102\n",
      "Accuracy: 0.3645\n",
      "epoch: 30, training loss: 1.4470179080963135, testing loss: 1.6595202684402466\n",
      "Accuracy: 0.3506\n",
      "epoch: 30, training loss: 1.4360556602478027, testing loss: 1.5726337432861328\n",
      "Accuracy: 0.4380\n",
      "epoch: 30, training loss: 1.377905249595642, testing loss: 1.5524717569351196\n",
      "Accuracy: 0.4613\n",
      "epoch: 30, training loss: 1.4790209531784058, testing loss: 1.6127004623413086\n",
      "Accuracy: 0.4373\n",
      "epoch: 30, training loss: 1.3970218896865845, testing loss: 1.5333755016326904\n",
      "Accuracy: 0.4638\n",
      "epoch: 30, training loss: 1.3806238174438477, testing loss: 1.496310830116272\n",
      "Accuracy: 0.4539\n",
      "epoch: 30, training loss: 1.3866357803344727, testing loss: 1.598131775856018\n",
      "Accuracy: 0.4314\n",
      "epoch: 30, training loss: 1.438002586364746, testing loss: 1.5186679363250732\n",
      "Accuracy: 0.4482\n",
      "epoch: 30, training loss: 1.4455702304840088, testing loss: 1.6286274194717407\n",
      "Accuracy: 0.4024\n",
      "epoch: 30, training loss: 1.3975733518600464, testing loss: 1.5530976057052612\n",
      "Accuracy: 0.4498\n",
      "epoch: 30, training loss: 1.4598939418792725, testing loss: 1.5701446533203125\n",
      "Accuracy: 0.4388\n",
      "epoch: 30, training loss: 1.3982048034667969, testing loss: 1.6057440042495728\n",
      "Accuracy: 0.4225\n",
      "epoch: 30, training loss: 1.4221153259277344, testing loss: 1.5753858089447021\n",
      "Accuracy: 0.4272\n",
      "epoch: 30, training loss: 1.3784854412078857, testing loss: 1.6576749086380005\n",
      "Accuracy: 0.4058\n",
      "epoch: 30, training loss: 1.4736545085906982, testing loss: 1.539727807044983\n",
      "Accuracy: 0.4485\n",
      "epoch: 30, training loss: 1.4037957191467285, testing loss: 1.5903987884521484\n",
      "Accuracy: 0.4706\n",
      "epoch: 30, training loss: 1.4546619653701782, testing loss: 1.5530366897583008\n",
      "Accuracy: 0.4225\n",
      "epoch: 30, training loss: 1.4755500555038452, testing loss: 1.5543264150619507\n",
      "Accuracy: 0.3951\n",
      "epoch: 30, training loss: 1.4708865880966187, testing loss: 1.4747774600982666\n",
      "Accuracy: 0.4777\n",
      "epoch: 30, training loss: 1.404692530632019, testing loss: 1.6044694185256958\n",
      "Accuracy: 0.4138\n",
      "epoch: 30, training loss: 1.4000598192214966, testing loss: 1.5480221509933472\n",
      "Accuracy: 0.4820\n",
      "epoch: 30, training loss: 1.4024014472961426, testing loss: 1.6277215480804443\n",
      "Accuracy: 0.4034\n",
      "epoch: 30, training loss: 1.5076117515563965, testing loss: 1.6264116764068604\n",
      "Accuracy: 0.4295\n",
      "epoch: 30, training loss: 1.4721500873565674, testing loss: 1.5063613653182983\n",
      "Accuracy: 0.4810\n",
      "epoch: 30, training loss: 1.4551669359207153, testing loss: 1.5942423343658447\n",
      "Accuracy: 0.4169\n",
      "epoch: 30, training loss: 1.4495846033096313, testing loss: 1.5799486637115479\n",
      "Accuracy: 0.4379\n",
      "epoch: 30, training loss: 1.4289674758911133, testing loss: 1.5337055921554565\n",
      "Accuracy: 0.4337\n",
      "epoch: 30, training loss: 1.420996069908142, testing loss: 1.5617923736572266\n",
      "Accuracy: 0.4373\n",
      "epoch: 30, training loss: 1.455472469329834, testing loss: 1.6258126497268677\n",
      "Accuracy: 0.4286\n",
      "epoch: 30, training loss: 1.5152509212493896, testing loss: 1.5030921697616577\n",
      "Accuracy: 0.4909\n",
      "epoch: 30, training loss: 1.4070475101470947, testing loss: 1.5408477783203125\n",
      "Accuracy: 0.4455\n",
      "epoch: 30, training loss: 1.4390522241592407, testing loss: 1.5186514854431152\n",
      "Accuracy: 0.4801\n",
      "epoch: 30, training loss: 1.488508701324463, testing loss: 1.615939736366272\n",
      "Accuracy: 0.4152\n",
      "epoch: 30, training loss: 1.4276854991912842, testing loss: 1.528041958808899\n",
      "Accuracy: 0.4497\n",
      "epoch: 30, training loss: 1.429340124130249, testing loss: 1.5023438930511475\n",
      "Accuracy: 0.4618\n",
      "epoch: 30, training loss: 1.422698736190796, testing loss: 1.5764262676239014\n",
      "Accuracy: 0.3945\n",
      "epoch: 30, training loss: 1.485019326210022, testing loss: 1.5648049116134644\n",
      "Accuracy: 0.4548\n",
      "epoch: 30, training loss: 1.5399526357650757, testing loss: 1.4759910106658936\n",
      "Accuracy: 0.4522\n",
      "epoch: 30, training loss: 1.4061627388000488, testing loss: 1.520073652267456\n",
      "Accuracy: 0.4560\n",
      "epoch: 30, training loss: 1.446866750717163, testing loss: 1.567949652671814\n",
      "Accuracy: 0.4209\n",
      "epoch: 30, training loss: 1.4734386205673218, testing loss: 1.6546117067337036\n",
      "Accuracy: 0.4377\n",
      "epoch: 30, training loss: 1.448644757270813, testing loss: 1.4747145175933838\n",
      "Accuracy: 0.4560\n",
      "epoch: 30, training loss: 1.4916287660598755, testing loss: 1.6140342950820923\n",
      "Accuracy: 0.3947\n",
      "epoch: 30, training loss: 1.4426651000976562, testing loss: 1.6587953567504883\n",
      "Accuracy: 0.3948\n",
      "epoch: 30, training loss: 1.3761941194534302, testing loss: 1.4939450025558472\n",
      "Accuracy: 0.4560\n",
      "epoch: 30, training loss: 1.467616081237793, testing loss: 1.5470762252807617\n",
      "Accuracy: 0.3987\n",
      "epoch: 30, training loss: 1.4763315916061401, testing loss: 1.5549477338790894\n",
      "Accuracy: 0.4557\n",
      "epoch: 30, training loss: 1.4012123346328735, testing loss: 1.5496962070465088\n",
      "Accuracy: 0.4414\n",
      "epoch: 30, training loss: 1.3993542194366455, testing loss: 1.660714030265808\n",
      "Accuracy: 0.3889\n",
      "epoch: 30, training loss: 1.476759433746338, testing loss: 1.528331995010376\n",
      "Accuracy: 0.4346\n",
      "epoch: 30, training loss: 1.4125516414642334, testing loss: 1.6611336469650269\n",
      "Accuracy: 0.4290\n",
      "epoch: 30, training loss: 1.414369821548462, testing loss: 1.529018521308899\n",
      "Accuracy: 0.4183\n",
      "epoch: 30, training loss: 1.48333740234375, testing loss: 1.4864237308502197\n",
      "Accuracy: 0.4860\n",
      "epoch: 40, training loss: 1.4040921926498413, testing loss: 1.535322904586792\n",
      "Accuracy: 0.4384\n",
      "epoch: 40, training loss: 1.3878023624420166, testing loss: 1.4867652654647827\n",
      "Accuracy: 0.4774\n",
      "epoch: 40, training loss: 1.474683403968811, testing loss: 1.5202577114105225\n",
      "Accuracy: 0.4651\n",
      "epoch: 40, training loss: 1.3911157846450806, testing loss: 1.5835471153259277\n",
      "Accuracy: 0.4299\n",
      "epoch: 40, training loss: 1.422316074371338, testing loss: 1.532051682472229\n",
      "Accuracy: 0.4829\n",
      "epoch: 40, training loss: 1.4097920656204224, testing loss: 1.6180598735809326\n",
      "Accuracy: 0.4387\n",
      "epoch: 40, training loss: 1.4486850500106812, testing loss: 1.5304899215698242\n",
      "Accuracy: 0.4066\n",
      "epoch: 40, training loss: 1.4140324592590332, testing loss: 1.6278852224349976\n",
      "Accuracy: 0.4373\n",
      "epoch: 40, training loss: 1.4663927555084229, testing loss: 1.6169971227645874\n",
      "Accuracy: 0.4304\n",
      "epoch: 40, training loss: 1.3976718187332153, testing loss: 1.6081916093826294\n",
      "Accuracy: 0.4352\n",
      "epoch: 40, training loss: 1.4000171422958374, testing loss: 1.5036553144454956\n",
      "Accuracy: 0.3980\n",
      "epoch: 40, training loss: 1.4584282636642456, testing loss: 1.55414879322052\n",
      "Accuracy: 0.4377\n",
      "epoch: 40, training loss: 1.4113661050796509, testing loss: 1.5629593133926392\n",
      "Accuracy: 0.4631\n",
      "epoch: 40, training loss: 1.4348231554031372, testing loss: 1.5801409482955933\n",
      "Accuracy: 0.4810\n",
      "epoch: 40, training loss: 1.4150532484054565, testing loss: 1.6221089363098145\n",
      "Accuracy: 0.3789\n",
      "epoch: 40, training loss: 1.3681915998458862, testing loss: 1.5353864431381226\n",
      "Accuracy: 0.4873\n",
      "epoch: 40, training loss: 1.3849815130233765, testing loss: 1.5654842853546143\n",
      "Accuracy: 0.4537\n",
      "epoch: 40, training loss: 1.46534264087677, testing loss: 1.5944480895996094\n",
      "Accuracy: 0.4362\n",
      "epoch: 40, training loss: 1.365744948387146, testing loss: 1.5866488218307495\n",
      "Accuracy: 0.4076\n",
      "epoch: 40, training loss: 1.4065394401550293, testing loss: 1.6643202304840088\n",
      "Accuracy: 0.4677\n",
      "epoch: 40, training loss: 1.3663225173950195, testing loss: 1.5668061971664429\n",
      "Accuracy: 0.4708\n",
      "epoch: 40, training loss: 1.448649287223816, testing loss: 1.5501960515975952\n",
      "Accuracy: 0.4677\n",
      "epoch: 40, training loss: 1.430902361869812, testing loss: 1.555968999862671\n",
      "Accuracy: 0.4371\n",
      "epoch: 40, training loss: 1.4681675434112549, testing loss: 1.5001869201660156\n",
      "Accuracy: 0.4675\n",
      "epoch: 40, training loss: 1.4323406219482422, testing loss: 1.6160117387771606\n",
      "Accuracy: 0.4502\n",
      "epoch: 40, training loss: 1.4004769325256348, testing loss: 1.5403456687927246\n",
      "Accuracy: 0.4537\n",
      "epoch: 40, training loss: 1.437538743019104, testing loss: 1.5769485235214233\n",
      "Accuracy: 0.4408\n",
      "epoch: 40, training loss: 1.468406319618225, testing loss: 1.6361377239227295\n",
      "Accuracy: 0.4167\n",
      "epoch: 40, training loss: 1.467729926109314, testing loss: 1.4542701244354248\n",
      "Accuracy: 0.4868\n",
      "epoch: 40, training loss: 1.463509202003479, testing loss: 1.500110387802124\n",
      "Accuracy: 0.5000\n",
      "epoch: 40, training loss: 1.4174575805664062, testing loss: 1.57213294506073\n",
      "Accuracy: 0.4620\n",
      "epoch: 40, training loss: 1.3639458417892456, testing loss: 1.548071265220642\n",
      "Accuracy: 0.4379\n",
      "epoch: 40, training loss: 1.4030448198318481, testing loss: 1.4599411487579346\n",
      "Accuracy: 0.4773\n",
      "epoch: 40, training loss: 1.3967636823654175, testing loss: 1.5048160552978516\n",
      "Accuracy: 0.4420\n",
      "epoch: 40, training loss: 1.4369218349456787, testing loss: 1.457116723060608\n",
      "Accuracy: 0.4803\n",
      "epoch: 40, training loss: 1.4691604375839233, testing loss: 1.6013774871826172\n",
      "Accuracy: 0.4172\n",
      "epoch: 40, training loss: 1.4693604707717896, testing loss: 1.5240527391433716\n",
      "Accuracy: 0.4638\n",
      "epoch: 40, training loss: 1.3671163320541382, testing loss: 1.5813090801239014\n",
      "Accuracy: 0.4169\n",
      "epoch: 40, training loss: 1.425669550895691, testing loss: 1.6298394203186035\n",
      "Accuracy: 0.4389\n",
      "epoch: 40, training loss: 1.4812723398208618, testing loss: 1.4859592914581299\n",
      "Accuracy: 0.4534\n",
      "epoch: 40, training loss: 1.3806575536727905, testing loss: 1.6543378829956055\n",
      "Accuracy: 0.4204\n",
      "epoch: 40, training loss: 1.4141206741333008, testing loss: 1.5988718271255493\n",
      "Accuracy: 0.4528\n",
      "epoch: 40, training loss: 1.4296596050262451, testing loss: 1.5315502882003784\n",
      "Accuracy: 0.4579\n",
      "epoch: 40, training loss: 1.4727668762207031, testing loss: 1.4333736896514893\n",
      "Accuracy: 0.4407\n",
      "epoch: 40, training loss: 1.403103232383728, testing loss: 1.5085148811340332\n",
      "Accuracy: 0.4730\n",
      "epoch: 40, training loss: 1.4134107828140259, testing loss: 1.5267349481582642\n",
      "Accuracy: 0.4494\n",
      "epoch: 40, training loss: 1.4039620161056519, testing loss: 1.5110912322998047\n",
      "Accuracy: 0.4922\n",
      "epoch: 40, training loss: 1.4127970933914185, testing loss: 1.5092034339904785\n",
      "Accuracy: 0.4630\n",
      "epoch: 40, training loss: 1.3808552026748657, testing loss: 1.5736974477767944\n",
      "Accuracy: 0.4193\n",
      "epoch: 40, training loss: 1.4296185970306396, testing loss: 1.4705493450164795\n",
      "Accuracy: 0.4346\n",
      "epoch: 40, training loss: 1.454332709312439, testing loss: 1.5769431591033936\n",
      "Accuracy: 0.4404\n",
      "epoch: 40, training loss: 1.3689278364181519, testing loss: 1.5259066820144653\n",
      "Accuracy: 0.4503\n",
      "epoch: 40, training loss: 1.4319175481796265, testing loss: 1.5868595838546753\n",
      "Accuracy: 0.4482\n",
      "epoch: 40, training loss: 1.4146884679794312, testing loss: 1.5958753824234009\n",
      "Accuracy: 0.4067\n",
      "epoch: 40, training loss: 1.398128628730774, testing loss: 1.5479001998901367\n",
      "Accuracy: 0.4564\n",
      "epoch: 40, training loss: 1.4170044660568237, testing loss: 1.5577346086502075\n",
      "Accuracy: 0.4222\n",
      "epoch: 40, training loss: 1.3893581628799438, testing loss: 1.6373209953308105\n",
      "Accuracy: 0.4343\n",
      "epoch: 40, training loss: 1.4092355966567993, testing loss: 1.5555424690246582\n",
      "Accuracy: 0.4669\n",
      "epoch: 40, training loss: 1.4424328804016113, testing loss: 1.6027034521102905\n",
      "Accuracy: 0.4869\n",
      "epoch: 40, training loss: 1.4082682132720947, testing loss: 1.6103719472885132\n",
      "Accuracy: 0.4386\n",
      "epoch: 40, training loss: 1.3452149629592896, testing loss: 1.6138129234313965\n",
      "Accuracy: 0.4039\n",
      "epoch: 40, training loss: 1.3738439083099365, testing loss: 1.5792219638824463\n",
      "Accuracy: 0.4422\n",
      "epoch: 40, training loss: 1.3964780569076538, testing loss: 1.492658257484436\n",
      "Accuracy: 0.4812\n",
      "epoch: 40, training loss: 1.442515254020691, testing loss: 1.6051347255706787\n",
      "Accuracy: 0.4172\n",
      "epoch: 40, training loss: 1.3930996656417847, testing loss: 1.5851826667785645\n",
      "Accuracy: 0.4636\n",
      "epoch: 40, training loss: 1.3758338689804077, testing loss: 1.554272174835205\n",
      "Accuracy: 0.4266\n",
      "epoch: 40, training loss: 1.3979120254516602, testing loss: 1.493835687637329\n",
      "Accuracy: 0.4522\n",
      "epoch: 40, training loss: 1.44972825050354, testing loss: 1.54666268825531\n",
      "Accuracy: 0.4494\n",
      "epoch: 40, training loss: 1.4286553859710693, testing loss: 1.6515083312988281\n",
      "Accuracy: 0.4211\n",
      "epoch: 40, training loss: 1.4137866497039795, testing loss: 1.6861180067062378\n",
      "Accuracy: 0.4130\n",
      "epoch: 40, training loss: 1.4438176155090332, testing loss: 1.6895859241485596\n",
      "Accuracy: 0.4399\n",
      "epoch: 40, training loss: 1.4108110666275024, testing loss: 1.729583501815796\n",
      "Accuracy: 0.4290\n",
      "epoch: 40, training loss: 1.381983757019043, testing loss: 1.635694146156311\n",
      "Accuracy: 0.4167\n",
      "epoch: 40, training loss: 1.4607269763946533, testing loss: 1.6142936944961548\n",
      "Accuracy: 0.4206\n",
      "epoch: 40, training loss: 1.3937715291976929, testing loss: 1.5313411951065063\n",
      "Accuracy: 0.4564\n",
      "epoch: 40, training loss: 1.410175085067749, testing loss: 1.4468499422073364\n",
      "Accuracy: 0.4513\n",
      "epoch: 40, training loss: 1.404807448387146, testing loss: 1.539043664932251\n",
      "Accuracy: 0.4281\n",
      "epoch: 40, training loss: 1.4007550477981567, testing loss: 1.550758957862854\n",
      "Accuracy: 0.4677\n",
      "epoch: 40, training loss: 1.3895270824432373, testing loss: 1.5634243488311768\n",
      "Accuracy: 0.4515\n",
      "epoch: 40, training loss: 1.391599178314209, testing loss: 1.4741710424423218\n",
      "Accuracy: 0.4969\n",
      "epoch: 40, training loss: 1.405078649520874, testing loss: 1.5785280466079712\n",
      "Accuracy: 0.4248\n",
      "epoch: 40, training loss: 1.4261786937713623, testing loss: 1.546532392501831\n",
      "Accuracy: 0.4381\n",
      "epoch: 40, training loss: 1.419387698173523, testing loss: 1.5668694972991943\n",
      "Accuracy: 0.4395\n",
      "epoch: 40, training loss: 1.4297327995300293, testing loss: 1.5339722633361816\n",
      "Accuracy: 0.4277\n",
      "epoch: 40, training loss: 1.4080965518951416, testing loss: 1.5524410009384155\n",
      "Accuracy: 0.3688\n",
      "epoch: 40, training loss: 1.3730945587158203, testing loss: 1.6644095182418823\n",
      "Accuracy: 0.3788\n",
      "epoch: 40, training loss: 1.4278918504714966, testing loss: 1.6975432634353638\n",
      "Accuracy: 0.4066\n",
      "epoch: 40, training loss: 1.4790958166122437, testing loss: 1.52197265625\n",
      "Accuracy: 0.4743\n",
      "epoch: 40, training loss: 1.411031723022461, testing loss: 1.5790362358093262\n",
      "Accuracy: 0.4189\n",
      "epoch: 40, training loss: 1.402378797531128, testing loss: 1.6364973783493042\n",
      "Accuracy: 0.4092\n",
      "epoch: 40, training loss: 1.392124891281128, testing loss: 1.52988600730896\n",
      "Accuracy: 0.4521\n",
      "epoch: 40, training loss: 1.362764835357666, testing loss: 1.5711910724639893\n",
      "Accuracy: 0.4613\n",
      "epoch: 40, training loss: 1.463123083114624, testing loss: 1.590448260307312\n",
      "Accuracy: 0.4649\n",
      "epoch: 40, training loss: 1.4167729616165161, testing loss: 1.6183593273162842\n",
      "Accuracy: 0.4174\n",
      "epoch: 40, training loss: 1.4294224977493286, testing loss: 1.583717703819275\n",
      "Accuracy: 0.4223\n",
      "epoch: 40, training loss: 1.4620468616485596, testing loss: 1.6106029748916626\n",
      "Accuracy: 0.4044\n",
      "epoch: 40, training loss: 1.4316293001174927, testing loss: 1.5499886274337769\n",
      "Accuracy: 0.4384\n",
      "epoch: 40, training loss: 1.3734720945358276, testing loss: 1.5287373065948486\n",
      "Accuracy: 0.4525\n",
      "epoch: 40, training loss: 1.4122512340545654, testing loss: 1.6435068845748901\n",
      "Accuracy: 0.4111\n",
      "epoch: 40, training loss: 1.420375108718872, testing loss: 1.6204445362091064\n",
      "Accuracy: 0.4373\n",
      "epoch: 50, training loss: 1.3674756288528442, testing loss: 1.5267724990844727\n",
      "Accuracy: 0.4551\n",
      "epoch: 50, training loss: 1.376118540763855, testing loss: 1.5868333578109741\n",
      "Accuracy: 0.4340\n",
      "epoch: 50, training loss: 1.388871192932129, testing loss: 1.5859153270721436\n",
      "Accuracy: 0.4335\n",
      "epoch: 50, training loss: 1.3763964176177979, testing loss: 1.5484369993209839\n",
      "Accuracy: 0.4653\n",
      "epoch: 50, training loss: 1.3791640996932983, testing loss: 1.6220765113830566\n",
      "Accuracy: 0.3719\n",
      "epoch: 50, training loss: 1.4026614427566528, testing loss: 1.5776009559631348\n",
      "Accuracy: 0.4420\n",
      "epoch: 50, training loss: 1.3384649753570557, testing loss: 1.5945627689361572\n",
      "Accuracy: 0.4429\n",
      "epoch: 50, training loss: 1.3968604803085327, testing loss: 1.6498546600341797\n",
      "Accuracy: 0.4007\n",
      "epoch: 50, training loss: 1.3605860471725464, testing loss: 1.5348578691482544\n",
      "Accuracy: 0.4598\n",
      "epoch: 50, training loss: 1.4284396171569824, testing loss: 1.4494976997375488\n",
      "Accuracy: 0.5229\n",
      "epoch: 50, training loss: 1.393573522567749, testing loss: 1.4975751638412476\n",
      "Accuracy: 0.5050\n",
      "epoch: 50, training loss: 1.3591537475585938, testing loss: 1.5044183731079102\n",
      "Accuracy: 0.4575\n",
      "epoch: 50, training loss: 1.3323802947998047, testing loss: 1.6502944231033325\n",
      "Accuracy: 0.4241\n",
      "epoch: 50, training loss: 1.3454853296279907, testing loss: 1.5329607725143433\n",
      "Accuracy: 0.4571\n",
      "epoch: 50, training loss: 1.3756344318389893, testing loss: 1.633405327796936\n",
      "Accuracy: 0.4423\n",
      "epoch: 50, training loss: 1.4541325569152832, testing loss: 1.5839765071868896\n",
      "Accuracy: 0.3957\n",
      "epoch: 50, training loss: 1.3811349868774414, testing loss: 1.6171653270721436\n",
      "Accuracy: 0.4430\n",
      "epoch: 50, training loss: 1.3374439477920532, testing loss: 1.5845956802368164\n",
      "Accuracy: 0.4230\n",
      "epoch: 50, training loss: 1.3781694173812866, testing loss: 1.5065441131591797\n",
      "Accuracy: 0.4018\n",
      "epoch: 50, training loss: 1.376466155052185, testing loss: 1.60004723072052\n",
      "Accuracy: 0.4331\n",
      "epoch: 50, training loss: 1.3512516021728516, testing loss: 1.5944608449935913\n",
      "Accuracy: 0.4259\n",
      "epoch: 50, training loss: 1.2896254062652588, testing loss: 1.5793274641036987\n",
      "Accuracy: 0.4592\n",
      "epoch: 50, training loss: 1.4026132822036743, testing loss: 1.6814215183258057\n",
      "Accuracy: 0.4228\n",
      "epoch: 50, training loss: 1.3766058683395386, testing loss: 1.674829125404358\n",
      "Accuracy: 0.4038\n",
      "epoch: 50, training loss: 1.3564890623092651, testing loss: 1.5330785512924194\n",
      "Accuracy: 0.4381\n",
      "epoch: 50, training loss: 1.397115707397461, testing loss: 1.6563901901245117\n",
      "Accuracy: 0.4116\n",
      "epoch: 50, training loss: 1.408882975578308, testing loss: 1.7035592794418335\n",
      "Accuracy: 0.4167\n",
      "epoch: 50, training loss: 1.3749476671218872, testing loss: 1.5925155878067017\n",
      "Accuracy: 0.3954\n",
      "epoch: 50, training loss: 1.3524770736694336, testing loss: 1.5698645114898682\n",
      "Accuracy: 0.4416\n",
      "epoch: 50, training loss: 1.3729579448699951, testing loss: 1.5539575815200806\n",
      "Accuracy: 0.4532\n",
      "epoch: 50, training loss: 1.360783338546753, testing loss: 1.601406455039978\n",
      "Accuracy: 0.4149\n",
      "epoch: 50, training loss: 1.3511240482330322, testing loss: 1.5950697660446167\n",
      "Accuracy: 0.4217\n",
      "epoch: 50, training loss: 1.4450932741165161, testing loss: 1.5862241983413696\n",
      "Accuracy: 0.4599\n",
      "epoch: 50, training loss: 1.391303300857544, testing loss: 1.4173223972320557\n",
      "Accuracy: 0.4966\n",
      "epoch: 50, training loss: 1.3704745769500732, testing loss: 1.6681935787200928\n",
      "Accuracy: 0.3994\n",
      "epoch: 50, training loss: 1.403409481048584, testing loss: 1.611257553100586\n",
      "Accuracy: 0.4384\n",
      "epoch: 50, training loss: 1.3819713592529297, testing loss: 1.5941495895385742\n",
      "Accuracy: 0.4532\n",
      "epoch: 50, training loss: 1.3785865306854248, testing loss: 1.6609352827072144\n",
      "Accuracy: 0.4416\n",
      "epoch: 50, training loss: 1.3593318462371826, testing loss: 1.598455786705017\n",
      "Accuracy: 0.4141\n",
      "epoch: 50, training loss: 1.4058209657669067, testing loss: 1.5714527368545532\n",
      "Accuracy: 0.4572\n",
      "epoch: 50, training loss: 1.3674969673156738, testing loss: 1.53217613697052\n",
      "Accuracy: 0.4509\n",
      "epoch: 50, training loss: 1.4004762172698975, testing loss: 1.4326810836791992\n",
      "Accuracy: 0.4752\n",
      "epoch: 50, training loss: 1.4079676866531372, testing loss: 1.6191600561141968\n",
      "Accuracy: 0.4308\n",
      "epoch: 50, training loss: 1.4136812686920166, testing loss: 1.6012969017028809\n",
      "Accuracy: 0.4125\n",
      "epoch: 50, training loss: 1.4096168279647827, testing loss: 1.6231672763824463\n",
      "Accuracy: 0.4527\n",
      "epoch: 50, training loss: 1.3906190395355225, testing loss: 1.5387486219406128\n",
      "Accuracy: 0.4597\n",
      "epoch: 50, training loss: 1.3781999349594116, testing loss: 1.566908836364746\n",
      "Accuracy: 0.4662\n",
      "epoch: 50, training loss: 1.3601574897766113, testing loss: 1.5712960958480835\n",
      "Accuracy: 0.4698\n",
      "epoch: 50, training loss: 1.406901478767395, testing loss: 1.56393301486969\n",
      "Accuracy: 0.4248\n",
      "epoch: 50, training loss: 1.330201268196106, testing loss: 1.6379083395004272\n",
      "Accuracy: 0.4112\n",
      "epoch: 50, training loss: 1.4578070640563965, testing loss: 1.5377055406570435\n",
      "Accuracy: 0.4433\n",
      "epoch: 50, training loss: 1.3769739866256714, testing loss: 1.4836781024932861\n",
      "Accuracy: 0.4940\n",
      "epoch: 50, training loss: 1.3671514987945557, testing loss: 1.6567420959472656\n",
      "Accuracy: 0.4389\n",
      "epoch: 50, training loss: 1.373033881187439, testing loss: 1.4501185417175293\n",
      "Accuracy: 0.4484\n",
      "epoch: 50, training loss: 1.4840867519378662, testing loss: 1.575033187866211\n",
      "Accuracy: 0.4299\n",
      "epoch: 50, training loss: 1.4369850158691406, testing loss: 1.6098284721374512\n",
      "Accuracy: 0.4257\n",
      "epoch: 50, training loss: 1.4741648435592651, testing loss: 1.6488192081451416\n",
      "Accuracy: 0.3746\n",
      "epoch: 50, training loss: 1.379927635192871, testing loss: 1.5625073909759521\n",
      "Accuracy: 0.4413\n",
      "epoch: 50, training loss: 1.4130300283432007, testing loss: 1.6415224075317383\n",
      "Accuracy: 0.4345\n",
      "epoch: 50, training loss: 1.3700424432754517, testing loss: 1.5409756898880005\n",
      "Accuracy: 0.4567\n",
      "epoch: 50, training loss: 1.3817064762115479, testing loss: 1.5298380851745605\n",
      "Accuracy: 0.4383\n",
      "epoch: 50, training loss: 1.4270578622817993, testing loss: 1.5511043071746826\n",
      "Accuracy: 0.4257\n",
      "epoch: 50, training loss: 1.3727017641067505, testing loss: 1.5643768310546875\n",
      "Accuracy: 0.4371\n",
      "epoch: 50, training loss: 1.3598018884658813, testing loss: 1.644201397895813\n",
      "Accuracy: 0.4138\n",
      "epoch: 50, training loss: 1.3710956573486328, testing loss: 1.6074923276901245\n",
      "Accuracy: 0.4026\n",
      "epoch: 50, training loss: 1.4319255352020264, testing loss: 1.614496111869812\n",
      "Accuracy: 0.4309\n",
      "epoch: 50, training loss: 1.3702267408370972, testing loss: 1.5852769613265991\n",
      "Accuracy: 0.4416\n",
      "epoch: 50, training loss: 1.3662667274475098, testing loss: 1.5187127590179443\n",
      "Accuracy: 0.4665\n",
      "epoch: 50, training loss: 1.388654351234436, testing loss: 1.5845301151275635\n",
      "Accuracy: 0.4487\n",
      "epoch: 50, training loss: 1.4426904916763306, testing loss: 1.5605623722076416\n",
      "Accuracy: 0.4072\n",
      "epoch: 50, training loss: 1.426807165145874, testing loss: 1.588335633277893\n",
      "Accuracy: 0.4638\n",
      "epoch: 50, training loss: 1.4413135051727295, testing loss: 1.730124831199646\n",
      "Accuracy: 0.3556\n",
      "epoch: 50, training loss: 1.4174740314483643, testing loss: 1.6765477657318115\n",
      "Accuracy: 0.3910\n",
      "epoch: 50, training loss: 1.3601088523864746, testing loss: 1.5974758863449097\n",
      "Accuracy: 0.4335\n",
      "epoch: 50, training loss: 1.415519118309021, testing loss: 1.5751274824142456\n",
      "Accuracy: 0.4583\n",
      "epoch: 50, training loss: 1.3756462335586548, testing loss: 1.5710337162017822\n",
      "Accuracy: 0.4466\n",
      "epoch: 50, training loss: 1.3548120260238647, testing loss: 1.6793972253799438\n",
      "Accuracy: 0.4365\n",
      "epoch: 50, training loss: 1.4541809558868408, testing loss: 1.6415138244628906\n",
      "Accuracy: 0.4602\n",
      "epoch: 50, training loss: 1.4595544338226318, testing loss: 1.620961308479309\n",
      "Accuracy: 0.4108\n",
      "epoch: 50, training loss: 1.4681960344314575, testing loss: 1.5413874387741089\n",
      "Accuracy: 0.4286\n",
      "epoch: 50, training loss: 1.3468912839889526, testing loss: 1.544577956199646\n",
      "Accuracy: 0.4495\n",
      "epoch: 50, training loss: 1.3585149049758911, testing loss: 1.5995688438415527\n",
      "Accuracy: 0.4352\n",
      "epoch: 50, training loss: 1.4113999605178833, testing loss: 1.5563424825668335\n",
      "Accuracy: 0.4326\n",
      "epoch: 50, training loss: 1.4548219442367554, testing loss: 1.5037826299667358\n",
      "Accuracy: 0.4861\n",
      "epoch: 50, training loss: 1.4073928594589233, testing loss: 1.5219894647598267\n",
      "Accuracy: 0.4613\n",
      "epoch: 50, training loss: 1.4119844436645508, testing loss: 1.5662354230880737\n",
      "Accuracy: 0.4419\n",
      "epoch: 50, training loss: 1.4280803203582764, testing loss: 1.5301954746246338\n",
      "Accuracy: 0.4520\n",
      "epoch: 50, training loss: 1.414410948753357, testing loss: 1.570198655128479\n",
      "Accuracy: 0.4423\n",
      "epoch: 50, training loss: 1.4254690408706665, testing loss: 1.5615102052688599\n",
      "Accuracy: 0.4149\n",
      "epoch: 50, training loss: 1.3972944021224976, testing loss: 1.6370582580566406\n",
      "Accuracy: 0.4267\n",
      "epoch: 50, training loss: 1.368166208267212, testing loss: 1.5558425188064575\n",
      "Accuracy: 0.4309\n",
      "epoch: 50, training loss: 1.3723291158676147, testing loss: 1.5974913835525513\n",
      "Accuracy: 0.4492\n",
      "epoch: 50, training loss: 1.380928874015808, testing loss: 1.4983702898025513\n",
      "Accuracy: 0.4518\n",
      "epoch: 50, training loss: 1.3755977153778076, testing loss: 1.5121417045593262\n",
      "Accuracy: 0.4956\n",
      "epoch: 50, training loss: 1.3699947595596313, testing loss: 1.5753586292266846\n",
      "Accuracy: 0.4161\n",
      "epoch: 50, training loss: 1.3826316595077515, testing loss: 1.6667377948760986\n",
      "Accuracy: 0.4099\n",
      "epoch: 50, training loss: 1.3861727714538574, testing loss: 1.5549962520599365\n",
      "Accuracy: 0.4332\n",
      "epoch: 50, training loss: 1.3938230276107788, testing loss: 1.603151798248291\n",
      "Accuracy: 0.4172\n",
      "epoch: 50, training loss: 1.3976832628250122, testing loss: 1.5899949073791504\n",
      "Accuracy: 0.4251\n",
      "epoch: 50, training loss: 1.3920965194702148, testing loss: 1.6544036865234375\n",
      "Accuracy: 0.4034\n",
      "epoch: 60, training loss: 1.3953592777252197, testing loss: 1.5507515668869019\n",
      "Accuracy: 0.4571\n",
      "epoch: 60, training loss: 1.3937147855758667, testing loss: 1.578559398651123\n",
      "Accuracy: 0.4346\n",
      "epoch: 60, training loss: 1.3642261028289795, testing loss: 1.6204859018325806\n",
      "Accuracy: 0.4510\n",
      "epoch: 60, training loss: 1.3786550760269165, testing loss: 1.7050607204437256\n",
      "Accuracy: 0.3779\n",
      "epoch: 60, training loss: 1.345549464225769, testing loss: 1.5999085903167725\n",
      "Accuracy: 0.4129\n",
      "epoch: 60, training loss: 1.3779805898666382, testing loss: 1.7264043092727661\n",
      "Accuracy: 0.3791\n",
      "epoch: 60, training loss: 1.2863118648529053, testing loss: 1.6031825542449951\n",
      "Accuracy: 0.4156\n",
      "epoch: 60, training loss: 1.4414812326431274, testing loss: 1.5598217248916626\n",
      "Accuracy: 0.4276\n",
      "epoch: 60, training loss: 1.4018597602844238, testing loss: 1.5708236694335938\n",
      "Accuracy: 0.4384\n",
      "epoch: 60, training loss: 1.3412904739379883, testing loss: 1.6341028213500977\n",
      "Accuracy: 0.4387\n",
      "epoch: 60, training loss: 1.3740770816802979, testing loss: 1.4590474367141724\n",
      "Accuracy: 0.4983\n",
      "epoch: 60, training loss: 1.3656402826309204, testing loss: 1.6559391021728516\n",
      "Accuracy: 0.4393\n",
      "epoch: 60, training loss: 1.3949737548828125, testing loss: 1.5843183994293213\n",
      "Accuracy: 0.4242\n",
      "epoch: 60, training loss: 1.4156908988952637, testing loss: 1.5106706619262695\n",
      "Accuracy: 0.4788\n",
      "epoch: 60, training loss: 1.3075507879257202, testing loss: 1.5764832496643066\n",
      "Accuracy: 0.4611\n",
      "epoch: 60, training loss: 1.3931236267089844, testing loss: 1.6612062454223633\n",
      "Accuracy: 0.4434\n",
      "epoch: 60, training loss: 1.3383009433746338, testing loss: 1.6379390954971313\n",
      "Accuracy: 0.3790\n",
      "epoch: 60, training loss: 1.3641743659973145, testing loss: 1.6766782999038696\n",
      "Accuracy: 0.3456\n",
      "epoch: 60, training loss: 1.3169162273406982, testing loss: 1.6673099994659424\n",
      "Accuracy: 0.3851\n",
      "epoch: 60, training loss: 1.40997314453125, testing loss: 1.503766655921936\n",
      "Accuracy: 0.4389\n",
      "epoch: 60, training loss: 1.3671932220458984, testing loss: 1.55790114402771\n",
      "Accuracy: 0.4689\n",
      "epoch: 60, training loss: 1.4240108728408813, testing loss: 1.538439154624939\n",
      "Accuracy: 0.4636\n",
      "epoch: 60, training loss: 1.457797646522522, testing loss: 1.738057017326355\n",
      "Accuracy: 0.4185\n",
      "epoch: 60, training loss: 1.3958117961883545, testing loss: 1.6584182977676392\n",
      "Accuracy: 0.3889\n",
      "epoch: 60, training loss: 1.3476159572601318, testing loss: 1.5246630907058716\n",
      "Accuracy: 0.4621\n",
      "epoch: 60, training loss: 1.3277137279510498, testing loss: 1.5628985166549683\n",
      "Accuracy: 0.4724\n",
      "epoch: 60, training loss: 1.3376191854476929, testing loss: 1.6712582111358643\n",
      "Accuracy: 0.4536\n",
      "epoch: 60, training loss: 1.3613269329071045, testing loss: 1.6071640253067017\n",
      "Accuracy: 0.4268\n",
      "epoch: 60, training loss: 1.3231462240219116, testing loss: 1.563941478729248\n",
      "Accuracy: 0.4234\n",
      "epoch: 60, training loss: 1.386460304260254, testing loss: 1.519054889678955\n",
      "Accuracy: 0.4525\n",
      "epoch: 60, training loss: 1.4331508874893188, testing loss: 1.804897427558899\n",
      "Accuracy: 0.3846\n",
      "epoch: 60, training loss: 1.4322317838668823, testing loss: 1.5709762573242188\n",
      "Accuracy: 0.4490\n",
      "epoch: 60, training loss: 1.42448091506958, testing loss: 1.573615550994873\n",
      "Accuracy: 0.4172\n",
      "epoch: 60, training loss: 1.36067533493042, testing loss: 1.6251506805419922\n",
      "Accuracy: 0.4241\n",
      "epoch: 60, training loss: 1.434231162071228, testing loss: 1.5467878580093384\n",
      "Accuracy: 0.4968\n",
      "epoch: 60, training loss: 1.4101842641830444, testing loss: 1.615431308746338\n",
      "Accuracy: 0.3885\n",
      "epoch: 60, training loss: 1.3719525337219238, testing loss: 1.6755131483078003\n",
      "Accuracy: 0.4228\n",
      "epoch: 60, training loss: 1.4570019245147705, testing loss: 1.5111675262451172\n",
      "Accuracy: 0.4465\n",
      "epoch: 60, training loss: 1.4108418226242065, testing loss: 1.5107388496398926\n",
      "Accuracy: 0.4656\n",
      "epoch: 60, training loss: 1.4611496925354004, testing loss: 1.659063696861267\n",
      "Accuracy: 0.4338\n",
      "epoch: 60, training loss: 1.4456814527511597, testing loss: 1.5582642555236816\n",
      "Accuracy: 0.4585\n",
      "epoch: 60, training loss: 1.4203791618347168, testing loss: 1.6577339172363281\n",
      "Accuracy: 0.4355\n",
      "epoch: 60, training loss: 1.3131868839263916, testing loss: 1.7165192365646362\n",
      "Accuracy: 0.4172\n",
      "epoch: 60, training loss: 1.3124297857284546, testing loss: 1.5512480735778809\n",
      "Accuracy: 0.4630\n",
      "epoch: 60, training loss: 1.3585866689682007, testing loss: 1.6832016706466675\n",
      "Accuracy: 0.4181\n",
      "epoch: 60, training loss: 1.3370059728622437, testing loss: 1.5882426500320435\n",
      "Accuracy: 0.4331\n",
      "epoch: 60, training loss: 1.3516387939453125, testing loss: 1.5650527477264404\n",
      "Accuracy: 0.4633\n",
      "epoch: 60, training loss: 1.4149277210235596, testing loss: 1.5846692323684692\n",
      "Accuracy: 0.3947\n",
      "epoch: 60, training loss: 1.3484044075012207, testing loss: 1.557378888130188\n",
      "Accuracy: 0.4767\n",
      "epoch: 60, training loss: 1.3632351160049438, testing loss: 1.5566799640655518\n",
      "Accuracy: 0.4348\n",
      "epoch: 60, training loss: 1.3345918655395508, testing loss: 1.5438501834869385\n",
      "Accuracy: 0.4281\n",
      "epoch: 60, training loss: 1.3980177640914917, testing loss: 1.6212397813796997\n",
      "Accuracy: 0.4082\n",
      "epoch: 60, training loss: 1.3117903470993042, testing loss: 1.5263350009918213\n",
      "Accuracy: 0.4551\n",
      "epoch: 60, training loss: 1.3526779413223267, testing loss: 1.5833134651184082\n",
      "Accuracy: 0.4467\n",
      "epoch: 60, training loss: 1.3669251203536987, testing loss: 1.5450763702392578\n",
      "Accuracy: 0.4768\n",
      "epoch: 60, training loss: 1.4282044172286987, testing loss: 1.7185969352722168\n",
      "Accuracy: 0.3801\n",
      "epoch: 60, training loss: 1.4271339178085327, testing loss: 1.569638967514038\n",
      "Accuracy: 0.3805\n",
      "epoch: 60, training loss: 1.3941638469696045, testing loss: 1.6903942823410034\n",
      "Accuracy: 0.4073\n",
      "epoch: 60, training loss: 1.2838166952133179, testing loss: 1.5451983213424683\n",
      "Accuracy: 0.4536\n",
      "epoch: 60, training loss: 1.4019988775253296, testing loss: 1.5605106353759766\n",
      "Accuracy: 0.4592\n",
      "epoch: 60, training loss: 1.3216174840927124, testing loss: 1.6195356845855713\n",
      "Accuracy: 0.4361\n",
      "epoch: 60, training loss: 1.3402371406555176, testing loss: 1.668829083442688\n",
      "Accuracy: 0.4247\n",
      "epoch: 60, training loss: 1.372668981552124, testing loss: 1.5389968156814575\n",
      "Accuracy: 0.4543\n",
      "epoch: 60, training loss: 1.3596614599227905, testing loss: 1.6910548210144043\n",
      "Accuracy: 0.3758\n",
      "epoch: 60, training loss: 1.3520790338516235, testing loss: 1.552668571472168\n",
      "Accuracy: 0.4156\n",
      "epoch: 60, training loss: 1.369023323059082, testing loss: 1.6088223457336426\n",
      "Accuracy: 0.4448\n",
      "epoch: 60, training loss: 1.4260385036468506, testing loss: 1.5550346374511719\n",
      "Accuracy: 0.4371\n",
      "epoch: 60, training loss: 1.3513482809066772, testing loss: 1.5730096101760864\n",
      "Accuracy: 0.4557\n",
      "epoch: 60, training loss: 1.3748373985290527, testing loss: 1.6472749710083008\n",
      "Accuracy: 0.4222\n",
      "epoch: 60, training loss: 1.3874413967132568, testing loss: 1.546704888343811\n",
      "Accuracy: 0.4465\n",
      "epoch: 60, training loss: 1.3759530782699585, testing loss: 1.5893521308898926\n",
      "Accuracy: 0.3993\n",
      "epoch: 60, training loss: 1.4693881273269653, testing loss: 1.3560702800750732\n",
      "Accuracy: 0.5094\n",
      "epoch: 60, training loss: 1.3439749479293823, testing loss: 1.6606615781784058\n",
      "Accuracy: 0.4419\n",
      "epoch: 60, training loss: 1.3514453172683716, testing loss: 1.552570104598999\n",
      "Accuracy: 0.4483\n",
      "epoch: 60, training loss: 1.389679193496704, testing loss: 1.6073689460754395\n",
      "Accuracy: 0.4110\n",
      "epoch: 60, training loss: 1.4069265127182007, testing loss: 1.6344757080078125\n",
      "Accuracy: 0.4534\n",
      "epoch: 60, training loss: 1.3156768083572388, testing loss: 1.6022205352783203\n",
      "Accuracy: 0.4349\n",
      "epoch: 60, training loss: 1.3263870477676392, testing loss: 1.6403034925460815\n",
      "Accuracy: 0.4387\n",
      "epoch: 60, training loss: 1.3187055587768555, testing loss: 1.7048046588897705\n",
      "Accuracy: 0.4107\n",
      "epoch: 60, training loss: 1.366634488105774, testing loss: 1.6587257385253906\n",
      "Accuracy: 0.4180\n",
      "epoch: 60, training loss: 1.3812519311904907, testing loss: 1.6100871562957764\n",
      "Accuracy: 0.4332\n",
      "epoch: 60, training loss: 1.390324354171753, testing loss: 1.6595348119735718\n",
      "Accuracy: 0.3791\n",
      "epoch: 60, training loss: 1.2895209789276123, testing loss: 1.6647082567214966\n",
      "Accuracy: 0.4526\n",
      "epoch: 60, training loss: 1.4082591533660889, testing loss: 1.640971302986145\n",
      "Accuracy: 0.4152\n",
      "epoch: 60, training loss: 1.3859522342681885, testing loss: 1.6765789985656738\n",
      "Accuracy: 0.3880\n",
      "epoch: 60, training loss: 1.349122405052185, testing loss: 1.6696019172668457\n",
      "Accuracy: 0.4179\n",
      "epoch: 60, training loss: 1.3171433210372925, testing loss: 1.6661052703857422\n",
      "Accuracy: 0.4199\n",
      "epoch: 60, training loss: 1.3228428363800049, testing loss: 1.6731348037719727\n",
      "Accuracy: 0.4007\n",
      "epoch: 60, training loss: 1.3780618906021118, testing loss: 1.6337652206420898\n",
      "Accuracy: 0.4483\n",
      "epoch: 60, training loss: 1.4316118955612183, testing loss: 1.6915355920791626\n",
      "Accuracy: 0.3939\n",
      "epoch: 60, training loss: 1.3904719352722168, testing loss: 1.5635218620300293\n",
      "Accuracy: 0.3945\n",
      "epoch: 60, training loss: 1.4119796752929688, testing loss: 1.5983250141143799\n",
      "Accuracy: 0.4138\n",
      "epoch: 60, training loss: 1.3438464403152466, testing loss: 1.580337643623352\n",
      "Accuracy: 0.4680\n",
      "epoch: 60, training loss: 1.3665417432785034, testing loss: 1.579372525215149\n",
      "Accuracy: 0.3889\n",
      "epoch: 60, training loss: 1.3466602563858032, testing loss: 1.7131505012512207\n",
      "Accuracy: 0.3694\n",
      "epoch: 60, training loss: 1.3418701887130737, testing loss: 1.6350330114364624\n",
      "Accuracy: 0.4416\n",
      "epoch: 60, training loss: 1.373162031173706, testing loss: 1.5464142560958862\n",
      "Accuracy: 0.4359\n",
      "epoch: 60, training loss: 1.3791604042053223, testing loss: 1.5399086475372314\n",
      "Accuracy: 0.4756\n",
      "epoch: 60, training loss: 1.3803895711898804, testing loss: 1.7760562896728516\n",
      "Accuracy: 0.3900\n",
      "epoch: 60, training loss: 1.3483986854553223, testing loss: 1.4956512451171875\n",
      "Accuracy: 0.4740\n",
      "epoch: 70, training loss: 1.339682698249817, testing loss: 1.5626239776611328\n",
      "Accuracy: 0.4734\n",
      "epoch: 70, training loss: 1.3898134231567383, testing loss: 1.6732916831970215\n",
      "Accuracy: 0.4172\n",
      "epoch: 70, training loss: 1.3742400407791138, testing loss: 1.5930850505828857\n",
      "Accuracy: 0.4503\n",
      "epoch: 70, training loss: 1.361436128616333, testing loss: 1.6915358304977417\n",
      "Accuracy: 0.4033\n",
      "epoch: 70, training loss: 1.330803632736206, testing loss: 1.6011371612548828\n",
      "Accuracy: 0.4654\n",
      "epoch: 70, training loss: 1.3908947706222534, testing loss: 1.703391432762146\n",
      "Accuracy: 0.4183\n",
      "epoch: 70, training loss: 1.3522967100143433, testing loss: 1.7207398414611816\n",
      "Accuracy: 0.3813\n",
      "epoch: 70, training loss: 1.3753875494003296, testing loss: 1.7189301252365112\n",
      "Accuracy: 0.4487\n",
      "epoch: 70, training loss: 1.3495999574661255, testing loss: 1.629701852798462\n",
      "Accuracy: 0.4361\n",
      "epoch: 70, training loss: 1.330161213874817, testing loss: 1.5647764205932617\n",
      "Accuracy: 0.4784\n",
      "epoch: 70, training loss: 1.3367536067962646, testing loss: 1.6327663660049438\n",
      "Accuracy: 0.4125\n",
      "epoch: 70, training loss: 1.3780604600906372, testing loss: 1.6249085664749146\n",
      "Accuracy: 0.4187\n",
      "epoch: 70, training loss: 1.358284592628479, testing loss: 1.4788432121276855\n",
      "Accuracy: 0.4922\n",
      "epoch: 70, training loss: 1.362072467803955, testing loss: 1.58646559715271\n",
      "Accuracy: 0.3933\n",
      "epoch: 70, training loss: 1.349635362625122, testing loss: 1.6775894165039062\n",
      "Accuracy: 0.4064\n",
      "epoch: 70, training loss: 1.383862853050232, testing loss: 1.5741630792617798\n",
      "Accuracy: 0.4894\n",
      "epoch: 70, training loss: 1.4050571918487549, testing loss: 1.6297188997268677\n",
      "Accuracy: 0.4407\n",
      "epoch: 70, training loss: 1.301841378211975, testing loss: 1.645324945449829\n",
      "Accuracy: 0.4268\n",
      "epoch: 70, training loss: 1.3187884092330933, testing loss: 1.5589206218719482\n",
      "Accuracy: 0.4026\n",
      "epoch: 70, training loss: 1.4091016054153442, testing loss: 1.599341630935669\n",
      "Accuracy: 0.4358\n",
      "epoch: 70, training loss: 1.3309001922607422, testing loss: 1.5846925973892212\n",
      "Accuracy: 0.4503\n",
      "epoch: 70, training loss: 1.3543952703475952, testing loss: 1.627466082572937\n",
      "Accuracy: 0.4462\n",
      "epoch: 70, training loss: 1.3430585861206055, testing loss: 1.538926124572754\n",
      "Accuracy: 0.4598\n",
      "epoch: 70, training loss: 1.2965949773788452, testing loss: 1.53849458694458\n",
      "Accuracy: 0.4483\n",
      "epoch: 70, training loss: 1.3915749788284302, testing loss: 1.5724352598190308\n",
      "Accuracy: 0.4675\n",
      "epoch: 70, training loss: 1.3352468013763428, testing loss: 1.6649612188339233\n",
      "Accuracy: 0.3956\n",
      "epoch: 70, training loss: 1.3851354122161865, testing loss: 1.64681875705719\n",
      "Accuracy: 0.4421\n",
      "epoch: 70, training loss: 1.3784769773483276, testing loss: 1.600431203842163\n",
      "Accuracy: 0.4415\n",
      "epoch: 70, training loss: 1.343576192855835, testing loss: 1.6676716804504395\n",
      "Accuracy: 0.4227\n",
      "epoch: 70, training loss: 1.3658968210220337, testing loss: 1.7053096294403076\n",
      "Accuracy: 0.3792\n",
      "epoch: 70, training loss: 1.398586630821228, testing loss: 1.6567137241363525\n",
      "Accuracy: 0.4291\n",
      "epoch: 70, training loss: 1.3197174072265625, testing loss: 1.5296269655227661\n",
      "Accuracy: 0.4375\n",
      "epoch: 70, training loss: 1.3667352199554443, testing loss: 1.5533524751663208\n",
      "Accuracy: 0.4505\n",
      "epoch: 70, training loss: 1.397265911102295, testing loss: 1.538097620010376\n",
      "Accuracy: 0.4603\n",
      "epoch: 70, training loss: 1.3207730054855347, testing loss: 1.5556707382202148\n",
      "Accuracy: 0.4412\n",
      "epoch: 70, training loss: 1.319332480430603, testing loss: 1.6214799880981445\n",
      "Accuracy: 0.4132\n",
      "epoch: 70, training loss: 1.3430193662643433, testing loss: 1.6022403240203857\n",
      "Accuracy: 0.4216\n",
      "epoch: 70, training loss: 1.373898983001709, testing loss: 1.6444002389907837\n",
      "Accuracy: 0.3929\n",
      "epoch: 70, training loss: 1.4730901718139648, testing loss: 1.5718196630477905\n",
      "Accuracy: 0.4295\n",
      "epoch: 70, training loss: 1.3537325859069824, testing loss: 1.5771126747131348\n",
      "Accuracy: 0.4489\n",
      "epoch: 70, training loss: 1.4172282218933105, testing loss: 1.5472180843353271\n",
      "Accuracy: 0.4742\n",
      "epoch: 70, training loss: 1.410252332687378, testing loss: 1.7042158842086792\n",
      "Accuracy: 0.3817\n",
      "epoch: 70, training loss: 1.393102765083313, testing loss: 1.6158028841018677\n",
      "Accuracy: 0.4392\n",
      "epoch: 70, training loss: 1.3723177909851074, testing loss: 1.6276339292526245\n",
      "Accuracy: 0.4156\n",
      "epoch: 70, training loss: 1.3654696941375732, testing loss: 1.553235650062561\n",
      "Accuracy: 0.4177\n",
      "epoch: 70, training loss: 1.3681005239486694, testing loss: 1.6453155279159546\n",
      "Accuracy: 0.4218\n",
      "epoch: 70, training loss: 1.346387505531311, testing loss: 1.638263463973999\n",
      "Accuracy: 0.4159\n",
      "epoch: 70, training loss: 1.307421088218689, testing loss: 1.6772997379302979\n",
      "Accuracy: 0.4092\n",
      "epoch: 70, training loss: 1.3308974504470825, testing loss: 1.630481243133545\n",
      "Accuracy: 0.4055\n",
      "epoch: 70, training loss: 1.3340216875076294, testing loss: 1.6184049844741821\n",
      "Accuracy: 0.4379\n",
      "epoch: 70, training loss: 1.3767343759536743, testing loss: 1.5315252542495728\n",
      "Accuracy: 0.4381\n",
      "epoch: 70, training loss: 1.3325313329696655, testing loss: 1.7341573238372803\n",
      "Accuracy: 0.3801\n",
      "epoch: 70, training loss: 1.3808280229568481, testing loss: 1.5403491258621216\n",
      "Accuracy: 0.4673\n",
      "epoch: 70, training loss: 1.323379397392273, testing loss: 1.5855764150619507\n",
      "Accuracy: 0.4092\n",
      "epoch: 70, training loss: 1.3674418926239014, testing loss: 1.6704012155532837\n",
      "Accuracy: 0.3838\n",
      "epoch: 70, training loss: 1.3783015012741089, testing loss: 1.5023516416549683\n",
      "Accuracy: 0.4599\n",
      "epoch: 70, training loss: 1.3112945556640625, testing loss: 1.6156535148620605\n",
      "Accuracy: 0.4742\n",
      "epoch: 70, training loss: 1.3247958421707153, testing loss: 1.5675818920135498\n",
      "Accuracy: 0.4644\n",
      "epoch: 70, training loss: 1.3810964822769165, testing loss: 1.5540757179260254\n",
      "Accuracy: 0.4479\n",
      "epoch: 70, training loss: 1.392306923866272, testing loss: 1.6954821348190308\n",
      "Accuracy: 0.4087\n",
      "epoch: 70, training loss: 1.3290382623672485, testing loss: 1.5350195169448853\n",
      "Accuracy: 0.4846\n",
      "epoch: 70, training loss: 1.3280916213989258, testing loss: 1.6034376621246338\n",
      "Accuracy: 0.4281\n",
      "epoch: 70, training loss: 1.3335638046264648, testing loss: 1.6855487823486328\n",
      "Accuracy: 0.4359\n",
      "epoch: 70, training loss: 1.352135181427002, testing loss: 1.635376214981079\n",
      "Accuracy: 0.4180\n",
      "epoch: 70, training loss: 1.417802095413208, testing loss: 1.5973948240280151\n",
      "Accuracy: 0.4557\n",
      "epoch: 70, training loss: 1.3679053783416748, testing loss: 1.5819127559661865\n",
      "Accuracy: 0.4437\n",
      "epoch: 70, training loss: 1.3541182279586792, testing loss: 1.530513882637024\n",
      "Accuracy: 0.4319\n",
      "epoch: 70, training loss: 1.380087971687317, testing loss: 1.6426823139190674\n",
      "Accuracy: 0.4316\n",
      "epoch: 70, training loss: 1.3970965147018433, testing loss: 1.5894262790679932\n",
      "Accuracy: 0.4585\n",
      "epoch: 70, training loss: 1.361992359161377, testing loss: 1.6124688386917114\n",
      "Accuracy: 0.4260\n",
      "epoch: 70, training loss: 1.3614860773086548, testing loss: 1.6142045259475708\n",
      "Accuracy: 0.4108\n",
      "epoch: 70, training loss: 1.3134428262710571, testing loss: 1.5846240520477295\n",
      "Accuracy: 0.4363\n",
      "epoch: 70, training loss: 1.3370037078857422, testing loss: 1.6622929573059082\n",
      "Accuracy: 0.4362\n",
      "epoch: 70, training loss: 1.3590666055679321, testing loss: 1.655324935913086\n",
      "Accuracy: 0.4420\n",
      "epoch: 70, training loss: 1.3762110471725464, testing loss: 1.6180789470672607\n",
      "Accuracy: 0.4658\n",
      "epoch: 70, training loss: 1.2949506044387817, testing loss: 1.6475170850753784\n",
      "Accuracy: 0.3738\n",
      "epoch: 70, training loss: 1.4587645530700684, testing loss: 1.5289438962936401\n",
      "Accuracy: 0.4595\n",
      "epoch: 70, training loss: 1.3503735065460205, testing loss: 1.6650327444076538\n",
      "Accuracy: 0.4570\n",
      "epoch: 70, training loss: 1.3702725172042847, testing loss: 1.6066324710845947\n",
      "Accuracy: 0.4639\n",
      "epoch: 70, training loss: 1.449404239654541, testing loss: 1.5523947477340698\n",
      "Accuracy: 0.4495\n",
      "epoch: 70, training loss: 1.3086655139923096, testing loss: 1.5442205667495728\n",
      "Accuracy: 0.4924\n",
      "epoch: 70, training loss: 1.377605676651001, testing loss: 1.5641562938690186\n",
      "Accuracy: 0.4426\n",
      "epoch: 70, training loss: 1.3043237924575806, testing loss: 1.5370361804962158\n",
      "Accuracy: 0.4478\n",
      "epoch: 70, training loss: 1.2928497791290283, testing loss: 1.6458922624588013\n",
      "Accuracy: 0.4317\n",
      "epoch: 70, training loss: 1.4242340326309204, testing loss: 1.5757663249969482\n",
      "Accuracy: 0.4317\n",
      "epoch: 70, training loss: 1.317270040512085, testing loss: 1.6058051586151123\n",
      "Accuracy: 0.4381\n",
      "epoch: 70, training loss: 1.3967076539993286, testing loss: 1.6436347961425781\n",
      "Accuracy: 0.4281\n",
      "epoch: 70, training loss: 1.345401406288147, testing loss: 1.5633667707443237\n",
      "Accuracy: 0.4602\n",
      "epoch: 70, training loss: 1.3415104150772095, testing loss: 1.5859341621398926\n",
      "Accuracy: 0.4526\n",
      "epoch: 70, training loss: 1.383636713027954, testing loss: 1.711384654045105\n",
      "Accuracy: 0.4644\n",
      "epoch: 70, training loss: 1.3184183835983276, testing loss: 1.6558271646499634\n",
      "Accuracy: 0.4343\n",
      "epoch: 70, training loss: 1.3817260265350342, testing loss: 1.5720733404159546\n",
      "Accuracy: 0.4217\n",
      "epoch: 70, training loss: 1.3948394060134888, testing loss: 1.6691906452178955\n",
      "Accuracy: 0.4226\n",
      "epoch: 70, training loss: 1.40436851978302, testing loss: 1.6479620933532715\n",
      "Accuracy: 0.4025\n",
      "epoch: 70, training loss: 1.3590538501739502, testing loss: 1.6230915784835815\n",
      "Accuracy: 0.4141\n",
      "epoch: 70, training loss: 1.3293421268463135, testing loss: 1.7686582803726196\n",
      "Accuracy: 0.3673\n",
      "epoch: 70, training loss: 1.3703620433807373, testing loss: 1.6565402746200562\n",
      "Accuracy: 0.3987\n",
      "epoch: 70, training loss: 1.3298383951187134, testing loss: 1.5751712322235107\n",
      "Accuracy: 0.4620\n",
      "epoch: 70, training loss: 1.3336433172225952, testing loss: 1.5647646188735962\n",
      "Accuracy: 0.4452\n",
      "epoch: 70, training loss: 1.3070003986358643, testing loss: 1.6614692211151123\n",
      "Accuracy: 0.3855\n",
      "epoch: 80, training loss: 1.3487969636917114, testing loss: 1.5732817649841309\n",
      "Accuracy: 0.4305\n",
      "epoch: 80, training loss: 1.3199390172958374, testing loss: 1.5738468170166016\n",
      "Accuracy: 0.4415\n",
      "epoch: 80, training loss: 1.3808858394622803, testing loss: 1.5511302947998047\n",
      "Accuracy: 0.3953\n",
      "epoch: 80, training loss: 1.3789538145065308, testing loss: 1.5972028970718384\n",
      "Accuracy: 0.4460\n",
      "epoch: 80, training loss: 1.3778222799301147, testing loss: 1.6213693618774414\n",
      "Accuracy: 0.4591\n",
      "epoch: 80, training loss: 1.3595575094223022, testing loss: 1.6405017375946045\n",
      "Accuracy: 0.4153\n",
      "epoch: 80, training loss: 1.2871347665786743, testing loss: 1.6117128133773804\n",
      "Accuracy: 0.4164\n",
      "epoch: 80, training loss: 1.4645098447799683, testing loss: 1.5624406337738037\n",
      "Accuracy: 0.4474\n",
      "epoch: 80, training loss: 1.3177992105484009, testing loss: 1.625364899635315\n",
      "Accuracy: 0.4182\n",
      "epoch: 80, training loss: 1.402284860610962, testing loss: 1.5776376724243164\n",
      "Accuracy: 0.4481\n",
      "epoch: 80, training loss: 1.2773747444152832, testing loss: 1.673252820968628\n",
      "Accuracy: 0.4130\n",
      "epoch: 80, training loss: 1.3211108446121216, testing loss: 1.6674470901489258\n",
      "Accuracy: 0.3538\n",
      "epoch: 80, training loss: 1.370965600013733, testing loss: 1.5827727317810059\n",
      "Accuracy: 0.4381\n",
      "epoch: 80, training loss: 1.3047523498535156, testing loss: 1.482638955116272\n",
      "Accuracy: 0.5034\n",
      "epoch: 80, training loss: 1.4176974296569824, testing loss: 1.5572123527526855\n",
      "Accuracy: 0.4150\n",
      "epoch: 80, training loss: 1.3046562671661377, testing loss: 1.6324471235275269\n",
      "Accuracy: 0.4310\n",
      "epoch: 80, training loss: 1.2764812707901, testing loss: 1.6549594402313232\n",
      "Accuracy: 0.4484\n",
      "epoch: 80, training loss: 1.3119332790374756, testing loss: 1.6109986305236816\n",
      "Accuracy: 0.4172\n",
      "epoch: 80, training loss: 1.3224889039993286, testing loss: 1.5397225618362427\n",
      "Accuracy: 0.4303\n",
      "epoch: 80, training loss: 1.2997907400131226, testing loss: 1.6505427360534668\n",
      "Accuracy: 0.4159\n",
      "epoch: 80, training loss: 1.4017199277877808, testing loss: 1.6505838632583618\n",
      "Accuracy: 0.3964\n",
      "epoch: 80, training loss: 1.3669203519821167, testing loss: 1.6909503936767578\n",
      "Accuracy: 0.4441\n",
      "epoch: 80, training loss: 1.359316110610962, testing loss: 1.6333765983581543\n",
      "Accuracy: 0.4091\n",
      "epoch: 80, training loss: 1.3479466438293457, testing loss: 1.602662444114685\n",
      "Accuracy: 0.4006\n",
      "epoch: 80, training loss: 1.3130923509597778, testing loss: 1.6680055856704712\n",
      "Accuracy: 0.3974\n",
      "epoch: 80, training loss: 1.310049057006836, testing loss: 1.6466156244277954\n",
      "Accuracy: 0.3792\n",
      "epoch: 80, training loss: 1.3911274671554565, testing loss: 1.6284397840499878\n",
      "Accuracy: 0.3898\n",
      "epoch: 80, training loss: 1.2888058423995972, testing loss: 1.5727639198303223\n",
      "Accuracy: 0.4153\n",
      "epoch: 80, training loss: 1.3007056713104248, testing loss: 1.534943699836731\n",
      "Accuracy: 0.4423\n",
      "epoch: 80, training loss: 1.3575645685195923, testing loss: 1.5296316146850586\n",
      "Accuracy: 0.4790\n",
      "epoch: 80, training loss: 1.3621975183486938, testing loss: 1.651247262954712\n",
      "Accuracy: 0.3994\n",
      "epoch: 80, training loss: 1.3569064140319824, testing loss: 1.548156499862671\n",
      "Accuracy: 0.4272\n",
      "epoch: 80, training loss: 1.3180371522903442, testing loss: 1.5981597900390625\n",
      "Accuracy: 0.4554\n",
      "epoch: 80, training loss: 1.3758271932601929, testing loss: 1.643531084060669\n",
      "Accuracy: 0.4676\n",
      "epoch: 80, training loss: 1.3676643371582031, testing loss: 1.6858283281326294\n",
      "Accuracy: 0.4295\n",
      "epoch: 80, training loss: 1.4339072704315186, testing loss: 1.6820170879364014\n",
      "Accuracy: 0.4254\n",
      "epoch: 80, training loss: 1.387489914894104, testing loss: 1.6623402833938599\n",
      "Accuracy: 0.4476\n",
      "epoch: 80, training loss: 1.384321689605713, testing loss: 1.720787525177002\n",
      "Accuracy: 0.4212\n",
      "epoch: 80, training loss: 1.3204816579818726, testing loss: 1.5482463836669922\n",
      "Accuracy: 0.4904\n",
      "epoch: 80, training loss: 1.3701205253601074, testing loss: 1.5497018098831177\n",
      "Accuracy: 0.4658\n",
      "epoch: 80, training loss: 1.4003647565841675, testing loss: 1.6441786289215088\n",
      "Accuracy: 0.4472\n",
      "epoch: 80, training loss: 1.3209574222564697, testing loss: 1.546478033065796\n",
      "Accuracy: 0.4419\n",
      "epoch: 80, training loss: 1.3543022871017456, testing loss: 1.5487273931503296\n",
      "Accuracy: 0.4677\n",
      "epoch: 80, training loss: 1.3034976720809937, testing loss: 1.5880193710327148\n",
      "Accuracy: 0.4542\n",
      "epoch: 80, training loss: 1.3620752096176147, testing loss: 1.5779005289077759\n",
      "Accuracy: 0.4695\n",
      "epoch: 80, training loss: 1.3801259994506836, testing loss: 1.7069647312164307\n",
      "Accuracy: 0.3916\n",
      "epoch: 80, training loss: 1.307646632194519, testing loss: 1.6038845777511597\n",
      "Accuracy: 0.4332\n",
      "epoch: 80, training loss: 1.3513994216918945, testing loss: 1.775053858757019\n",
      "Accuracy: 0.3591\n",
      "epoch: 80, training loss: 1.3436217308044434, testing loss: 1.643784523010254\n",
      "Accuracy: 0.3894\n",
      "epoch: 80, training loss: 1.364500641822815, testing loss: 1.5949516296386719\n",
      "Accuracy: 0.4626\n",
      "epoch: 80, training loss: 1.3669031858444214, testing loss: 1.4882651567459106\n",
      "Accuracy: 0.4773\n",
      "epoch: 80, training loss: 1.3400518894195557, testing loss: 1.6873877048492432\n",
      "Accuracy: 0.4013\n",
      "epoch: 80, training loss: 1.2997024059295654, testing loss: 1.6210640668869019\n",
      "Accuracy: 0.4274\n",
      "epoch: 80, training loss: 1.3842198848724365, testing loss: 1.564752459526062\n",
      "Accuracy: 0.4451\n",
      "epoch: 80, training loss: 1.3122903108596802, testing loss: 1.6471831798553467\n",
      "Accuracy: 0.4221\n",
      "epoch: 80, training loss: 1.3697110414505005, testing loss: 1.6565996408462524\n",
      "Accuracy: 0.4031\n",
      "epoch: 80, training loss: 1.3470762968063354, testing loss: 1.5185010433197021\n",
      "Accuracy: 0.4387\n",
      "epoch: 80, training loss: 1.2858428955078125, testing loss: 1.6598657369613647\n",
      "Accuracy: 0.4486\n",
      "epoch: 80, training loss: 1.4064196348190308, testing loss: 1.5260637998580933\n",
      "Accuracy: 0.4494\n",
      "epoch: 80, training loss: 1.383768081665039, testing loss: 1.641445279121399\n",
      "Accuracy: 0.4263\n",
      "epoch: 80, training loss: 1.3067259788513184, testing loss: 1.716873049736023\n",
      "Accuracy: 0.3994\n",
      "epoch: 80, training loss: 1.413793921470642, testing loss: 1.5617570877075195\n",
      "Accuracy: 0.4564\n",
      "epoch: 80, training loss: 1.317887306213379, testing loss: 1.5752248764038086\n",
      "Accuracy: 0.4933\n",
      "epoch: 80, training loss: 1.322412133216858, testing loss: 1.6015995740890503\n",
      "Accuracy: 0.4437\n",
      "epoch: 80, training loss: 1.3018383979797363, testing loss: 1.7413663864135742\n",
      "Accuracy: 0.4048\n",
      "epoch: 80, training loss: 1.3712546825408936, testing loss: 1.6613227128982544\n",
      "Accuracy: 0.4238\n",
      "epoch: 80, training loss: 1.3889797925949097, testing loss: 1.7059063911437988\n",
      "Accuracy: 0.4179\n",
      "epoch: 80, training loss: 1.3623546361923218, testing loss: 1.5852372646331787\n",
      "Accuracy: 0.4290\n",
      "epoch: 80, training loss: 1.3957819938659668, testing loss: 1.5275096893310547\n",
      "Accuracy: 0.5000\n",
      "epoch: 80, training loss: 1.3374285697937012, testing loss: 1.5294750928878784\n",
      "Accuracy: 0.4870\n",
      "epoch: 80, training loss: 1.3450082540512085, testing loss: 1.5300729274749756\n",
      "Accuracy: 0.4473\n",
      "epoch: 80, training loss: 1.3422640562057495, testing loss: 1.7031421661376953\n",
      "Accuracy: 0.4112\n",
      "epoch: 80, training loss: 1.3302061557769775, testing loss: 1.654009222984314\n",
      "Accuracy: 0.4389\n",
      "epoch: 80, training loss: 1.3531652688980103, testing loss: 1.6249088048934937\n",
      "Accuracy: 0.4276\n",
      "epoch: 80, training loss: 1.3934394121170044, testing loss: 1.6953974962234497\n",
      "Accuracy: 0.4383\n",
      "epoch: 80, training loss: 1.2999459505081177, testing loss: 1.7104355096817017\n",
      "Accuracy: 0.3785\n",
      "epoch: 80, training loss: 1.377009391784668, testing loss: 1.6668963432312012\n",
      "Accuracy: 0.3535\n",
      "epoch: 80, training loss: 1.4026750326156616, testing loss: 1.5861996412277222\n",
      "Accuracy: 0.4085\n",
      "epoch: 80, training loss: 1.3096168041229248, testing loss: 1.7307878732681274\n",
      "Accuracy: 0.4195\n",
      "epoch: 80, training loss: 1.3203896284103394, testing loss: 1.4903569221496582\n",
      "Accuracy: 0.4473\n",
      "epoch: 80, training loss: 1.3469161987304688, testing loss: 1.6910427808761597\n",
      "Accuracy: 0.3746\n",
      "epoch: 80, training loss: 1.3797943592071533, testing loss: 1.7055011987686157\n",
      "Accuracy: 0.4006\n",
      "epoch: 80, training loss: 1.3077048063278198, testing loss: 1.5669022798538208\n",
      "Accuracy: 0.4353\n",
      "epoch: 80, training loss: 1.349319577217102, testing loss: 1.7508933544158936\n",
      "Accuracy: 0.4159\n",
      "epoch: 80, training loss: 1.4094514846801758, testing loss: 1.6374701261520386\n",
      "Accuracy: 0.4221\n",
      "epoch: 80, training loss: 1.4132493734359741, testing loss: 1.6072982549667358\n",
      "Accuracy: 0.4164\n",
      "epoch: 80, training loss: 1.371752142906189, testing loss: 1.5110647678375244\n",
      "Accuracy: 0.4658\n",
      "epoch: 80, training loss: 1.312742829322815, testing loss: 1.6675605773925781\n",
      "Accuracy: 0.4194\n",
      "epoch: 80, training loss: 1.3248040676116943, testing loss: 1.5897685289382935\n",
      "Accuracy: 0.4405\n",
      "epoch: 80, training loss: 1.3092598915100098, testing loss: 1.6493953466415405\n",
      "Accuracy: 0.4326\n",
      "epoch: 80, training loss: 1.2728033065795898, testing loss: 1.7305797338485718\n",
      "Accuracy: 0.3956\n",
      "epoch: 80, training loss: 1.4109445810317993, testing loss: 1.6172655820846558\n",
      "Accuracy: 0.4037\n",
      "epoch: 80, training loss: 1.3526405096054077, testing loss: 1.749287486076355\n",
      "Accuracy: 0.4252\n",
      "epoch: 80, training loss: 1.3724348545074463, testing loss: 1.5817737579345703\n",
      "Accuracy: 0.4486\n",
      "epoch: 80, training loss: 1.30417799949646, testing loss: 1.5898584127426147\n",
      "Accuracy: 0.4290\n",
      "epoch: 80, training loss: 1.3475598096847534, testing loss: 1.610466480255127\n",
      "Accuracy: 0.4352\n",
      "epoch: 80, training loss: 1.346187949180603, testing loss: 1.5043352842330933\n",
      "Accuracy: 0.4257\n",
      "epoch: 80, training loss: 1.324792742729187, testing loss: 1.6052547693252563\n",
      "Accuracy: 0.4238\n",
      "epoch: 80, training loss: 1.456705093383789, testing loss: 1.657826542854309\n",
      "Accuracy: 0.4281\n",
      "epoch: 80, training loss: 1.3767579793930054, testing loss: 1.5464609861373901\n",
      "Accuracy: 0.4300\n",
      "epoch: 90, training loss: 1.3777353763580322, testing loss: 1.6351572275161743\n",
      "Accuracy: 0.4603\n",
      "epoch: 90, training loss: 1.291948676109314, testing loss: 1.599592924118042\n",
      "Accuracy: 0.4451\n",
      "epoch: 90, training loss: 1.289237380027771, testing loss: 1.488627552986145\n",
      "Accuracy: 0.4914\n",
      "epoch: 90, training loss: 1.389039397239685, testing loss: 1.5547080039978027\n",
      "Accuracy: 0.4502\n",
      "epoch: 90, training loss: 1.3196818828582764, testing loss: 1.5940896272659302\n",
      "Accuracy: 0.4007\n",
      "epoch: 90, training loss: 1.3753297328948975, testing loss: 1.7321618795394897\n",
      "Accuracy: 0.3947\n",
      "epoch: 90, training loss: 1.3039238452911377, testing loss: 1.5432989597320557\n",
      "Accuracy: 0.4394\n",
      "epoch: 90, training loss: 1.3818072080612183, testing loss: 1.5841008424758911\n",
      "Accuracy: 0.4028\n",
      "epoch: 90, training loss: 1.339638352394104, testing loss: 1.6567862033843994\n",
      "Accuracy: 0.4041\n",
      "epoch: 90, training loss: 1.3032054901123047, testing loss: 1.684473991394043\n",
      "Accuracy: 0.4241\n",
      "epoch: 90, training loss: 1.3618886470794678, testing loss: 1.658233880996704\n",
      "Accuracy: 0.4046\n",
      "epoch: 90, training loss: 1.286463737487793, testing loss: 1.6172196865081787\n",
      "Accuracy: 0.4389\n",
      "epoch: 90, training loss: 1.3775171041488647, testing loss: 1.7081496715545654\n",
      "Accuracy: 0.4140\n",
      "epoch: 90, training loss: 1.3937163352966309, testing loss: 1.5976039171218872\n",
      "Accuracy: 0.4570\n",
      "epoch: 90, training loss: 1.3460180759429932, testing loss: 1.612850308418274\n",
      "Accuracy: 0.3974\n",
      "epoch: 90, training loss: 1.3768192529678345, testing loss: 1.6511579751968384\n",
      "Accuracy: 0.3987\n",
      "epoch: 90, training loss: 1.3414089679718018, testing loss: 1.5834826231002808\n",
      "Accuracy: 0.4495\n",
      "epoch: 90, training loss: 1.264273762702942, testing loss: 1.5857845544815063\n",
      "Accuracy: 0.4658\n",
      "epoch: 90, training loss: 1.3423352241516113, testing loss: 1.6403114795684814\n",
      "Accuracy: 0.4427\n",
      "epoch: 90, training loss: 1.3462257385253906, testing loss: 1.6528011560440063\n",
      "Accuracy: 0.4172\n",
      "epoch: 90, training loss: 1.3691768646240234, testing loss: 1.5666769742965698\n",
      "Accuracy: 0.4517\n",
      "epoch: 90, training loss: 1.2947936058044434, testing loss: 1.6740542650222778\n",
      "Accuracy: 0.4059\n",
      "epoch: 90, training loss: 1.3834470510482788, testing loss: 1.6187776327133179\n",
      "Accuracy: 0.4405\n",
      "epoch: 90, training loss: 1.3657160997390747, testing loss: 1.6168822050094604\n",
      "Accuracy: 0.3946\n",
      "epoch: 90, training loss: 1.3080755472183228, testing loss: 1.5543862581253052\n",
      "Accuracy: 0.4164\n",
      "epoch: 90, training loss: 1.2981926202774048, testing loss: 1.5663751363754272\n",
      "Accuracy: 0.4263\n",
      "epoch: 90, training loss: 1.3428601026535034, testing loss: 1.6956007480621338\n",
      "Accuracy: 0.4153\n",
      "epoch: 90, training loss: 1.3370548486709595, testing loss: 1.6468416452407837\n",
      "Accuracy: 0.3914\n",
      "epoch: 90, training loss: 1.345179557800293, testing loss: 1.5808145999908447\n",
      "Accuracy: 0.4478\n",
      "epoch: 90, training loss: 1.3378864526748657, testing loss: 1.721564769744873\n",
      "Accuracy: 0.4045\n",
      "epoch: 90, training loss: 1.3217201232910156, testing loss: 1.645249366760254\n",
      "Accuracy: 0.4382\n",
      "epoch: 90, training loss: 1.338945984840393, testing loss: 1.5992966890335083\n",
      "Accuracy: 0.4033\n",
      "epoch: 90, training loss: 1.2693943977355957, testing loss: 1.6302629709243774\n",
      "Accuracy: 0.4177\n",
      "epoch: 90, training loss: 1.3421273231506348, testing loss: 1.5563122034072876\n",
      "Accuracy: 0.4983\n",
      "epoch: 90, training loss: 1.294893741607666, testing loss: 1.5937880277633667\n",
      "Accuracy: 0.4167\n",
      "epoch: 90, training loss: 1.2950611114501953, testing loss: 1.6932326555252075\n",
      "Accuracy: 0.4196\n",
      "epoch: 90, training loss: 1.359405279159546, testing loss: 1.5129427909851074\n",
      "Accuracy: 0.4669\n",
      "epoch: 90, training loss: 1.348035454750061, testing loss: 1.5930051803588867\n",
      "Accuracy: 0.3746\n",
      "epoch: 90, training loss: 1.3241665363311768, testing loss: 1.5955052375793457\n",
      "Accuracy: 0.4795\n",
      "epoch: 90, training loss: 1.3372682332992554, testing loss: 1.5302107334136963\n",
      "Accuracy: 0.4249\n",
      "epoch: 90, training loss: 1.2808690071105957, testing loss: 1.68701171875\n",
      "Accuracy: 0.4323\n",
      "epoch: 90, training loss: 1.3776686191558838, testing loss: 1.588105320930481\n",
      "Accuracy: 0.4420\n",
      "epoch: 90, training loss: 1.4575165510177612, testing loss: 1.583543062210083\n",
      "Accuracy: 0.4381\n",
      "epoch: 90, training loss: 1.2748218774795532, testing loss: 1.504191279411316\n",
      "Accuracy: 0.4810\n",
      "epoch: 90, training loss: 1.3533152341842651, testing loss: 1.546985149383545\n",
      "Accuracy: 0.4682\n",
      "epoch: 90, training loss: 1.3104169368743896, testing loss: 1.605060338973999\n",
      "Accuracy: 0.4299\n",
      "epoch: 90, training loss: 1.2468314170837402, testing loss: 1.6536898612976074\n",
      "Accuracy: 0.4307\n",
      "epoch: 90, training loss: 1.3218973875045776, testing loss: 1.6064449548721313\n",
      "Accuracy: 0.4485\n",
      "epoch: 90, training loss: 1.314158320426941, testing loss: 1.6679878234863281\n",
      "Accuracy: 0.3550\n",
      "epoch: 90, training loss: 1.3401708602905273, testing loss: 1.6425179243087769\n",
      "Accuracy: 0.3937\n",
      "epoch: 90, training loss: 1.2845044136047363, testing loss: 1.5950195789337158\n",
      "Accuracy: 0.4156\n",
      "epoch: 90, training loss: 1.3029868602752686, testing loss: 1.7310723066329956\n",
      "Accuracy: 0.3889\n",
      "epoch: 90, training loss: 1.3535929918289185, testing loss: 1.5867151021957397\n",
      "Accuracy: 0.4554\n",
      "epoch: 90, training loss: 1.3064043521881104, testing loss: 1.5944286584854126\n",
      "Accuracy: 0.4114\n",
      "epoch: 90, training loss: 1.377107858657837, testing loss: 1.6637756824493408\n",
      "Accuracy: 0.4079\n",
      "epoch: 90, training loss: 1.3545924425125122, testing loss: 1.6210237741470337\n",
      "Accuracy: 0.4146\n",
      "epoch: 90, training loss: 1.3855277299880981, testing loss: 1.5965769290924072\n",
      "Accuracy: 0.4348\n",
      "epoch: 90, training loss: 1.3659512996673584, testing loss: 1.5752426385879517\n",
      "Accuracy: 0.4849\n",
      "epoch: 90, training loss: 1.2984744310379028, testing loss: 1.7594821453094482\n",
      "Accuracy: 0.3994\n",
      "epoch: 90, training loss: 1.3168251514434814, testing loss: 1.52028226852417\n",
      "Accuracy: 0.4437\n",
      "epoch: 90, training loss: 1.3740345239639282, testing loss: 1.6333365440368652\n",
      "Accuracy: 0.3702\n",
      "epoch: 90, training loss: 1.2976791858673096, testing loss: 1.5630863904953003\n",
      "Accuracy: 0.4521\n",
      "epoch: 90, training loss: 1.3050166368484497, testing loss: 1.6012294292449951\n",
      "Accuracy: 0.4038\n",
      "epoch: 90, training loss: 1.382291316986084, testing loss: 1.6640698909759521\n",
      "Accuracy: 0.4423\n",
      "epoch: 90, training loss: 1.3054630756378174, testing loss: 1.6414576768875122\n",
      "Accuracy: 0.4059\n",
      "epoch: 90, training loss: 1.344220757484436, testing loss: 1.616220474243164\n",
      "Accuracy: 0.3688\n",
      "epoch: 90, training loss: 1.311750054359436, testing loss: 1.7725456953048706\n",
      "Accuracy: 0.3583\n",
      "epoch: 90, training loss: 1.3851892948150635, testing loss: 1.5537470579147339\n",
      "Accuracy: 0.4808\n",
      "epoch: 90, training loss: 1.27595055103302, testing loss: 1.6712754964828491\n",
      "Accuracy: 0.4413\n",
      "epoch: 90, training loss: 1.3390750885009766, testing loss: 1.654576301574707\n",
      "Accuracy: 0.4152\n",
      "epoch: 90, training loss: 1.303020715713501, testing loss: 1.6916098594665527\n",
      "Accuracy: 0.4560\n",
      "epoch: 90, training loss: 1.3297128677368164, testing loss: 1.5665314197540283\n",
      "Accuracy: 0.4321\n",
      "epoch: 90, training loss: 1.3189656734466553, testing loss: 1.5052748918533325\n",
      "Accuracy: 0.4220\n",
      "epoch: 90, training loss: 1.3050729036331177, testing loss: 1.545638084411621\n",
      "Accuracy: 0.4747\n",
      "epoch: 90, training loss: 1.310157060623169, testing loss: 1.6133475303649902\n",
      "Accuracy: 0.4281\n",
      "epoch: 90, training loss: 1.3488408327102661, testing loss: 1.5669039487838745\n",
      "Accuracy: 0.4876\n",
      "epoch: 90, training loss: 1.3648897409439087, testing loss: 1.6579639911651611\n",
      "Accuracy: 0.4187\n",
      "epoch: 90, training loss: 1.3643689155578613, testing loss: 1.5976450443267822\n",
      "Accuracy: 0.4178\n",
      "epoch: 90, training loss: 1.3478809595108032, testing loss: 1.562552571296692\n",
      "Accuracy: 0.4277\n",
      "epoch: 90, training loss: 1.3722296953201294, testing loss: 1.5111333131790161\n",
      "Accuracy: 0.4444\n",
      "epoch: 90, training loss: 1.3562687635421753, testing loss: 1.6790410280227661\n",
      "Accuracy: 0.3621\n",
      "epoch: 90, training loss: 1.3089063167572021, testing loss: 1.6229630708694458\n",
      "Accuracy: 0.3893\n",
      "epoch: 90, training loss: 1.309983491897583, testing loss: 1.6646236181259155\n",
      "Accuracy: 0.4262\n",
      "epoch: 90, training loss: 1.2887147665023804, testing loss: 1.6033105850219727\n",
      "Accuracy: 0.4349\n",
      "epoch: 90, training loss: 1.3202403783798218, testing loss: 1.5000406503677368\n",
      "Accuracy: 0.4524\n",
      "epoch: 90, training loss: 1.303544282913208, testing loss: 1.7218769788742065\n",
      "Accuracy: 0.4155\n",
      "epoch: 90, training loss: 1.2955288887023926, testing loss: 1.6500250101089478\n",
      "Accuracy: 0.4019\n",
      "epoch: 90, training loss: 1.3564614057540894, testing loss: 1.6001057624816895\n",
      "Accuracy: 0.3981\n",
      "epoch: 90, training loss: 1.2935205698013306, testing loss: 1.5582622289657593\n",
      "Accuracy: 0.4268\n",
      "epoch: 90, training loss: 1.323710560798645, testing loss: 1.6298513412475586\n",
      "Accuracy: 0.4524\n",
      "epoch: 90, training loss: 1.3883455991744995, testing loss: 1.6363639831542969\n",
      "Accuracy: 0.4305\n",
      "epoch: 90, training loss: 1.2726243734359741, testing loss: 1.6629390716552734\n",
      "Accuracy: 0.4127\n",
      "epoch: 90, training loss: 1.379245400428772, testing loss: 1.5637933015823364\n",
      "Accuracy: 0.3715\n",
      "epoch: 90, training loss: 1.3386605978012085, testing loss: 1.6908637285232544\n",
      "Accuracy: 0.4153\n",
      "epoch: 90, training loss: 1.4181877374649048, testing loss: 1.622405767440796\n",
      "Accuracy: 0.4067\n",
      "epoch: 90, training loss: 1.3036144971847534, testing loss: 1.5311312675476074\n",
      "Accuracy: 0.4167\n",
      "epoch: 90, training loss: 1.346472144126892, testing loss: 1.6169341802597046\n",
      "Accuracy: 0.4139\n",
      "epoch: 90, training loss: 1.3304729461669922, testing loss: 1.641255259513855\n",
      "Accuracy: 0.4317\n",
      "epoch: 90, training loss: 1.3841596841812134, testing loss: 1.627532720565796\n",
      "Accuracy: 0.4329\n",
      "epoch: 90, training loss: 1.3230335712432861, testing loss: 1.6404571533203125\n",
      "Accuracy: 0.4071\n",
      "epoch: 100, training loss: 1.2937705516815186, testing loss: 1.6995320320129395\n",
      "Accuracy: 0.4337\n",
      "epoch: 100, training loss: 1.3391811847686768, testing loss: 1.556527853012085\n",
      "Accuracy: 0.4363\n",
      "epoch: 100, training loss: 1.3079503774642944, testing loss: 1.6111687421798706\n",
      "Accuracy: 0.4389\n",
      "epoch: 100, training loss: 1.372836709022522, testing loss: 1.6201196908950806\n",
      "Accuracy: 0.4452\n",
      "epoch: 100, training loss: 1.337339997291565, testing loss: 1.534792423248291\n",
      "Accuracy: 0.4185\n",
      "epoch: 100, training loss: 1.3569461107254028, testing loss: 1.6232314109802246\n",
      "Accuracy: 0.4349\n",
      "epoch: 100, training loss: 1.2443379163742065, testing loss: 1.6548385620117188\n",
      "Accuracy: 0.4212\n",
      "epoch: 100, training loss: 1.3269264698028564, testing loss: 1.6609394550323486\n",
      "Accuracy: 0.4427\n",
      "epoch: 100, training loss: 1.3235176801681519, testing loss: 1.5830053091049194\n",
      "Accuracy: 0.4751\n",
      "epoch: 100, training loss: 1.4030542373657227, testing loss: 1.5904520750045776\n",
      "Accuracy: 0.4045\n",
      "epoch: 100, training loss: 1.334517240524292, testing loss: 1.5586214065551758\n",
      "Accuracy: 0.4169\n",
      "epoch: 100, training loss: 1.3287535905838013, testing loss: 1.5605193376541138\n",
      "Accuracy: 0.4139\n",
      "epoch: 100, training loss: 1.3088253736495972, testing loss: 1.586134672164917\n",
      "Accuracy: 0.4455\n",
      "epoch: 100, training loss: 1.3228141069412231, testing loss: 1.5364594459533691\n",
      "Accuracy: 0.4328\n",
      "epoch: 100, training loss: 1.359796166419983, testing loss: 1.6727901697158813\n",
      "Accuracy: 0.4369\n",
      "epoch: 100, training loss: 1.3392598628997803, testing loss: 1.5676612854003906\n",
      "Accuracy: 0.4678\n",
      "epoch: 100, training loss: 1.2766413688659668, testing loss: 1.4624650478363037\n",
      "Accuracy: 0.4819\n",
      "epoch: 100, training loss: 1.2984803915023804, testing loss: 1.5267812013626099\n",
      "Accuracy: 0.4672\n",
      "epoch: 100, training loss: 1.3885889053344727, testing loss: 1.6856108903884888\n",
      "Accuracy: 0.4048\n",
      "epoch: 100, training loss: 1.3595607280731201, testing loss: 1.597342848777771\n",
      "Accuracy: 0.4120\n",
      "epoch: 100, training loss: 1.361642599105835, testing loss: 1.725645661354065\n",
      "Accuracy: 0.3988\n",
      "epoch: 100, training loss: 1.310713291168213, testing loss: 1.5340882539749146\n",
      "Accuracy: 0.4910\n",
      "epoch: 100, training loss: 1.3159289360046387, testing loss: 1.6098581552505493\n",
      "Accuracy: 0.3910\n",
      "epoch: 100, training loss: 1.3381379842758179, testing loss: 1.62928307056427\n",
      "Accuracy: 0.4315\n",
      "epoch: 100, training loss: 1.249066710472107, testing loss: 1.703341007232666\n",
      "Accuracy: 0.3966\n",
      "epoch: 100, training loss: 1.3041801452636719, testing loss: 1.6056652069091797\n",
      "Accuracy: 0.4582\n",
      "epoch: 100, training loss: 1.2756166458129883, testing loss: 1.5358175039291382\n",
      "Accuracy: 0.4422\n",
      "epoch: 100, training loss: 1.327438235282898, testing loss: 1.6004014015197754\n",
      "Accuracy: 0.4217\n",
      "epoch: 100, training loss: 1.2544879913330078, testing loss: 1.6939126253128052\n",
      "Accuracy: 0.4290\n",
      "epoch: 100, training loss: 1.3384088277816772, testing loss: 1.5637685060501099\n",
      "Accuracy: 0.4548\n",
      "epoch: 100, training loss: 1.2822695970535278, testing loss: 1.6539758443832397\n",
      "Accuracy: 0.4367\n",
      "epoch: 100, training loss: 1.3158077001571655, testing loss: 1.4969291687011719\n",
      "Accuracy: 0.4286\n",
      "epoch: 100, training loss: 1.2873693704605103, testing loss: 1.6213701963424683\n",
      "Accuracy: 0.3933\n",
      "epoch: 100, training loss: 1.3801136016845703, testing loss: 1.6166319847106934\n",
      "Accuracy: 0.3894\n",
      "epoch: 100, training loss: 1.3148133754730225, testing loss: 1.6720752716064453\n",
      "Accuracy: 0.4108\n",
      "epoch: 100, training loss: 1.3911181688308716, testing loss: 1.8258721828460693\n",
      "Accuracy: 0.3828\n",
      "epoch: 100, training loss: 1.2719124555587769, testing loss: 1.7690110206604004\n",
      "Accuracy: 0.4052\n",
      "epoch: 100, training loss: 1.2794758081436157, testing loss: 1.5498871803283691\n",
      "Accuracy: 0.4164\n",
      "epoch: 100, training loss: 1.3599740266799927, testing loss: 1.5477746725082397\n",
      "Accuracy: 0.4448\n",
      "epoch: 100, training loss: 1.2850854396820068, testing loss: 1.4749362468719482\n",
      "Accuracy: 0.4983\n",
      "epoch: 100, training loss: 1.2629714012145996, testing loss: 1.582672119140625\n",
      "Accuracy: 0.4333\n",
      "epoch: 100, training loss: 1.308543086051941, testing loss: 1.5501853227615356\n",
      "Accuracy: 0.4563\n",
      "epoch: 100, training loss: 1.3238670825958252, testing loss: 1.6150965690612793\n",
      "Accuracy: 0.3943\n",
      "epoch: 100, training loss: 1.2920230627059937, testing loss: 1.528415322303772\n",
      "Accuracy: 0.3968\n",
      "epoch: 100, training loss: 1.3621387481689453, testing loss: 1.6020987033843994\n",
      "Accuracy: 0.4277\n",
      "epoch: 100, training loss: 1.3079197406768799, testing loss: 1.5946029424667358\n",
      "Accuracy: 0.4615\n",
      "epoch: 100, training loss: 1.34695565700531, testing loss: 1.6159602403640747\n",
      "Accuracy: 0.4349\n",
      "epoch: 100, training loss: 1.3052576780319214, testing loss: 1.6846274137496948\n",
      "Accuracy: 0.4239\n",
      "epoch: 100, training loss: 1.3229328393936157, testing loss: 1.6187915802001953\n",
      "Accuracy: 0.4007\n",
      "epoch: 100, training loss: 1.3184677362442017, testing loss: 1.5881901979446411\n",
      "Accuracy: 0.4321\n",
      "epoch: 100, training loss: 1.3110488653182983, testing loss: 1.6421313285827637\n",
      "Accuracy: 0.3891\n",
      "epoch: 100, training loss: 1.3251011371612549, testing loss: 1.629098653793335\n",
      "Accuracy: 0.4241\n",
      "epoch: 100, training loss: 1.3415099382400513, testing loss: 1.6450482606887817\n",
      "Accuracy: 0.3994\n",
      "epoch: 100, training loss: 1.3093469142913818, testing loss: 1.4869664907455444\n",
      "Accuracy: 0.4508\n",
      "epoch: 100, training loss: 1.356493353843689, testing loss: 1.6471788883209229\n",
      "Accuracy: 0.4024\n",
      "epoch: 100, training loss: 1.3580905199050903, testing loss: 1.6129826307296753\n",
      "Accuracy: 0.4371\n",
      "epoch: 100, training loss: 1.3990395069122314, testing loss: 1.6987165212631226\n",
      "Accuracy: 0.4073\n",
      "epoch: 100, training loss: 1.3947035074234009, testing loss: 1.636095404624939\n",
      "Accuracy: 0.4262\n",
      "epoch: 100, training loss: 1.3010458946228027, testing loss: 1.5183255672454834\n",
      "Accuracy: 0.4422\n",
      "epoch: 100, training loss: 1.2938296794891357, testing loss: 1.7364622354507446\n",
      "Accuracy: 0.4072\n",
      "epoch: 100, training loss: 1.2894240617752075, testing loss: 1.6789036989212036\n",
      "Accuracy: 0.4343\n",
      "epoch: 100, training loss: 1.3766822814941406, testing loss: 1.5092344284057617\n",
      "Accuracy: 0.4055\n",
      "epoch: 100, training loss: 1.2824101448059082, testing loss: 1.7026827335357666\n",
      "Accuracy: 0.4369\n",
      "epoch: 100, training loss: 1.3278154134750366, testing loss: 1.5561151504516602\n",
      "Accuracy: 0.4337\n",
      "epoch: 100, training loss: 1.3777029514312744, testing loss: 1.7802900075912476\n",
      "Accuracy: 0.3854\n",
      "epoch: 100, training loss: 1.3201338052749634, testing loss: 1.540858268737793\n",
      "Accuracy: 0.4606\n",
      "epoch: 100, training loss: 1.2790204286575317, testing loss: 1.6673057079315186\n",
      "Accuracy: 0.4161\n",
      "epoch: 100, training loss: 1.2830458879470825, testing loss: 1.5467588901519775\n",
      "Accuracy: 0.4367\n",
      "epoch: 100, training loss: 1.305677056312561, testing loss: 1.5018713474273682\n",
      "Accuracy: 0.4639\n",
      "epoch: 100, training loss: 1.3537594079971313, testing loss: 1.658368706703186\n",
      "Accuracy: 0.4566\n",
      "epoch: 100, training loss: 1.334262728691101, testing loss: 1.62093186378479\n",
      "Accuracy: 0.4130\n",
      "epoch: 100, training loss: 1.359052062034607, testing loss: 1.7720760107040405\n",
      "Accuracy: 0.3762\n",
      "epoch: 100, training loss: 1.2977898120880127, testing loss: 1.662092685699463\n",
      "Accuracy: 0.4000\n",
      "epoch: 100, training loss: 1.30181086063385, testing loss: 1.5921028852462769\n",
      "Accuracy: 0.4557\n",
      "epoch: 100, training loss: 1.3183822631835938, testing loss: 1.68907630443573\n",
      "Accuracy: 0.4161\n",
      "epoch: 100, training loss: 1.2585115432739258, testing loss: 1.6042195558547974\n",
      "Accuracy: 0.4152\n",
      "epoch: 100, training loss: 1.2649967670440674, testing loss: 1.7264997959136963\n",
      "Accuracy: 0.4502\n",
      "epoch: 100, training loss: 1.3119184970855713, testing loss: 1.637246012687683\n",
      "Accuracy: 0.3987\n",
      "epoch: 100, training loss: 1.3747355937957764, testing loss: 1.7765672206878662\n",
      "Accuracy: 0.4024\n",
      "epoch: 100, training loss: 1.353663682937622, testing loss: 1.6418406963348389\n",
      "Accuracy: 0.4159\n",
      "epoch: 100, training loss: 1.3098794221878052, testing loss: 1.6192182302474976\n",
      "Accuracy: 0.4412\n",
      "epoch: 100, training loss: 1.3100770711898804, testing loss: 1.6632826328277588\n",
      "Accuracy: 0.3897\n",
      "epoch: 100, training loss: 1.3103870153427124, testing loss: 1.7504521608352661\n",
      "Accuracy: 0.3431\n",
      "epoch: 100, training loss: 1.3552871942520142, testing loss: 1.5202378034591675\n",
      "Accuracy: 0.5033\n",
      "epoch: 100, training loss: 1.339809536933899, testing loss: 1.512223243713379\n",
      "Accuracy: 0.4543\n",
      "epoch: 100, training loss: 1.3361272811889648, testing loss: 1.6876866817474365\n",
      "Accuracy: 0.4393\n",
      "epoch: 100, training loss: 1.2979322671890259, testing loss: 1.5579105615615845\n",
      "Accuracy: 0.4542\n",
      "epoch: 100, training loss: 1.3386150598526, testing loss: 1.5806646347045898\n",
      "Accuracy: 0.4480\n",
      "epoch: 100, training loss: 1.3682608604431152, testing loss: 1.6520992517471313\n",
      "Accuracy: 0.4237\n",
      "epoch: 100, training loss: 1.3393703699111938, testing loss: 1.5305969715118408\n",
      "Accuracy: 0.4890\n",
      "epoch: 100, training loss: 1.3345049619674683, testing loss: 1.6650831699371338\n",
      "Accuracy: 0.4031\n",
      "epoch: 100, training loss: 1.3900972604751587, testing loss: 1.649598240852356\n",
      "Accuracy: 0.4295\n",
      "epoch: 100, training loss: 1.3524430990219116, testing loss: 1.6187578439712524\n",
      "Accuracy: 0.4756\n",
      "epoch: 100, training loss: 1.3120577335357666, testing loss: 1.534149169921875\n",
      "Accuracy: 0.4600\n",
      "epoch: 100, training loss: 1.3083837032318115, testing loss: 1.5558240413665771\n",
      "Accuracy: 0.4563\n",
      "epoch: 100, training loss: 1.4100635051727295, testing loss: 1.6390857696533203\n",
      "Accuracy: 0.4172\n",
      "epoch: 100, training loss: 1.3456629514694214, testing loss: 1.6666733026504517\n",
      "Accuracy: 0.3951\n",
      "epoch: 100, training loss: 1.3180030584335327, testing loss: 1.6824047565460205\n",
      "Accuracy: 0.4375\n",
      "epoch: 100, training loss: 1.3278709650039673, testing loss: 1.5727806091308594\n",
      "Accuracy: 0.4262\n",
      "epoch: 100, training loss: 1.294316053390503, testing loss: 1.6517256498336792\n",
      "Accuracy: 0.4296\n",
      "epoch: 110, training loss: 1.3870209455490112, testing loss: 1.6306893825531006\n",
      "Accuracy: 0.3882\n",
      "epoch: 110, training loss: 1.2603282928466797, testing loss: 1.6081691980361938\n",
      "Accuracy: 0.4389\n",
      "epoch: 110, training loss: 1.3388372659683228, testing loss: 1.5643203258514404\n",
      "Accuracy: 0.4459\n",
      "epoch: 110, training loss: 1.3481590747833252, testing loss: 1.6160554885864258\n",
      "Accuracy: 0.4320\n",
      "epoch: 110, training loss: 1.3593182563781738, testing loss: 1.5236964225769043\n",
      "Accuracy: 0.4713\n",
      "epoch: 110, training loss: 1.284827709197998, testing loss: 1.5644561052322388\n",
      "Accuracy: 0.4645\n",
      "epoch: 110, training loss: 1.3641127347946167, testing loss: 1.6560797691345215\n",
      "Accuracy: 0.4050\n",
      "epoch: 110, training loss: 1.3102360963821411, testing loss: 1.7298834323883057\n",
      "Accuracy: 0.3957\n",
      "epoch: 110, training loss: 1.3878871202468872, testing loss: 1.6901516914367676\n",
      "Accuracy: 0.4014\n",
      "epoch: 110, training loss: 1.275815486907959, testing loss: 1.6540803909301758\n",
      "Accuracy: 0.4254\n",
      "epoch: 110, training loss: 1.2757536172866821, testing loss: 1.5842413902282715\n",
      "Accuracy: 0.4561\n",
      "epoch: 110, training loss: 1.253865122795105, testing loss: 1.6639786958694458\n",
      "Accuracy: 0.4305\n",
      "epoch: 110, training loss: 1.3247884511947632, testing loss: 1.5719869136810303\n",
      "Accuracy: 0.4351\n",
      "epoch: 110, training loss: 1.3360121250152588, testing loss: 1.6386457681655884\n",
      "Accuracy: 0.4025\n",
      "epoch: 110, training loss: 1.3782390356063843, testing loss: 1.5025917291641235\n",
      "Accuracy: 0.4518\n",
      "epoch: 110, training loss: 1.3519729375839233, testing loss: 1.6640658378601074\n",
      "Accuracy: 0.4146\n",
      "epoch: 110, training loss: 1.316357135772705, testing loss: 1.5616775751113892\n",
      "Accuracy: 0.4230\n",
      "epoch: 110, training loss: 1.3707478046417236, testing loss: 1.6029497385025024\n",
      "Accuracy: 0.4164\n",
      "epoch: 110, training loss: 1.315701961517334, testing loss: 1.621483325958252\n",
      "Accuracy: 0.4438\n",
      "epoch: 110, training loss: 1.357515811920166, testing loss: 1.6528316736221313\n",
      "Accuracy: 0.4262\n",
      "epoch: 110, training loss: 1.292603611946106, testing loss: 1.5331617593765259\n",
      "Accuracy: 0.4479\n",
      "epoch: 110, training loss: 1.2793537378311157, testing loss: 1.5493065118789673\n",
      "Accuracy: 0.4540\n",
      "epoch: 110, training loss: 1.2955372333526611, testing loss: 1.5627388954162598\n",
      "Accuracy: 0.4295\n",
      "epoch: 110, training loss: 1.3352620601654053, testing loss: 1.6709517240524292\n",
      "Accuracy: 0.4361\n",
      "epoch: 110, training loss: 1.2792550325393677, testing loss: 1.6490310430526733\n",
      "Accuracy: 0.4557\n",
      "epoch: 110, training loss: 1.3976669311523438, testing loss: 1.469834804534912\n",
      "Accuracy: 0.4669\n",
      "epoch: 110, training loss: 1.2903320789337158, testing loss: 1.7055085897445679\n",
      "Accuracy: 0.3841\n",
      "epoch: 110, training loss: 1.2997571229934692, testing loss: 1.6565943956375122\n",
      "Accuracy: 0.4410\n",
      "epoch: 110, training loss: 1.3063850402832031, testing loss: 1.6748316287994385\n",
      "Accuracy: 0.3866\n",
      "epoch: 110, training loss: 1.3476403951644897, testing loss: 1.5502279996871948\n",
      "Accuracy: 0.4259\n",
      "epoch: 110, training loss: 1.3200517892837524, testing loss: 1.6489969491958618\n",
      "Accuracy: 0.3994\n",
      "epoch: 110, training loss: 1.3499137163162231, testing loss: 1.6589370965957642\n",
      "Accuracy: 0.4377\n",
      "epoch: 110, training loss: 1.3618005514144897, testing loss: 1.6640145778656006\n",
      "Accuracy: 0.4183\n",
      "epoch: 110, training loss: 1.3253508806228638, testing loss: 1.5936081409454346\n",
      "Accuracy: 0.4257\n",
      "epoch: 110, training loss: 1.2776663303375244, testing loss: 1.6774210929870605\n",
      "Accuracy: 0.4476\n",
      "epoch: 110, training loss: 1.37937331199646, testing loss: 1.6122711896896362\n",
      "Accuracy: 0.4444\n",
      "epoch: 110, training loss: 1.3087728023529053, testing loss: 1.7189509868621826\n",
      "Accuracy: 0.4730\n",
      "epoch: 110, training loss: 1.3027174472808838, testing loss: 1.6849803924560547\n",
      "Accuracy: 0.4006\n",
      "epoch: 110, training loss: 1.3336118459701538, testing loss: 1.5716930627822876\n",
      "Accuracy: 0.4131\n",
      "epoch: 110, training loss: 1.293600082397461, testing loss: 1.672664999961853\n",
      "Accuracy: 0.4488\n",
      "epoch: 110, training loss: 1.396795630455017, testing loss: 1.585825800895691\n",
      "Accuracy: 0.4489\n",
      "epoch: 110, training loss: 1.2783303260803223, testing loss: 1.5788390636444092\n",
      "Accuracy: 0.4314\n",
      "epoch: 110, training loss: 1.3284366130828857, testing loss: 1.5241562128067017\n",
      "Accuracy: 0.4356\n",
      "epoch: 110, training loss: 1.3444459438323975, testing loss: 1.6682652235031128\n",
      "Accuracy: 0.4028\n",
      "epoch: 110, training loss: 1.2986334562301636, testing loss: 1.596495270729065\n",
      "Accuracy: 0.4458\n",
      "epoch: 110, training loss: 1.2764662504196167, testing loss: 1.6427574157714844\n",
      "Accuracy: 0.3974\n",
      "epoch: 110, training loss: 1.3166282176971436, testing loss: 1.7038207054138184\n",
      "Accuracy: 0.4054\n",
      "epoch: 110, training loss: 1.3203039169311523, testing loss: 1.721112847328186\n",
      "Accuracy: 0.3806\n",
      "epoch: 110, training loss: 1.3347523212432861, testing loss: 1.5755503177642822\n",
      "Accuracy: 0.4463\n",
      "epoch: 110, training loss: 1.3542814254760742, testing loss: 1.6294219493865967\n",
      "Accuracy: 0.4235\n",
      "epoch: 110, training loss: 1.3332830667495728, testing loss: 1.808927059173584\n",
      "Accuracy: 0.3571\n",
      "epoch: 110, training loss: 1.3911741971969604, testing loss: 1.6151002645492554\n",
      "Accuracy: 0.4241\n",
      "epoch: 110, training loss: 1.3085416555404663, testing loss: 1.613284945487976\n",
      "Accuracy: 0.4618\n",
      "epoch: 110, training loss: 1.3239575624465942, testing loss: 1.466111660003662\n",
      "Accuracy: 0.4674\n",
      "epoch: 110, training loss: 1.3792319297790527, testing loss: 1.5152978897094727\n",
      "Accuracy: 0.4745\n",
      "epoch: 110, training loss: 1.3206408023834229, testing loss: 1.5685557126998901\n",
      "Accuracy: 0.4355\n",
      "epoch: 110, training loss: 1.3064074516296387, testing loss: 1.5835132598876953\n",
      "Accuracy: 0.4395\n",
      "epoch: 110, training loss: 1.3157382011413574, testing loss: 1.539064884185791\n",
      "Accuracy: 0.4267\n",
      "epoch: 110, training loss: 1.2892025709152222, testing loss: 1.6125845909118652\n",
      "Accuracy: 0.4188\n",
      "epoch: 110, training loss: 1.3178240060806274, testing loss: 1.7086223363876343\n",
      "Accuracy: 0.4257\n",
      "epoch: 110, training loss: 1.3735049962997437, testing loss: 1.5063567161560059\n",
      "Accuracy: 0.4464\n",
      "epoch: 110, training loss: 1.3393237590789795, testing loss: 1.7483028173446655\n",
      "Accuracy: 0.3903\n",
      "epoch: 110, training loss: 1.2536293268203735, testing loss: 1.5840158462524414\n",
      "Accuracy: 0.4634\n",
      "epoch: 110, training loss: 1.4152166843414307, testing loss: 1.5993192195892334\n",
      "Accuracy: 0.4423\n",
      "epoch: 110, training loss: 1.3216032981872559, testing loss: 1.6579761505126953\n",
      "Accuracy: 0.4272\n",
      "epoch: 110, training loss: 1.334090232849121, testing loss: 1.5708975791931152\n",
      "Accuracy: 0.4332\n",
      "epoch: 110, training loss: 1.2847071886062622, testing loss: 1.7356489896774292\n",
      "Accuracy: 0.4290\n",
      "epoch: 110, training loss: 1.341751217842102, testing loss: 1.7176296710968018\n",
      "Accuracy: 0.4181\n",
      "epoch: 110, training loss: 1.2949143648147583, testing loss: 1.7072519063949585\n",
      "Accuracy: 0.3722\n",
      "epoch: 110, training loss: 1.268370270729065, testing loss: 1.6353195905685425\n",
      "Accuracy: 0.4441\n",
      "epoch: 110, training loss: 1.3155789375305176, testing loss: 1.6598584651947021\n",
      "Accuracy: 0.4103\n",
      "epoch: 110, training loss: 1.3030966520309448, testing loss: 1.5917130708694458\n",
      "Accuracy: 0.4164\n",
      "epoch: 110, training loss: 1.3154549598693848, testing loss: 1.738663911819458\n",
      "Accuracy: 0.3919\n",
      "epoch: 110, training loss: 1.3730789422988892, testing loss: 1.7083284854888916\n",
      "Accuracy: 0.4141\n",
      "epoch: 110, training loss: 1.3044594526290894, testing loss: 1.5269707441329956\n",
      "Accuracy: 0.4932\n",
      "epoch: 110, training loss: 1.2636719942092896, testing loss: 1.5945305824279785\n",
      "Accuracy: 0.3920\n",
      "epoch: 110, training loss: 1.265355110168457, testing loss: 1.5505741834640503\n",
      "Accuracy: 0.4543\n",
      "epoch: 110, training loss: 1.307041049003601, testing loss: 1.5542516708374023\n",
      "Accuracy: 0.4514\n",
      "epoch: 110, training loss: 1.309940218925476, testing loss: 1.797881841659546\n",
      "Accuracy: 0.4509\n",
      "epoch: 110, training loss: 1.3580149412155151, testing loss: 1.5431631803512573\n",
      "Accuracy: 0.4816\n",
      "epoch: 110, training loss: 1.3056609630584717, testing loss: 1.6035524606704712\n",
      "Accuracy: 0.4290\n",
      "epoch: 110, training loss: 1.261108160018921, testing loss: 1.5454331636428833\n",
      "Accuracy: 0.4271\n",
      "epoch: 110, training loss: 1.3280739784240723, testing loss: 1.578134298324585\n",
      "Accuracy: 0.4455\n",
      "epoch: 110, training loss: 1.3129994869232178, testing loss: 1.6870472431182861\n",
      "Accuracy: 0.3913\n",
      "epoch: 110, training loss: 1.3111486434936523, testing loss: 1.6411848068237305\n",
      "Accuracy: 0.4384\n",
      "epoch: 110, training loss: 1.3498846292495728, testing loss: 1.686928391456604\n",
      "Accuracy: 0.4129\n",
      "epoch: 110, training loss: 1.3272974491119385, testing loss: 1.5416066646575928\n",
      "Accuracy: 0.4984\n",
      "epoch: 110, training loss: 1.3243279457092285, testing loss: 1.6353375911712646\n",
      "Accuracy: 0.4403\n",
      "epoch: 110, training loss: 1.3613444566726685, testing loss: 1.6553101539611816\n",
      "Accuracy: 0.4026\n",
      "epoch: 110, training loss: 1.321581482887268, testing loss: 1.5370917320251465\n",
      "Accuracy: 0.4628\n",
      "epoch: 110, training loss: 1.2225452661514282, testing loss: 1.5911399126052856\n",
      "Accuracy: 0.4571\n",
      "epoch: 110, training loss: 1.2547680139541626, testing loss: 1.6500310897827148\n",
      "Accuracy: 0.4229\n",
      "epoch: 110, training loss: 1.3334418535232544, testing loss: 1.650073528289795\n",
      "Accuracy: 0.4371\n",
      "epoch: 110, training loss: 1.3458232879638672, testing loss: 1.6121002435684204\n",
      "Accuracy: 0.4204\n",
      "epoch: 110, training loss: 1.2858941555023193, testing loss: 1.5527640581130981\n",
      "Accuracy: 0.4681\n",
      "epoch: 110, training loss: 1.3425647020339966, testing loss: 1.689151644706726\n",
      "Accuracy: 0.4441\n",
      "epoch: 110, training loss: 1.3780341148376465, testing loss: 1.6644359827041626\n",
      "Accuracy: 0.4133\n",
      "epoch: 110, training loss: 1.2733010053634644, testing loss: 1.5153529644012451\n",
      "Accuracy: 0.4765\n",
      "epoch: 110, training loss: 1.3225784301757812, testing loss: 1.627563238143921\n",
      "Accuracy: 0.4126\n",
      "epoch: 110, training loss: 1.3471853733062744, testing loss: 1.5559502840042114\n",
      "Accuracy: 0.4232\n",
      "epoch: 220, training loss: 1.2943789958953857, testing loss: 1.639739751815796\n",
      "Accuracy: 0.4277\n",
      "epoch: 220, training loss: 1.2727669477462769, testing loss: 1.7366058826446533\n",
      "Accuracy: 0.4180\n",
      "epoch: 220, training loss: 1.270232081413269, testing loss: 1.8058103322982788\n",
      "Accuracy: 0.3323\n",
      "epoch: 220, training loss: 1.3044538497924805, testing loss: 1.6843351125717163\n",
      "Accuracy: 0.4286\n",
      "epoch: 220, training loss: 1.2443883419036865, testing loss: 1.763257622718811\n",
      "Accuracy: 0.3867\n",
      "epoch: 220, training loss: 1.2601664066314697, testing loss: 1.6101223230361938\n",
      "Accuracy: 0.4458\n",
      "epoch: 220, training loss: 1.2855005264282227, testing loss: 1.7685130834579468\n",
      "Accuracy: 0.4190\n",
      "epoch: 220, training loss: 1.3337515592575073, testing loss: 1.622282862663269\n",
      "Accuracy: 0.4744\n",
      "epoch: 220, training loss: 1.2550207376480103, testing loss: 1.6122522354125977\n",
      "Accuracy: 0.4413\n",
      "epoch: 220, training loss: 1.2919589281082153, testing loss: 1.7161245346069336\n",
      "Accuracy: 0.4007\n",
      "epoch: 220, training loss: 1.3088996410369873, testing loss: 1.9288904666900635\n",
      "Accuracy: 0.4206\n",
      "epoch: 220, training loss: 1.2062705755233765, testing loss: 1.77103590965271\n",
      "Accuracy: 0.4299\n",
      "epoch: 220, training loss: 1.2679978609085083, testing loss: 1.7065972089767456\n",
      "Accuracy: 0.4042\n",
      "epoch: 220, training loss: 1.3379456996917725, testing loss: 1.686032772064209\n",
      "Accuracy: 0.4290\n",
      "epoch: 220, training loss: 1.244706630706787, testing loss: 1.6224039793014526\n",
      "Accuracy: 0.4301\n",
      "epoch: 220, training loss: 1.3162033557891846, testing loss: 1.6374610662460327\n",
      "Accuracy: 0.4965\n",
      "epoch: 220, training loss: 1.3231056928634644, testing loss: 1.6975045204162598\n",
      "Accuracy: 0.4204\n",
      "epoch: 220, training loss: 1.310903787612915, testing loss: 1.5422093868255615\n",
      "Accuracy: 0.4589\n",
      "epoch: 220, training loss: 1.2666672468185425, testing loss: 1.550873041152954\n",
      "Accuracy: 0.4337\n",
      "epoch: 220, training loss: 1.3381457328796387, testing loss: 1.6023343801498413\n",
      "Accuracy: 0.4232\n",
      "epoch: 220, training loss: 1.3285521268844604, testing loss: 1.6281898021697998\n",
      "Accuracy: 0.4069\n",
      "epoch: 220, training loss: 1.218665361404419, testing loss: 1.643200397491455\n",
      "Accuracy: 0.3981\n",
      "epoch: 220, training loss: 1.2828465700149536, testing loss: 1.674782633781433\n",
      "Accuracy: 0.4302\n",
      "epoch: 220, training loss: 1.2861783504486084, testing loss: 1.7608953714370728\n",
      "Accuracy: 0.4394\n",
      "epoch: 220, training loss: 1.3019365072250366, testing loss: 1.6657882928848267\n",
      "Accuracy: 0.4186\n",
      "epoch: 220, training loss: 1.2847763299942017, testing loss: 1.7448770999908447\n",
      "Accuracy: 0.3838\n",
      "epoch: 220, training loss: 1.3165512084960938, testing loss: 1.7085236310958862\n",
      "Accuracy: 0.4026\n",
      "epoch: 220, training loss: 1.2576396465301514, testing loss: 1.593798041343689\n",
      "Accuracy: 0.4578\n",
      "epoch: 220, training loss: 1.360292673110962, testing loss: 1.6917099952697754\n",
      "Accuracy: 0.4026\n",
      "epoch: 220, training loss: 1.2927165031433105, testing loss: 1.6006139516830444\n",
      "Accuracy: 0.3932\n",
      "epoch: 220, training loss: 1.266296625137329, testing loss: 1.695020079612732\n",
      "Accuracy: 0.4684\n",
      "epoch: 220, training loss: 1.3741402626037598, testing loss: 1.5742383003234863\n",
      "Accuracy: 0.4400\n",
      "epoch: 220, training loss: 1.2318273782730103, testing loss: 1.5699162483215332\n",
      "Accuracy: 0.4164\n",
      "epoch: 220, training loss: 1.244150161743164, testing loss: 1.7659953832626343\n",
      "Accuracy: 0.4114\n",
      "epoch: 220, training loss: 1.29715096950531, testing loss: 1.574714183807373\n",
      "Accuracy: 0.4272\n",
      "epoch: 220, training loss: 1.255730152130127, testing loss: 1.6368858814239502\n",
      "Accuracy: 0.4032\n",
      "epoch: 220, training loss: 1.2627800703048706, testing loss: 1.7308063507080078\n",
      "Accuracy: 0.4598\n",
      "epoch: 220, training loss: 1.2053289413452148, testing loss: 1.6075907945632935\n",
      "Accuracy: 0.4393\n",
      "epoch: 220, training loss: 1.2952579259872437, testing loss: 1.648114800453186\n",
      "Accuracy: 0.4290\n",
      "epoch: 220, training loss: 1.3125288486480713, testing loss: 1.5935349464416504\n",
      "Accuracy: 0.4028\n",
      "epoch: 220, training loss: 1.2339355945587158, testing loss: 1.608808159828186\n",
      "Accuracy: 0.4528\n",
      "epoch: 220, training loss: 1.193193793296814, testing loss: 1.789433240890503\n",
      "Accuracy: 0.3962\n",
      "epoch: 220, training loss: 1.2125377655029297, testing loss: 1.655548334121704\n",
      "Accuracy: 0.4286\n",
      "epoch: 220, training loss: 1.3482739925384521, testing loss: 1.670637845993042\n",
      "Accuracy: 0.4066\n",
      "epoch: 220, training loss: 1.3504916429519653, testing loss: 1.7183045148849487\n",
      "Accuracy: 0.4441\n",
      "epoch: 220, training loss: 1.2052388191223145, testing loss: 1.5502796173095703\n",
      "Accuracy: 0.4631\n",
      "epoch: 220, training loss: 1.2636895179748535, testing loss: 1.6540052890777588\n",
      "Accuracy: 0.4013\n",
      "epoch: 220, training loss: 1.311619520187378, testing loss: 1.6813924312591553\n",
      "Accuracy: 0.4048\n",
      "epoch: 220, training loss: 1.2734787464141846, testing loss: 1.672871708869934\n",
      "Accuracy: 0.4144\n",
      "epoch: 220, training loss: 1.292785406112671, testing loss: 1.6517223119735718\n",
      "Accuracy: 0.4232\n",
      "epoch: 220, training loss: 1.3321573734283447, testing loss: 1.5567455291748047\n",
      "Accuracy: 0.4654\n",
      "epoch: 220, training loss: 1.2823137044906616, testing loss: 1.74774169921875\n",
      "Accuracy: 0.3841\n",
      "epoch: 220, training loss: 1.2672966718673706, testing loss: 1.6296048164367676\n",
      "Accuracy: 0.4579\n",
      "epoch: 220, training loss: 1.236423134803772, testing loss: 1.5599172115325928\n",
      "Accuracy: 0.4548\n",
      "epoch: 220, training loss: 1.2409064769744873, testing loss: 1.6119540929794312\n",
      "Accuracy: 0.4685\n",
      "epoch: 220, training loss: 1.1960443258285522, testing loss: 1.5898751020431519\n",
      "Accuracy: 0.4366\n",
      "epoch: 220, training loss: 1.2690813541412354, testing loss: 1.7927531003952026\n",
      "Accuracy: 0.3767\n",
      "epoch: 220, training loss: 1.284402847290039, testing loss: 1.6537623405456543\n",
      "Accuracy: 0.4865\n",
      "epoch: 220, training loss: 1.2411624193191528, testing loss: 1.6044504642486572\n",
      "Accuracy: 0.4124\n",
      "epoch: 220, training loss: 1.2784849405288696, testing loss: 1.6693179607391357\n",
      "Accuracy: 0.3591\n",
      "epoch: 220, training loss: 1.227677822113037, testing loss: 1.7402215003967285\n",
      "Accuracy: 0.3914\n",
      "epoch: 220, training loss: 1.276939034461975, testing loss: 1.6464892625808716\n",
      "Accuracy: 0.4161\n",
      "epoch: 220, training loss: 1.2192531824111938, testing loss: 1.7233800888061523\n",
      "Accuracy: 0.3880\n",
      "epoch: 220, training loss: 1.2324273586273193, testing loss: 1.6270575523376465\n",
      "Accuracy: 0.4305\n",
      "epoch: 220, training loss: 1.3086862564086914, testing loss: 1.673305630683899\n",
      "Accuracy: 0.4076\n",
      "epoch: 220, training loss: 1.21831476688385, testing loss: 1.642089605331421\n",
      "Accuracy: 0.4230\n",
      "epoch: 220, training loss: 1.2495874166488647, testing loss: 1.6524577140808105\n",
      "Accuracy: 0.3958\n",
      "epoch: 220, training loss: 1.3201977014541626, testing loss: 1.5644538402557373\n",
      "Accuracy: 0.4800\n",
      "epoch: 220, training loss: 1.288760781288147, testing loss: 1.6681289672851562\n",
      "Accuracy: 0.4143\n",
      "epoch: 220, training loss: 1.2715171575546265, testing loss: 1.6326119899749756\n",
      "Accuracy: 0.4253\n",
      "epoch: 220, training loss: 1.2613228559494019, testing loss: 1.7629650831222534\n",
      "Accuracy: 0.4108\n",
      "epoch: 220, training loss: 1.1900867223739624, testing loss: 1.7408338785171509\n",
      "Accuracy: 0.3948\n",
      "epoch: 220, training loss: 1.2931830883026123, testing loss: 1.789203405380249\n",
      "Accuracy: 0.4196\n",
      "epoch: 220, training loss: 1.2360467910766602, testing loss: 1.6956583261489868\n",
      "Accuracy: 0.4076\n",
      "epoch: 220, training loss: 1.229029893875122, testing loss: 1.6908423900604248\n",
      "Accuracy: 0.4172\n",
      "epoch: 220, training loss: 1.2543474435806274, testing loss: 1.5657029151916504\n",
      "Accuracy: 0.4658\n",
      "epoch: 220, training loss: 1.285320520401001, testing loss: 1.730973482131958\n",
      "Accuracy: 0.4448\n",
      "epoch: 220, training loss: 1.2921613454818726, testing loss: 1.6693966388702393\n",
      "Accuracy: 0.4006\n",
      "epoch: 220, training loss: 1.4008440971374512, testing loss: 1.6948164701461792\n",
      "Accuracy: 0.4352\n",
      "epoch: 220, training loss: 1.241093635559082, testing loss: 1.5439931154251099\n",
      "Accuracy: 0.4419\n",
      "epoch: 220, training loss: 1.3209649324417114, testing loss: 1.6471108198165894\n",
      "Accuracy: 0.4158\n",
      "epoch: 220, training loss: 1.2394323348999023, testing loss: 1.5904134511947632\n",
      "Accuracy: 0.4674\n",
      "epoch: 220, training loss: 1.2952384948730469, testing loss: 1.6858726739883423\n",
      "Accuracy: 0.4266\n",
      "epoch: 220, training loss: 1.3171464204788208, testing loss: 1.7286487817764282\n",
      "Accuracy: 0.3746\n",
      "epoch: 220, training loss: 1.3223985433578491, testing loss: 1.6788482666015625\n",
      "Accuracy: 0.4281\n",
      "epoch: 220, training loss: 1.215342402458191, testing loss: 1.6060569286346436\n",
      "Accuracy: 0.4506\n",
      "epoch: 220, training loss: 1.2571306228637695, testing loss: 1.60489821434021\n",
      "Accuracy: 0.4111\n",
      "epoch: 220, training loss: 1.2588977813720703, testing loss: 1.6455317735671997\n",
      "Accuracy: 0.4262\n",
      "epoch: 220, training loss: 1.200514554977417, testing loss: 1.7499085664749146\n",
      "Accuracy: 0.3981\n",
      "epoch: 220, training loss: 1.2633898258209229, testing loss: 1.6555942296981812\n",
      "Accuracy: 0.4377\n",
      "epoch: 220, training loss: 1.2979557514190674, testing loss: 1.757960557937622\n",
      "Accuracy: 0.4058\n",
      "epoch: 220, training loss: 1.2915297746658325, testing loss: 1.6309149265289307\n",
      "Accuracy: 0.4161\n",
      "epoch: 220, training loss: 1.277593731880188, testing loss: 1.6184921264648438\n",
      "Accuracy: 0.4125\n",
      "epoch: 220, training loss: 1.2458505630493164, testing loss: 1.6611759662628174\n",
      "Accuracy: 0.4227\n",
      "epoch: 220, training loss: 1.2843389511108398, testing loss: 1.6715266704559326\n",
      "Accuracy: 0.4375\n",
      "epoch: 220, training loss: 1.252751350402832, testing loss: 1.6610050201416016\n",
      "Accuracy: 0.4228\n",
      "epoch: 220, training loss: 1.3287183046340942, testing loss: 1.698648452758789\n",
      "Accuracy: 0.3891\n",
      "epoch: 220, training loss: 1.3012380599975586, testing loss: 1.5362491607666016\n",
      "Accuracy: 0.4664\n",
      "epoch: 220, training loss: 1.336458683013916, testing loss: 1.5950888395309448\n",
      "Accuracy: 0.4006\n",
      "epoch: 220, training loss: 1.2872729301452637, testing loss: 1.649800419807434\n",
      "Accuracy: 0.4318\n",
      "epoch: 230, training loss: 1.2952065467834473, testing loss: 1.6335625648498535\n",
      "Accuracy: 0.4785\n",
      "epoch: 230, training loss: 1.3173034191131592, testing loss: 1.7672398090362549\n",
      "Accuracy: 0.3920\n",
      "epoch: 230, training loss: 1.2862801551818848, testing loss: 1.6601715087890625\n",
      "Accuracy: 0.4653\n",
      "epoch: 230, training loss: 1.3178123235702515, testing loss: 1.671557068824768\n",
      "Accuracy: 0.4038\n",
      "epoch: 230, training loss: 1.2596296072006226, testing loss: 1.7051362991333008\n",
      "Accuracy: 0.3880\n",
      "epoch: 230, training loss: 1.259230613708496, testing loss: 1.4860692024230957\n",
      "Accuracy: 0.4423\n",
      "epoch: 230, training loss: 1.2772105932235718, testing loss: 1.6137158870697021\n",
      "Accuracy: 0.4156\n",
      "epoch: 230, training loss: 1.2940665483474731, testing loss: 1.7341399192810059\n",
      "Accuracy: 0.4028\n",
      "epoch: 230, training loss: 1.2869371175765991, testing loss: 1.7274062633514404\n",
      "Accuracy: 0.4250\n",
      "epoch: 230, training loss: 1.2748116254806519, testing loss: 1.7063798904418945\n",
      "Accuracy: 0.3469\n",
      "epoch: 230, training loss: 1.1908515691757202, testing loss: 1.7287468910217285\n",
      "Accuracy: 0.4105\n",
      "epoch: 230, training loss: 1.3269301652908325, testing loss: 1.710900068283081\n",
      "Accuracy: 0.3935\n",
      "epoch: 230, training loss: 1.3293416500091553, testing loss: 1.7131444215774536\n",
      "Accuracy: 0.3818\n",
      "epoch: 230, training loss: 1.326535940170288, testing loss: 1.7922232151031494\n",
      "Accuracy: 0.3576\n",
      "epoch: 230, training loss: 1.2369672060012817, testing loss: 1.7542511224746704\n",
      "Accuracy: 0.3932\n",
      "epoch: 230, training loss: 1.3206002712249756, testing loss: 1.673134446144104\n",
      "Accuracy: 0.4080\n",
      "epoch: 230, training loss: 1.269034743309021, testing loss: 1.7197303771972656\n",
      "Accuracy: 0.4263\n",
      "epoch: 230, training loss: 1.2805753946304321, testing loss: 1.5514469146728516\n",
      "Accuracy: 0.4814\n",
      "epoch: 230, training loss: 1.2830408811569214, testing loss: 1.758322834968567\n",
      "Accuracy: 0.4047\n",
      "epoch: 230, training loss: 1.2767601013183594, testing loss: 1.7010364532470703\n",
      "Accuracy: 0.4545\n",
      "epoch: 230, training loss: 1.285858392715454, testing loss: 1.6441456079483032\n",
      "Accuracy: 0.4235\n",
      "epoch: 230, training loss: 1.290926456451416, testing loss: 1.6364445686340332\n",
      "Accuracy: 0.4540\n",
      "epoch: 230, training loss: 1.3462812900543213, testing loss: 1.7517812252044678\n",
      "Accuracy: 0.3945\n",
      "epoch: 230, training loss: 1.2329785823822021, testing loss: 1.6516084671020508\n",
      "Accuracy: 0.4218\n",
      "epoch: 230, training loss: 1.2273857593536377, testing loss: 1.5519531965255737\n",
      "Accuracy: 0.4477\n",
      "epoch: 230, training loss: 1.2632472515106201, testing loss: 1.729382038116455\n",
      "Accuracy: 0.4228\n",
      "epoch: 230, training loss: 1.3558807373046875, testing loss: 1.635265588760376\n",
      "Accuracy: 0.3901\n",
      "epoch: 230, training loss: 1.346081018447876, testing loss: 1.568833589553833\n",
      "Accuracy: 0.3986\n",
      "epoch: 230, training loss: 1.3252687454223633, testing loss: 1.6576645374298096\n",
      "Accuracy: 0.4475\n",
      "epoch: 230, training loss: 1.2404199838638306, testing loss: 1.8521560430526733\n",
      "Accuracy: 0.3802\n",
      "epoch: 230, training loss: 1.2729817628860474, testing loss: 1.773408055305481\n",
      "Accuracy: 0.4040\n",
      "epoch: 230, training loss: 1.2749301195144653, testing loss: 1.6271929740905762\n",
      "Accuracy: 0.3994\n",
      "epoch: 230, training loss: 1.3401650190353394, testing loss: 1.5733766555786133\n",
      "Accuracy: 0.4078\n",
      "epoch: 230, training loss: 1.272831678390503, testing loss: 1.7348581552505493\n",
      "Accuracy: 0.3793\n",
      "epoch: 230, training loss: 1.2717729806900024, testing loss: 1.666183352470398\n",
      "Accuracy: 0.4414\n",
      "epoch: 230, training loss: 1.265854001045227, testing loss: 1.6116926670074463\n",
      "Accuracy: 0.4448\n",
      "epoch: 230, training loss: 1.2626579999923706, testing loss: 1.65887451171875\n",
      "Accuracy: 0.4464\n",
      "epoch: 230, training loss: 1.3026121854782104, testing loss: 1.5506218671798706\n",
      "Accuracy: 0.4526\n",
      "epoch: 230, training loss: 1.2975926399230957, testing loss: 1.6708675622940063\n",
      "Accuracy: 0.3981\n",
      "epoch: 230, training loss: 1.3078829050064087, testing loss: 1.6616860628128052\n",
      "Accuracy: 0.4148\n",
      "epoch: 230, training loss: 1.230305790901184, testing loss: 1.5634347200393677\n",
      "Accuracy: 0.4059\n",
      "epoch: 230, training loss: 1.3067600727081299, testing loss: 1.7568652629852295\n",
      "Accuracy: 0.3678\n",
      "epoch: 230, training loss: 1.304299235343933, testing loss: 1.6858928203582764\n",
      "Accuracy: 0.4437\n",
      "epoch: 230, training loss: 1.2303507328033447, testing loss: 1.6667046546936035\n",
      "Accuracy: 0.4683\n",
      "epoch: 230, training loss: 1.3179278373718262, testing loss: 1.685909390449524\n",
      "Accuracy: 0.3686\n",
      "epoch: 230, training loss: 1.353739857673645, testing loss: 1.6178596019744873\n",
      "Accuracy: 0.4191\n",
      "epoch: 230, training loss: 1.2969814538955688, testing loss: 1.8444514274597168\n",
      "Accuracy: 0.4121\n",
      "epoch: 230, training loss: 1.2999110221862793, testing loss: 1.789159893989563\n",
      "Accuracy: 0.3796\n",
      "epoch: 230, training loss: 1.273163080215454, testing loss: 1.856056809425354\n",
      "Accuracy: 0.3873\n",
      "epoch: 230, training loss: 1.2615596055984497, testing loss: 1.6570370197296143\n",
      "Accuracy: 0.4418\n",
      "epoch: 230, training loss: 1.2471669912338257, testing loss: 1.5296382904052734\n",
      "Accuracy: 0.4437\n",
      "epoch: 230, training loss: 1.3121100664138794, testing loss: 1.58432936668396\n",
      "Accuracy: 0.4145\n",
      "epoch: 230, training loss: 1.2865813970565796, testing loss: 1.6631792783737183\n",
      "Accuracy: 0.4188\n",
      "epoch: 230, training loss: 1.2394193410873413, testing loss: 1.7847181558609009\n",
      "Accuracy: 0.3885\n",
      "epoch: 230, training loss: 1.2691386938095093, testing loss: 1.686050295829773\n",
      "Accuracy: 0.4090\n",
      "epoch: 230, training loss: 1.2681618928909302, testing loss: 1.4442260265350342\n",
      "Accuracy: 0.4851\n",
      "epoch: 230, training loss: 1.270251989364624, testing loss: 1.6860644817352295\n",
      "Accuracy: 0.3945\n",
      "epoch: 230, training loss: 1.3404672145843506, testing loss: 1.667104959487915\n",
      "Accuracy: 0.4266\n",
      "epoch: 230, training loss: 1.28165602684021, testing loss: 1.6128307580947876\n",
      "Accuracy: 0.4156\n",
      "epoch: 230, training loss: 1.2677572965621948, testing loss: 1.681754469871521\n",
      "Accuracy: 0.4150\n",
      "epoch: 230, training loss: 1.2447394132614136, testing loss: 1.6598082780838013\n",
      "Accuracy: 0.3951\n",
      "epoch: 230, training loss: 1.273700475692749, testing loss: 1.649717092514038\n",
      "Accuracy: 0.4346\n",
      "epoch: 230, training loss: 1.3016903400421143, testing loss: 1.7370498180389404\n",
      "Accuracy: 0.4373\n",
      "epoch: 230, training loss: 1.2584216594696045, testing loss: 1.603684902191162\n",
      "Accuracy: 0.4319\n",
      "epoch: 230, training loss: 1.346528172492981, testing loss: 1.615461826324463\n",
      "Accuracy: 0.3939\n",
      "epoch: 230, training loss: 1.3275132179260254, testing loss: 1.5999164581298828\n",
      "Accuracy: 0.4207\n",
      "epoch: 230, training loss: 1.3244404792785645, testing loss: 1.6983497142791748\n",
      "Accuracy: 0.4636\n",
      "epoch: 230, training loss: 1.2761826515197754, testing loss: 1.6960721015930176\n",
      "Accuracy: 0.3870\n",
      "epoch: 230, training loss: 1.239089846611023, testing loss: 1.7062747478485107\n",
      "Accuracy: 0.3796\n",
      "epoch: 230, training loss: 1.3110706806182861, testing loss: 1.6392297744750977\n",
      "Accuracy: 0.4047\n",
      "epoch: 230, training loss: 1.2767025232315063, testing loss: 1.6479969024658203\n",
      "Accuracy: 0.3829\n",
      "epoch: 230, training loss: 1.312512993812561, testing loss: 1.7153784036636353\n",
      "Accuracy: 0.4101\n",
      "epoch: 230, training loss: 1.2635811567306519, testing loss: 1.5549238920211792\n",
      "Accuracy: 0.3931\n",
      "epoch: 230, training loss: 1.3311494588851929, testing loss: 1.5395866632461548\n",
      "Accuracy: 0.4890\n",
      "epoch: 230, training loss: 1.3014464378356934, testing loss: 1.6589288711547852\n",
      "Accuracy: 0.4296\n",
      "epoch: 230, training loss: 1.259822964668274, testing loss: 1.6203153133392334\n",
      "Accuracy: 0.4106\n",
      "epoch: 230, training loss: 1.2584917545318604, testing loss: 1.6835734844207764\n",
      "Accuracy: 0.4221\n",
      "epoch: 230, training loss: 1.2129640579223633, testing loss: 1.7147260904312134\n",
      "Accuracy: 0.4272\n",
      "epoch: 230, training loss: 1.3253569602966309, testing loss: 1.7073736190795898\n",
      "Accuracy: 0.3993\n",
      "epoch: 230, training loss: 1.2448570728302002, testing loss: 1.7234736680984497\n",
      "Accuracy: 0.3739\n",
      "epoch: 230, training loss: 1.186155080795288, testing loss: 1.604619026184082\n",
      "Accuracy: 0.4119\n",
      "epoch: 230, training loss: 1.3005322217941284, testing loss: 1.6279022693634033\n",
      "Accuracy: 0.4150\n",
      "epoch: 230, training loss: 1.2769596576690674, testing loss: 1.7122918367385864\n",
      "Accuracy: 0.4035\n",
      "epoch: 230, training loss: 1.2468308210372925, testing loss: 1.6968234777450562\n",
      "Accuracy: 0.4444\n",
      "epoch: 230, training loss: 1.300750732421875, testing loss: 1.8301692008972168\n",
      "Accuracy: 0.3841\n",
      "epoch: 230, training loss: 1.2599040269851685, testing loss: 1.7502450942993164\n",
      "Accuracy: 0.4320\n",
      "epoch: 230, training loss: 1.2739843130111694, testing loss: 1.7662415504455566\n",
      "Accuracy: 0.4013\n",
      "epoch: 230, training loss: 1.2952710390090942, testing loss: 1.592902660369873\n",
      "Accuracy: 0.4479\n",
      "epoch: 230, training loss: 1.3449747562408447, testing loss: 1.7431437969207764\n",
      "Accuracy: 0.3693\n",
      "epoch: 230, training loss: 1.2806788682937622, testing loss: 1.8064005374908447\n",
      "Accuracy: 0.3464\n",
      "epoch: 230, training loss: 1.3133193254470825, testing loss: 1.7416250705718994\n",
      "Accuracy: 0.4104\n",
      "epoch: 230, training loss: 1.3142632246017456, testing loss: 1.5924596786499023\n",
      "Accuracy: 0.4239\n",
      "epoch: 230, training loss: 1.269943356513977, testing loss: 1.6146631240844727\n",
      "Accuracy: 0.4531\n",
      "epoch: 230, training loss: 1.2740325927734375, testing loss: 1.670227289199829\n",
      "Accuracy: 0.3924\n",
      "epoch: 230, training loss: 1.2777574062347412, testing loss: 1.5984352827072144\n",
      "Accuracy: 0.4562\n",
      "epoch: 230, training loss: 1.3610517978668213, testing loss: 1.625818133354187\n",
      "Accuracy: 0.4424\n",
      "epoch: 230, training loss: 1.2239291667938232, testing loss: 1.6057060956954956\n",
      "Accuracy: 0.4277\n",
      "epoch: 230, training loss: 1.3349168300628662, testing loss: 1.7511528730392456\n",
      "Accuracy: 0.3904\n",
      "epoch: 230, training loss: 1.3217817544937134, testing loss: 1.6525249481201172\n",
      "Accuracy: 0.4597\n",
      "epoch: 230, training loss: 1.2121896743774414, testing loss: 1.6283185482025146\n",
      "Accuracy: 0.4156\n",
      "epoch: 240, training loss: 1.2534613609313965, testing loss: 1.6370409727096558\n",
      "Accuracy: 0.3934\n",
      "epoch: 240, training loss: 1.2557733058929443, testing loss: 1.5790736675262451\n",
      "Accuracy: 0.4486\n",
      "epoch: 240, training loss: 1.2334116697311401, testing loss: 1.6747767925262451\n",
      "Accuracy: 0.4073\n",
      "epoch: 240, training loss: 1.2485142946243286, testing loss: 1.7259979248046875\n",
      "Accuracy: 0.4084\n",
      "epoch: 240, training loss: 1.2448909282684326, testing loss: 1.718994379043579\n",
      "Accuracy: 0.4019\n",
      "epoch: 240, training loss: 1.2523964643478394, testing loss: 1.7031683921813965\n",
      "Accuracy: 0.3983\n",
      "epoch: 240, training loss: 1.2568979263305664, testing loss: 1.6965045928955078\n",
      "Accuracy: 0.4239\n",
      "epoch: 240, training loss: 1.214775562286377, testing loss: 1.6768770217895508\n",
      "Accuracy: 0.4079\n",
      "epoch: 240, training loss: 1.2244939804077148, testing loss: 1.5475133657455444\n",
      "Accuracy: 0.4885\n",
      "epoch: 240, training loss: 1.2814077138900757, testing loss: 1.6987100839614868\n",
      "Accuracy: 0.4272\n",
      "epoch: 240, training loss: 1.2744011878967285, testing loss: 1.7108347415924072\n",
      "Accuracy: 0.4072\n",
      "epoch: 240, training loss: 1.2325537204742432, testing loss: 1.8129544258117676\n",
      "Accuracy: 0.4268\n",
      "epoch: 240, training loss: 1.2584501504898071, testing loss: 1.671971321105957\n",
      "Accuracy: 0.4257\n",
      "epoch: 240, training loss: 1.2933449745178223, testing loss: 1.6407990455627441\n",
      "Accuracy: 0.4696\n",
      "epoch: 240, training loss: 1.306895136833191, testing loss: 1.6019121408462524\n",
      "Accuracy: 0.4434\n",
      "epoch: 240, training loss: 1.3464679718017578, testing loss: 1.6708123683929443\n",
      "Accuracy: 0.3876\n",
      "epoch: 240, training loss: 1.206929087638855, testing loss: 1.6219629049301147\n",
      "Accuracy: 0.4207\n",
      "epoch: 240, training loss: 1.2045269012451172, testing loss: 1.6625643968582153\n",
      "Accuracy: 0.4177\n",
      "epoch: 240, training loss: 1.303268551826477, testing loss: 1.7107441425323486\n",
      "Accuracy: 0.4147\n",
      "epoch: 240, training loss: 1.3279638290405273, testing loss: 1.6856664419174194\n",
      "Accuracy: 0.3774\n",
      "epoch: 240, training loss: 1.323659896850586, testing loss: 1.6942329406738281\n",
      "Accuracy: 0.4187\n",
      "epoch: 240, training loss: 1.2567799091339111, testing loss: 1.8385623693466187\n",
      "Accuracy: 0.4348\n",
      "epoch: 240, training loss: 1.2596492767333984, testing loss: 1.702881097793579\n",
      "Accuracy: 0.4209\n",
      "epoch: 240, training loss: 1.2231194972991943, testing loss: 1.631072998046875\n",
      "Accuracy: 0.4605\n",
      "epoch: 240, training loss: 1.2938015460968018, testing loss: 1.7204153537750244\n",
      "Accuracy: 0.3973\n",
      "epoch: 240, training loss: 1.2755261659622192, testing loss: 1.7613688707351685\n",
      "Accuracy: 0.4146\n",
      "epoch: 240, training loss: 1.3960685729980469, testing loss: 1.6511800289154053\n",
      "Accuracy: 0.4190\n",
      "epoch: 240, training loss: 1.2551360130310059, testing loss: 1.6950032711029053\n",
      "Accuracy: 0.3849\n",
      "epoch: 240, training loss: 1.291195273399353, testing loss: 1.6087934970855713\n",
      "Accuracy: 0.4469\n",
      "epoch: 240, training loss: 1.3288429975509644, testing loss: 1.7799584865570068\n",
      "Accuracy: 0.3830\n",
      "epoch: 240, training loss: 1.3236552476882935, testing loss: 1.6557203531265259\n",
      "Accuracy: 0.4571\n",
      "epoch: 240, training loss: 1.30986750125885, testing loss: 1.5942330360412598\n",
      "Accuracy: 0.4259\n",
      "epoch: 240, training loss: 1.2109428644180298, testing loss: 1.6900990009307861\n",
      "Accuracy: 0.4184\n",
      "epoch: 240, training loss: 1.1831501722335815, testing loss: 1.7739609479904175\n",
      "Accuracy: 0.4393\n",
      "epoch: 240, training loss: 1.2588008642196655, testing loss: 1.5517903566360474\n",
      "Accuracy: 0.4708\n",
      "epoch: 240, training loss: 1.3073375225067139, testing loss: 1.6231328248977661\n",
      "Accuracy: 0.4080\n",
      "epoch: 240, training loss: 1.2434511184692383, testing loss: 1.6071507930755615\n",
      "Accuracy: 0.4248\n",
      "epoch: 240, training loss: 1.3219280242919922, testing loss: 1.6388332843780518\n",
      "Accuracy: 0.4552\n",
      "epoch: 240, training loss: 1.2558658123016357, testing loss: 1.6108932495117188\n",
      "Accuracy: 0.4103\n",
      "epoch: 240, training loss: 1.2826118469238281, testing loss: 1.6187522411346436\n",
      "Accuracy: 0.4733\n",
      "epoch: 240, training loss: 1.2766882181167603, testing loss: 1.5862531661987305\n",
      "Accuracy: 0.4183\n",
      "epoch: 240, training loss: 1.3220211267471313, testing loss: 1.728627324104309\n",
      "Accuracy: 0.3958\n",
      "epoch: 240, training loss: 1.2899047136306763, testing loss: 1.6794216632843018\n",
      "Accuracy: 0.4897\n",
      "epoch: 240, training loss: 1.2767128944396973, testing loss: 1.767461895942688\n",
      "Accuracy: 0.3821\n",
      "epoch: 240, training loss: 1.2101821899414062, testing loss: 1.7628859281539917\n",
      "Accuracy: 0.3933\n",
      "epoch: 240, training loss: 1.2818241119384766, testing loss: 1.5825108289718628\n",
      "Accuracy: 0.4196\n",
      "epoch: 240, training loss: 1.3058332204818726, testing loss: 1.6887941360473633\n",
      "Accuracy: 0.4207\n",
      "epoch: 240, training loss: 1.3381046056747437, testing loss: 1.7520253658294678\n",
      "Accuracy: 0.3980\n",
      "epoch: 240, training loss: 1.232917070388794, testing loss: 1.5783659219741821\n",
      "Accuracy: 0.4221\n",
      "epoch: 240, training loss: 1.2878206968307495, testing loss: 1.6848458051681519\n",
      "Accuracy: 0.4422\n",
      "epoch: 240, training loss: 1.295163869857788, testing loss: 1.6393576860427856\n",
      "Accuracy: 0.4469\n",
      "epoch: 240, training loss: 1.2134206295013428, testing loss: 1.698056936264038\n",
      "Accuracy: 0.4291\n",
      "epoch: 240, training loss: 1.2860954999923706, testing loss: 1.6466970443725586\n",
      "Accuracy: 0.3816\n",
      "epoch: 240, training loss: 1.3265116214752197, testing loss: 1.6871532201766968\n",
      "Accuracy: 0.4244\n",
      "epoch: 240, training loss: 1.2640125751495361, testing loss: 1.6304082870483398\n",
      "Accuracy: 0.4375\n",
      "epoch: 240, training loss: 1.3350132703781128, testing loss: 1.5568516254425049\n",
      "Accuracy: 0.4295\n",
      "epoch: 240, training loss: 1.3404244184494019, testing loss: 1.772239089012146\n",
      "Accuracy: 0.3785\n",
      "epoch: 240, training loss: 1.262697458267212, testing loss: 1.670524001121521\n",
      "Accuracy: 0.4146\n",
      "epoch: 240, training loss: 1.2909857034683228, testing loss: 1.669887661933899\n",
      "Accuracy: 0.3910\n",
      "epoch: 240, training loss: 1.2914540767669678, testing loss: 1.781612515449524\n",
      "Accuracy: 0.3819\n",
      "epoch: 240, training loss: 1.31208074092865, testing loss: 1.7057667970657349\n",
      "Accuracy: 0.4053\n",
      "epoch: 240, training loss: 1.257093071937561, testing loss: 1.6409136056900024\n",
      "Accuracy: 0.4312\n",
      "epoch: 240, training loss: 1.2811412811279297, testing loss: 1.747064471244812\n",
      "Accuracy: 0.4007\n",
      "epoch: 240, training loss: 1.2733672857284546, testing loss: 1.6826329231262207\n",
      "Accuracy: 0.4121\n",
      "epoch: 240, training loss: 1.2800211906433105, testing loss: 1.5735230445861816\n",
      "Accuracy: 0.4531\n",
      "epoch: 240, training loss: 1.3086775541305542, testing loss: 1.6135071516036987\n",
      "Accuracy: 0.3981\n",
      "epoch: 240, training loss: 1.244543194770813, testing loss: 1.746302604675293\n",
      "Accuracy: 0.4198\n",
      "epoch: 240, training loss: 1.2936230897903442, testing loss: 1.8035680055618286\n",
      "Accuracy: 0.3870\n",
      "epoch: 240, training loss: 1.279146432876587, testing loss: 1.6777478456497192\n",
      "Accuracy: 0.4277\n",
      "epoch: 240, training loss: 1.3195425271987915, testing loss: 1.714267373085022\n",
      "Accuracy: 0.4204\n",
      "epoch: 240, training loss: 1.2925838232040405, testing loss: 1.6346503496170044\n",
      "Accuracy: 0.3913\n",
      "epoch: 240, training loss: 1.2699888944625854, testing loss: 1.666858196258545\n",
      "Accuracy: 0.4341\n",
      "epoch: 240, training loss: 1.2872792482376099, testing loss: 1.7401319742202759\n",
      "Accuracy: 0.4300\n",
      "epoch: 240, training loss: 1.306606650352478, testing loss: 1.6729512214660645\n",
      "Accuracy: 0.3660\n",
      "epoch: 240, training loss: 1.267519474029541, testing loss: 1.7385450601577759\n",
      "Accuracy: 0.4610\n",
      "epoch: 240, training loss: 1.29280424118042, testing loss: 1.6372661590576172\n",
      "Accuracy: 0.4082\n",
      "epoch: 240, training loss: 1.2699406147003174, testing loss: 1.7925506830215454\n",
      "Accuracy: 0.4121\n",
      "epoch: 240, training loss: 1.2736256122589111, testing loss: 1.6409860849380493\n",
      "Accuracy: 0.4786\n",
      "epoch: 240, training loss: 1.3169922828674316, testing loss: 1.7330424785614014\n",
      "Accuracy: 0.4114\n",
      "epoch: 240, training loss: 1.2986865043640137, testing loss: 1.684744954109192\n",
      "Accuracy: 0.4458\n",
      "epoch: 240, training loss: 1.225480079650879, testing loss: 1.6240729093551636\n",
      "Accuracy: 0.4197\n",
      "epoch: 240, training loss: 1.2735648155212402, testing loss: 1.6974576711654663\n",
      "Accuracy: 0.4691\n",
      "epoch: 240, training loss: 1.239533543586731, testing loss: 1.89818274974823\n",
      "Accuracy: 0.3839\n",
      "epoch: 240, training loss: 1.326533555984497, testing loss: 1.5789687633514404\n",
      "Accuracy: 0.4477\n",
      "epoch: 240, training loss: 1.2202658653259277, testing loss: 1.6676921844482422\n",
      "Accuracy: 0.4500\n",
      "epoch: 240, training loss: 1.2762346267700195, testing loss: 1.6986829042434692\n",
      "Accuracy: 0.4000\n",
      "epoch: 240, training loss: 1.257732629776001, testing loss: 1.7320667505264282\n",
      "Accuracy: 0.4058\n",
      "epoch: 240, training loss: 1.2679821252822876, testing loss: 1.510790467262268\n",
      "Accuracy: 0.4521\n",
      "epoch: 240, training loss: 1.2309935092926025, testing loss: 1.625494360923767\n",
      "Accuracy: 0.4646\n",
      "epoch: 240, training loss: 1.3093152046203613, testing loss: 1.7501988410949707\n",
      "Accuracy: 0.3817\n",
      "epoch: 240, training loss: 1.2781932353973389, testing loss: 1.6230422258377075\n",
      "Accuracy: 0.3906\n",
      "epoch: 240, training loss: 1.3073759078979492, testing loss: 1.5274505615234375\n",
      "Accuracy: 0.4475\n",
      "epoch: 240, training loss: 1.2750037908554077, testing loss: 1.6108306646347046\n",
      "Accuracy: 0.4081\n",
      "epoch: 240, training loss: 1.2801886796951294, testing loss: 1.6855273246765137\n",
      "Accuracy: 0.4194\n",
      "epoch: 240, training loss: 1.2473421096801758, testing loss: 1.643465518951416\n",
      "Accuracy: 0.4671\n",
      "epoch: 240, training loss: 1.296227216720581, testing loss: 1.7965681552886963\n",
      "Accuracy: 0.4149\n",
      "epoch: 240, training loss: 1.2621980905532837, testing loss: 1.6057360172271729\n",
      "Accuracy: 0.4257\n",
      "epoch: 240, training loss: 1.3148728609085083, testing loss: 1.6327818632125854\n",
      "Accuracy: 0.4471\n",
      "epoch: 240, training loss: 1.244126319885254, testing loss: 1.6974073648452759\n",
      "Accuracy: 0.4267\n",
      "epoch: 240, training loss: 1.3251347541809082, testing loss: 1.6659868955612183\n",
      "Accuracy: 0.4669\n",
      "epoch: 250, training loss: 1.2823381423950195, testing loss: 1.7990232706069946\n",
      "Accuracy: 0.3851\n",
      "epoch: 250, training loss: 1.2872332334518433, testing loss: 1.5486232042312622\n",
      "Accuracy: 0.4618\n",
      "epoch: 250, training loss: 1.2682164907455444, testing loss: 1.6563166379928589\n",
      "Accuracy: 0.4105\n",
      "epoch: 250, training loss: 1.3149023056030273, testing loss: 1.667025089263916\n",
      "Accuracy: 0.3783\n",
      "epoch: 250, training loss: 1.2414127588272095, testing loss: 1.625481128692627\n",
      "Accuracy: 0.4232\n",
      "epoch: 250, training loss: 1.3149200677871704, testing loss: 1.706409215927124\n",
      "Accuracy: 0.3875\n",
      "epoch: 250, training loss: 1.2652610540390015, testing loss: 1.7436531782150269\n",
      "Accuracy: 0.3831\n",
      "epoch: 250, training loss: 1.2613145112991333, testing loss: 1.6100883483886719\n",
      "Accuracy: 0.4601\n",
      "epoch: 250, training loss: 1.2953072786331177, testing loss: 1.755874752998352\n",
      "Accuracy: 0.4209\n",
      "epoch: 250, training loss: 1.2314831018447876, testing loss: 1.7628957033157349\n",
      "Accuracy: 0.3931\n",
      "epoch: 250, training loss: 1.2780163288116455, testing loss: 1.8228527307510376\n",
      "Accuracy: 0.3672\n",
      "epoch: 250, training loss: 1.2367678880691528, testing loss: 1.7686498165130615\n",
      "Accuracy: 0.3645\n",
      "epoch: 250, training loss: 1.23663330078125, testing loss: 1.621457815170288\n",
      "Accuracy: 0.4618\n",
      "epoch: 250, training loss: 1.2681756019592285, testing loss: 1.7003551721572876\n",
      "Accuracy: 0.4218\n",
      "epoch: 250, training loss: 1.2590447664260864, testing loss: 1.7729005813598633\n",
      "Accuracy: 0.3970\n",
      "epoch: 250, training loss: 1.2996299266815186, testing loss: 1.5729739665985107\n",
      "Accuracy: 0.4611\n",
      "epoch: 250, training loss: 1.2473374605178833, testing loss: 1.6224687099456787\n",
      "Accuracy: 0.4286\n",
      "epoch: 250, training loss: 1.315451979637146, testing loss: 1.7955466508865356\n",
      "Accuracy: 0.4246\n",
      "epoch: 250, training loss: 1.321426272392273, testing loss: 1.6437337398529053\n",
      "Accuracy: 0.3718\n",
      "epoch: 250, training loss: 1.2800514698028564, testing loss: 1.6331849098205566\n",
      "Accuracy: 0.4334\n",
      "epoch: 250, training loss: 1.233902096748352, testing loss: 1.8649321794509888\n",
      "Accuracy: 0.3931\n",
      "epoch: 250, training loss: 1.338081955909729, testing loss: 1.6919951438903809\n",
      "Accuracy: 0.3973\n",
      "epoch: 250, training loss: 1.227745771408081, testing loss: 1.6638634204864502\n",
      "Accuracy: 0.4248\n",
      "epoch: 250, training loss: 1.299002766609192, testing loss: 1.7088234424591064\n",
      "Accuracy: 0.4363\n",
      "epoch: 250, training loss: 1.2631542682647705, testing loss: 1.6999591588974\n",
      "Accuracy: 0.4061\n",
      "epoch: 250, training loss: 1.2483288049697876, testing loss: 1.7800581455230713\n",
      "Accuracy: 0.3803\n",
      "epoch: 250, training loss: 1.286946415901184, testing loss: 1.6689305305480957\n",
      "Accuracy: 0.3993\n",
      "epoch: 250, training loss: 1.264021635055542, testing loss: 1.797864556312561\n",
      "Accuracy: 0.4055\n",
      "epoch: 250, training loss: 1.3621562719345093, testing loss: 1.6247122287750244\n",
      "Accuracy: 0.4209\n",
      "epoch: 250, training loss: 1.2786004543304443, testing loss: 1.6891385316848755\n",
      "Accuracy: 0.4409\n",
      "epoch: 250, training loss: 1.298778772354126, testing loss: 1.7471147775650024\n",
      "Accuracy: 0.4006\n",
      "epoch: 250, training loss: 1.3080017566680908, testing loss: 1.7756320238113403\n",
      "Accuracy: 0.4369\n",
      "epoch: 250, training loss: 1.2676301002502441, testing loss: 1.568582534790039\n",
      "Accuracy: 0.4497\n",
      "epoch: 250, training loss: 1.304413914680481, testing loss: 1.726946473121643\n",
      "Accuracy: 0.4601\n",
      "epoch: 250, training loss: 1.3353456258773804, testing loss: 1.6666505336761475\n",
      "Accuracy: 0.3723\n",
      "epoch: 250, training loss: 1.2526681423187256, testing loss: 1.6364105939865112\n",
      "Accuracy: 0.4770\n",
      "epoch: 250, training loss: 1.3198260068893433, testing loss: 1.7761515378952026\n",
      "Accuracy: 0.3794\n",
      "epoch: 250, training loss: 1.3191909790039062, testing loss: 1.762745976448059\n",
      "Accuracy: 0.4085\n",
      "epoch: 250, training loss: 1.271779179573059, testing loss: 1.6760746240615845\n",
      "Accuracy: 0.4461\n",
      "epoch: 250, training loss: 1.2692067623138428, testing loss: 1.6950483322143555\n",
      "Accuracy: 0.4006\n",
      "epoch: 250, training loss: 1.270288348197937, testing loss: 1.7085270881652832\n",
      "Accuracy: 0.4488\n",
      "epoch: 250, training loss: 1.2782996892929077, testing loss: 1.7021294832229614\n",
      "Accuracy: 0.4122\n",
      "epoch: 250, training loss: 1.2903046607971191, testing loss: 1.6290441751480103\n",
      "Accuracy: 0.4242\n",
      "epoch: 250, training loss: 1.2681682109832764, testing loss: 1.6164501905441284\n",
      "Accuracy: 0.3938\n",
      "epoch: 250, training loss: 1.3210984468460083, testing loss: 1.664216160774231\n",
      "Accuracy: 0.4152\n",
      "epoch: 250, training loss: 1.31021249294281, testing loss: 1.709761619567871\n",
      "Accuracy: 0.3722\n",
      "epoch: 250, training loss: 1.2462170124053955, testing loss: 1.6658461093902588\n",
      "Accuracy: 0.4241\n",
      "epoch: 250, training loss: 1.2576050758361816, testing loss: 1.6921768188476562\n",
      "Accuracy: 0.4158\n",
      "epoch: 250, training loss: 1.284578800201416, testing loss: 1.611017107963562\n",
      "Accuracy: 0.4494\n",
      "epoch: 250, training loss: 1.2006230354309082, testing loss: 1.6700749397277832\n",
      "Accuracy: 0.4057\n",
      "epoch: 250, training loss: 1.3559595346450806, testing loss: 1.7282363176345825\n",
      "Accuracy: 0.4071\n",
      "epoch: 250, training loss: 1.2811484336853027, testing loss: 1.6729936599731445\n",
      "Accuracy: 0.4352\n",
      "epoch: 250, training loss: 1.2544916868209839, testing loss: 1.6642329692840576\n",
      "Accuracy: 0.4272\n",
      "epoch: 250, training loss: 1.3158047199249268, testing loss: 1.619071125984192\n",
      "Accuracy: 0.4485\n",
      "epoch: 250, training loss: 1.2693568468093872, testing loss: 1.5359944105148315\n",
      "Accuracy: 0.4823\n",
      "epoch: 250, training loss: 1.2603870630264282, testing loss: 1.6281416416168213\n",
      "Accuracy: 0.4358\n",
      "epoch: 250, training loss: 1.2645972967147827, testing loss: 1.6832265853881836\n",
      "Accuracy: 0.3925\n",
      "epoch: 250, training loss: 1.326141357421875, testing loss: 1.612187385559082\n",
      "Accuracy: 0.4576\n",
      "epoch: 250, training loss: 1.2754014730453491, testing loss: 1.7110897302627563\n",
      "Accuracy: 0.4207\n",
      "epoch: 250, training loss: 1.2971384525299072, testing loss: 1.6367771625518799\n",
      "Accuracy: 0.4125\n",
      "epoch: 250, training loss: 1.2832672595977783, testing loss: 1.6082476377487183\n",
      "Accuracy: 0.4281\n",
      "epoch: 250, training loss: 1.290090799331665, testing loss: 1.6475642919540405\n",
      "Accuracy: 0.3693\n",
      "epoch: 250, training loss: 1.2323474884033203, testing loss: 1.660375952720642\n",
      "Accuracy: 0.4483\n",
      "epoch: 250, training loss: 1.2477312088012695, testing loss: 1.6406217813491821\n",
      "Accuracy: 0.3692\n",
      "epoch: 250, training loss: 1.3718875646591187, testing loss: 1.6397989988327026\n",
      "Accuracy: 0.4323\n",
      "epoch: 250, training loss: 1.2595653533935547, testing loss: 1.6821000576019287\n",
      "Accuracy: 0.3967\n",
      "epoch: 250, training loss: 1.3227412700653076, testing loss: 1.7559640407562256\n",
      "Accuracy: 0.3926\n",
      "epoch: 250, training loss: 1.2679811716079712, testing loss: 1.6467020511627197\n",
      "Accuracy: 0.4127\n",
      "epoch: 250, training loss: 1.248334527015686, testing loss: 1.6179444789886475\n",
      "Accuracy: 0.4344\n",
      "epoch: 250, training loss: 1.271749496459961, testing loss: 1.670858383178711\n",
      "Accuracy: 0.4120\n",
      "epoch: 250, training loss: 1.233254313468933, testing loss: 1.60490882396698\n",
      "Accuracy: 0.4276\n",
      "epoch: 250, training loss: 1.2914552688598633, testing loss: 1.7148422002792358\n",
      "Accuracy: 0.4031\n",
      "epoch: 250, training loss: 1.2756025791168213, testing loss: 1.6720209121704102\n",
      "Accuracy: 0.3942\n",
      "epoch: 250, training loss: 1.220859169960022, testing loss: 1.5877485275268555\n",
      "Accuracy: 0.4458\n",
      "epoch: 250, training loss: 1.2898526191711426, testing loss: 1.6506404876708984\n",
      "Accuracy: 0.4259\n",
      "epoch: 250, training loss: 1.2402760982513428, testing loss: 1.7477713823318481\n",
      "Accuracy: 0.3912\n",
      "epoch: 250, training loss: 1.2605078220367432, testing loss: 1.7552435398101807\n",
      "Accuracy: 0.3960\n",
      "epoch: 250, training loss: 1.297833800315857, testing loss: 1.918389916419983\n",
      "Accuracy: 0.3994\n",
      "epoch: 250, training loss: 1.3202829360961914, testing loss: 1.73343026638031\n",
      "Accuracy: 0.3946\n",
      "epoch: 250, training loss: 1.3258190155029297, testing loss: 1.8353747129440308\n",
      "Accuracy: 0.3773\n",
      "epoch: 250, training loss: 1.2942354679107666, testing loss: 1.774639368057251\n",
      "Accuracy: 0.3558\n",
      "epoch: 250, training loss: 1.2581676244735718, testing loss: 1.6970654726028442\n",
      "Accuracy: 0.4028\n",
      "epoch: 250, training loss: 1.283935785293579, testing loss: 1.6851640939712524\n",
      "Accuracy: 0.4108\n",
      "epoch: 250, training loss: 1.2442959547042847, testing loss: 1.709363341331482\n",
      "Accuracy: 0.4242\n",
      "epoch: 250, training loss: 1.2598756551742554, testing loss: 1.6769745349884033\n",
      "Accuracy: 0.3994\n",
      "epoch: 250, training loss: 1.2607290744781494, testing loss: 1.640384554862976\n",
      "Accuracy: 0.4393\n",
      "epoch: 250, training loss: 1.227238416671753, testing loss: 1.7346872091293335\n",
      "Accuracy: 0.4112\n",
      "epoch: 250, training loss: 1.344048261642456, testing loss: 1.700991153717041\n",
      "Accuracy: 0.4471\n",
      "epoch: 250, training loss: 1.2742477655410767, testing loss: 1.689916729927063\n",
      "Accuracy: 0.4628\n",
      "epoch: 250, training loss: 1.2974332571029663, testing loss: 1.659235954284668\n",
      "Accuracy: 0.4777\n",
      "epoch: 250, training loss: 1.2535173892974854, testing loss: 1.7402801513671875\n",
      "Accuracy: 0.4034\n",
      "epoch: 250, training loss: 1.31121027469635, testing loss: 1.7276227474212646\n",
      "Accuracy: 0.4484\n",
      "epoch: 250, training loss: 1.1769793033599854, testing loss: 1.7063969373703003\n",
      "Accuracy: 0.3956\n",
      "epoch: 250, training loss: 1.2707594633102417, testing loss: 1.7475088834762573\n",
      "Accuracy: 0.4277\n",
      "epoch: 250, training loss: 1.2612736225128174, testing loss: 1.724136471748352\n",
      "Accuracy: 0.4102\n",
      "epoch: 250, training loss: 1.3250283002853394, testing loss: 1.6694235801696777\n",
      "Accuracy: 0.4145\n",
      "epoch: 250, training loss: 1.2849810123443604, testing loss: 1.5276708602905273\n",
      "Accuracy: 0.4441\n",
      "epoch: 250, training loss: 1.2853084802627563, testing loss: 1.732983112335205\n",
      "Accuracy: 0.4357\n",
      "epoch: 250, training loss: 1.3253449201583862, testing loss: 1.7047804594039917\n",
      "Accuracy: 0.3873\n",
      "epoch: 250, training loss: 1.284353256225586, testing loss: 1.6827032566070557\n",
      "Accuracy: 0.4559\n",
      "epoch: 260, training loss: 1.327665090560913, testing loss: 1.6784733533859253\n",
      "Accuracy: 0.4159\n",
      "epoch: 260, training loss: 1.222131371498108, testing loss: 1.6398766040802002\n",
      "Accuracy: 0.4363\n",
      "epoch: 260, training loss: 1.266829252243042, testing loss: 1.7323006391525269\n",
      "Accuracy: 0.4136\n",
      "epoch: 260, training loss: 1.2621376514434814, testing loss: 1.6112509965896606\n",
      "Accuracy: 0.4502\n",
      "epoch: 260, training loss: 1.2547094821929932, testing loss: 1.6578471660614014\n",
      "Accuracy: 0.4065\n",
      "epoch: 260, training loss: 1.274914264678955, testing loss: 1.6615508794784546\n",
      "Accuracy: 0.4237\n",
      "epoch: 260, training loss: 1.3046995401382446, testing loss: 1.6265249252319336\n",
      "Accuracy: 0.4448\n",
      "epoch: 260, training loss: 1.232979416847229, testing loss: 1.6012545824050903\n",
      "Accuracy: 0.4725\n",
      "epoch: 260, training loss: 1.2852725982666016, testing loss: 1.804508924484253\n",
      "Accuracy: 0.3916\n",
      "epoch: 260, training loss: 1.2402957677841187, testing loss: 1.6683470010757446\n",
      "Accuracy: 0.4276\n",
      "epoch: 260, training loss: 1.271082878112793, testing loss: 1.691498875617981\n",
      "Accuracy: 0.3828\n",
      "epoch: 260, training loss: 1.3106147050857544, testing loss: 1.6894506216049194\n",
      "Accuracy: 0.4007\n",
      "epoch: 260, training loss: 1.234628677368164, testing loss: 1.6459486484527588\n",
      "Accuracy: 0.4150\n",
      "epoch: 260, training loss: 1.2454497814178467, testing loss: 1.6774779558181763\n",
      "Accuracy: 0.4138\n",
      "epoch: 260, training loss: 1.2402855157852173, testing loss: 1.6248583793640137\n",
      "Accuracy: 0.4194\n",
      "epoch: 260, training loss: 1.265160083770752, testing loss: 1.6105502843856812\n",
      "Accuracy: 0.4408\n",
      "epoch: 260, training loss: 1.34992516040802, testing loss: 1.6373194456100464\n",
      "Accuracy: 0.4286\n",
      "epoch: 260, training loss: 1.2501636743545532, testing loss: 1.8863658905029297\n",
      "Accuracy: 0.4007\n",
      "epoch: 260, training loss: 1.275593876838684, testing loss: 1.74169921875\n",
      "Accuracy: 0.4114\n",
      "epoch: 260, training loss: 1.2559468746185303, testing loss: 1.6505006551742554\n",
      "Accuracy: 0.4130\n",
      "epoch: 260, training loss: 1.3556649684906006, testing loss: 1.5877597332000732\n",
      "Accuracy: 0.4669\n",
      "epoch: 260, training loss: 1.2182557582855225, testing loss: 1.582760214805603\n",
      "Accuracy: 0.4922\n",
      "epoch: 260, training loss: 1.3200950622558594, testing loss: 1.7395410537719727\n",
      "Accuracy: 0.3481\n",
      "epoch: 260, training loss: 1.2393900156021118, testing loss: 1.6201372146606445\n",
      "Accuracy: 0.4188\n",
      "epoch: 260, training loss: 1.201000690460205, testing loss: 1.6178454160690308\n",
      "Accuracy: 0.4329\n",
      "epoch: 260, training loss: 1.3150659799575806, testing loss: 1.569920301437378\n",
      "Accuracy: 0.4486\n",
      "epoch: 260, training loss: 1.2422807216644287, testing loss: 1.6505424976348877\n",
      "Accuracy: 0.4224\n",
      "epoch: 260, training loss: 1.3032618761062622, testing loss: 1.6205098628997803\n",
      "Accuracy: 0.4409\n",
      "epoch: 260, training loss: 1.2763744592666626, testing loss: 1.6616053581237793\n",
      "Accuracy: 0.4148\n",
      "epoch: 260, training loss: 1.3077120780944824, testing loss: 1.4722695350646973\n",
      "Accuracy: 0.4324\n",
      "epoch: 260, training loss: 1.3265182971954346, testing loss: 1.68840754032135\n",
      "Accuracy: 0.4579\n",
      "epoch: 260, training loss: 1.2941912412643433, testing loss: 1.5320442914962769\n",
      "Accuracy: 0.4483\n",
      "epoch: 260, training loss: 1.3891305923461914, testing loss: 1.793527364730835\n",
      "Accuracy: 0.4241\n",
      "epoch: 260, training loss: 1.3460739850997925, testing loss: 1.5527961254119873\n",
      "Accuracy: 0.4258\n",
      "epoch: 260, training loss: 1.3132193088531494, testing loss: 1.8226993083953857\n",
      "Accuracy: 0.3750\n",
      "epoch: 260, training loss: 1.2693631649017334, testing loss: 1.7474894523620605\n",
      "Accuracy: 0.3714\n",
      "epoch: 260, training loss: 1.269933819770813, testing loss: 1.6588681936264038\n",
      "Accuracy: 0.3953\n",
      "epoch: 260, training loss: 1.3285801410675049, testing loss: 1.6789480447769165\n",
      "Accuracy: 0.4407\n",
      "epoch: 260, training loss: 1.248199224472046, testing loss: 1.683323860168457\n",
      "Accuracy: 0.4379\n",
      "epoch: 260, training loss: 1.296548843383789, testing loss: 1.644311785697937\n",
      "Accuracy: 0.4471\n",
      "epoch: 260, training loss: 1.2730138301849365, testing loss: 1.5160504579544067\n",
      "Accuracy: 0.4190\n",
      "epoch: 260, training loss: 1.3005330562591553, testing loss: 1.6408331394195557\n",
      "Accuracy: 0.4295\n",
      "epoch: 260, training loss: 1.289581537246704, testing loss: 1.7257128953933716\n",
      "Accuracy: 0.4241\n",
      "epoch: 260, training loss: 1.272915005683899, testing loss: 1.6419047117233276\n",
      "Accuracy: 0.4150\n",
      "epoch: 260, training loss: 1.237330436706543, testing loss: 1.5623767375946045\n",
      "Accuracy: 0.4842\n",
      "epoch: 260, training loss: 1.2626378536224365, testing loss: 1.6081470251083374\n",
      "Accuracy: 0.4252\n",
      "epoch: 260, training loss: 1.2682995796203613, testing loss: 1.6452150344848633\n",
      "Accuracy: 0.3916\n",
      "epoch: 260, training loss: 1.250859260559082, testing loss: 1.6396230459213257\n",
      "Accuracy: 0.4313\n",
      "epoch: 260, training loss: 1.2916064262390137, testing loss: 1.8004181385040283\n",
      "Accuracy: 0.4042\n",
      "epoch: 260, training loss: 1.2443323135375977, testing loss: 1.65577232837677\n",
      "Accuracy: 0.4209\n",
      "epoch: 260, training loss: 1.3084322214126587, testing loss: 1.7782341241836548\n",
      "Accuracy: 0.3738\n",
      "epoch: 260, training loss: 1.2708725929260254, testing loss: 1.7640060186386108\n",
      "Accuracy: 0.3750\n",
      "epoch: 260, training loss: 1.272434115409851, testing loss: 1.7135881185531616\n",
      "Accuracy: 0.4272\n",
      "epoch: 260, training loss: 1.2997416257858276, testing loss: 1.5306485891342163\n",
      "Accuracy: 0.4853\n",
      "epoch: 260, training loss: 1.1949925422668457, testing loss: 1.6506105661392212\n",
      "Accuracy: 0.4091\n",
      "epoch: 260, training loss: 1.308984398841858, testing loss: 1.803043246269226\n",
      "Accuracy: 0.3974\n",
      "epoch: 260, training loss: 1.2575337886810303, testing loss: 1.585632562637329\n",
      "Accuracy: 0.4505\n",
      "epoch: 260, training loss: 1.308850884437561, testing loss: 1.720295786857605\n",
      "Accuracy: 0.4381\n",
      "epoch: 260, training loss: 1.268978476524353, testing loss: 1.652053952217102\n",
      "Accuracy: 0.4006\n",
      "epoch: 260, training loss: 1.296287178993225, testing loss: 1.6447662115097046\n",
      "Accuracy: 0.4182\n",
      "epoch: 260, training loss: 1.3607087135314941, testing loss: 1.8024286031723022\n",
      "Accuracy: 0.3613\n",
      "epoch: 260, training loss: 1.2838592529296875, testing loss: 1.7139626741409302\n",
      "Accuracy: 0.3975\n",
      "epoch: 260, training loss: 1.330113172531128, testing loss: 1.6818920373916626\n",
      "Accuracy: 0.3709\n",
      "epoch: 260, training loss: 1.3071844577789307, testing loss: 1.6249268054962158\n",
      "Accuracy: 0.3993\n",
      "epoch: 260, training loss: 1.3632205724716187, testing loss: 1.7105158567428589\n",
      "Accuracy: 0.4147\n",
      "epoch: 260, training loss: 1.2898681163787842, testing loss: 1.5528773069381714\n",
      "Accuracy: 0.4452\n",
      "epoch: 260, training loss: 1.2604459524154663, testing loss: 1.7347614765167236\n",
      "Accuracy: 0.3837\n",
      "epoch: 260, training loss: 1.302165150642395, testing loss: 1.7169840335845947\n",
      "Accuracy: 0.3737\n",
      "epoch: 260, training loss: 1.2885253429412842, testing loss: 1.6755175590515137\n",
      "Accuracy: 0.4065\n",
      "epoch: 260, training loss: 1.2689908742904663, testing loss: 1.636237382888794\n",
      "Accuracy: 0.4551\n",
      "epoch: 260, training loss: 1.2106786966323853, testing loss: 1.6165528297424316\n",
      "Accuracy: 0.4828\n",
      "epoch: 260, training loss: 1.269317865371704, testing loss: 1.7167026996612549\n",
      "Accuracy: 0.4030\n",
      "epoch: 260, training loss: 1.2296886444091797, testing loss: 1.6262909173965454\n",
      "Accuracy: 0.4184\n",
      "epoch: 260, training loss: 1.2688862085342407, testing loss: 1.7136224508285522\n",
      "Accuracy: 0.3821\n",
      "epoch: 260, training loss: 1.2778698205947876, testing loss: 1.6072665452957153\n",
      "Accuracy: 0.4702\n",
      "epoch: 260, training loss: 1.3001055717468262, testing loss: 1.6058443784713745\n",
      "Accuracy: 0.4348\n",
      "epoch: 260, training loss: 1.2775006294250488, testing loss: 1.7216342687606812\n",
      "Accuracy: 0.3838\n",
      "epoch: 260, training loss: 1.277992844581604, testing loss: 1.5126019716262817\n",
      "Accuracy: 0.4543\n",
      "epoch: 260, training loss: 1.2476457357406616, testing loss: 1.654061198234558\n",
      "Accuracy: 0.4114\n",
      "epoch: 260, training loss: 1.3137385845184326, testing loss: 1.6251330375671387\n",
      "Accuracy: 0.4183\n",
      "epoch: 260, training loss: 1.2719281911849976, testing loss: 1.6113357543945312\n",
      "Accuracy: 0.4071\n",
      "epoch: 260, training loss: 1.2510051727294922, testing loss: 1.6817083358764648\n",
      "Accuracy: 0.4326\n",
      "epoch: 260, training loss: 1.32715904712677, testing loss: 1.654821515083313\n",
      "Accuracy: 0.4361\n",
      "epoch: 260, training loss: 1.2493621110916138, testing loss: 1.8300092220306396\n",
      "Accuracy: 0.3878\n",
      "epoch: 260, training loss: 1.2556630373001099, testing loss: 1.7594276666641235\n",
      "Accuracy: 0.3849\n",
      "epoch: 260, training loss: 1.2522163391113281, testing loss: 1.6583682298660278\n",
      "Accuracy: 0.4073\n",
      "epoch: 260, training loss: 1.2708269357681274, testing loss: 1.662974238395691\n",
      "Accuracy: 0.4064\n",
      "epoch: 260, training loss: 1.339453935623169, testing loss: 1.6457786560058594\n",
      "Accuracy: 0.4578\n",
      "epoch: 260, training loss: 1.219792127609253, testing loss: 1.7295268774032593\n",
      "Accuracy: 0.4253\n",
      "epoch: 260, training loss: 1.326090931892395, testing loss: 1.7215949296951294\n",
      "Accuracy: 0.3627\n",
      "epoch: 260, training loss: 1.2419098615646362, testing loss: 1.646999716758728\n",
      "Accuracy: 0.4778\n",
      "epoch: 260, training loss: 1.2714488506317139, testing loss: 1.8501054048538208\n",
      "Accuracy: 0.3925\n",
      "epoch: 260, training loss: 1.2920212745666504, testing loss: 1.6294127702713013\n",
      "Accuracy: 0.4476\n",
      "epoch: 260, training loss: 1.2119606733322144, testing loss: 1.6918489933013916\n",
      "Accuracy: 0.4247\n",
      "epoch: 260, training loss: 1.329698920249939, testing loss: 1.670269250869751\n",
      "Accuracy: 0.4320\n",
      "epoch: 260, training loss: 1.212996482849121, testing loss: 1.6703581809997559\n",
      "Accuracy: 0.4277\n",
      "epoch: 260, training loss: 1.2378207445144653, testing loss: 1.5415847301483154\n",
      "Accuracy: 0.4371\n",
      "epoch: 260, training loss: 1.230096459388733, testing loss: 1.8173431158065796\n",
      "Accuracy: 0.3949\n",
      "epoch: 260, training loss: 1.2436474561691284, testing loss: 1.6593153476715088\n",
      "Accuracy: 0.4290\n",
      "epoch: 260, training loss: 1.210390329360962, testing loss: 1.8246376514434814\n",
      "Accuracy: 0.4153\n",
      "epoch: 270, training loss: 1.220323920249939, testing loss: 1.5490349531173706\n",
      "Accuracy: 0.4919\n",
      "epoch: 270, training loss: 1.255466103553772, testing loss: 1.605519413948059\n",
      "Accuracy: 0.4644\n",
      "epoch: 270, training loss: 1.2868303060531616, testing loss: 1.6320953369140625\n",
      "Accuracy: 0.4423\n",
      "epoch: 270, training loss: 1.252660870552063, testing loss: 1.612326979637146\n",
      "Accuracy: 0.4553\n",
      "epoch: 270, training loss: 1.3394296169281006, testing loss: 1.654126524925232\n",
      "Accuracy: 0.4125\n",
      "epoch: 270, training loss: 1.3102121353149414, testing loss: 1.5975594520568848\n",
      "Accuracy: 0.4448\n",
      "epoch: 270, training loss: 1.1682816743850708, testing loss: 1.7251440286636353\n",
      "Accuracy: 0.4143\n",
      "epoch: 270, training loss: 1.1922162771224976, testing loss: 1.6812939643859863\n",
      "Accuracy: 0.4371\n",
      "epoch: 270, training loss: 1.2772564888000488, testing loss: 1.6460310220718384\n",
      "Accuracy: 0.4242\n",
      "epoch: 270, training loss: 1.2212438583374023, testing loss: 1.7222702503204346\n",
      "Accuracy: 0.3981\n",
      "epoch: 270, training loss: 1.2720052003860474, testing loss: 1.7833338975906372\n",
      "Accuracy: 0.4323\n",
      "epoch: 270, training loss: 1.3456283807754517, testing loss: 1.6786879301071167\n",
      "Accuracy: 0.4355\n",
      "epoch: 270, training loss: 1.242679476737976, testing loss: 1.7972426414489746\n",
      "Accuracy: 0.4030\n",
      "epoch: 270, training loss: 1.28762686252594, testing loss: 1.644287347793579\n",
      "Accuracy: 0.4444\n",
      "epoch: 270, training loss: 1.325769305229187, testing loss: 1.7498220205307007\n",
      "Accuracy: 0.4086\n",
      "epoch: 270, training loss: 1.2906259298324585, testing loss: 1.7932788133621216\n",
      "Accuracy: 0.4006\n",
      "epoch: 270, training loss: 1.2757569551467896, testing loss: 1.6993203163146973\n",
      "Accuracy: 0.4121\n",
      "epoch: 270, training loss: 1.2020602226257324, testing loss: 1.6576083898544312\n",
      "Accuracy: 0.4013\n",
      "epoch: 270, training loss: 1.2722375392913818, testing loss: 1.6494807004928589\n",
      "Accuracy: 0.4228\n",
      "epoch: 270, training loss: 1.2179561853408813, testing loss: 1.743354082107544\n",
      "Accuracy: 0.3859\n",
      "epoch: 270, training loss: 1.2719367742538452, testing loss: 1.6796048879623413\n",
      "Accuracy: 0.3885\n",
      "epoch: 270, training loss: 1.2695443630218506, testing loss: 1.660258173942566\n",
      "Accuracy: 0.3988\n",
      "epoch: 270, training loss: 1.2490081787109375, testing loss: 1.6321340799331665\n",
      "Accuracy: 0.4523\n",
      "epoch: 270, training loss: 1.283228874206543, testing loss: 1.6412888765335083\n",
      "Accuracy: 0.4049\n",
      "epoch: 270, training loss: 1.265456199645996, testing loss: 1.5894516706466675\n",
      "Accuracy: 0.4291\n",
      "epoch: 270, training loss: 1.2629071474075317, testing loss: 1.5953221321105957\n",
      "Accuracy: 0.4267\n",
      "epoch: 270, training loss: 1.2649799585342407, testing loss: 1.6486767530441284\n",
      "Accuracy: 0.3987\n",
      "epoch: 270, training loss: 1.3137112855911255, testing loss: 1.6098297834396362\n",
      "Accuracy: 0.4097\n",
      "epoch: 270, training loss: 1.3494731187820435, testing loss: 1.7177351713180542\n",
      "Accuracy: 0.3882\n",
      "epoch: 270, training loss: 1.3227320909500122, testing loss: 1.6922309398651123\n",
      "Accuracy: 0.4137\n",
      "epoch: 270, training loss: 1.2803919315338135, testing loss: 1.570314884185791\n",
      "Accuracy: 0.4434\n",
      "epoch: 270, training loss: 1.354886770248413, testing loss: 1.728162407875061\n",
      "Accuracy: 0.3889\n",
      "epoch: 270, training loss: 1.2623839378356934, testing loss: 1.725391149520874\n",
      "Accuracy: 0.4338\n",
      "epoch: 270, training loss: 1.22075617313385, testing loss: 1.7304813861846924\n",
      "Accuracy: 0.4064\n",
      "epoch: 270, training loss: 1.2797185182571411, testing loss: 1.6340633630752563\n",
      "Accuracy: 0.4365\n",
      "epoch: 270, training loss: 1.2688546180725098, testing loss: 1.651638388633728\n",
      "Accuracy: 0.4513\n",
      "epoch: 270, training loss: 1.2450696229934692, testing loss: 1.5511558055877686\n",
      "Accuracy: 0.4263\n",
      "epoch: 270, training loss: 1.2335262298583984, testing loss: 1.7292976379394531\n",
      "Accuracy: 0.4286\n",
      "epoch: 270, training loss: 1.241394281387329, testing loss: 1.6028083562850952\n",
      "Accuracy: 0.4247\n",
      "epoch: 270, training loss: 1.3357535600662231, testing loss: 1.6933709383010864\n",
      "Accuracy: 0.4199\n",
      "epoch: 270, training loss: 1.2625027894973755, testing loss: 1.6735564470291138\n",
      "Accuracy: 0.4226\n",
      "epoch: 270, training loss: 1.3012980222702026, testing loss: 1.6582577228546143\n",
      "Accuracy: 0.4303\n",
      "epoch: 270, training loss: 1.2744415998458862, testing loss: 1.640376329421997\n",
      "Accuracy: 0.4300\n",
      "epoch: 270, training loss: 1.2700307369232178, testing loss: 1.611484408378601\n",
      "Accuracy: 0.3926\n",
      "epoch: 270, training loss: 1.3110381364822388, testing loss: 1.644547939300537\n",
      "Accuracy: 0.4267\n",
      "epoch: 270, training loss: 1.247350811958313, testing loss: 1.7552101612091064\n",
      "Accuracy: 0.4247\n",
      "epoch: 270, training loss: 1.2587729692459106, testing loss: 1.7280762195587158\n",
      "Accuracy: 0.4281\n",
      "epoch: 270, training loss: 1.322845220565796, testing loss: 1.7182953357696533\n",
      "Accuracy: 0.4006\n",
      "epoch: 270, training loss: 1.319085955619812, testing loss: 1.729244589805603\n",
      "Accuracy: 0.3918\n",
      "epoch: 270, training loss: 1.330068588256836, testing loss: 1.5782560110092163\n",
      "Accuracy: 0.4334\n",
      "epoch: 270, training loss: 1.2105807065963745, testing loss: 1.5885647535324097\n",
      "Accuracy: 0.4522\n",
      "epoch: 270, training loss: 1.2523057460784912, testing loss: 1.6688300371170044\n",
      "Accuracy: 0.4167\n",
      "epoch: 270, training loss: 1.3028942346572876, testing loss: 1.6513792276382446\n",
      "Accuracy: 0.3846\n",
      "epoch: 270, training loss: 1.2597965002059937, testing loss: 1.6077336072921753\n",
      "Accuracy: 0.4226\n",
      "epoch: 270, training loss: 1.2578296661376953, testing loss: 1.7609964609146118\n",
      "Accuracy: 0.3950\n",
      "epoch: 270, training loss: 1.2876229286193848, testing loss: 1.7495163679122925\n",
      "Accuracy: 0.3754\n",
      "epoch: 270, training loss: 1.2640750408172607, testing loss: 1.4741086959838867\n",
      "Accuracy: 0.4722\n",
      "epoch: 270, training loss: 1.3008040189743042, testing loss: 1.682313084602356\n",
      "Accuracy: 0.4174\n",
      "epoch: 270, training loss: 1.2700213193893433, testing loss: 1.7775769233703613\n",
      "Accuracy: 0.3836\n",
      "epoch: 270, training loss: 1.177241563796997, testing loss: 1.6458501815795898\n",
      "Accuracy: 0.4006\n",
      "epoch: 270, training loss: 1.2995359897613525, testing loss: 1.6774330139160156\n",
      "Accuracy: 0.4120\n",
      "epoch: 270, training loss: 1.323340654373169, testing loss: 1.769805908203125\n",
      "Accuracy: 0.4248\n",
      "epoch: 270, training loss: 1.2535333633422852, testing loss: 1.626889944076538\n",
      "Accuracy: 0.4267\n",
      "epoch: 270, training loss: 1.2793066501617432, testing loss: 1.608289361000061\n",
      "Accuracy: 0.4178\n",
      "epoch: 270, training loss: 1.2432703971862793, testing loss: 1.8796076774597168\n",
      "Accuracy: 0.3779\n",
      "epoch: 270, training loss: 1.240543007850647, testing loss: 1.685078501701355\n",
      "Accuracy: 0.4261\n",
      "epoch: 270, training loss: 1.2914994955062866, testing loss: 1.7588038444519043\n",
      "Accuracy: 0.3903\n",
      "epoch: 270, training loss: 1.2818846702575684, testing loss: 1.5760316848754883\n",
      "Accuracy: 0.4340\n",
      "epoch: 270, training loss: 1.29947829246521, testing loss: 1.6856588125228882\n",
      "Accuracy: 0.3774\n",
      "epoch: 270, training loss: 1.3033443689346313, testing loss: 1.6437091827392578\n",
      "Accuracy: 0.4462\n",
      "epoch: 270, training loss: 1.2564092874526978, testing loss: 1.6637986898422241\n",
      "Accuracy: 0.4365\n",
      "epoch: 270, training loss: 1.2381442785263062, testing loss: 1.701295018196106\n",
      "Accuracy: 0.3720\n",
      "epoch: 270, training loss: 1.2437158823013306, testing loss: 1.6860076189041138\n",
      "Accuracy: 0.4062\n",
      "epoch: 270, training loss: 1.3190631866455078, testing loss: 1.7147390842437744\n",
      "Accuracy: 0.4089\n",
      "epoch: 270, training loss: 1.3261017799377441, testing loss: 1.6355586051940918\n",
      "Accuracy: 0.4205\n",
      "epoch: 270, training loss: 1.2830206155776978, testing loss: 1.7788658142089844\n",
      "Accuracy: 0.3932\n",
      "epoch: 270, training loss: 1.262786626815796, testing loss: 1.586116075515747\n",
      "Accuracy: 0.4243\n",
      "epoch: 270, training loss: 1.2641651630401611, testing loss: 1.704877495765686\n",
      "Accuracy: 0.4143\n",
      "epoch: 270, training loss: 1.3061888217926025, testing loss: 1.5587934255599976\n",
      "Accuracy: 0.4804\n",
      "epoch: 270, training loss: 1.183288812637329, testing loss: 1.6511679887771606\n",
      "Accuracy: 0.4233\n",
      "epoch: 270, training loss: 1.2500184774398804, testing loss: 1.660129189491272\n",
      "Accuracy: 0.4575\n",
      "epoch: 270, training loss: 1.2239052057266235, testing loss: 1.7697111368179321\n",
      "Accuracy: 0.3877\n",
      "epoch: 270, training loss: 1.3184001445770264, testing loss: 1.7578884363174438\n",
      "Accuracy: 0.4281\n",
      "epoch: 270, training loss: 1.2179603576660156, testing loss: 1.6789953708648682\n",
      "Accuracy: 0.3841\n",
      "epoch: 270, training loss: 1.332477331161499, testing loss: 1.6113029718399048\n",
      "Accuracy: 0.3929\n",
      "epoch: 270, training loss: 1.194663405418396, testing loss: 1.5864145755767822\n",
      "Accuracy: 0.4512\n",
      "epoch: 270, training loss: 1.2265511751174927, testing loss: 1.6724011898040771\n",
      "Accuracy: 0.4127\n",
      "epoch: 270, training loss: 1.2616517543792725, testing loss: 1.5883517265319824\n",
      "Accuracy: 0.4660\n",
      "epoch: 270, training loss: 1.214996337890625, testing loss: 1.700927972793579\n",
      "Accuracy: 0.4587\n",
      "epoch: 270, training loss: 1.2463247776031494, testing loss: 1.714152216911316\n",
      "Accuracy: 0.3799\n",
      "epoch: 270, training loss: 1.241965889930725, testing loss: 1.6750694513320923\n",
      "Accuracy: 0.3743\n",
      "epoch: 270, training loss: 1.229814052581787, testing loss: 1.545005440711975\n",
      "Accuracy: 0.4695\n",
      "epoch: 270, training loss: 1.225372552871704, testing loss: 1.6690773963928223\n",
      "Accuracy: 0.4075\n",
      "epoch: 270, training loss: 1.4017324447631836, testing loss: 1.7252976894378662\n",
      "Accuracy: 0.4184\n",
      "epoch: 270, training loss: 1.2267272472381592, testing loss: 1.6510462760925293\n",
      "Accuracy: 0.4103\n",
      "epoch: 270, training loss: 1.2492049932479858, testing loss: 1.7155765295028687\n",
      "Accuracy: 0.4444\n",
      "epoch: 270, training loss: 1.2799216508865356, testing loss: 1.709474802017212\n",
      "Accuracy: 0.4205\n",
      "epoch: 270, training loss: 1.230594515800476, testing loss: 1.7875101566314697\n",
      "Accuracy: 0.4211\n",
      "epoch: 270, training loss: 1.2917181253433228, testing loss: 1.6951664686203003\n",
      "Accuracy: 0.3993\n",
      "epoch: 270, training loss: 1.3763985633850098, testing loss: 1.6660590171813965\n",
      "Accuracy: 0.4493\n",
      "epoch: 280, training loss: 1.3157873153686523, testing loss: 1.6259905099868774\n",
      "Accuracy: 0.4608\n",
      "epoch: 280, training loss: 1.2905399799346924, testing loss: 1.7102179527282715\n",
      "Accuracy: 0.4227\n",
      "epoch: 280, training loss: 1.1992475986480713, testing loss: 1.7069705724716187\n",
      "Accuracy: 0.4145\n",
      "epoch: 280, training loss: 1.2517393827438354, testing loss: 1.6313109397888184\n",
      "Accuracy: 0.4331\n",
      "epoch: 280, training loss: 1.279489517211914, testing loss: 1.7947938442230225\n",
      "Accuracy: 0.4085\n",
      "epoch: 280, training loss: 1.2636504173278809, testing loss: 1.6158372163772583\n",
      "Accuracy: 0.4416\n",
      "epoch: 280, training loss: 1.2645436525344849, testing loss: 1.6544232368469238\n",
      "Accuracy: 0.4034\n",
      "epoch: 280, training loss: 1.2452502250671387, testing loss: 1.6517030000686646\n",
      "Accuracy: 0.4665\n",
      "epoch: 280, training loss: 1.2936054468154907, testing loss: 1.7237666845321655\n",
      "Accuracy: 0.3922\n",
      "epoch: 280, training loss: 1.2354260683059692, testing loss: 1.5181946754455566\n",
      "Accuracy: 0.4615\n",
      "epoch: 280, training loss: 1.3324973583221436, testing loss: 1.7096344232559204\n",
      "Accuracy: 0.4534\n",
      "epoch: 280, training loss: 1.1948091983795166, testing loss: 1.7695626020431519\n",
      "Accuracy: 0.3841\n",
      "epoch: 280, training loss: 1.2658796310424805, testing loss: 1.6621286869049072\n",
      "Accuracy: 0.4282\n",
      "epoch: 280, training loss: 1.2080848217010498, testing loss: 1.6568487882614136\n",
      "Accuracy: 0.4257\n",
      "epoch: 280, training loss: 1.2678483724594116, testing loss: 1.7309494018554688\n",
      "Accuracy: 0.3969\n",
      "epoch: 280, training loss: 1.2244186401367188, testing loss: 1.609818696975708\n",
      "Accuracy: 0.4525\n",
      "epoch: 280, training loss: 1.2618721723556519, testing loss: 1.8804051876068115\n",
      "Accuracy: 0.3898\n",
      "epoch: 280, training loss: 1.319506287574768, testing loss: 1.6667771339416504\n",
      "Accuracy: 0.4320\n",
      "epoch: 280, training loss: 1.244138240814209, testing loss: 1.7618257999420166\n",
      "Accuracy: 0.3980\n",
      "epoch: 280, training loss: 1.2068567276000977, testing loss: 1.7291972637176514\n",
      "Accuracy: 0.3808\n",
      "epoch: 280, training loss: 1.282043218612671, testing loss: 1.6601701974868774\n",
      "Accuracy: 0.4451\n",
      "epoch: 280, training loss: 1.2705646753311157, testing loss: 1.571474552154541\n",
      "Accuracy: 0.4468\n",
      "epoch: 280, training loss: 1.3418878316879272, testing loss: 1.6105246543884277\n",
      "Accuracy: 0.4458\n",
      "epoch: 280, training loss: 1.2723431587219238, testing loss: 1.7907239198684692\n",
      "Accuracy: 0.4123\n",
      "epoch: 280, training loss: 1.2203274965286255, testing loss: 1.6691415309906006\n",
      "Accuracy: 0.4267\n",
      "epoch: 280, training loss: 1.2911709547042847, testing loss: 1.6113793849945068\n",
      "Accuracy: 0.4140\n",
      "epoch: 280, training loss: 1.2565919160842896, testing loss: 1.645125150680542\n",
      "Accuracy: 0.4286\n",
      "epoch: 280, training loss: 1.2634718418121338, testing loss: 1.6484602689743042\n",
      "Accuracy: 0.3976\n",
      "epoch: 280, training loss: 1.2492905855178833, testing loss: 1.7220954895019531\n",
      "Accuracy: 0.4154\n",
      "epoch: 280, training loss: 1.3265036344528198, testing loss: 1.6497256755828857\n",
      "Accuracy: 0.4281\n",
      "epoch: 280, training loss: 1.3299319744110107, testing loss: 1.7241928577423096\n",
      "Accuracy: 0.4105\n",
      "epoch: 280, training loss: 1.2677419185638428, testing loss: 1.7299143075942993\n",
      "Accuracy: 0.4506\n",
      "epoch: 280, training loss: 1.256239652633667, testing loss: 1.6989479064941406\n",
      "Accuracy: 0.4351\n",
      "epoch: 280, training loss: 1.2484867572784424, testing loss: 1.637774109840393\n",
      "Accuracy: 0.4259\n",
      "epoch: 280, training loss: 1.2585225105285645, testing loss: 1.683741569519043\n",
      "Accuracy: 0.4502\n",
      "epoch: 280, training loss: 1.2267680168151855, testing loss: 1.7696064710617065\n",
      "Accuracy: 0.3705\n",
      "epoch: 280, training loss: 1.2644643783569336, testing loss: 1.5704145431518555\n",
      "Accuracy: 0.4967\n",
      "epoch: 280, training loss: 1.2776365280151367, testing loss: 1.7917752265930176\n",
      "Accuracy: 0.4223\n",
      "epoch: 280, training loss: 1.2247599363327026, testing loss: 1.774527907371521\n",
      "Accuracy: 0.3926\n",
      "epoch: 280, training loss: 1.3450630903244019, testing loss: 1.7310720682144165\n",
      "Accuracy: 0.3903\n",
      "epoch: 280, training loss: 1.2593107223510742, testing loss: 1.8643927574157715\n",
      "Accuracy: 0.4153\n",
      "epoch: 280, training loss: 1.2627872228622437, testing loss: 1.633614182472229\n",
      "Accuracy: 0.4459\n",
      "epoch: 280, training loss: 1.2409472465515137, testing loss: 1.707929253578186\n",
      "Accuracy: 0.4290\n",
      "epoch: 280, training loss: 1.2690502405166626, testing loss: 1.6591498851776123\n",
      "Accuracy: 0.4451\n",
      "epoch: 280, training loss: 1.208242416381836, testing loss: 1.6325900554656982\n",
      "Accuracy: 0.4540\n",
      "epoch: 280, training loss: 1.265232801437378, testing loss: 1.6715750694274902\n",
      "Accuracy: 0.3993\n",
      "epoch: 280, training loss: 1.2147184610366821, testing loss: 1.7574193477630615\n",
      "Accuracy: 0.4034\n",
      "epoch: 280, training loss: 1.281562089920044, testing loss: 1.764586329460144\n",
      "Accuracy: 0.3973\n",
      "epoch: 280, training loss: 1.2398813962936401, testing loss: 1.7566983699798584\n",
      "Accuracy: 0.3695\n",
      "epoch: 280, training loss: 1.2837938070297241, testing loss: 1.5772026777267456\n",
      "Accuracy: 0.4646\n",
      "epoch: 280, training loss: 1.246184229850769, testing loss: 1.728090763092041\n",
      "Accuracy: 0.3871\n",
      "epoch: 280, training loss: 1.2753618955612183, testing loss: 1.7516818046569824\n",
      "Accuracy: 0.3954\n",
      "epoch: 280, training loss: 1.2845344543457031, testing loss: 1.6637346744537354\n",
      "Accuracy: 0.3897\n",
      "epoch: 280, training loss: 1.258700966835022, testing loss: 1.680478811264038\n",
      "Accuracy: 0.4045\n",
      "epoch: 280, training loss: 1.231561541557312, testing loss: 1.7213248014450073\n",
      "Accuracy: 0.4068\n",
      "epoch: 280, training loss: 1.2139837741851807, testing loss: 1.6923277378082275\n",
      "Accuracy: 0.4357\n",
      "epoch: 280, training loss: 1.3097033500671387, testing loss: 1.5772775411605835\n",
      "Accuracy: 0.4422\n",
      "epoch: 280, training loss: 1.2104185819625854, testing loss: 1.6405128240585327\n",
      "Accuracy: 0.4299\n",
      "epoch: 280, training loss: 1.26946222782135, testing loss: 1.7841182947158813\n",
      "Accuracy: 0.4678\n",
      "epoch: 280, training loss: 1.3509306907653809, testing loss: 1.6439929008483887\n",
      "Accuracy: 0.3836\n",
      "epoch: 280, training loss: 1.2547063827514648, testing loss: 1.7128496170043945\n",
      "Accuracy: 0.3872\n",
      "epoch: 280, training loss: 1.2772040367126465, testing loss: 1.8054944276809692\n",
      "Accuracy: 0.3765\n",
      "epoch: 280, training loss: 1.2142095565795898, testing loss: 1.7933026552200317\n",
      "Accuracy: 0.4022\n",
      "epoch: 280, training loss: 1.2480454444885254, testing loss: 1.6938421726226807\n",
      "Accuracy: 0.4266\n",
      "epoch: 280, training loss: 1.2490381002426147, testing loss: 1.6912137269973755\n",
      "Accuracy: 0.3986\n",
      "epoch: 280, training loss: 1.2538355588912964, testing loss: 1.5790847539901733\n",
      "Accuracy: 0.4542\n",
      "epoch: 280, training loss: 1.247685194015503, testing loss: 1.6066088676452637\n",
      "Accuracy: 0.4203\n",
      "epoch: 280, training loss: 1.2236522436141968, testing loss: 1.6960904598236084\n",
      "Accuracy: 0.4159\n",
      "epoch: 280, training loss: 1.2532048225402832, testing loss: 1.666616439819336\n",
      "Accuracy: 0.4103\n",
      "epoch: 280, training loss: 1.305120587348938, testing loss: 1.7302227020263672\n",
      "Accuracy: 0.4193\n",
      "epoch: 280, training loss: 1.2516744136810303, testing loss: 1.6357123851776123\n",
      "Accuracy: 0.4154\n",
      "epoch: 280, training loss: 1.3386850357055664, testing loss: 1.7574888467788696\n",
      "Accuracy: 0.4267\n",
      "epoch: 280, training loss: 1.2391694784164429, testing loss: 1.6153273582458496\n",
      "Accuracy: 0.4319\n",
      "epoch: 280, training loss: 1.2834240198135376, testing loss: 1.6186347007751465\n",
      "Accuracy: 0.3952\n",
      "epoch: 280, training loss: 1.2868571281433105, testing loss: 1.7057957649230957\n",
      "Accuracy: 0.4052\n",
      "epoch: 280, training loss: 1.2754969596862793, testing loss: 1.6632264852523804\n",
      "Accuracy: 0.3931\n",
      "epoch: 280, training loss: 1.2788015604019165, testing loss: 1.7413381338119507\n",
      "Accuracy: 0.4114\n",
      "epoch: 280, training loss: 1.2562260627746582, testing loss: 1.62149178981781\n",
      "Accuracy: 0.4417\n",
      "epoch: 280, training loss: 1.3610934019088745, testing loss: 1.8045613765716553\n",
      "Accuracy: 0.3816\n",
      "epoch: 280, training loss: 1.2723126411437988, testing loss: 1.7560670375823975\n",
      "Accuracy: 0.4154\n",
      "epoch: 280, training loss: 1.2972747087478638, testing loss: 1.9316705465316772\n",
      "Accuracy: 0.3513\n",
      "epoch: 280, training loss: 1.2497045993804932, testing loss: 1.6180781126022339\n",
      "Accuracy: 0.4308\n",
      "epoch: 280, training loss: 1.232332468032837, testing loss: 1.780885934829712\n",
      "Accuracy: 0.4097\n",
      "epoch: 280, training loss: 1.1953930854797363, testing loss: 1.631659984588623\n",
      "Accuracy: 0.4157\n",
      "epoch: 280, training loss: 1.2700374126434326, testing loss: 1.6063517332077026\n",
      "Accuracy: 0.4355\n",
      "epoch: 280, training loss: 1.2255481481552124, testing loss: 1.8015623092651367\n",
      "Accuracy: 0.3889\n",
      "epoch: 280, training loss: 1.293772578239441, testing loss: 1.6143351793289185\n",
      "Accuracy: 0.4422\n",
      "epoch: 280, training loss: 1.2115089893341064, testing loss: 1.704420566558838\n",
      "Accuracy: 0.4050\n",
      "epoch: 280, training loss: 1.2612824440002441, testing loss: 1.7363942861557007\n",
      "Accuracy: 0.3981\n",
      "epoch: 280, training loss: 1.2420896291732788, testing loss: 1.735144019126892\n",
      "Accuracy: 0.4131\n",
      "epoch: 280, training loss: 1.240302562713623, testing loss: 1.621177315711975\n",
      "Accuracy: 0.3935\n",
      "epoch: 280, training loss: 1.2390302419662476, testing loss: 1.5763520002365112\n",
      "Accuracy: 0.4230\n",
      "epoch: 280, training loss: 1.279272198677063, testing loss: 1.7304351329803467\n",
      "Accuracy: 0.3966\n",
      "epoch: 280, training loss: 1.249226689338684, testing loss: 1.6606718301773071\n",
      "Accuracy: 0.4000\n",
      "epoch: 280, training loss: 1.224740743637085, testing loss: 1.6496515274047852\n",
      "Accuracy: 0.4772\n",
      "epoch: 280, training loss: 1.2864128351211548, testing loss: 1.7413406372070312\n",
      "Accuracy: 0.4252\n",
      "epoch: 280, training loss: 1.2348244190216064, testing loss: 1.6783597469329834\n",
      "Accuracy: 0.4528\n",
      "epoch: 280, training loss: 1.2854121923446655, testing loss: 1.696382999420166\n",
      "Accuracy: 0.4123\n",
      "epoch: 280, training loss: 1.2470424175262451, testing loss: 1.6922553777694702\n",
      "Accuracy: 0.4175\n",
      "epoch: 280, training loss: 1.3175718784332275, testing loss: 1.7270885705947876\n",
      "Accuracy: 0.4459\n",
      "epoch: 290, training loss: 1.2244025468826294, testing loss: 1.8313449621200562\n",
      "Accuracy: 0.4211\n",
      "epoch: 290, training loss: 1.24303138256073, testing loss: 1.6561661958694458\n",
      "Accuracy: 0.4404\n",
      "epoch: 290, training loss: 1.2275707721710205, testing loss: 1.6549880504608154\n",
      "Accuracy: 0.3887\n",
      "epoch: 290, training loss: 1.2892522811889648, testing loss: 1.6184195280075073\n",
      "Accuracy: 0.4247\n",
      "epoch: 290, training loss: 1.2995799779891968, testing loss: 1.5079712867736816\n",
      "Accuracy: 0.4659\n",
      "epoch: 290, training loss: 1.273774266242981, testing loss: 1.6225701570510864\n",
      "Accuracy: 0.4441\n",
      "epoch: 290, training loss: 1.236177682876587, testing loss: 1.8122577667236328\n",
      "Accuracy: 0.3712\n",
      "epoch: 290, training loss: 1.2423367500305176, testing loss: 1.8671798706054688\n",
      "Accuracy: 0.3739\n",
      "epoch: 290, training loss: 1.340102195739746, testing loss: 1.5961153507232666\n",
      "Accuracy: 0.4034\n",
      "epoch: 290, training loss: 1.2774754762649536, testing loss: 1.7030770778656006\n",
      "Accuracy: 0.4469\n",
      "epoch: 290, training loss: 1.2234077453613281, testing loss: 1.6089805364608765\n",
      "Accuracy: 0.4660\n",
      "epoch: 290, training loss: 1.2337404489517212, testing loss: 1.6649764776229858\n",
      "Accuracy: 0.4186\n",
      "epoch: 290, training loss: 1.3454627990722656, testing loss: 1.5832923650741577\n",
      "Accuracy: 0.4205\n",
      "epoch: 290, training loss: 1.3383362293243408, testing loss: 1.6044751405715942\n",
      "Accuracy: 0.4620\n",
      "epoch: 290, training loss: 1.2809678316116333, testing loss: 1.7486239671707153\n",
      "Accuracy: 0.3951\n",
      "epoch: 290, training loss: 1.2768893241882324, testing loss: 1.646741271018982\n",
      "Accuracy: 0.3800\n",
      "epoch: 290, training loss: 1.2794932126998901, testing loss: 1.642164945602417\n",
      "Accuracy: 0.4294\n",
      "epoch: 290, training loss: 1.3120625019073486, testing loss: 1.6436498165130615\n",
      "Accuracy: 0.4068\n",
      "epoch: 290, training loss: 1.2267457246780396, testing loss: 1.7614326477050781\n",
      "Accuracy: 0.4153\n",
      "epoch: 290, training loss: 1.2740124464035034, testing loss: 1.7148447036743164\n",
      "Accuracy: 0.4091\n",
      "epoch: 290, training loss: 1.2389682531356812, testing loss: 1.7914758920669556\n",
      "Accuracy: 0.3947\n",
      "epoch: 290, training loss: 1.2565644979476929, testing loss: 1.704313039779663\n",
      "Accuracy: 0.3993\n",
      "epoch: 290, training loss: 1.2988673448562622, testing loss: 1.7483004331588745\n",
      "Accuracy: 0.3935\n",
      "epoch: 290, training loss: 1.2470427751541138, testing loss: 1.836395502090454\n",
      "Accuracy: 0.3790\n",
      "epoch: 290, training loss: 1.2724100351333618, testing loss: 1.6538562774658203\n",
      "Accuracy: 0.4387\n",
      "epoch: 290, training loss: 1.2198536396026611, testing loss: 1.7273139953613281\n",
      "Accuracy: 0.4088\n",
      "epoch: 290, training loss: 1.2340060472488403, testing loss: 1.6133447885513306\n",
      "Accuracy: 0.4714\n",
      "epoch: 290, training loss: 1.2608928680419922, testing loss: 1.7324237823486328\n",
      "Accuracy: 0.4209\n",
      "epoch: 290, training loss: 1.3020131587982178, testing loss: 1.7488148212432861\n",
      "Accuracy: 0.3970\n",
      "epoch: 290, training loss: 1.2242991924285889, testing loss: 1.6301343441009521\n",
      "Accuracy: 0.3891\n",
      "epoch: 290, training loss: 1.2604438066482544, testing loss: 1.5305174589157104\n",
      "Accuracy: 0.4575\n",
      "epoch: 290, training loss: 1.2317358255386353, testing loss: 1.6907848119735718\n",
      "Accuracy: 0.4040\n",
      "epoch: 290, training loss: 1.2881989479064941, testing loss: 1.7212685346603394\n",
      "Accuracy: 0.4384\n",
      "epoch: 290, training loss: 1.3370870351791382, testing loss: 1.7010648250579834\n",
      "Accuracy: 0.3804\n",
      "epoch: 290, training loss: 1.3017247915267944, testing loss: 1.6151378154754639\n",
      "Accuracy: 0.4060\n",
      "epoch: 290, training loss: 1.214743733406067, testing loss: 1.690863013267517\n",
      "Accuracy: 0.4092\n",
      "epoch: 290, training loss: 1.2509442567825317, testing loss: 1.645768165588379\n",
      "Accuracy: 0.4562\n",
      "epoch: 290, training loss: 1.325525164604187, testing loss: 1.825378656387329\n",
      "Accuracy: 0.4075\n",
      "epoch: 290, training loss: 1.324478030204773, testing loss: 1.7837907075881958\n",
      "Accuracy: 0.3994\n",
      "epoch: 290, training loss: 1.267057180404663, testing loss: 1.5953291654586792\n",
      "Accuracy: 0.4152\n",
      "epoch: 290, training loss: 1.3004677295684814, testing loss: 1.7377878427505493\n",
      "Accuracy: 0.4114\n",
      "epoch: 290, training loss: 1.3087466955184937, testing loss: 1.6946628093719482\n",
      "Accuracy: 0.4176\n",
      "epoch: 290, training loss: 1.2138025760650635, testing loss: 1.6798605918884277\n",
      "Accuracy: 0.3960\n",
      "epoch: 290, training loss: 1.3356186151504517, testing loss: 1.6930251121520996\n",
      "Accuracy: 0.4232\n",
      "epoch: 290, training loss: 1.297415018081665, testing loss: 1.7474788427352905\n",
      "Accuracy: 0.4257\n",
      "epoch: 290, training loss: 1.3011493682861328, testing loss: 1.5716389417648315\n",
      "Accuracy: 0.5250\n",
      "epoch: 290, training loss: 1.2804641723632812, testing loss: 1.691631555557251\n",
      "Accuracy: 0.4281\n",
      "epoch: 290, training loss: 1.2478734254837036, testing loss: 1.6624256372451782\n",
      "Accuracy: 0.4420\n",
      "epoch: 290, training loss: 1.2814003229141235, testing loss: 1.6841926574707031\n",
      "Accuracy: 0.3926\n",
      "epoch: 290, training loss: 1.2202696800231934, testing loss: 1.60122549533844\n",
      "Accuracy: 0.4725\n",
      "epoch: 290, training loss: 1.3030991554260254, testing loss: 1.627758502960205\n",
      "Accuracy: 0.4300\n",
      "epoch: 290, training loss: 1.2257611751556396, testing loss: 1.6818513870239258\n",
      "Accuracy: 0.4250\n",
      "epoch: 290, training loss: 1.2822858095169067, testing loss: 1.6961669921875\n",
      "Accuracy: 0.3969\n",
      "epoch: 290, training loss: 1.2472405433654785, testing loss: 1.6750065088272095\n",
      "Accuracy: 0.4290\n",
      "epoch: 290, training loss: 1.246469497680664, testing loss: 1.5640431642532349\n",
      "Accuracy: 0.4371\n",
      "epoch: 290, training loss: 1.206079125404358, testing loss: 1.7443181276321411\n",
      "Accuracy: 0.4338\n",
      "epoch: 290, training loss: 1.2268705368041992, testing loss: 1.6039009094238281\n",
      "Accuracy: 0.4606\n",
      "epoch: 290, training loss: 1.28156578540802, testing loss: 1.6888214349746704\n",
      "Accuracy: 0.4334\n",
      "epoch: 290, training loss: 1.2840808629989624, testing loss: 1.6933846473693848\n",
      "Accuracy: 0.4608\n",
      "epoch: 290, training loss: 1.303342342376709, testing loss: 1.7552680969238281\n",
      "Accuracy: 0.3896\n",
      "epoch: 290, training loss: 1.199835181236267, testing loss: 1.6823452711105347\n",
      "Accuracy: 0.4050\n",
      "epoch: 290, training loss: 1.2725757360458374, testing loss: 1.6857225894927979\n",
      "Accuracy: 0.4455\n",
      "epoch: 290, training loss: 1.273880958557129, testing loss: 1.727961540222168\n",
      "Accuracy: 0.4091\n",
      "epoch: 290, training loss: 1.230718970298767, testing loss: 1.6168118715286255\n",
      "Accuracy: 0.4212\n",
      "epoch: 290, training loss: 1.2496787309646606, testing loss: 1.6407859325408936\n",
      "Accuracy: 0.4119\n",
      "epoch: 290, training loss: 1.2108404636383057, testing loss: 1.7611572742462158\n",
      "Accuracy: 0.4216\n",
      "epoch: 290, training loss: 1.255434274673462, testing loss: 1.7512435913085938\n",
      "Accuracy: 0.4153\n",
      "epoch: 290, training loss: 1.2514592409133911, testing loss: 1.7665401697158813\n",
      "Accuracy: 0.3832\n",
      "epoch: 290, training loss: 1.283780813217163, testing loss: 1.6756092309951782\n",
      "Accuracy: 0.4045\n",
      "epoch: 290, training loss: 1.2993718385696411, testing loss: 1.6860493421554565\n",
      "Accuracy: 0.4032\n",
      "epoch: 290, training loss: 1.2436285018920898, testing loss: 1.7636529207229614\n",
      "Accuracy: 0.3805\n",
      "epoch: 290, training loss: 1.2548586130142212, testing loss: 1.6863237619400024\n",
      "Accuracy: 0.4195\n",
      "epoch: 290, training loss: 1.30977463722229, testing loss: 1.624525547027588\n",
      "Accuracy: 0.4381\n",
      "epoch: 290, training loss: 1.2869939804077148, testing loss: 1.7849342823028564\n",
      "Accuracy: 0.4148\n",
      "epoch: 290, training loss: 1.2674906253814697, testing loss: 1.6809537410736084\n",
      "Accuracy: 0.4217\n",
      "epoch: 290, training loss: 1.2413322925567627, testing loss: 1.7429617643356323\n",
      "Accuracy: 0.4058\n",
      "epoch: 290, training loss: 1.1909143924713135, testing loss: 1.663838505744934\n",
      "Accuracy: 0.4367\n",
      "epoch: 290, training loss: 1.2553186416625977, testing loss: 1.586667776107788\n",
      "Accuracy: 0.4749\n",
      "epoch: 290, training loss: 1.3050340414047241, testing loss: 1.7611833810806274\n",
      "Accuracy: 0.3969\n",
      "epoch: 290, training loss: 1.273384928703308, testing loss: 1.5825941562652588\n",
      "Accuracy: 0.4477\n",
      "epoch: 290, training loss: 1.2470042705535889, testing loss: 1.7096760272979736\n",
      "Accuracy: 0.4267\n",
      "epoch: 290, training loss: 1.283103108406067, testing loss: 1.7389708757400513\n",
      "Accuracy: 0.4268\n",
      "epoch: 290, training loss: 1.2095204591751099, testing loss: 1.7107828855514526\n",
      "Accuracy: 0.4405\n",
      "epoch: 290, training loss: 1.3528026342391968, testing loss: 1.5854121446609497\n",
      "Accuracy: 0.4342\n",
      "epoch: 290, training loss: 1.2907094955444336, testing loss: 1.6610037088394165\n",
      "Accuracy: 0.4050\n",
      "epoch: 290, training loss: 1.2095341682434082, testing loss: 1.6045382022857666\n",
      "Accuracy: 0.4500\n",
      "epoch: 290, training loss: 1.301697850227356, testing loss: 1.6103687286376953\n",
      "Accuracy: 0.4263\n",
      "epoch: 290, training loss: 1.2202763557434082, testing loss: 1.596786379814148\n",
      "Accuracy: 0.4937\n",
      "epoch: 290, training loss: 1.2286485433578491, testing loss: 1.7216953039169312\n",
      "Accuracy: 0.4195\n",
      "epoch: 290, training loss: 1.1860389709472656, testing loss: 1.6294164657592773\n",
      "Accuracy: 0.4387\n",
      "epoch: 290, training loss: 1.2444124221801758, testing loss: 1.6956106424331665\n",
      "Accuracy: 0.4197\n",
      "epoch: 290, training loss: 1.385921835899353, testing loss: 1.8528310060501099\n",
      "Accuracy: 0.4221\n",
      "epoch: 290, training loss: 1.328285574913025, testing loss: 1.681193232536316\n",
      "Accuracy: 0.4254\n",
      "epoch: 290, training loss: 1.2739614248275757, testing loss: 1.7036939859390259\n",
      "Accuracy: 0.4037\n",
      "epoch: 290, training loss: 1.2791523933410645, testing loss: 1.5886893272399902\n",
      "Accuracy: 0.4308\n",
      "epoch: 290, training loss: 1.2352927923202515, testing loss: 1.6962907314300537\n",
      "Accuracy: 0.4182\n",
      "epoch: 290, training loss: 1.2602511644363403, testing loss: 1.615685224533081\n",
      "Accuracy: 0.4202\n",
      "epoch: 290, training loss: 1.2221617698669434, testing loss: 1.7057969570159912\n",
      "Accuracy: 0.4498\n",
      "epoch: 290, training loss: 1.2651622295379639, testing loss: 1.6873053312301636\n",
      "Accuracy: 0.4082\n",
      "epoch: 290, training loss: 1.2246471643447876, testing loss: 1.7505831718444824\n",
      "Accuracy: 0.3782\n",
      "epoch: 300, training loss: 1.2869174480438232, testing loss: 1.6814335584640503\n",
      "Accuracy: 0.4349\n",
      "epoch: 300, training loss: 1.2304136753082275, testing loss: 1.7264540195465088\n",
      "Accuracy: 0.4286\n",
      "epoch: 300, training loss: 1.2785531282424927, testing loss: 1.6154899597167969\n",
      "Accuracy: 0.4139\n",
      "epoch: 300, training loss: 1.2491538524627686, testing loss: 1.558359980583191\n",
      "Accuracy: 0.4373\n",
      "epoch: 300, training loss: 1.304422378540039, testing loss: 1.4672733545303345\n",
      "Accuracy: 0.4885\n",
      "epoch: 300, training loss: 1.2398675680160522, testing loss: 1.586206078529358\n",
      "Accuracy: 0.4879\n",
      "epoch: 300, training loss: 1.2233995199203491, testing loss: 1.8116810321807861\n",
      "Accuracy: 0.4665\n",
      "epoch: 300, training loss: 1.3441662788391113, testing loss: 1.5758413076400757\n",
      "Accuracy: 0.4199\n",
      "epoch: 300, training loss: 1.2546690702438354, testing loss: 1.6771506071090698\n",
      "Accuracy: 0.4224\n",
      "epoch: 300, training loss: 1.2181439399719238, testing loss: 1.6311194896697998\n",
      "Accuracy: 0.4074\n",
      "epoch: 300, training loss: 1.3312499523162842, testing loss: 1.6744914054870605\n",
      "Accuracy: 0.4304\n",
      "epoch: 300, training loss: 1.2705743312835693, testing loss: 1.6289066076278687\n",
      "Accuracy: 0.4367\n",
      "epoch: 300, training loss: 1.3075470924377441, testing loss: 1.632241129875183\n",
      "Accuracy: 0.4308\n",
      "epoch: 300, training loss: 1.3612810373306274, testing loss: 1.7150421142578125\n",
      "Accuracy: 0.4058\n",
      "epoch: 300, training loss: 1.2595767974853516, testing loss: 1.666127324104309\n",
      "Accuracy: 0.4409\n",
      "epoch: 300, training loss: 1.2657431364059448, testing loss: 1.701053261756897\n",
      "Accuracy: 0.4075\n",
      "epoch: 300, training loss: 1.236365795135498, testing loss: 1.7307182550430298\n",
      "Accuracy: 0.4161\n",
      "epoch: 300, training loss: 1.2818244695663452, testing loss: 1.680647373199463\n",
      "Accuracy: 0.4273\n",
      "epoch: 300, training loss: 1.2514379024505615, testing loss: 1.6945909261703491\n",
      "Accuracy: 0.3899\n",
      "epoch: 300, training loss: 1.246976613998413, testing loss: 1.7894096374511719\n",
      "Accuracy: 0.4204\n",
      "epoch: 300, training loss: 1.2785282135009766, testing loss: 1.629057765007019\n",
      "Accuracy: 0.4780\n",
      "epoch: 300, training loss: 1.2126835584640503, testing loss: 1.6387163400650024\n",
      "Accuracy: 0.4045\n",
      "epoch: 300, training loss: 1.2303721904754639, testing loss: 1.714357852935791\n",
      "Accuracy: 0.3985\n",
      "epoch: 300, training loss: 1.203434944152832, testing loss: 1.6705342531204224\n",
      "Accuracy: 0.4204\n",
      "epoch: 300, training loss: 1.2811992168426514, testing loss: 1.6527767181396484\n",
      "Accuracy: 0.4038\n",
      "epoch: 300, training loss: 1.2541933059692383, testing loss: 1.7171471118927002\n",
      "Accuracy: 0.4185\n",
      "epoch: 300, training loss: 1.2852602005004883, testing loss: 1.646608591079712\n",
      "Accuracy: 0.4164\n",
      "epoch: 300, training loss: 1.2270077466964722, testing loss: 1.5923510789871216\n",
      "Accuracy: 0.4295\n",
      "epoch: 300, training loss: 1.2621515989303589, testing loss: 1.6690280437469482\n",
      "Accuracy: 0.4392\n",
      "epoch: 300, training loss: 1.3213074207305908, testing loss: 1.6184147596359253\n",
      "Accuracy: 0.4209\n",
      "epoch: 300, training loss: 1.2845855951309204, testing loss: 1.5696138143539429\n",
      "Accuracy: 0.4263\n",
      "epoch: 300, training loss: 1.2771053314208984, testing loss: 1.7274271249771118\n",
      "Accuracy: 0.3987\n",
      "epoch: 300, training loss: 1.3430498838424683, testing loss: 1.7615073919296265\n",
      "Accuracy: 0.3666\n",
      "epoch: 300, training loss: 1.2803927659988403, testing loss: 1.6184301376342773\n",
      "Accuracy: 0.4045\n",
      "epoch: 300, training loss: 1.284227728843689, testing loss: 1.6157162189483643\n",
      "Accuracy: 0.4379\n",
      "epoch: 300, training loss: 1.347011923789978, testing loss: 1.668686866760254\n",
      "Accuracy: 0.4039\n",
      "epoch: 300, training loss: 1.2361849546432495, testing loss: 1.8299962282180786\n",
      "Accuracy: 0.4361\n",
      "epoch: 300, training loss: 1.3040589094161987, testing loss: 1.7879137992858887\n",
      "Accuracy: 0.4286\n",
      "epoch: 300, training loss: 1.362026333808899, testing loss: 1.7421168088912964\n",
      "Accuracy: 0.4043\n",
      "epoch: 300, training loss: 1.2687153816223145, testing loss: 1.6430338621139526\n",
      "Accuracy: 0.4491\n",
      "epoch: 300, training loss: 1.256083369255066, testing loss: 1.7298249006271362\n",
      "Accuracy: 0.3968\n",
      "epoch: 300, training loss: 1.2604320049285889, testing loss: 1.6229479312896729\n",
      "Accuracy: 0.4351\n",
      "epoch: 300, training loss: 1.2661911249160767, testing loss: 1.7900558710098267\n",
      "Accuracy: 0.4231\n",
      "epoch: 300, training loss: 1.271353006362915, testing loss: 1.9114376306533813\n",
      "Accuracy: 0.3601\n",
      "epoch: 300, training loss: 1.3013339042663574, testing loss: 1.6389524936676025\n",
      "Accuracy: 0.4478\n",
      "epoch: 300, training loss: 1.2752084732055664, testing loss: 1.5433807373046875\n",
      "Accuracy: 0.4331\n",
      "epoch: 300, training loss: 1.2985413074493408, testing loss: 1.684727668762207\n",
      "Accuracy: 0.4212\n",
      "epoch: 300, training loss: 1.2715429067611694, testing loss: 1.7002686262130737\n",
      "Accuracy: 0.3904\n",
      "epoch: 300, training loss: 1.3409337997436523, testing loss: 1.6817861795425415\n",
      "Accuracy: 0.4377\n",
      "epoch: 300, training loss: 1.2147637605667114, testing loss: 1.637925148010254\n",
      "Accuracy: 0.4276\n",
      "epoch: 300, training loss: 1.2900978326797485, testing loss: 1.7767833471298218\n",
      "Accuracy: 0.3973\n",
      "epoch: 300, training loss: 1.382289171218872, testing loss: 1.6315679550170898\n",
      "Accuracy: 0.4161\n",
      "epoch: 300, training loss: 1.318480134010315, testing loss: 1.7235969305038452\n",
      "Accuracy: 0.3841\n",
      "epoch: 300, training loss: 1.276506781578064, testing loss: 1.715393304824829\n",
      "Accuracy: 0.4078\n",
      "epoch: 300, training loss: 1.2350572347640991, testing loss: 1.7205123901367188\n",
      "Accuracy: 0.4406\n",
      "epoch: 300, training loss: 1.3005834817886353, testing loss: 1.6615544557571411\n",
      "Accuracy: 0.4773\n",
      "epoch: 300, training loss: 1.1975470781326294, testing loss: 1.6591845750808716\n",
      "Accuracy: 0.4063\n",
      "epoch: 300, training loss: 1.2555084228515625, testing loss: 1.8336807489395142\n",
      "Accuracy: 0.3387\n",
      "epoch: 300, training loss: 1.354905366897583, testing loss: 1.726222038269043\n",
      "Accuracy: 0.4037\n",
      "epoch: 300, training loss: 1.3062177896499634, testing loss: 1.8438913822174072\n",
      "Accuracy: 0.3643\n",
      "epoch: 300, training loss: 1.2735062837600708, testing loss: 1.652912974357605\n",
      "Accuracy: 0.4563\n",
      "epoch: 300, training loss: 1.304360270500183, testing loss: 1.7979711294174194\n",
      "Accuracy: 0.4444\n",
      "epoch: 300, training loss: 1.253995418548584, testing loss: 1.804575800895691\n",
      "Accuracy: 0.4013\n",
      "epoch: 300, training loss: 1.2774947881698608, testing loss: 1.7241894006729126\n",
      "Accuracy: 0.4182\n",
      "epoch: 300, training loss: 1.2458423376083374, testing loss: 1.6340641975402832\n",
      "Accuracy: 0.4502\n",
      "epoch: 300, training loss: 1.3672040700912476, testing loss: 1.6006137132644653\n",
      "Accuracy: 0.4237\n",
      "epoch: 300, training loss: 1.2844388484954834, testing loss: 1.6000206470489502\n",
      "Accuracy: 0.4068\n",
      "epoch: 300, training loss: 1.247058629989624, testing loss: 1.5971382856369019\n",
      "Accuracy: 0.4561\n",
      "epoch: 300, training loss: 1.2933597564697266, testing loss: 1.6750961542129517\n",
      "Accuracy: 0.4106\n",
      "epoch: 300, training loss: 1.3070849180221558, testing loss: 1.6340699195861816\n",
      "Accuracy: 0.4766\n",
      "epoch: 300, training loss: 1.2499966621398926, testing loss: 1.5350871086120605\n",
      "Accuracy: 0.4462\n",
      "epoch: 300, training loss: 1.3194278478622437, testing loss: 1.6918532848358154\n",
      "Accuracy: 0.3969\n",
      "epoch: 300, training loss: 1.255752682685852, testing loss: 1.619822382926941\n",
      "Accuracy: 0.4146\n",
      "epoch: 300, training loss: 1.3110833168029785, testing loss: 1.6830687522888184\n",
      "Accuracy: 0.4062\n",
      "epoch: 300, training loss: 1.2108304500579834, testing loss: 1.6621276140213013\n",
      "Accuracy: 0.4128\n",
      "epoch: 300, training loss: 1.2828326225280762, testing loss: 1.654514193534851\n",
      "Accuracy: 0.4519\n",
      "epoch: 300, training loss: 1.2732435464859009, testing loss: 1.7009823322296143\n",
      "Accuracy: 0.4290\n",
      "epoch: 300, training loss: 1.295527696609497, testing loss: 1.7523870468139648\n",
      "Accuracy: 0.3925\n",
      "epoch: 300, training loss: 1.2436171770095825, testing loss: 1.667471170425415\n",
      "Accuracy: 0.4104\n",
      "epoch: 300, training loss: 1.222222089767456, testing loss: 1.622426152229309\n",
      "Accuracy: 0.4387\n",
      "epoch: 300, training loss: 1.2287436723709106, testing loss: 1.637641429901123\n",
      "Accuracy: 0.3879\n",
      "epoch: 300, training loss: 1.2634488344192505, testing loss: 1.9811758995056152\n",
      "Accuracy: 0.4013\n",
      "epoch: 300, training loss: 1.2790979146957397, testing loss: 1.503623604774475\n",
      "Accuracy: 0.4474\n",
      "epoch: 300, training loss: 1.2252860069274902, testing loss: 1.7775731086730957\n",
      "Accuracy: 0.4328\n",
      "epoch: 300, training loss: 1.2335693836212158, testing loss: 1.6705175638198853\n",
      "Accuracy: 0.4328\n",
      "epoch: 300, training loss: 1.2456028461456299, testing loss: 1.6230865716934204\n",
      "Accuracy: 0.4613\n",
      "epoch: 300, training loss: 1.2931312322616577, testing loss: 1.5729211568832397\n",
      "Accuracy: 0.4549\n",
      "epoch: 300, training loss: 1.2399879693984985, testing loss: 1.6583960056304932\n",
      "Accuracy: 0.4385\n",
      "epoch: 300, training loss: 1.3554115295410156, testing loss: 1.7145887613296509\n",
      "Accuracy: 0.4193\n",
      "epoch: 300, training loss: 1.2420060634613037, testing loss: 1.7009297609329224\n",
      "Accuracy: 0.4627\n",
      "epoch: 300, training loss: 1.3110634088516235, testing loss: 1.6613454818725586\n",
      "Accuracy: 0.4291\n",
      "epoch: 300, training loss: 1.2929644584655762, testing loss: 1.763069987297058\n",
      "Accuracy: 0.3742\n",
      "epoch: 300, training loss: 1.213749647140503, testing loss: 1.6617088317871094\n",
      "Accuracy: 0.4340\n",
      "epoch: 300, training loss: 1.2896409034729004, testing loss: 1.6262186765670776\n",
      "Accuracy: 0.4400\n",
      "epoch: 300, training loss: 1.2778278589248657, testing loss: 1.6961066722869873\n",
      "Accuracy: 0.4069\n",
      "epoch: 300, training loss: 1.2287126779556274, testing loss: 1.6417826414108276\n",
      "Accuracy: 0.4464\n",
      "epoch: 300, training loss: 1.2289941310882568, testing loss: 1.6882619857788086\n",
      "Accuracy: 0.4299\n",
      "epoch: 300, training loss: 1.3373963832855225, testing loss: 1.7752685546875\n",
      "Accuracy: 0.3982\n",
      "epoch: 300, training loss: 1.2584627866744995, testing loss: 1.7069954872131348\n",
      "Accuracy: 0.4130\n",
      "epoch: 300, training loss: 1.294555425643921, testing loss: 1.636191725730896\n",
      "Accuracy: 0.4785\n",
      "epoch: 310, training loss: 1.3144187927246094, testing loss: 1.6139891147613525\n",
      "Accuracy: 0.4077\n",
      "epoch: 310, training loss: 1.2907154560089111, testing loss: 1.6948977708816528\n",
      "Accuracy: 0.3986\n",
      "epoch: 310, training loss: 1.3466483354568481, testing loss: 1.653475284576416\n",
      "Accuracy: 0.4111\n",
      "epoch: 310, training loss: 1.2557079792022705, testing loss: 1.781270980834961\n",
      "Accuracy: 0.4092\n",
      "epoch: 310, training loss: 1.2182133197784424, testing loss: 1.6529364585876465\n",
      "Accuracy: 0.4000\n",
      "epoch: 310, training loss: 1.3139675855636597, testing loss: 1.7508087158203125\n",
      "Accuracy: 0.3833\n",
      "epoch: 310, training loss: 1.2694737911224365, testing loss: 1.6071614027023315\n",
      "Accuracy: 0.4194\n",
      "epoch: 310, training loss: 1.2417279481887817, testing loss: 1.667255163192749\n",
      "Accuracy: 0.4207\n",
      "epoch: 310, training loss: 1.2578080892562866, testing loss: 1.6130998134613037\n",
      "Accuracy: 0.4673\n",
      "epoch: 310, training loss: 1.2268545627593994, testing loss: 1.8083293437957764\n",
      "Accuracy: 0.4056\n",
      "epoch: 310, training loss: 1.250737190246582, testing loss: 1.7724921703338623\n",
      "Accuracy: 0.4403\n",
      "epoch: 310, training loss: 1.2870441675186157, testing loss: 1.6373580694198608\n",
      "Accuracy: 0.4071\n",
      "epoch: 310, training loss: 1.2619913816452026, testing loss: 1.710037112236023\n",
      "Accuracy: 0.4136\n",
      "epoch: 310, training loss: 1.1828373670578003, testing loss: 1.687207818031311\n",
      "Accuracy: 0.4403\n",
      "epoch: 310, training loss: 1.2774255275726318, testing loss: 1.6829779148101807\n",
      "Accuracy: 0.3937\n",
      "epoch: 310, training loss: 1.2177648544311523, testing loss: 1.6924489736557007\n",
      "Accuracy: 0.4389\n",
      "epoch: 310, training loss: 1.2273091077804565, testing loss: 1.7217004299163818\n",
      "Accuracy: 0.4427\n",
      "epoch: 310, training loss: 1.2838654518127441, testing loss: 1.8297066688537598\n",
      "Accuracy: 0.4172\n",
      "epoch: 310, training loss: 1.2962177991867065, testing loss: 1.704677700996399\n",
      "Accuracy: 0.4007\n",
      "epoch: 310, training loss: 1.263222575187683, testing loss: 1.5633735656738281\n",
      "Accuracy: 0.4581\n",
      "epoch: 310, training loss: 1.2944037914276123, testing loss: 1.51265549659729\n",
      "Accuracy: 0.4603\n",
      "epoch: 310, training loss: 1.3015613555908203, testing loss: 1.6744649410247803\n",
      "Accuracy: 0.4179\n",
      "epoch: 310, training loss: 1.2892229557037354, testing loss: 1.6548187732696533\n",
      "Accuracy: 0.3994\n",
      "epoch: 310, training loss: 1.3302059173583984, testing loss: 1.7487980127334595\n",
      "Accuracy: 0.4037\n",
      "epoch: 310, training loss: 1.3553814888000488, testing loss: 1.667041540145874\n",
      "Accuracy: 0.4058\n",
      "epoch: 310, training loss: 1.2396398782730103, testing loss: 1.7785818576812744\n",
      "Accuracy: 0.3957\n",
      "epoch: 310, training loss: 1.297739863395691, testing loss: 1.7404067516326904\n",
      "Accuracy: 0.4073\n",
      "epoch: 310, training loss: 1.260596752166748, testing loss: 1.6574558019638062\n",
      "Accuracy: 0.4158\n",
      "epoch: 310, training loss: 1.2011868953704834, testing loss: 1.7670249938964844\n",
      "Accuracy: 0.3892\n",
      "epoch: 310, training loss: 1.3055459260940552, testing loss: 1.6231579780578613\n",
      "Accuracy: 0.4462\n",
      "epoch: 310, training loss: 1.2809165716171265, testing loss: 1.6990917921066284\n",
      "Accuracy: 0.4180\n",
      "epoch: 310, training loss: 1.224443793296814, testing loss: 1.6408498287200928\n",
      "Accuracy: 0.4551\n",
      "epoch: 310, training loss: 1.2823429107666016, testing loss: 1.6508690118789673\n",
      "Accuracy: 0.4281\n",
      "epoch: 310, training loss: 1.2683000564575195, testing loss: 1.6817145347595215\n",
      "Accuracy: 0.4006\n",
      "epoch: 310, training loss: 1.2266875505447388, testing loss: 1.6600390672683716\n",
      "Accuracy: 0.4399\n",
      "epoch: 310, training loss: 1.228524088859558, testing loss: 1.7551319599151611\n",
      "Accuracy: 0.3919\n",
      "epoch: 310, training loss: 1.2225489616394043, testing loss: 1.6585092544555664\n",
      "Accuracy: 0.4140\n",
      "epoch: 310, training loss: 1.353621482849121, testing loss: 1.647792935371399\n",
      "Accuracy: 0.4555\n",
      "epoch: 310, training loss: 1.1776050329208374, testing loss: 1.7450007200241089\n",
      "Accuracy: 0.4164\n",
      "epoch: 310, training loss: 1.3555798530578613, testing loss: 1.6635538339614868\n",
      "Accuracy: 0.4252\n",
      "epoch: 310, training loss: 1.2653989791870117, testing loss: 1.9086201190948486\n",
      "Accuracy: 0.3919\n",
      "epoch: 310, training loss: 1.1988840103149414, testing loss: 1.7106568813323975\n",
      "Accuracy: 0.3963\n",
      "epoch: 310, training loss: 1.2785338163375854, testing loss: 1.6635345220565796\n",
      "Accuracy: 0.4317\n",
      "epoch: 310, training loss: 1.2979265451431274, testing loss: 1.718348503112793\n",
      "Accuracy: 0.3866\n",
      "epoch: 310, training loss: 1.3312647342681885, testing loss: 1.6496291160583496\n",
      "Accuracy: 0.4742\n",
      "epoch: 310, training loss: 1.2423200607299805, testing loss: 1.6712595224380493\n",
      "Accuracy: 0.4303\n",
      "epoch: 310, training loss: 1.2701616287231445, testing loss: 1.6718404293060303\n",
      "Accuracy: 0.4281\n",
      "epoch: 310, training loss: 1.2859405279159546, testing loss: 1.711076021194458\n",
      "Accuracy: 0.3868\n",
      "epoch: 310, training loss: 1.2870062589645386, testing loss: 1.6496498584747314\n",
      "Accuracy: 0.4073\n",
      "epoch: 310, training loss: 1.1887799501419067, testing loss: 1.7629005908966064\n",
      "Accuracy: 0.4073\n",
      "epoch: 310, training loss: 1.2340539693832397, testing loss: 1.573956847190857\n",
      "Accuracy: 0.4181\n",
      "epoch: 310, training loss: 1.24497389793396, testing loss: 1.7622365951538086\n",
      "Accuracy: 0.3742\n",
      "epoch: 310, training loss: 1.252327799797058, testing loss: 1.6098319292068481\n",
      "Accuracy: 0.4211\n",
      "epoch: 310, training loss: 1.2725028991699219, testing loss: 1.7652593851089478\n",
      "Accuracy: 0.3940\n",
      "epoch: 310, training loss: 1.3165841102600098, testing loss: 1.591766119003296\n",
      "Accuracy: 0.4365\n",
      "epoch: 310, training loss: 1.2202601432800293, testing loss: 1.6828500032424927\n",
      "Accuracy: 0.4121\n",
      "epoch: 310, training loss: 1.2253057956695557, testing loss: 1.8504999876022339\n",
      "Accuracy: 0.4178\n",
      "epoch: 310, training loss: 1.2803977727890015, testing loss: 1.6571255922317505\n",
      "Accuracy: 0.4216\n",
      "epoch: 310, training loss: 1.2449390888214111, testing loss: 1.6590731143951416\n",
      "Accuracy: 0.4198\n",
      "epoch: 310, training loss: 1.263054609298706, testing loss: 1.6470794677734375\n",
      "Accuracy: 0.4531\n",
      "epoch: 310, training loss: 1.2994310855865479, testing loss: 1.667414903640747\n",
      "Accuracy: 0.4189\n",
      "epoch: 310, training loss: 1.2760604619979858, testing loss: 1.5951319932937622\n",
      "Accuracy: 0.4492\n",
      "epoch: 310, training loss: 1.2628942728042603, testing loss: 1.7386761903762817\n",
      "Accuracy: 0.4328\n",
      "epoch: 310, training loss: 1.3058291673660278, testing loss: 1.6770570278167725\n",
      "Accuracy: 0.4286\n",
      "epoch: 310, training loss: 1.1973241567611694, testing loss: 1.5924690961837769\n",
      "Accuracy: 0.4491\n",
      "epoch: 310, training loss: 1.332818627357483, testing loss: 1.6652458906173706\n",
      "Accuracy: 0.4202\n",
      "epoch: 310, training loss: 1.3295561075210571, testing loss: 1.6583002805709839\n",
      "Accuracy: 0.4331\n",
      "epoch: 310, training loss: 1.2406940460205078, testing loss: 1.6891303062438965\n",
      "Accuracy: 0.3974\n",
      "epoch: 310, training loss: 1.2476401329040527, testing loss: 1.626344084739685\n",
      "Accuracy: 0.3906\n",
      "epoch: 310, training loss: 1.2535854578018188, testing loss: 1.6346638202667236\n",
      "Accuracy: 0.4112\n",
      "epoch: 310, training loss: 1.3418418169021606, testing loss: 1.770383596420288\n",
      "Accuracy: 0.4124\n",
      "epoch: 310, training loss: 1.250165343284607, testing loss: 1.6628400087356567\n",
      "Accuracy: 0.3799\n",
      "epoch: 310, training loss: 1.2726705074310303, testing loss: 1.7047746181488037\n",
      "Accuracy: 0.3994\n",
      "epoch: 310, training loss: 1.218658447265625, testing loss: 1.5937368869781494\n",
      "Accuracy: 0.3905\n",
      "epoch: 310, training loss: 1.3470934629440308, testing loss: 1.8015457391738892\n",
      "Accuracy: 0.4433\n",
      "epoch: 310, training loss: 1.3058316707611084, testing loss: 1.7494672536849976\n",
      "Accuracy: 0.3642\n",
      "epoch: 310, training loss: 1.3101972341537476, testing loss: 1.7339763641357422\n",
      "Accuracy: 0.4487\n",
      "epoch: 310, training loss: 1.2128177881240845, testing loss: 1.7457093000411987\n",
      "Accuracy: 0.3798\n",
      "epoch: 310, training loss: 1.3078895807266235, testing loss: 1.711155891418457\n",
      "Accuracy: 0.4481\n",
      "epoch: 310, training loss: 1.2276959419250488, testing loss: 1.8395737409591675\n",
      "Accuracy: 0.3883\n",
      "epoch: 310, training loss: 1.2961926460266113, testing loss: 1.6531553268432617\n",
      "Accuracy: 0.4033\n",
      "epoch: 310, training loss: 1.2989047765731812, testing loss: 1.6198625564575195\n",
      "Accuracy: 0.4045\n",
      "epoch: 310, training loss: 1.3083257675170898, testing loss: 1.6316688060760498\n",
      "Accuracy: 0.4077\n",
      "epoch: 310, training loss: 1.2755998373031616, testing loss: 1.6152602434158325\n",
      "Accuracy: 0.4528\n",
      "epoch: 310, training loss: 1.2508842945098877, testing loss: 1.7348997592926025\n",
      "Accuracy: 0.4027\n",
      "epoch: 310, training loss: 1.3387335538864136, testing loss: 1.6377586126327515\n",
      "Accuracy: 0.3877\n",
      "epoch: 310, training loss: 1.2477178573608398, testing loss: 1.7461978197097778\n",
      "Accuracy: 0.4123\n",
      "epoch: 310, training loss: 1.2897244691848755, testing loss: 1.6365511417388916\n",
      "Accuracy: 0.4385\n",
      "epoch: 310, training loss: 1.1806981563568115, testing loss: 1.8065392971038818\n",
      "Accuracy: 0.4136\n",
      "epoch: 310, training loss: 1.2945213317871094, testing loss: 1.6846089363098145\n",
      "Accuracy: 0.4681\n",
      "epoch: 310, training loss: 1.2928121089935303, testing loss: 1.7293134927749634\n",
      "Accuracy: 0.4092\n",
      "epoch: 310, training loss: 1.3368645906448364, testing loss: 1.667955756187439\n",
      "Accuracy: 0.4466\n",
      "epoch: 310, training loss: 1.316805362701416, testing loss: 1.694762110710144\n",
      "Accuracy: 0.4077\n",
      "epoch: 310, training loss: 1.2980616092681885, testing loss: 1.759704351425171\n",
      "Accuracy: 0.3924\n",
      "epoch: 310, training loss: 1.2378718852996826, testing loss: 1.603496789932251\n",
      "Accuracy: 0.4459\n",
      "epoch: 310, training loss: 1.2593227624893188, testing loss: 1.6630443334579468\n",
      "Accuracy: 0.4328\n",
      "epoch: 310, training loss: 1.191450595855713, testing loss: 1.6794646978378296\n",
      "Accuracy: 0.3980\n",
      "epoch: 310, training loss: 1.2341351509094238, testing loss: 1.533888578414917\n",
      "Accuracy: 0.4484\n",
      "epoch: 310, training loss: 1.2917232513427734, testing loss: 1.5872639417648315\n",
      "Accuracy: 0.4633\n",
      "epoch: 310, training loss: 1.2683337926864624, testing loss: 1.6915416717529297\n",
      "Accuracy: 0.4281\n",
      "epoch: 320, training loss: 1.2365375757217407, testing loss: 1.6624393463134766\n",
      "Accuracy: 0.4175\n",
      "epoch: 320, training loss: 1.2008851766586304, testing loss: 1.8003826141357422\n",
      "Accuracy: 0.3878\n",
      "epoch: 320, training loss: 1.2286194562911987, testing loss: 1.7941370010375977\n",
      "Accuracy: 0.4013\n",
      "epoch: 320, training loss: 1.2395521402359009, testing loss: 1.6162950992584229\n",
      "Accuracy: 0.4270\n",
      "epoch: 320, training loss: 1.28805410861969, testing loss: 1.655689001083374\n",
      "Accuracy: 0.4140\n",
      "epoch: 320, training loss: 1.2571768760681152, testing loss: 1.8004671335220337\n",
      "Accuracy: 0.4153\n",
      "epoch: 320, training loss: 1.2406048774719238, testing loss: 1.5571975708007812\n",
      "Accuracy: 0.4563\n",
      "epoch: 320, training loss: 1.2819280624389648, testing loss: 1.7503759860992432\n",
      "Accuracy: 0.4245\n",
      "epoch: 320, training loss: 1.270805835723877, testing loss: 1.7772018909454346\n",
      "Accuracy: 0.3854\n",
      "epoch: 320, training loss: 1.294546127319336, testing loss: 1.5980466604232788\n",
      "Accuracy: 0.3871\n",
      "epoch: 320, training loss: 1.2800337076187134, testing loss: 1.4877468347549438\n",
      "Accuracy: 0.4735\n",
      "epoch: 320, training loss: 1.2653783559799194, testing loss: 1.7036479711532593\n",
      "Accuracy: 0.4195\n",
      "epoch: 320, training loss: 1.256807804107666, testing loss: 1.7349635362625122\n",
      "Accuracy: 0.4290\n",
      "epoch: 320, training loss: 1.2172808647155762, testing loss: 1.6575887203216553\n",
      "Accuracy: 0.4108\n",
      "epoch: 320, training loss: 1.2232586145401, testing loss: 1.5075174570083618\n",
      "Accuracy: 0.4780\n",
      "epoch: 320, training loss: 1.3259185552597046, testing loss: 1.6543545722961426\n",
      "Accuracy: 0.4290\n",
      "epoch: 320, training loss: 1.2548271417617798, testing loss: 1.6497193574905396\n",
      "Accuracy: 0.4774\n",
      "epoch: 320, training loss: 1.296809196472168, testing loss: 1.6360806226730347\n",
      "Accuracy: 0.3785\n",
      "epoch: 320, training loss: 1.2621631622314453, testing loss: 1.647109866142273\n",
      "Accuracy: 0.4602\n",
      "epoch: 320, training loss: 1.2914646863937378, testing loss: 1.7876014709472656\n",
      "Accuracy: 0.3701\n",
      "epoch: 320, training loss: 1.2273955345153809, testing loss: 1.6526328325271606\n",
      "Accuracy: 0.4158\n",
      "epoch: 320, training loss: 1.2878230810165405, testing loss: 1.7759190797805786\n",
      "Accuracy: 0.4266\n",
      "epoch: 320, training loss: 1.2032393217086792, testing loss: 1.7490907907485962\n",
      "Accuracy: 0.4000\n",
      "epoch: 320, training loss: 1.268595814704895, testing loss: 1.6386022567749023\n",
      "Accuracy: 0.4221\n",
      "epoch: 320, training loss: 1.3240691423416138, testing loss: 1.6038752794265747\n",
      "Accuracy: 0.4211\n",
      "epoch: 320, training loss: 1.2920295000076294, testing loss: 1.6649380922317505\n",
      "Accuracy: 0.4430\n",
      "epoch: 320, training loss: 1.2184898853302002, testing loss: 1.7673981189727783\n",
      "Accuracy: 0.3813\n",
      "epoch: 320, training loss: 1.294429063796997, testing loss: 1.7645930051803589\n",
      "Accuracy: 0.3662\n",
      "epoch: 320, training loss: 1.2837822437286377, testing loss: 1.6216984987258911\n",
      "Accuracy: 0.4110\n",
      "epoch: 320, training loss: 1.2229039669036865, testing loss: 1.7782793045043945\n",
      "Accuracy: 0.3409\n",
      "epoch: 320, training loss: 1.278490662574768, testing loss: 1.695841908454895\n",
      "Accuracy: 0.4551\n",
      "epoch: 320, training loss: 1.180313229560852, testing loss: 1.70588219165802\n",
      "Accuracy: 0.4110\n",
      "epoch: 320, training loss: 1.2281246185302734, testing loss: 1.6443159580230713\n",
      "Accuracy: 0.4826\n",
      "epoch: 320, training loss: 1.2761150598526, testing loss: 1.5474259853363037\n",
      "Accuracy: 0.4778\n",
      "epoch: 320, training loss: 1.2636994123458862, testing loss: 1.8531547784805298\n",
      "Accuracy: 0.4054\n",
      "epoch: 320, training loss: 1.2574827671051025, testing loss: 1.692294955253601\n",
      "Accuracy: 0.4294\n",
      "epoch: 320, training loss: 1.295424222946167, testing loss: 1.6984375715255737\n",
      "Accuracy: 0.4007\n",
      "epoch: 320, training loss: 1.1458427906036377, testing loss: 1.740132212638855\n",
      "Accuracy: 0.3942\n",
      "epoch: 320, training loss: 1.282595157623291, testing loss: 1.6110944747924805\n",
      "Accuracy: 0.4420\n",
      "epoch: 320, training loss: 1.2555562257766724, testing loss: 1.6945332288742065\n",
      "Accuracy: 0.3544\n",
      "epoch: 320, training loss: 1.214078426361084, testing loss: 1.59980309009552\n",
      "Accuracy: 0.4116\n",
      "epoch: 320, training loss: 1.3037923574447632, testing loss: 1.652486801147461\n",
      "Accuracy: 0.4110\n",
      "epoch: 320, training loss: 1.247795820236206, testing loss: 1.657236099243164\n",
      "Accuracy: 0.4490\n",
      "epoch: 320, training loss: 1.2866578102111816, testing loss: 1.6491105556488037\n",
      "Accuracy: 0.4539\n",
      "epoch: 320, training loss: 1.2295671701431274, testing loss: 1.5665258169174194\n",
      "Accuracy: 0.4543\n",
      "epoch: 320, training loss: 1.2548227310180664, testing loss: 1.6480300426483154\n",
      "Accuracy: 0.4212\n",
      "epoch: 320, training loss: 1.2482982873916626, testing loss: 1.5318562984466553\n",
      "Accuracy: 0.4456\n",
      "epoch: 320, training loss: 1.3101258277893066, testing loss: 1.561150074005127\n",
      "Accuracy: 0.4824\n",
      "epoch: 320, training loss: 1.201093316078186, testing loss: 1.6232404708862305\n",
      "Accuracy: 0.4467\n",
      "epoch: 320, training loss: 1.2495591640472412, testing loss: 1.6471894979476929\n",
      "Accuracy: 0.4183\n",
      "epoch: 320, training loss: 1.260773777961731, testing loss: 1.6409937143325806\n",
      "Accuracy: 0.3980\n",
      "epoch: 320, training loss: 1.243753433227539, testing loss: 1.7444268465042114\n",
      "Accuracy: 0.4863\n",
      "epoch: 320, training loss: 1.2028608322143555, testing loss: 1.779003381729126\n",
      "Accuracy: 0.4185\n",
      "epoch: 320, training loss: 1.2707664966583252, testing loss: 1.5593557357788086\n",
      "Accuracy: 0.4567\n",
      "epoch: 320, training loss: 1.2733347415924072, testing loss: 1.6969761848449707\n",
      "Accuracy: 0.4222\n",
      "epoch: 320, training loss: 1.3629961013793945, testing loss: 1.7382802963256836\n",
      "Accuracy: 0.3969\n",
      "epoch: 320, training loss: 1.252402901649475, testing loss: 1.6669743061065674\n",
      "Accuracy: 0.3905\n",
      "epoch: 320, training loss: 1.2395880222320557, testing loss: 1.6703238487243652\n",
      "Accuracy: 0.4169\n",
      "epoch: 320, training loss: 1.2976983785629272, testing loss: 1.7155035734176636\n",
      "Accuracy: 0.4109\n",
      "epoch: 320, training loss: 1.2677289247512817, testing loss: 1.7228809595108032\n",
      "Accuracy: 0.3844\n",
      "epoch: 320, training loss: 1.2275075912475586, testing loss: 1.6266627311706543\n",
      "Accuracy: 0.4361\n",
      "epoch: 320, training loss: 1.3008092641830444, testing loss: 1.683695912361145\n",
      "Accuracy: 0.3974\n",
      "epoch: 320, training loss: 1.2404743432998657, testing loss: 1.776798129081726\n",
      "Accuracy: 0.4230\n",
      "epoch: 320, training loss: 1.2373253107070923, testing loss: 1.658020257949829\n",
      "Accuracy: 0.4169\n",
      "epoch: 320, training loss: 1.30635404586792, testing loss: 1.6875962018966675\n",
      "Accuracy: 0.4148\n",
      "epoch: 320, training loss: 1.2417558431625366, testing loss: 1.5937556028366089\n",
      "Accuracy: 0.4082\n",
      "epoch: 320, training loss: 1.285518765449524, testing loss: 1.7097622156143188\n",
      "Accuracy: 0.4437\n",
      "epoch: 320, training loss: 1.2702116966247559, testing loss: 1.6249126195907593\n",
      "Accuracy: 0.3849\n",
      "epoch: 320, training loss: 1.2378389835357666, testing loss: 1.7299113273620605\n",
      "Accuracy: 0.4290\n",
      "epoch: 320, training loss: 1.4382710456848145, testing loss: 1.5767759084701538\n",
      "Accuracy: 0.4508\n",
      "epoch: 320, training loss: 1.2362384796142578, testing loss: 1.609899640083313\n",
      "Accuracy: 0.4267\n",
      "epoch: 320, training loss: 1.2588911056518555, testing loss: 1.760484218597412\n",
      "Accuracy: 0.3574\n",
      "epoch: 320, training loss: 1.2360119819641113, testing loss: 1.7419533729553223\n",
      "Accuracy: 0.3780\n",
      "epoch: 320, training loss: 1.2042919397354126, testing loss: 1.7343597412109375\n",
      "Accuracy: 0.4306\n",
      "epoch: 320, training loss: 1.2596439123153687, testing loss: 1.6670467853546143\n",
      "Accuracy: 0.4123\n",
      "epoch: 320, training loss: 1.2516957521438599, testing loss: 1.4985828399658203\n",
      "Accuracy: 0.4571\n",
      "epoch: 320, training loss: 1.2368820905685425, testing loss: 1.5688735246658325\n",
      "Accuracy: 0.4528\n",
      "epoch: 320, training loss: 1.2371538877487183, testing loss: 1.742173433303833\n",
      "Accuracy: 0.3972\n",
      "epoch: 320, training loss: 1.2281537055969238, testing loss: 1.7456650733947754\n",
      "Accuracy: 0.4178\n",
      "epoch: 320, training loss: 1.29374361038208, testing loss: 1.6590591669082642\n",
      "Accuracy: 0.4067\n",
      "epoch: 320, training loss: 1.2746306657791138, testing loss: 1.7774193286895752\n",
      "Accuracy: 0.3920\n",
      "epoch: 320, training loss: 1.1929643154144287, testing loss: 1.763379454612732\n",
      "Accuracy: 0.4419\n",
      "epoch: 320, training loss: 1.1866294145584106, testing loss: 1.7465814352035522\n",
      "Accuracy: 0.4051\n",
      "epoch: 320, training loss: 1.336303949356079, testing loss: 1.5447697639465332\n",
      "Accuracy: 0.5104\n",
      "epoch: 320, training loss: 1.21895432472229, testing loss: 1.5883899927139282\n",
      "Accuracy: 0.4301\n",
      "epoch: 320, training loss: 1.3238617181777954, testing loss: 1.7298041582107544\n",
      "Accuracy: 0.4069\n",
      "epoch: 320, training loss: 1.2957288026809692, testing loss: 1.6141945123672485\n",
      "Accuracy: 0.4669\n",
      "epoch: 320, training loss: 1.322037935256958, testing loss: 1.6333214044570923\n",
      "Accuracy: 0.4593\n",
      "epoch: 320, training loss: 1.3053230047225952, testing loss: 1.6153746843338013\n",
      "Accuracy: 0.3906\n",
      "epoch: 320, training loss: 1.2468087673187256, testing loss: 1.6366370916366577\n",
      "Accuracy: 0.4263\n",
      "epoch: 320, training loss: 1.2622696161270142, testing loss: 1.613523244857788\n",
      "Accuracy: 0.4167\n",
      "epoch: 320, training loss: 1.2645769119262695, testing loss: 1.7546021938323975\n",
      "Accuracy: 0.4058\n",
      "epoch: 320, training loss: 1.2705745697021484, testing loss: 1.6624623537063599\n",
      "Accuracy: 0.4339\n",
      "epoch: 320, training loss: 1.3413527011871338, testing loss: 1.6343015432357788\n",
      "Accuracy: 0.4125\n",
      "epoch: 320, training loss: 1.296364426612854, testing loss: 1.6133159399032593\n",
      "Accuracy: 0.4112\n",
      "epoch: 320, training loss: 1.2720402479171753, testing loss: 1.673173427581787\n",
      "Accuracy: 0.4106\n",
      "epoch: 320, training loss: 1.2517768144607544, testing loss: 1.739329218864441\n",
      "Accuracy: 0.4019\n",
      "epoch: 320, training loss: 1.31509530544281, testing loss: 1.7443026304244995\n",
      "Accuracy: 0.3733\n",
      "epoch: 320, training loss: 1.2672451734542847, testing loss: 1.5663866996765137\n",
      "Accuracy: 0.4211\n",
      "epoch: 320, training loss: 1.3194605112075806, testing loss: 1.7654885053634644\n",
      "Accuracy: 0.4089\n",
      "epoch: 330, training loss: 1.2965985536575317, testing loss: 1.740816593170166\n",
      "Accuracy: 0.4483\n",
      "epoch: 330, training loss: 1.1975797414779663, testing loss: 1.7503834962844849\n",
      "Accuracy: 0.4245\n",
      "epoch: 330, training loss: 1.225471019744873, testing loss: 1.6506171226501465\n",
      "Accuracy: 0.4324\n",
      "epoch: 330, training loss: 1.2921080589294434, testing loss: 1.8262965679168701\n",
      "Accuracy: 0.4167\n",
      "epoch: 330, training loss: 1.2565720081329346, testing loss: 1.6251397132873535\n",
      "Accuracy: 0.4522\n",
      "epoch: 330, training loss: 1.2453426122665405, testing loss: 1.6269350051879883\n",
      "Accuracy: 0.4136\n",
      "epoch: 330, training loss: 1.2649378776550293, testing loss: 1.5668038129806519\n",
      "Accuracy: 0.4759\n",
      "epoch: 330, training loss: 1.2830941677093506, testing loss: 1.622438669204712\n",
      "Accuracy: 0.4221\n",
      "epoch: 330, training loss: 1.3573157787322998, testing loss: 1.6670117378234863\n",
      "Accuracy: 0.4583\n",
      "epoch: 330, training loss: 1.2625935077667236, testing loss: 1.6291619539260864\n",
      "Accuracy: 0.4503\n",
      "epoch: 330, training loss: 1.207761526107788, testing loss: 1.7298493385314941\n",
      "Accuracy: 0.4121\n",
      "epoch: 330, training loss: 1.2667189836502075, testing loss: 1.7790592908859253\n",
      "Accuracy: 0.4291\n",
      "epoch: 330, training loss: 1.2467925548553467, testing loss: 1.6272271871566772\n",
      "Accuracy: 0.4618\n",
      "epoch: 330, training loss: 1.2490278482437134, testing loss: 1.636128306388855\n",
      "Accuracy: 0.4739\n",
      "epoch: 330, training loss: 1.231034278869629, testing loss: 1.6150773763656616\n",
      "Accuracy: 0.4088\n",
      "epoch: 330, training loss: 1.297998309135437, testing loss: 1.6609512567520142\n",
      "Accuracy: 0.4184\n",
      "epoch: 330, training loss: 1.245464563369751, testing loss: 1.674477219581604\n",
      "Accuracy: 0.4527\n",
      "epoch: 330, training loss: 1.218845248222351, testing loss: 1.7134997844696045\n",
      "Accuracy: 0.4081\n",
      "epoch: 330, training loss: 1.214471697807312, testing loss: 1.700408697128296\n",
      "Accuracy: 0.4246\n",
      "epoch: 330, training loss: 1.291002869606018, testing loss: 1.65143883228302\n",
      "Accuracy: 0.4194\n",
      "epoch: 330, training loss: 1.2246067523956299, testing loss: 1.681854009628296\n",
      "Accuracy: 0.3994\n",
      "epoch: 330, training loss: 1.2401329278945923, testing loss: 1.6231811046600342\n",
      "Accuracy: 0.4161\n",
      "epoch: 330, training loss: 1.238684058189392, testing loss: 1.5813199281692505\n",
      "Accuracy: 0.4603\n",
      "epoch: 330, training loss: 1.3078031539916992, testing loss: 1.5902190208435059\n",
      "Accuracy: 0.4359\n",
      "epoch: 330, training loss: 1.2674944400787354, testing loss: 1.7101976871490479\n",
      "Accuracy: 0.4159\n",
      "epoch: 330, training loss: 1.243947148323059, testing loss: 1.7040714025497437\n",
      "Accuracy: 0.4082\n",
      "epoch: 330, training loss: 1.2463808059692383, testing loss: 1.7209601402282715\n",
      "Accuracy: 0.3918\n",
      "epoch: 330, training loss: 1.2875436544418335, testing loss: 1.663345456123352\n",
      "Accuracy: 0.4431\n",
      "epoch: 330, training loss: 1.2444754838943481, testing loss: 1.6954681873321533\n",
      "Accuracy: 0.4151\n",
      "epoch: 330, training loss: 1.2402156591415405, testing loss: 1.6714084148406982\n",
      "Accuracy: 0.4735\n",
      "epoch: 330, training loss: 1.2368172407150269, testing loss: 1.7157078981399536\n",
      "Accuracy: 0.3987\n",
      "epoch: 330, training loss: 1.3252313137054443, testing loss: 1.697066068649292\n",
      "Accuracy: 0.3746\n",
      "epoch: 330, training loss: 1.2761226892471313, testing loss: 1.599981427192688\n",
      "Accuracy: 0.4075\n",
      "epoch: 330, training loss: 1.2530755996704102, testing loss: 1.7490198612213135\n",
      "Accuracy: 0.4581\n",
      "epoch: 330, training loss: 1.2166979312896729, testing loss: 1.6589062213897705\n",
      "Accuracy: 0.4169\n",
      "epoch: 330, training loss: 1.2765774726867676, testing loss: 1.7895162105560303\n",
      "Accuracy: 0.3901\n",
      "epoch: 330, training loss: 1.2289457321166992, testing loss: 1.7307342290878296\n",
      "Accuracy: 0.3893\n",
      "epoch: 330, training loss: 1.2919117212295532, testing loss: 1.6960296630859375\n",
      "Accuracy: 0.4178\n",
      "epoch: 330, training loss: 1.2633100748062134, testing loss: 1.6775542497634888\n",
      "Accuracy: 0.4270\n",
      "epoch: 330, training loss: 1.2353483438491821, testing loss: 1.6331896781921387\n",
      "Accuracy: 0.3708\n",
      "epoch: 330, training loss: 1.300424575805664, testing loss: 1.7409546375274658\n",
      "Accuracy: 0.3793\n",
      "epoch: 330, training loss: 1.2498670816421509, testing loss: 1.7684874534606934\n",
      "Accuracy: 0.3864\n",
      "epoch: 330, training loss: 1.2929726839065552, testing loss: 1.6306535005569458\n",
      "Accuracy: 0.4677\n",
      "epoch: 330, training loss: 1.2745939493179321, testing loss: 1.6182392835617065\n",
      "Accuracy: 0.4427\n",
      "epoch: 330, training loss: 1.299282193183899, testing loss: 1.6616735458374023\n",
      "Accuracy: 0.4217\n",
      "epoch: 330, training loss: 1.2315553426742554, testing loss: 1.669230580329895\n",
      "Accuracy: 0.4142\n",
      "epoch: 330, training loss: 1.2931612730026245, testing loss: 1.6335376501083374\n",
      "Accuracy: 0.4221\n",
      "epoch: 330, training loss: 1.2078272104263306, testing loss: 1.643540620803833\n",
      "Accuracy: 0.4433\n",
      "epoch: 330, training loss: 1.2703980207443237, testing loss: 1.6638572216033936\n",
      "Accuracy: 0.4343\n",
      "epoch: 330, training loss: 1.3116028308868408, testing loss: 1.6910858154296875\n",
      "Accuracy: 0.3816\n",
      "epoch: 330, training loss: 1.279946208000183, testing loss: 1.7586374282836914\n",
      "Accuracy: 0.3627\n",
      "epoch: 330, training loss: 1.3078405857086182, testing loss: 1.645524024963379\n",
      "Accuracy: 0.4383\n",
      "epoch: 330, training loss: 1.3082355260849, testing loss: 1.7425458431243896\n",
      "Accuracy: 0.4177\n",
      "epoch: 330, training loss: 1.3159024715423584, testing loss: 1.6667380332946777\n",
      "Accuracy: 0.4073\n",
      "epoch: 330, training loss: 1.2932710647583008, testing loss: 1.7001631259918213\n",
      "Accuracy: 0.3905\n",
      "epoch: 330, training loss: 1.2296003103256226, testing loss: 1.6886011362075806\n",
      "Accuracy: 0.4441\n",
      "epoch: 330, training loss: 1.3394219875335693, testing loss: 1.6457481384277344\n",
      "Accuracy: 0.4050\n",
      "epoch: 330, training loss: 1.3040319681167603, testing loss: 1.6230006217956543\n",
      "Accuracy: 0.4644\n",
      "epoch: 330, training loss: 1.267634630203247, testing loss: 1.6680091619491577\n",
      "Accuracy: 0.4461\n",
      "epoch: 330, training loss: 1.2655024528503418, testing loss: 1.735738754272461\n",
      "Accuracy: 0.4006\n",
      "epoch: 330, training loss: 1.2549978494644165, testing loss: 1.7356045246124268\n",
      "Accuracy: 0.4340\n",
      "epoch: 330, training loss: 1.2928816080093384, testing loss: 1.567244291305542\n",
      "Accuracy: 0.4389\n",
      "epoch: 330, training loss: 1.217065453529358, testing loss: 1.6528633832931519\n",
      "Accuracy: 0.4228\n",
      "epoch: 330, training loss: 1.2403316497802734, testing loss: 1.7437632083892822\n",
      "Accuracy: 0.4069\n",
      "epoch: 330, training loss: 1.3017377853393555, testing loss: 1.7469840049743652\n",
      "Accuracy: 0.4181\n",
      "epoch: 330, training loss: 1.2418118715286255, testing loss: 1.621911644935608\n",
      "Accuracy: 0.4000\n",
      "epoch: 330, training loss: 1.3079876899719238, testing loss: 1.6474270820617676\n",
      "Accuracy: 0.3628\n",
      "epoch: 330, training loss: 1.2862095832824707, testing loss: 1.6710240840911865\n",
      "Accuracy: 0.4116\n",
      "epoch: 330, training loss: 1.2733737230300903, testing loss: 1.6117537021636963\n",
      "Accuracy: 0.4142\n",
      "epoch: 330, training loss: 1.1884874105453491, testing loss: 1.723799228668213\n",
      "Accuracy: 0.4245\n",
      "epoch: 330, training loss: 1.2670178413391113, testing loss: 1.661226511001587\n",
      "Accuracy: 0.4126\n",
      "epoch: 330, training loss: 1.3071887493133545, testing loss: 1.702951192855835\n",
      "Accuracy: 0.4156\n",
      "epoch: 330, training loss: 1.3341163396835327, testing loss: 1.700242519378662\n",
      "Accuracy: 0.3820\n",
      "epoch: 330, training loss: 1.225211262702942, testing loss: 1.7457244396209717\n",
      "Accuracy: 0.3956\n",
      "epoch: 330, training loss: 1.2760909795761108, testing loss: 1.7081539630889893\n",
      "Accuracy: 0.3793\n",
      "epoch: 330, training loss: 1.2796590328216553, testing loss: 1.548069715499878\n",
      "Accuracy: 0.4031\n",
      "epoch: 330, training loss: 1.2095850706100464, testing loss: 1.7359018325805664\n",
      "Accuracy: 0.3796\n",
      "epoch: 330, training loss: 1.2539057731628418, testing loss: 1.8256499767303467\n",
      "Accuracy: 0.4167\n",
      "epoch: 330, training loss: 1.3206164836883545, testing loss: 1.8275721073150635\n",
      "Accuracy: 0.4130\n",
      "epoch: 330, training loss: 1.2729129791259766, testing loss: 1.6041117906570435\n",
      "Accuracy: 0.4560\n",
      "epoch: 330, training loss: 1.278246283531189, testing loss: 1.7180442810058594\n",
      "Accuracy: 0.4261\n",
      "epoch: 330, training loss: 1.232011079788208, testing loss: 1.6890252828598022\n",
      "Accuracy: 0.4444\n",
      "epoch: 330, training loss: 1.2871721982955933, testing loss: 1.666532278060913\n",
      "Accuracy: 0.4316\n",
      "epoch: 330, training loss: 1.2090024948120117, testing loss: 1.738267183303833\n",
      "Accuracy: 0.3882\n",
      "epoch: 330, training loss: 1.3456295728683472, testing loss: 1.764025092124939\n",
      "Accuracy: 0.4656\n",
      "epoch: 330, training loss: 1.2656152248382568, testing loss: 1.7106311321258545\n",
      "Accuracy: 0.4164\n",
      "epoch: 330, training loss: 1.281528115272522, testing loss: 1.6122695207595825\n",
      "Accuracy: 0.4682\n",
      "epoch: 330, training loss: 1.239686131477356, testing loss: 1.701293706893921\n",
      "Accuracy: 0.3774\n",
      "epoch: 330, training loss: 1.1856247186660767, testing loss: 1.7093123197555542\n",
      "Accuracy: 0.3792\n",
      "epoch: 330, training loss: 1.2646300792694092, testing loss: 1.8762400150299072\n",
      "Accuracy: 0.3924\n",
      "epoch: 330, training loss: 1.2416324615478516, testing loss: 1.6404753923416138\n",
      "Accuracy: 0.4537\n",
      "epoch: 330, training loss: 1.2754011154174805, testing loss: 1.7234151363372803\n",
      "Accuracy: 0.3952\n",
      "epoch: 330, training loss: 1.1669764518737793, testing loss: 1.6805025339126587\n",
      "Accuracy: 0.4073\n",
      "epoch: 330, training loss: 1.3306351900100708, testing loss: 1.595682144165039\n",
      "Accuracy: 0.4239\n",
      "epoch: 330, training loss: 1.2374472618103027, testing loss: 1.8373947143554688\n",
      "Accuracy: 0.3814\n",
      "epoch: 330, training loss: 1.2228941917419434, testing loss: 1.6717301607131958\n",
      "Accuracy: 0.4036\n",
      "epoch: 330, training loss: 1.313599944114685, testing loss: 1.7820920944213867\n",
      "Accuracy: 0.4107\n",
      "epoch: 330, training loss: 1.2568178176879883, testing loss: 1.6179908514022827\n",
      "Accuracy: 0.4437\n",
      "epoch: 330, training loss: 1.2771509885787964, testing loss: 1.6749147176742554\n",
      "Accuracy: 0.4441\n",
      "epoch: 330, training loss: 1.2271391153335571, testing loss: 1.6570309400558472\n",
      "Accuracy: 0.4466\n",
      "epoch: 340, training loss: 1.3008557558059692, testing loss: 1.6552783250808716\n",
      "Accuracy: 0.3923\n",
      "epoch: 340, training loss: 1.238032341003418, testing loss: 1.6149524450302124\n",
      "Accuracy: 0.5034\n",
      "epoch: 340, training loss: 1.302095890045166, testing loss: 1.712195873260498\n",
      "Accuracy: 0.3988\n",
      "epoch: 340, training loss: 1.253238558769226, testing loss: 1.7465481758117676\n",
      "Accuracy: 0.3599\n",
      "epoch: 340, training loss: 1.2628412246704102, testing loss: 1.7229268550872803\n",
      "Accuracy: 0.4334\n",
      "epoch: 340, training loss: 1.3003531694412231, testing loss: 1.6949211359024048\n",
      "Accuracy: 0.3980\n",
      "epoch: 340, training loss: 1.312556266784668, testing loss: 1.592882513999939\n",
      "Accuracy: 0.4364\n",
      "epoch: 340, training loss: 1.3324902057647705, testing loss: 1.6503939628601074\n",
      "Accuracy: 0.4164\n",
      "epoch: 340, training loss: 1.2847068309783936, testing loss: 1.6353141069412231\n",
      "Accuracy: 0.4362\n",
      "epoch: 340, training loss: 1.2981462478637695, testing loss: 1.618074893951416\n",
      "Accuracy: 0.4215\n",
      "epoch: 340, training loss: 1.2796417474746704, testing loss: 1.6806303262710571\n",
      "Accuracy: 0.4269\n",
      "epoch: 340, training loss: 1.2883528470993042, testing loss: 1.758568286895752\n",
      "Accuracy: 0.3954\n",
      "epoch: 340, training loss: 1.3736400604248047, testing loss: 1.6979492902755737\n",
      "Accuracy: 0.4089\n",
      "epoch: 340, training loss: 1.264977216720581, testing loss: 1.7087281942367554\n",
      "Accuracy: 0.4348\n",
      "epoch: 340, training loss: 1.3050620555877686, testing loss: 1.7051934003829956\n",
      "Accuracy: 0.4426\n",
      "epoch: 340, training loss: 1.2647737264633179, testing loss: 1.567097544670105\n",
      "Accuracy: 0.4212\n",
      "epoch: 340, training loss: 1.2507859468460083, testing loss: 1.785833477973938\n",
      "Accuracy: 0.4040\n",
      "epoch: 340, training loss: 1.2703783512115479, testing loss: 1.7176555395126343\n",
      "Accuracy: 0.3954\n",
      "epoch: 340, training loss: 1.2915419340133667, testing loss: 1.7603797912597656\n",
      "Accuracy: 0.3846\n",
      "epoch: 340, training loss: 1.253466010093689, testing loss: 1.6485475301742554\n",
      "Accuracy: 0.4281\n",
      "epoch: 340, training loss: 1.225441575050354, testing loss: 1.652138590812683\n",
      "Accuracy: 0.4233\n",
      "epoch: 340, training loss: 1.2685989141464233, testing loss: 1.810683012008667\n",
      "Accuracy: 0.4314\n",
      "epoch: 340, training loss: 1.290129542350769, testing loss: 1.7024580240249634\n",
      "Accuracy: 0.4130\n",
      "epoch: 340, training loss: 1.3265221118927002, testing loss: 1.7359315156936646\n",
      "Accuracy: 0.3894\n",
      "epoch: 340, training loss: 1.2082926034927368, testing loss: 1.5579400062561035\n",
      "Accuracy: 0.4581\n",
      "epoch: 340, training loss: 1.2543143033981323, testing loss: 1.5457148551940918\n",
      "Accuracy: 0.4411\n",
      "epoch: 340, training loss: 1.2028288841247559, testing loss: 1.7290501594543457\n",
      "Accuracy: 0.4281\n",
      "epoch: 340, training loss: 1.2600548267364502, testing loss: 1.8336430788040161\n",
      "Accuracy: 0.4172\n",
      "epoch: 340, training loss: 1.2893545627593994, testing loss: 1.6954530477523804\n",
      "Accuracy: 0.4191\n",
      "epoch: 340, training loss: 1.250452995300293, testing loss: 1.834226131439209\n",
      "Accuracy: 0.4093\n",
      "epoch: 340, training loss: 1.2852288484573364, testing loss: 1.6410365104675293\n",
      "Accuracy: 0.4012\n",
      "epoch: 340, training loss: 1.2671595811843872, testing loss: 1.7424254417419434\n",
      "Accuracy: 0.3731\n",
      "epoch: 340, training loss: 1.2520623207092285, testing loss: 1.56076180934906\n",
      "Accuracy: 0.4266\n",
      "epoch: 340, training loss: 1.3102563619613647, testing loss: 1.7680294513702393\n",
      "Accuracy: 0.4261\n",
      "epoch: 340, training loss: 1.2652496099472046, testing loss: 1.7171114683151245\n",
      "Accuracy: 0.4167\n",
      "epoch: 340, training loss: 1.265064001083374, testing loss: 1.7366863489151\n",
      "Accuracy: 0.4332\n",
      "epoch: 340, training loss: 1.2715718746185303, testing loss: 1.7194042205810547\n",
      "Accuracy: 0.4475\n",
      "epoch: 340, training loss: 1.243483066558838, testing loss: 1.5731502771377563\n",
      "Accuracy: 0.4337\n",
      "epoch: 340, training loss: 1.2046605348587036, testing loss: 1.672380805015564\n",
      "Accuracy: 0.4444\n",
      "epoch: 340, training loss: 1.2077844142913818, testing loss: 1.6844638586044312\n",
      "Accuracy: 0.4343\n",
      "epoch: 340, training loss: 1.286190390586853, testing loss: 1.693955659866333\n",
      "Accuracy: 0.4323\n",
      "epoch: 340, training loss: 1.2376669645309448, testing loss: 1.621606469154358\n",
      "Accuracy: 0.4381\n",
      "epoch: 340, training loss: 1.217997670173645, testing loss: 1.7148996591567993\n",
      "Accuracy: 0.4358\n",
      "epoch: 340, training loss: 1.2891979217529297, testing loss: 1.7269231081008911\n",
      "Accuracy: 0.3805\n",
      "epoch: 340, training loss: 1.3081666231155396, testing loss: 1.5545786619186401\n",
      "Accuracy: 0.4319\n",
      "epoch: 340, training loss: 1.2253080606460571, testing loss: 1.7159311771392822\n",
      "Accuracy: 0.4086\n",
      "epoch: 340, training loss: 1.2654318809509277, testing loss: 1.728376030921936\n",
      "Accuracy: 0.4355\n",
      "epoch: 340, training loss: 1.2704933881759644, testing loss: 1.7329907417297363\n",
      "Accuracy: 0.4413\n",
      "epoch: 340, training loss: 1.3089321851730347, testing loss: 1.705236792564392\n",
      "Accuracy: 0.4065\n",
      "epoch: 340, training loss: 1.2428444623947144, testing loss: 1.665501356124878\n",
      "Accuracy: 0.4397\n",
      "epoch: 340, training loss: 1.2155946493148804, testing loss: 1.6710833311080933\n",
      "Accuracy: 0.4286\n",
      "epoch: 340, training loss: 1.2591240406036377, testing loss: 1.6062325239181519\n",
      "Accuracy: 0.4660\n",
      "epoch: 340, training loss: 1.2539228200912476, testing loss: 1.6436073780059814\n",
      "Accuracy: 0.4809\n",
      "epoch: 340, training loss: 1.2386540174484253, testing loss: 1.694463849067688\n",
      "Accuracy: 0.4444\n",
      "epoch: 340, training loss: 1.2125557661056519, testing loss: 1.7588517665863037\n",
      "Accuracy: 0.4398\n",
      "epoch: 340, training loss: 1.2448257207870483, testing loss: 1.6362366676330566\n",
      "Accuracy: 0.4490\n",
      "epoch: 340, training loss: 1.2872707843780518, testing loss: 1.6521596908569336\n",
      "Accuracy: 0.4351\n",
      "epoch: 340, training loss: 1.277871012687683, testing loss: 1.6912171840667725\n",
      "Accuracy: 0.3808\n",
      "epoch: 340, training loss: 1.3100506067276, testing loss: 1.6737635135650635\n",
      "Accuracy: 0.4161\n",
      "epoch: 340, training loss: 1.2795743942260742, testing loss: 1.7048639059066772\n",
      "Accuracy: 0.4581\n",
      "epoch: 340, training loss: 1.258283019065857, testing loss: 1.6036217212677002\n",
      "Accuracy: 0.4073\n",
      "epoch: 340, training loss: 1.3197053670883179, testing loss: 1.6935076713562012\n",
      "Accuracy: 0.3917\n",
      "epoch: 340, training loss: 1.297633171081543, testing loss: 1.734598994255066\n",
      "Accuracy: 0.3797\n",
      "epoch: 340, training loss: 1.274489164352417, testing loss: 1.7444435358047485\n",
      "Accuracy: 0.4068\n",
      "epoch: 340, training loss: 1.2982258796691895, testing loss: 1.5792677402496338\n",
      "Accuracy: 0.4211\n",
      "epoch: 340, training loss: 1.2552138566970825, testing loss: 1.7155237197875977\n",
      "Accuracy: 0.4110\n",
      "epoch: 340, training loss: 1.290112853050232, testing loss: 1.6294264793395996\n",
      "Accuracy: 0.4236\n",
      "epoch: 340, training loss: 1.2078478336334229, testing loss: 1.747092843055725\n",
      "Accuracy: 0.3987\n",
      "epoch: 340, training loss: 1.3230717182159424, testing loss: 1.630797028541565\n",
      "Accuracy: 0.4734\n",
      "epoch: 340, training loss: 1.2425898313522339, testing loss: 1.6661698818206787\n",
      "Accuracy: 0.4317\n",
      "epoch: 340, training loss: 1.2110965251922607, testing loss: 1.673627257347107\n",
      "Accuracy: 0.4014\n",
      "epoch: 340, training loss: 1.3372907638549805, testing loss: 1.5192561149597168\n",
      "Accuracy: 0.4490\n",
      "epoch: 340, training loss: 1.2540843486785889, testing loss: 1.7543832063674927\n",
      "Accuracy: 0.4149\n",
      "epoch: 340, training loss: 1.2504918575286865, testing loss: 1.6585934162139893\n",
      "Accuracy: 0.3861\n",
      "epoch: 340, training loss: 1.2580031156539917, testing loss: 1.6735469102859497\n",
      "Accuracy: 0.4207\n",
      "epoch: 340, training loss: 1.2073090076446533, testing loss: 1.7684504985809326\n",
      "Accuracy: 0.3885\n",
      "epoch: 340, training loss: 1.162656545639038, testing loss: 1.6252256631851196\n",
      "Accuracy: 0.4337\n",
      "epoch: 340, training loss: 1.2615864276885986, testing loss: 1.6551804542541504\n",
      "Accuracy: 0.4515\n",
      "epoch: 340, training loss: 1.371206521987915, testing loss: 1.840172529220581\n",
      "Accuracy: 0.3691\n",
      "epoch: 340, training loss: 1.2502808570861816, testing loss: 1.7464231252670288\n",
      "Accuracy: 0.4482\n",
      "epoch: 340, training loss: 1.2793506383895874, testing loss: 1.7234230041503906\n",
      "Accuracy: 0.3527\n",
      "epoch: 340, training loss: 1.289674162864685, testing loss: 1.6700043678283691\n",
      "Accuracy: 0.4170\n",
      "epoch: 340, training loss: 1.230307936668396, testing loss: 1.6413768529891968\n",
      "Accuracy: 0.4138\n",
      "epoch: 340, training loss: 1.2961612939834595, testing loss: 1.6096525192260742\n",
      "Accuracy: 0.4399\n",
      "epoch: 340, training loss: 1.3354161977767944, testing loss: 1.7336783409118652\n",
      "Accuracy: 0.4177\n",
      "epoch: 340, training loss: 1.2948720455169678, testing loss: 1.7913649082183838\n",
      "Accuracy: 0.4369\n",
      "epoch: 340, training loss: 1.3156623840332031, testing loss: 1.6832759380340576\n",
      "Accuracy: 0.4316\n",
      "epoch: 340, training loss: 1.2629802227020264, testing loss: 1.643455147743225\n",
      "Accuracy: 0.4519\n",
      "epoch: 340, training loss: 1.3172155618667603, testing loss: 1.7121044397354126\n",
      "Accuracy: 0.4036\n",
      "epoch: 340, training loss: 1.217956781387329, testing loss: 1.693066120147705\n",
      "Accuracy: 0.3605\n",
      "epoch: 340, training loss: 1.2428194284439087, testing loss: 1.7076035737991333\n",
      "Accuracy: 0.3767\n",
      "epoch: 340, training loss: 1.3394709825515747, testing loss: 1.6833711862564087\n",
      "Accuracy: 0.3910\n",
      "epoch: 340, training loss: 1.2706793546676636, testing loss: 1.666731357574463\n",
      "Accuracy: 0.4545\n",
      "epoch: 340, training loss: 1.2451506853103638, testing loss: 1.5868923664093018\n",
      "Accuracy: 0.4498\n",
      "epoch: 340, training loss: 1.2319273948669434, testing loss: 1.7487468719482422\n",
      "Accuracy: 0.4180\n",
      "epoch: 340, training loss: 1.2633147239685059, testing loss: 1.734336256980896\n",
      "Accuracy: 0.4085\n",
      "epoch: 340, training loss: 1.284859538078308, testing loss: 1.776227593421936\n",
      "Accuracy: 0.4314\n",
      "epoch: 340, training loss: 1.2733911275863647, testing loss: 1.7628475427627563\n",
      "Accuracy: 0.3707\n",
      "epoch: 340, training loss: 1.2418779134750366, testing loss: 1.754751205444336\n",
      "Accuracy: 0.3962\n",
      "epoch: 340, training loss: 1.2235956192016602, testing loss: 1.559631586074829\n",
      "Accuracy: 0.4785\n",
      "epoch: 350, training loss: 1.2943835258483887, testing loss: 1.745262861251831\n",
      "Accuracy: 0.4066\n",
      "epoch: 350, training loss: 1.2850284576416016, testing loss: 1.688562273979187\n",
      "Accuracy: 0.4013\n",
      "epoch: 350, training loss: 1.2843631505966187, testing loss: 1.7202235460281372\n",
      "Accuracy: 0.4058\n",
      "epoch: 350, training loss: 1.2324366569519043, testing loss: 1.6904164552688599\n",
      "Accuracy: 0.3722\n",
      "epoch: 350, training loss: 1.3360387086868286, testing loss: 1.598548412322998\n",
      "Accuracy: 0.4299\n",
      "epoch: 350, training loss: 1.2847503423690796, testing loss: 1.7797237634658813\n",
      "Accuracy: 0.4007\n",
      "epoch: 350, training loss: 1.255380392074585, testing loss: 1.7310280799865723\n",
      "Accuracy: 0.4613\n",
      "epoch: 350, training loss: 1.2286744117736816, testing loss: 1.6123048067092896\n",
      "Accuracy: 0.4545\n",
      "epoch: 350, training loss: 1.2843471765518188, testing loss: 1.666858196258545\n",
      "Accuracy: 0.4475\n",
      "epoch: 350, training loss: 1.2447593212127686, testing loss: 1.7145920991897583\n",
      "Accuracy: 0.3779\n",
      "epoch: 350, training loss: 1.2335100173950195, testing loss: 1.6286066770553589\n",
      "Accuracy: 0.3894\n",
      "epoch: 350, training loss: 1.2139496803283691, testing loss: 1.5565595626831055\n",
      "Accuracy: 0.4125\n",
      "epoch: 350, training loss: 1.2472246885299683, testing loss: 1.6887187957763672\n",
      "Accuracy: 0.4228\n",
      "epoch: 350, training loss: 1.253631591796875, testing loss: 1.645544409751892\n",
      "Accuracy: 0.4226\n",
      "epoch: 350, training loss: 1.2317944765090942, testing loss: 1.642223596572876\n",
      "Accuracy: 0.4259\n",
      "epoch: 350, training loss: 1.2747515439987183, testing loss: 1.6803873777389526\n",
      "Accuracy: 0.4259\n",
      "epoch: 350, training loss: 1.2590996026992798, testing loss: 1.6296498775482178\n",
      "Accuracy: 0.4421\n",
      "epoch: 350, training loss: 1.188269853591919, testing loss: 1.563336968421936\n",
      "Accuracy: 0.4601\n",
      "epoch: 350, training loss: 1.2277952432632446, testing loss: 1.6780567169189453\n",
      "Accuracy: 0.3763\n",
      "epoch: 350, training loss: 1.2431762218475342, testing loss: 1.6764742136001587\n",
      "Accuracy: 0.4345\n",
      "epoch: 350, training loss: 1.2447929382324219, testing loss: 1.7874071598052979\n",
      "Accuracy: 0.3975\n",
      "epoch: 350, training loss: 1.3071956634521484, testing loss: 1.7691646814346313\n",
      "Accuracy: 0.4221\n",
      "epoch: 350, training loss: 1.2517852783203125, testing loss: 1.7896078824996948\n",
      "Accuracy: 0.4379\n",
      "epoch: 350, training loss: 1.2935854196548462, testing loss: 1.7541558742523193\n",
      "Accuracy: 0.3927\n",
      "epoch: 350, training loss: 1.2820968627929688, testing loss: 1.7498884201049805\n",
      "Accuracy: 0.4097\n",
      "epoch: 350, training loss: 1.2525854110717773, testing loss: 1.7879220247268677\n",
      "Accuracy: 0.3922\n",
      "epoch: 350, training loss: 1.2942402362823486, testing loss: 1.6129999160766602\n",
      "Accuracy: 0.4260\n",
      "epoch: 350, training loss: 1.2614659070968628, testing loss: 1.6423161029815674\n",
      "Accuracy: 0.3973\n",
      "epoch: 350, training loss: 1.2393367290496826, testing loss: 1.7140920162200928\n",
      "Accuracy: 0.4175\n",
      "epoch: 350, training loss: 1.2910940647125244, testing loss: 1.6672453880310059\n",
      "Accuracy: 0.4164\n",
      "epoch: 350, training loss: 1.2375805377960205, testing loss: 1.7243647575378418\n",
      "Accuracy: 0.4369\n",
      "epoch: 350, training loss: 1.2740672826766968, testing loss: 1.6502867937088013\n",
      "Accuracy: 0.3917\n",
      "epoch: 350, training loss: 1.3360371589660645, testing loss: 1.8169044256210327\n",
      "Accuracy: 0.3787\n",
      "epoch: 350, training loss: 1.2901443243026733, testing loss: 1.832200050354004\n",
      "Accuracy: 0.4174\n",
      "epoch: 350, training loss: 1.2620463371276855, testing loss: 1.6890546083450317\n",
      "Accuracy: 0.4236\n",
      "epoch: 350, training loss: 1.2565431594848633, testing loss: 1.6088515520095825\n",
      "Accuracy: 0.4560\n",
      "epoch: 350, training loss: 1.351741909980774, testing loss: 1.6507172584533691\n",
      "Accuracy: 0.4032\n",
      "epoch: 350, training loss: 1.2820640802383423, testing loss: 1.8402546644210815\n",
      "Accuracy: 0.3883\n",
      "epoch: 350, training loss: 1.3075937032699585, testing loss: 1.5188125371932983\n",
      "Accuracy: 0.4359\n",
      "epoch: 350, training loss: 1.197281002998352, testing loss: 1.638637661933899\n",
      "Accuracy: 0.4032\n",
      "epoch: 350, training loss: 1.203879952430725, testing loss: 1.6418557167053223\n",
      "Accuracy: 0.4247\n",
      "epoch: 350, training loss: 1.2907984256744385, testing loss: 1.704494833946228\n",
      "Accuracy: 0.4315\n",
      "epoch: 350, training loss: 1.2750240564346313, testing loss: 1.6672823429107666\n",
      "Accuracy: 0.4299\n",
      "epoch: 350, training loss: 1.1728359460830688, testing loss: 1.6776407957077026\n",
      "Accuracy: 0.4207\n",
      "epoch: 350, training loss: 1.229050874710083, testing loss: 1.8105634450912476\n",
      "Accuracy: 0.4062\n",
      "epoch: 350, training loss: 1.2713876962661743, testing loss: 1.746725082397461\n",
      "Accuracy: 0.3651\n",
      "epoch: 350, training loss: 1.38612961769104, testing loss: 1.637290596961975\n",
      "Accuracy: 0.4084\n",
      "epoch: 350, training loss: 1.2592321634292603, testing loss: 1.7496027946472168\n",
      "Accuracy: 0.4356\n",
      "epoch: 350, training loss: 1.210286259651184, testing loss: 1.7289931774139404\n",
      "Accuracy: 0.4319\n",
      "epoch: 350, training loss: 1.2647240161895752, testing loss: 1.7597659826278687\n",
      "Accuracy: 0.4032\n",
      "epoch: 350, training loss: 1.2600009441375732, testing loss: 1.601952314376831\n",
      "Accuracy: 0.4291\n",
      "epoch: 350, training loss: 1.267574667930603, testing loss: 1.6495041847229004\n",
      "Accuracy: 0.4309\n",
      "epoch: 350, training loss: 1.2762916088104248, testing loss: 1.6847503185272217\n",
      "Accuracy: 0.4086\n",
      "epoch: 350, training loss: 1.2370973825454712, testing loss: 1.662233591079712\n",
      "Accuracy: 0.3889\n",
      "epoch: 350, training loss: 1.2901326417922974, testing loss: 1.7366666793823242\n",
      "Accuracy: 0.3875\n",
      "epoch: 350, training loss: 1.290907621383667, testing loss: 1.6174951791763306\n",
      "Accuracy: 0.4295\n",
      "epoch: 350, training loss: 1.3076480627059937, testing loss: 1.6077604293823242\n",
      "Accuracy: 0.4197\n",
      "epoch: 350, training loss: 1.30415678024292, testing loss: 1.5665199756622314\n",
      "Accuracy: 0.4488\n",
      "epoch: 350, training loss: 1.267869472503662, testing loss: 1.6539362668991089\n",
      "Accuracy: 0.4209\n",
      "epoch: 350, training loss: 1.2597936391830444, testing loss: 1.624735713005066\n",
      "Accuracy: 0.4268\n",
      "epoch: 350, training loss: 1.2703605890274048, testing loss: 1.639068841934204\n",
      "Accuracy: 0.4304\n",
      "epoch: 350, training loss: 1.282294511795044, testing loss: 1.6276252269744873\n",
      "Accuracy: 0.3994\n",
      "epoch: 350, training loss: 1.2246968746185303, testing loss: 1.7027496099472046\n",
      "Accuracy: 0.4204\n",
      "epoch: 350, training loss: 1.238057255744934, testing loss: 1.5455135107040405\n",
      "Accuracy: 0.4459\n",
      "epoch: 350, training loss: 1.270282506942749, testing loss: 1.7684168815612793\n",
      "Accuracy: 0.3904\n",
      "epoch: 350, training loss: 1.2763510942459106, testing loss: 1.7020970582962036\n",
      "Accuracy: 0.4225\n",
      "epoch: 350, training loss: 1.2209914922714233, testing loss: 1.6910881996154785\n",
      "Accuracy: 0.4276\n",
      "epoch: 350, training loss: 1.2907521724700928, testing loss: 1.8338929414749146\n",
      "Accuracy: 0.4180\n",
      "epoch: 350, training loss: 1.2731069326400757, testing loss: 1.6632143259048462\n",
      "Accuracy: 0.4125\n",
      "epoch: 350, training loss: 1.2729984521865845, testing loss: 1.6337518692016602\n",
      "Accuracy: 0.3674\n",
      "epoch: 350, training loss: 1.29345703125, testing loss: 1.693123698234558\n",
      "Accuracy: 0.4389\n",
      "epoch: 350, training loss: 1.2155593633651733, testing loss: 1.6115686893463135\n",
      "Accuracy: 0.4813\n",
      "epoch: 350, training loss: 1.2410954236984253, testing loss: 1.7554421424865723\n",
      "Accuracy: 0.3938\n",
      "epoch: 350, training loss: 1.2015578746795654, testing loss: 1.795918345451355\n",
      "Accuracy: 0.4095\n",
      "epoch: 350, training loss: 1.270708680152893, testing loss: 1.6689645051956177\n",
      "Accuracy: 0.3937\n",
      "epoch: 350, training loss: 1.2422581911087036, testing loss: 1.7061818838119507\n",
      "Accuracy: 0.4340\n",
      "epoch: 350, training loss: 1.2402364015579224, testing loss: 1.7652301788330078\n",
      "Accuracy: 0.3696\n",
      "epoch: 350, training loss: 1.292399287223816, testing loss: 1.7123138904571533\n",
      "Accuracy: 0.4146\n",
      "epoch: 350, training loss: 1.1551167964935303, testing loss: 1.6815531253814697\n",
      "Accuracy: 0.3923\n",
      "epoch: 350, training loss: 1.2046972513198853, testing loss: 1.62236487865448\n",
      "Accuracy: 0.4424\n",
      "epoch: 350, training loss: 1.2436389923095703, testing loss: 1.6472951173782349\n",
      "Accuracy: 0.4548\n",
      "epoch: 350, training loss: 1.2613402605056763, testing loss: 1.6626763343811035\n",
      "Accuracy: 0.4167\n",
      "epoch: 350, training loss: 1.2947590351104736, testing loss: 1.6650898456573486\n",
      "Accuracy: 0.4399\n",
      "epoch: 350, training loss: 1.222032904624939, testing loss: 1.764443278312683\n",
      "Accuracy: 0.4221\n",
      "epoch: 350, training loss: 1.2632477283477783, testing loss: 1.7098379135131836\n",
      "Accuracy: 0.4412\n",
      "epoch: 350, training loss: 1.246809482574463, testing loss: 1.7273659706115723\n",
      "Accuracy: 0.4220\n",
      "epoch: 350, training loss: 1.2950894832611084, testing loss: 1.680674433708191\n",
      "Accuracy: 0.3839\n",
      "epoch: 350, training loss: 1.2279820442199707, testing loss: 1.7372232675552368\n",
      "Accuracy: 0.4142\n",
      "epoch: 350, training loss: 1.2454867362976074, testing loss: 1.6294188499450684\n",
      "Accuracy: 0.3987\n",
      "epoch: 350, training loss: 1.2501064538955688, testing loss: 1.7047760486602783\n",
      "Accuracy: 0.4233\n",
      "epoch: 350, training loss: 1.277768850326538, testing loss: 1.6168255805969238\n",
      "Accuracy: 0.4433\n",
      "epoch: 350, training loss: 1.295381784439087, testing loss: 1.792970895767212\n",
      "Accuracy: 0.3864\n",
      "epoch: 350, training loss: 1.28053879737854, testing loss: 1.7673406600952148\n",
      "Accuracy: 0.4081\n",
      "epoch: 350, training loss: 1.2649489641189575, testing loss: 1.6125125885009766\n",
      "Accuracy: 0.4263\n",
      "epoch: 350, training loss: 1.2307902574539185, testing loss: 1.692781925201416\n",
      "Accuracy: 0.4236\n",
      "epoch: 350, training loss: 1.2246206998825073, testing loss: 1.628688097000122\n",
      "Accuracy: 0.4231\n",
      "epoch: 350, training loss: 1.2663143873214722, testing loss: 1.7578455209732056\n",
      "Accuracy: 0.4262\n",
      "epoch: 350, training loss: 1.3051350116729736, testing loss: 1.6410614252090454\n",
      "Accuracy: 0.4309\n",
      "epoch: 350, training loss: 1.309134840965271, testing loss: 1.7004077434539795\n",
      "Accuracy: 0.4243\n",
      "epoch: 350, training loss: 1.2618070840835571, testing loss: 1.6415185928344727\n",
      "Accuracy: 0.4404\n",
      "epoch: 360, training loss: 1.3167325258255005, testing loss: 1.6377543210983276\n",
      "Accuracy: 0.4762\n",
      "epoch: 360, training loss: 1.2330950498580933, testing loss: 1.7005174160003662\n",
      "Accuracy: 0.4203\n",
      "epoch: 360, training loss: 1.2213407754898071, testing loss: 1.693253517150879\n",
      "Accuracy: 0.4497\n",
      "epoch: 360, training loss: 1.2509714365005493, testing loss: 1.6685055494308472\n",
      "Accuracy: 0.4249\n",
      "epoch: 360, training loss: 1.2707242965698242, testing loss: 1.6776180267333984\n",
      "Accuracy: 0.4618\n",
      "epoch: 360, training loss: 1.3030507564544678, testing loss: 1.7110207080841064\n",
      "Accuracy: 0.4231\n",
      "epoch: 360, training loss: 1.2641578912734985, testing loss: 1.6636621952056885\n",
      "Accuracy: 0.4067\n",
      "epoch: 360, training loss: 1.142660140991211, testing loss: 1.6017851829528809\n",
      "Accuracy: 0.4020\n",
      "epoch: 360, training loss: 1.237191081047058, testing loss: 1.7087433338165283\n",
      "Accuracy: 0.4101\n",
      "epoch: 360, training loss: 1.2691103219985962, testing loss: 1.7762269973754883\n",
      "Accuracy: 0.4471\n",
      "epoch: 360, training loss: 1.2421785593032837, testing loss: 1.6536680459976196\n",
      "Accuracy: 0.4257\n",
      "epoch: 360, training loss: 1.2590436935424805, testing loss: 1.6466046571731567\n",
      "Accuracy: 0.4488\n",
      "epoch: 360, training loss: 1.2428534030914307, testing loss: 1.7258172035217285\n",
      "Accuracy: 0.3993\n",
      "epoch: 360, training loss: 1.3207703828811646, testing loss: 1.7759135961532593\n",
      "Accuracy: 0.3754\n",
      "epoch: 360, training loss: 1.1712921857833862, testing loss: 1.7716596126556396\n",
      "Accuracy: 0.3955\n",
      "epoch: 360, training loss: 1.338192105293274, testing loss: 1.6338800191879272\n",
      "Accuracy: 0.4214\n",
      "epoch: 360, training loss: 1.2325252294540405, testing loss: 1.867281436920166\n",
      "Accuracy: 0.3466\n",
      "epoch: 360, training loss: 1.2476003170013428, testing loss: 1.6512744426727295\n",
      "Accuracy: 0.4095\n",
      "epoch: 360, training loss: 1.195403814315796, testing loss: 1.718104600906372\n",
      "Accuracy: 0.3611\n",
      "epoch: 360, training loss: 1.287231683731079, testing loss: 1.6455074548721313\n",
      "Accuracy: 0.3906\n",
      "epoch: 360, training loss: 1.2395473718643188, testing loss: 1.5777430534362793\n",
      "Accuracy: 0.4286\n",
      "epoch: 360, training loss: 1.2670270204544067, testing loss: 1.7225980758666992\n",
      "Accuracy: 0.3885\n",
      "epoch: 360, training loss: 1.2520939111709595, testing loss: 1.6294770240783691\n",
      "Accuracy: 0.4325\n",
      "epoch: 360, training loss: 1.2717443704605103, testing loss: 1.7072679996490479\n",
      "Accuracy: 0.4272\n",
      "epoch: 360, training loss: 1.3004481792449951, testing loss: 1.8215641975402832\n",
      "Accuracy: 0.4000\n",
      "epoch: 360, training loss: 1.2810810804367065, testing loss: 1.7722188234329224\n",
      "Accuracy: 0.4043\n",
      "epoch: 360, training loss: 1.210007667541504, testing loss: 1.7751911878585815\n",
      "Accuracy: 0.4054\n",
      "epoch: 360, training loss: 1.2685242891311646, testing loss: 1.7000257968902588\n",
      "Accuracy: 0.4067\n",
      "epoch: 360, training loss: 1.2440812587738037, testing loss: 1.6703604459762573\n",
      "Accuracy: 0.4315\n",
      "epoch: 360, training loss: 1.2211283445358276, testing loss: 1.6277238130569458\n",
      "Accuracy: 0.4212\n",
      "epoch: 360, training loss: 1.2751716375350952, testing loss: 1.6763169765472412\n",
      "Accuracy: 0.4182\n",
      "epoch: 360, training loss: 1.294628620147705, testing loss: 1.6297699213027954\n",
      "Accuracy: 0.4315\n",
      "epoch: 360, training loss: 1.2498137950897217, testing loss: 1.6162645816802979\n",
      "Accuracy: 0.4669\n",
      "epoch: 360, training loss: 1.2426714897155762, testing loss: 1.7071378231048584\n",
      "Accuracy: 0.4250\n",
      "epoch: 360, training loss: 1.2586742639541626, testing loss: 1.6292692422866821\n",
      "Accuracy: 0.4512\n",
      "epoch: 360, training loss: 1.2744866609573364, testing loss: 1.6297948360443115\n",
      "Accuracy: 0.4018\n",
      "epoch: 360, training loss: 1.2983421087265015, testing loss: 1.7440756559371948\n",
      "Accuracy: 0.4110\n",
      "epoch: 360, training loss: 1.3202366828918457, testing loss: 1.5554789304733276\n",
      "Accuracy: 0.4673\n",
      "epoch: 360, training loss: 1.2466609477996826, testing loss: 1.5986204147338867\n",
      "Accuracy: 0.4401\n",
      "epoch: 360, training loss: 1.3036216497421265, testing loss: 1.7339375019073486\n",
      "Accuracy: 0.4468\n",
      "epoch: 360, training loss: 1.2416576147079468, testing loss: 1.7632005214691162\n",
      "Accuracy: 0.3941\n",
      "epoch: 360, training loss: 1.2304435968399048, testing loss: 1.6021569967269897\n",
      "Accuracy: 0.4773\n",
      "epoch: 360, training loss: 1.256839632987976, testing loss: 1.6705561876296997\n",
      "Accuracy: 0.3961\n",
      "epoch: 360, training loss: 1.2030214071273804, testing loss: 1.719260334968567\n",
      "Accuracy: 0.4205\n",
      "epoch: 360, training loss: 1.2551313638687134, testing loss: 1.7565761804580688\n",
      "Accuracy: 0.4217\n",
      "epoch: 360, training loss: 1.2368019819259644, testing loss: 1.6434050798416138\n",
      "Accuracy: 0.3953\n",
      "epoch: 360, training loss: 1.235892653465271, testing loss: 1.712526798248291\n",
      "Accuracy: 0.4409\n",
      "epoch: 360, training loss: 1.2505911588668823, testing loss: 1.7108279466629028\n",
      "Accuracy: 0.3735\n",
      "epoch: 360, training loss: 1.2900465726852417, testing loss: 1.7165931463241577\n",
      "Accuracy: 0.4146\n",
      "epoch: 360, training loss: 1.2721686363220215, testing loss: 1.670170783996582\n",
      "Accuracy: 0.4365\n",
      "epoch: 360, training loss: 1.2134392261505127, testing loss: 1.660923719406128\n",
      "Accuracy: 0.3986\n",
      "epoch: 360, training loss: 1.2584971189498901, testing loss: 1.7294158935546875\n",
      "Accuracy: 0.4038\n",
      "epoch: 360, training loss: 1.2824913263320923, testing loss: 1.5230871438980103\n",
      "Accuracy: 0.4754\n",
      "epoch: 360, training loss: 1.2744935750961304, testing loss: 1.7246054410934448\n",
      "Accuracy: 0.4039\n",
      "epoch: 360, training loss: 1.243728756904602, testing loss: 1.7435047626495361\n",
      "Accuracy: 0.3994\n",
      "epoch: 360, training loss: 1.1922321319580078, testing loss: 1.6757586002349854\n",
      "Accuracy: 0.4249\n",
      "epoch: 360, training loss: 1.2473483085632324, testing loss: 1.785804271697998\n",
      "Accuracy: 0.3917\n",
      "epoch: 360, training loss: 1.2554926872253418, testing loss: 1.6174119710922241\n",
      "Accuracy: 0.4421\n",
      "epoch: 360, training loss: 1.272101879119873, testing loss: 1.5436173677444458\n",
      "Accuracy: 0.4423\n",
      "epoch: 360, training loss: 1.2120529413223267, testing loss: 1.6669816970825195\n",
      "Accuracy: 0.4042\n",
      "epoch: 360, training loss: 1.2828972339630127, testing loss: 1.7425647974014282\n",
      "Accuracy: 0.4119\n",
      "epoch: 360, training loss: 1.323562502861023, testing loss: 1.6875739097595215\n",
      "Accuracy: 0.4286\n",
      "epoch: 360, training loss: 1.2553037405014038, testing loss: 1.8188358545303345\n",
      "Accuracy: 0.4194\n",
      "epoch: 360, training loss: 1.3079971075057983, testing loss: 1.7076478004455566\n",
      "Accuracy: 0.4276\n",
      "epoch: 360, training loss: 1.2242581844329834, testing loss: 1.8571821451187134\n",
      "Accuracy: 0.3905\n",
      "epoch: 360, training loss: 1.2015093564987183, testing loss: 1.6415445804595947\n",
      "Accuracy: 0.4178\n",
      "epoch: 360, training loss: 1.2277004718780518, testing loss: 1.7714959383010864\n",
      "Accuracy: 0.3973\n",
      "epoch: 360, training loss: 1.2208683490753174, testing loss: 1.7405561208724976\n",
      "Accuracy: 0.3940\n",
      "epoch: 360, training loss: 1.2037752866744995, testing loss: 1.705214500427246\n",
      "Accuracy: 0.4390\n",
      "epoch: 360, training loss: 1.2795852422714233, testing loss: 1.5884034633636475\n",
      "Accuracy: 0.4527\n",
      "epoch: 360, training loss: 1.2511305809020996, testing loss: 1.6384345293045044\n",
      "Accuracy: 0.4125\n",
      "epoch: 360, training loss: 1.2472285032272339, testing loss: 1.7962042093276978\n",
      "Accuracy: 0.3846\n",
      "epoch: 360, training loss: 1.3142409324645996, testing loss: 1.7674063444137573\n",
      "Accuracy: 0.4054\n",
      "epoch: 360, training loss: 1.2173563241958618, testing loss: 1.7167541980743408\n",
      "Accuracy: 0.3814\n",
      "epoch: 360, training loss: 1.2210954427719116, testing loss: 1.6273573637008667\n",
      "Accuracy: 0.4263\n",
      "epoch: 360, training loss: 1.2810542583465576, testing loss: 1.6933646202087402\n",
      "Accuracy: 0.4306\n",
      "epoch: 360, training loss: 1.2899926900863647, testing loss: 1.6906906366348267\n",
      "Accuracy: 0.4181\n",
      "epoch: 360, training loss: 1.2515432834625244, testing loss: 1.740146279335022\n",
      "Accuracy: 0.4272\n",
      "epoch: 360, training loss: 1.2912214994430542, testing loss: 1.8674839735031128\n",
      "Accuracy: 0.3876\n",
      "epoch: 360, training loss: 1.2787026166915894, testing loss: 1.7921056747436523\n",
      "Accuracy: 0.4154\n",
      "epoch: 360, training loss: 1.2793124914169312, testing loss: 1.6385983228683472\n",
      "Accuracy: 0.4373\n",
      "epoch: 360, training loss: 1.1897300481796265, testing loss: 1.6631841659545898\n",
      "Accuracy: 0.4031\n",
      "epoch: 360, training loss: 1.3187721967697144, testing loss: 1.7394179105758667\n",
      "Accuracy: 0.3774\n",
      "epoch: 360, training loss: 1.2130690813064575, testing loss: 1.7681598663330078\n",
      "Accuracy: 0.4051\n",
      "epoch: 360, training loss: 1.3279075622558594, testing loss: 1.830737829208374\n",
      "Accuracy: 0.4371\n",
      "epoch: 360, training loss: 1.234841227531433, testing loss: 1.7528003454208374\n",
      "Accuracy: 0.4315\n",
      "epoch: 360, training loss: 1.226149559020996, testing loss: 1.682361364364624\n",
      "Accuracy: 0.3766\n",
      "epoch: 360, training loss: 1.226544976234436, testing loss: 1.6067602634429932\n",
      "Accuracy: 0.4800\n",
      "epoch: 360, training loss: 1.2449712753295898, testing loss: 1.5532681941986084\n",
      "Accuracy: 0.4589\n",
      "epoch: 360, training loss: 1.2999260425567627, testing loss: 1.783902883529663\n",
      "Accuracy: 0.4062\n",
      "epoch: 360, training loss: 1.2349367141723633, testing loss: 1.8404022455215454\n",
      "Accuracy: 0.3422\n",
      "epoch: 360, training loss: 1.2328895330429077, testing loss: 1.70601224899292\n",
      "Accuracy: 0.4514\n",
      "epoch: 360, training loss: 1.307342767715454, testing loss: 1.5524948835372925\n",
      "Accuracy: 0.4512\n",
      "epoch: 360, training loss: 1.2873904705047607, testing loss: 1.7720746994018555\n",
      "Accuracy: 0.4047\n",
      "epoch: 360, training loss: 1.1917766332626343, testing loss: 1.7743016481399536\n",
      "Accuracy: 0.4572\n",
      "epoch: 360, training loss: 1.2524455785751343, testing loss: 1.683833122253418\n",
      "Accuracy: 0.4161\n",
      "epoch: 360, training loss: 1.3231674432754517, testing loss: 1.8392324447631836\n",
      "Accuracy: 0.3899\n",
      "epoch: 360, training loss: 1.2786062955856323, testing loss: 1.5531641244888306\n",
      "Accuracy: 0.4342\n",
      "epoch: 360, training loss: 1.2417912483215332, testing loss: 1.6664677858352661\n",
      "Accuracy: 0.4141\n",
      "epoch: 360, training loss: 1.2083451747894287, testing loss: 1.6336381435394287\n",
      "Accuracy: 0.4448\n",
      "epoch: 370, training loss: 1.257890224456787, testing loss: 1.6639422178268433\n",
      "Accuracy: 0.4381\n",
      "epoch: 370, training loss: 1.2506191730499268, testing loss: 1.7508536577224731\n",
      "Accuracy: 0.4071\n",
      "epoch: 370, training loss: 1.314813494682312, testing loss: 1.6674050092697144\n",
      "Accuracy: 0.3849\n",
      "epoch: 370, training loss: 1.2256723642349243, testing loss: 1.8167771100997925\n",
      "Accuracy: 0.3585\n",
      "epoch: 370, training loss: 1.1896302700042725, testing loss: 1.8641771078109741\n",
      "Accuracy: 0.4055\n",
      "epoch: 370, training loss: 1.2329021692276, testing loss: 1.8006268739700317\n",
      "Accuracy: 0.4296\n",
      "epoch: 370, training loss: 1.3366098403930664, testing loss: 1.7369070053100586\n",
      "Accuracy: 0.3914\n",
      "epoch: 370, training loss: 1.2451646327972412, testing loss: 1.7203773260116577\n",
      "Accuracy: 0.3805\n",
      "epoch: 370, training loss: 1.2634574174880981, testing loss: 1.692162036895752\n",
      "Accuracy: 0.4433\n",
      "epoch: 370, training loss: 1.2076776027679443, testing loss: 1.7073644399642944\n",
      "Accuracy: 0.3652\n",
      "epoch: 370, training loss: 1.2616771459579468, testing loss: 1.624446153640747\n",
      "Accuracy: 0.4820\n",
      "epoch: 370, training loss: 1.3037537336349487, testing loss: 1.7994564771652222\n",
      "Accuracy: 0.4181\n",
      "epoch: 370, training loss: 1.3016539812088013, testing loss: 1.6458200216293335\n",
      "Accuracy: 0.4505\n",
      "epoch: 370, training loss: 1.180398941040039, testing loss: 1.695317268371582\n",
      "Accuracy: 0.4437\n",
      "epoch: 370, training loss: 1.2798675298690796, testing loss: 1.7147189378738403\n",
      "Accuracy: 0.4212\n",
      "epoch: 370, training loss: 1.2334579229354858, testing loss: 1.6632938385009766\n",
      "Accuracy: 0.4303\n",
      "epoch: 370, training loss: 1.3003008365631104, testing loss: 1.576735258102417\n",
      "Accuracy: 0.4181\n",
      "epoch: 370, training loss: 1.2853667736053467, testing loss: 1.7230167388916016\n",
      "Accuracy: 0.3799\n",
      "epoch: 370, training loss: 1.2818632125854492, testing loss: 1.6842963695526123\n",
      "Accuracy: 0.4694\n",
      "epoch: 370, training loss: 1.2897964715957642, testing loss: 1.6139745712280273\n",
      "Accuracy: 0.4427\n",
      "epoch: 370, training loss: 1.2447353601455688, testing loss: 1.6780976057052612\n",
      "Accuracy: 0.3853\n",
      "epoch: 370, training loss: 1.175284504890442, testing loss: 1.6195937395095825\n",
      "Accuracy: 0.4300\n",
      "epoch: 370, training loss: 1.2626787424087524, testing loss: 1.6604419946670532\n",
      "Accuracy: 0.4589\n",
      "epoch: 370, training loss: 1.3040887117385864, testing loss: 1.6091269254684448\n",
      "Accuracy: 0.4536\n",
      "epoch: 370, training loss: 1.3211456537246704, testing loss: 1.7878326177597046\n",
      "Accuracy: 0.3988\n",
      "epoch: 370, training loss: 1.2180577516555786, testing loss: 1.674042820930481\n",
      "Accuracy: 0.4114\n",
      "epoch: 370, training loss: 1.309698224067688, testing loss: 1.7733265161514282\n",
      "Accuracy: 0.3722\n",
      "epoch: 370, training loss: 1.2471628189086914, testing loss: 1.6771832704544067\n",
      "Accuracy: 0.4244\n",
      "epoch: 370, training loss: 1.3095945119857788, testing loss: 1.607017993927002\n",
      "Accuracy: 0.4243\n",
      "epoch: 370, training loss: 1.2665529251098633, testing loss: 1.8103505373001099\n",
      "Accuracy: 0.4145\n",
      "epoch: 370, training loss: 1.3062015771865845, testing loss: 1.756226658821106\n",
      "Accuracy: 0.4377\n",
      "epoch: 370, training loss: 1.2852462530136108, testing loss: 1.670908808708191\n",
      "Accuracy: 0.4256\n",
      "epoch: 370, training loss: 1.2796516418457031, testing loss: 1.735571026802063\n",
      "Accuracy: 0.4169\n",
      "epoch: 370, training loss: 1.2729871273040771, testing loss: 1.670960783958435\n",
      "Accuracy: 0.4564\n",
      "epoch: 370, training loss: 1.2482177019119263, testing loss: 1.6625230312347412\n",
      "Accuracy: 0.4116\n",
      "epoch: 370, training loss: 1.2505487203598022, testing loss: 1.6977427005767822\n",
      "Accuracy: 0.4006\n",
      "epoch: 370, training loss: 1.272318959236145, testing loss: 1.7422080039978027\n",
      "Accuracy: 0.3821\n",
      "epoch: 370, training loss: 1.2184096574783325, testing loss: 1.6634693145751953\n",
      "Accuracy: 0.4540\n",
      "epoch: 370, training loss: 1.3119611740112305, testing loss: 1.7185384035110474\n",
      "Accuracy: 0.4329\n",
      "epoch: 370, training loss: 1.3250713348388672, testing loss: 1.680578351020813\n",
      "Accuracy: 0.4246\n",
      "epoch: 370, training loss: 1.2589750289916992, testing loss: 1.7607448101043701\n",
      "Accuracy: 0.3848\n",
      "epoch: 370, training loss: 1.4098646640777588, testing loss: 1.533595085144043\n",
      "Accuracy: 0.4799\n",
      "epoch: 370, training loss: 1.3132411241531372, testing loss: 1.565259575843811\n",
      "Accuracy: 0.4369\n",
      "epoch: 370, training loss: 1.238095998764038, testing loss: 1.7178963422775269\n",
      "Accuracy: 0.4142\n",
      "epoch: 370, training loss: 1.2434600591659546, testing loss: 1.643120527267456\n",
      "Accuracy: 0.4043\n",
      "epoch: 370, training loss: 1.1716959476470947, testing loss: 1.6861510276794434\n",
      "Accuracy: 0.4156\n",
      "epoch: 370, training loss: 1.2046871185302734, testing loss: 1.7046486139297485\n",
      "Accuracy: 0.4118\n",
      "epoch: 370, training loss: 1.208898901939392, testing loss: 1.7622284889221191\n",
      "Accuracy: 0.3945\n",
      "epoch: 370, training loss: 1.2993797063827515, testing loss: 1.7075434923171997\n",
      "Accuracy: 0.4089\n",
      "epoch: 370, training loss: 1.1728755235671997, testing loss: 1.7509030103683472\n",
      "Accuracy: 0.4309\n",
      "epoch: 370, training loss: 1.1851664781570435, testing loss: 1.5386935472488403\n",
      "Accuracy: 0.4829\n",
      "epoch: 370, training loss: 1.335569977760315, testing loss: 1.523868203163147\n",
      "Accuracy: 0.4920\n",
      "epoch: 370, training loss: 1.2296479940414429, testing loss: 1.601125955581665\n",
      "Accuracy: 0.4595\n",
      "epoch: 370, training loss: 1.2607815265655518, testing loss: 1.7100013494491577\n",
      "Accuracy: 0.3921\n",
      "epoch: 370, training loss: 1.2793806791305542, testing loss: 1.6333876848220825\n",
      "Accuracy: 0.4230\n",
      "epoch: 370, training loss: 1.2404234409332275, testing loss: 1.6580853462219238\n",
      "Accuracy: 0.4118\n",
      "epoch: 370, training loss: 1.2942078113555908, testing loss: 1.7267396450042725\n",
      "Accuracy: 0.4088\n",
      "epoch: 370, training loss: 1.257388710975647, testing loss: 1.7228659391403198\n",
      "Accuracy: 0.4181\n",
      "epoch: 370, training loss: 1.2908767461776733, testing loss: 1.7184405326843262\n",
      "Accuracy: 0.4000\n",
      "epoch: 370, training loss: 1.3055998086929321, testing loss: 1.6514114141464233\n",
      "Accuracy: 0.4038\n",
      "epoch: 370, training loss: 1.252453327178955, testing loss: 1.6905899047851562\n",
      "Accuracy: 0.4200\n",
      "epoch: 370, training loss: 1.2557636499404907, testing loss: 1.6185519695281982\n",
      "Accuracy: 0.4056\n",
      "epoch: 370, training loss: 1.2939989566802979, testing loss: 1.6369202136993408\n",
      "Accuracy: 0.4026\n",
      "epoch: 370, training loss: 1.2191168069839478, testing loss: 1.7037689685821533\n",
      "Accuracy: 0.4169\n",
      "epoch: 370, training loss: 1.3058216571807861, testing loss: 1.7952229976654053\n",
      "Accuracy: 0.4159\n",
      "epoch: 370, training loss: 1.2255537509918213, testing loss: 1.6562796831130981\n",
      "Accuracy: 0.4025\n",
      "epoch: 370, training loss: 1.272328495979309, testing loss: 1.6134668588638306\n",
      "Accuracy: 0.4116\n",
      "epoch: 370, training loss: 1.2032105922698975, testing loss: 1.648573637008667\n",
      "Accuracy: 0.4375\n",
      "epoch: 370, training loss: 1.1984589099884033, testing loss: 1.7666289806365967\n",
      "Accuracy: 0.4267\n",
      "epoch: 370, training loss: 1.2193294763565063, testing loss: 1.699148178100586\n",
      "Accuracy: 0.4787\n",
      "epoch: 370, training loss: 1.2663016319274902, testing loss: 1.6996876001358032\n",
      "Accuracy: 0.4375\n",
      "epoch: 370, training loss: 1.2806702852249146, testing loss: 1.6654940843582153\n",
      "Accuracy: 0.3975\n",
      "epoch: 370, training loss: 1.2944245338439941, testing loss: 1.6253430843353271\n",
      "Accuracy: 0.4153\n",
      "epoch: 370, training loss: 1.2375766038894653, testing loss: 1.5301181077957153\n",
      "Accuracy: 0.4527\n",
      "epoch: 370, training loss: 1.33165442943573, testing loss: 1.5413398742675781\n",
      "Accuracy: 0.4603\n",
      "epoch: 370, training loss: 1.2790724039077759, testing loss: 1.6369839906692505\n",
      "Accuracy: 0.4464\n",
      "epoch: 370, training loss: 1.2552450895309448, testing loss: 1.6536887884140015\n",
      "Accuracy: 0.4070\n",
      "epoch: 370, training loss: 1.3030787706375122, testing loss: 1.7730379104614258\n",
      "Accuracy: 0.3876\n",
      "epoch: 370, training loss: 1.318515419960022, testing loss: 1.558316946029663\n",
      "Accuracy: 0.4595\n",
      "epoch: 370, training loss: 1.2444510459899902, testing loss: 1.6760449409484863\n",
      "Accuracy: 0.3887\n",
      "epoch: 370, training loss: 1.256519079208374, testing loss: 1.551209807395935\n",
      "Accuracy: 0.4441\n",
      "epoch: 370, training loss: 1.330578327178955, testing loss: 1.715744972229004\n",
      "Accuracy: 0.3846\n",
      "epoch: 370, training loss: 1.2299391031265259, testing loss: 1.628897786140442\n",
      "Accuracy: 0.4429\n",
      "epoch: 370, training loss: 1.2453628778457642, testing loss: 1.7811602354049683\n",
      "Accuracy: 0.3824\n",
      "epoch: 370, training loss: 1.2663670778274536, testing loss: 1.6891063451766968\n",
      "Accuracy: 0.4322\n",
      "epoch: 370, training loss: 1.2592663764953613, testing loss: 1.698438286781311\n",
      "Accuracy: 0.4548\n",
      "epoch: 370, training loss: 1.3022725582122803, testing loss: 1.7305023670196533\n",
      "Accuracy: 0.3750\n",
      "epoch: 370, training loss: 1.2662975788116455, testing loss: 1.7240718603134155\n",
      "Accuracy: 0.4033\n",
      "epoch: 370, training loss: 1.2157586812973022, testing loss: 1.7356595993041992\n",
      "Accuracy: 0.3877\n",
      "epoch: 370, training loss: 1.2883473634719849, testing loss: 1.6835397481918335\n",
      "Accuracy: 0.4019\n",
      "epoch: 370, training loss: 1.2294621467590332, testing loss: 1.7115622758865356\n",
      "Accuracy: 0.4497\n",
      "epoch: 370, training loss: 1.2460497617721558, testing loss: 1.5582062005996704\n",
      "Accuracy: 0.4484\n",
      "epoch: 370, training loss: 1.23301100730896, testing loss: 1.7254441976547241\n",
      "Accuracy: 0.4107\n",
      "epoch: 370, training loss: 1.2642650604248047, testing loss: 1.791265606880188\n",
      "Accuracy: 0.4000\n",
      "epoch: 370, training loss: 1.2928431034088135, testing loss: 1.6189026832580566\n",
      "Accuracy: 0.4281\n",
      "epoch: 370, training loss: 1.2805184125900269, testing loss: 1.6913344860076904\n",
      "Accuracy: 0.4625\n",
      "epoch: 370, training loss: 1.2202657461166382, testing loss: 1.6793538331985474\n",
      "Accuracy: 0.4295\n",
      "epoch: 370, training loss: 1.2672615051269531, testing loss: 1.6897460222244263\n",
      "Accuracy: 0.4125\n",
      "epoch: 370, training loss: 1.2567890882492065, testing loss: 1.7566635608673096\n",
      "Accuracy: 0.4006\n",
      "epoch: 370, training loss: 1.2436540126800537, testing loss: 1.7387516498565674\n",
      "Accuracy: 0.4058\n",
      "epoch: 380, training loss: 1.2539856433868408, testing loss: 1.5669187307357788\n",
      "Accuracy: 0.4231\n",
      "epoch: 380, training loss: 1.262113094329834, testing loss: 1.8018462657928467\n",
      "Accuracy: 0.4207\n",
      "epoch: 380, training loss: 1.233586311340332, testing loss: 1.6974538564682007\n",
      "Accuracy: 0.4107\n",
      "epoch: 380, training loss: 1.2534735202789307, testing loss: 1.7133840322494507\n",
      "Accuracy: 0.3765\n",
      "epoch: 380, training loss: 1.326997995376587, testing loss: 1.7100073099136353\n",
      "Accuracy: 0.3799\n",
      "epoch: 380, training loss: 1.3207913637161255, testing loss: 1.678180456161499\n",
      "Accuracy: 0.3836\n",
      "epoch: 380, training loss: 1.2510567903518677, testing loss: 1.7191165685653687\n",
      "Accuracy: 0.4390\n",
      "epoch: 380, training loss: 1.2449308633804321, testing loss: 1.8470089435577393\n",
      "Accuracy: 0.4281\n",
      "epoch: 380, training loss: 1.2702372074127197, testing loss: 1.5789551734924316\n",
      "Accuracy: 0.4252\n",
      "epoch: 380, training loss: 1.2798207998275757, testing loss: 1.6795930862426758\n",
      "Accuracy: 0.4247\n",
      "epoch: 380, training loss: 1.2136569023132324, testing loss: 1.9048333168029785\n",
      "Accuracy: 0.4294\n",
      "epoch: 380, training loss: 1.2019942998886108, testing loss: 1.6852140426635742\n",
      "Accuracy: 0.4043\n",
      "epoch: 380, training loss: 1.2737226486206055, testing loss: 1.680816650390625\n",
      "Accuracy: 0.3939\n",
      "epoch: 380, training loss: 1.2159689664840698, testing loss: 1.647585391998291\n",
      "Accuracy: 0.4252\n",
      "epoch: 380, training loss: 1.2011135816574097, testing loss: 1.5858818292617798\n",
      "Accuracy: 0.4542\n",
      "epoch: 380, training loss: 1.2215828895568848, testing loss: 1.6389802694320679\n",
      "Accuracy: 0.4369\n",
      "epoch: 380, training loss: 1.2474242448806763, testing loss: 1.7127788066864014\n",
      "Accuracy: 0.4495\n",
      "epoch: 380, training loss: 1.2788875102996826, testing loss: 1.5849461555480957\n",
      "Accuracy: 0.4317\n",
      "epoch: 380, training loss: 1.217824101448059, testing loss: 1.7614850997924805\n",
      "Accuracy: 0.3605\n",
      "epoch: 380, training loss: 1.2108829021453857, testing loss: 1.8981748819351196\n",
      "Accuracy: 0.4087\n",
      "epoch: 380, training loss: 1.2560917139053345, testing loss: 1.7111952304840088\n",
      "Accuracy: 0.3871\n",
      "epoch: 380, training loss: 1.227051019668579, testing loss: 1.5831025838851929\n",
      "Accuracy: 0.4539\n",
      "epoch: 380, training loss: 1.2823156118392944, testing loss: 1.722474217414856\n",
      "Accuracy: 0.3807\n",
      "epoch: 380, training loss: 1.216057538986206, testing loss: 1.6052255630493164\n",
      "Accuracy: 0.4396\n",
      "epoch: 380, training loss: 1.2960249185562134, testing loss: 1.7049977779388428\n",
      "Accuracy: 0.4241\n",
      "epoch: 380, training loss: 1.2149200439453125, testing loss: 1.644499659538269\n",
      "Accuracy: 0.4202\n",
      "epoch: 380, training loss: 1.2911416292190552, testing loss: 1.665419340133667\n",
      "Accuracy: 0.4337\n",
      "epoch: 380, training loss: 1.2623891830444336, testing loss: 1.6618623733520508\n",
      "Accuracy: 0.4211\n",
      "epoch: 380, training loss: 1.2801328897476196, testing loss: 1.727014422416687\n",
      "Accuracy: 0.4105\n",
      "epoch: 380, training loss: 1.2511768341064453, testing loss: 1.6245003938674927\n",
      "Accuracy: 0.4132\n",
      "epoch: 380, training loss: 1.278683066368103, testing loss: 1.637361764907837\n",
      "Accuracy: 0.3710\n",
      "epoch: 380, training loss: 1.2408390045166016, testing loss: 1.716802954673767\n",
      "Accuracy: 0.3962\n",
      "epoch: 380, training loss: 1.2525004148483276, testing loss: 1.6976276636123657\n",
      "Accuracy: 0.4333\n",
      "epoch: 380, training loss: 1.2271835803985596, testing loss: 1.784443736076355\n",
      "Accuracy: 0.3706\n",
      "epoch: 380, training loss: 1.2955546379089355, testing loss: 1.6030787229537964\n",
      "Accuracy: 0.4628\n",
      "epoch: 380, training loss: 1.3158835172653198, testing loss: 1.60483717918396\n",
      "Accuracy: 0.4286\n",
      "epoch: 380, training loss: 1.2461082935333252, testing loss: 1.6714279651641846\n",
      "Accuracy: 0.4172\n",
      "epoch: 380, training loss: 1.2505327463150024, testing loss: 1.6351693868637085\n",
      "Accuracy: 0.4476\n",
      "epoch: 380, training loss: 1.1994197368621826, testing loss: 1.72779381275177\n",
      "Accuracy: 0.4234\n",
      "epoch: 380, training loss: 1.2744008302688599, testing loss: 1.775622844696045\n",
      "Accuracy: 0.3987\n",
      "epoch: 380, training loss: 1.2941694259643555, testing loss: 1.8810582160949707\n",
      "Accuracy: 0.3711\n",
      "epoch: 380, training loss: 1.1699225902557373, testing loss: 1.7448458671569824\n",
      "Accuracy: 0.4416\n",
      "epoch: 380, training loss: 1.191038727760315, testing loss: 1.8859518766403198\n",
      "Accuracy: 0.3654\n",
      "epoch: 380, training loss: 1.2799460887908936, testing loss: 1.6271979808807373\n",
      "Accuracy: 0.4448\n",
      "epoch: 380, training loss: 1.2425047159194946, testing loss: 1.6837217807769775\n",
      "Accuracy: 0.4416\n",
      "epoch: 380, training loss: 1.2805392742156982, testing loss: 1.6533033847808838\n",
      "Accuracy: 0.4503\n",
      "epoch: 380, training loss: 1.1849702596664429, testing loss: 1.6534397602081299\n",
      "Accuracy: 0.4515\n",
      "epoch: 380, training loss: 1.2974026203155518, testing loss: 1.7183557748794556\n",
      "Accuracy: 0.4286\n",
      "epoch: 380, training loss: 1.269316554069519, testing loss: 1.6019659042358398\n",
      "Accuracy: 0.4345\n",
      "epoch: 380, training loss: 1.3440985679626465, testing loss: 1.7198578119277954\n",
      "Accuracy: 0.4172\n",
      "epoch: 380, training loss: 1.2797520160675049, testing loss: 1.5176746845245361\n",
      "Accuracy: 0.4687\n",
      "epoch: 380, training loss: 1.2140390872955322, testing loss: 1.7877845764160156\n",
      "Accuracy: 0.3754\n",
      "epoch: 380, training loss: 1.2667571306228638, testing loss: 1.7857340574264526\n",
      "Accuracy: 0.3867\n",
      "epoch: 380, training loss: 1.2650976181030273, testing loss: 1.5986168384552002\n",
      "Accuracy: 0.4236\n",
      "epoch: 380, training loss: 1.196567177772522, testing loss: 1.7623356580734253\n",
      "Accuracy: 0.4232\n",
      "epoch: 380, training loss: 1.3025424480438232, testing loss: 1.8088144063949585\n",
      "Accuracy: 0.4327\n",
      "epoch: 380, training loss: 1.2635478973388672, testing loss: 1.830011248588562\n",
      "Accuracy: 0.3762\n",
      "epoch: 380, training loss: 1.2774477005004883, testing loss: 1.6197117567062378\n",
      "Accuracy: 0.4796\n",
      "epoch: 380, training loss: 1.188391923904419, testing loss: 1.6552815437316895\n",
      "Accuracy: 0.4620\n",
      "epoch: 380, training loss: 1.1728966236114502, testing loss: 1.7364685535430908\n",
      "Accuracy: 0.4314\n",
      "epoch: 380, training loss: 1.201389193534851, testing loss: 1.6016652584075928\n",
      "Accuracy: 0.4732\n",
      "epoch: 380, training loss: 1.2694984674453735, testing loss: 1.7469091415405273\n",
      "Accuracy: 0.4000\n",
      "epoch: 380, training loss: 1.303343653678894, testing loss: 1.8408973217010498\n",
      "Accuracy: 0.4290\n",
      "epoch: 380, training loss: 1.2694162130355835, testing loss: 1.6802427768707275\n",
      "Accuracy: 0.4155\n",
      "epoch: 380, training loss: 1.3041996955871582, testing loss: 1.6699432134628296\n",
      "Accuracy: 0.4263\n",
      "epoch: 380, training loss: 1.2263102531433105, testing loss: 1.7040096521377563\n",
      "Accuracy: 0.4525\n",
      "epoch: 380, training loss: 1.224767804145813, testing loss: 1.7924050092697144\n",
      "Accuracy: 0.4169\n",
      "epoch: 380, training loss: 1.2964155673980713, testing loss: 1.6975634098052979\n",
      "Accuracy: 0.4105\n",
      "epoch: 380, training loss: 1.2312946319580078, testing loss: 1.540583848953247\n",
      "Accuracy: 0.4689\n",
      "epoch: 380, training loss: 1.178110957145691, testing loss: 1.8579031229019165\n",
      "Accuracy: 0.3833\n",
      "epoch: 380, training loss: 1.2780331373214722, testing loss: 1.6205271482467651\n",
      "Accuracy: 0.4467\n",
      "epoch: 380, training loss: 1.2880440950393677, testing loss: 1.7041412591934204\n",
      "Accuracy: 0.4199\n",
      "epoch: 380, training loss: 1.1984727382659912, testing loss: 1.7328989505767822\n",
      "Accuracy: 0.3922\n",
      "epoch: 380, training loss: 1.2667838335037231, testing loss: 1.7455028295516968\n",
      "Accuracy: 0.3917\n",
      "epoch: 380, training loss: 1.2747284173965454, testing loss: 1.7222778797149658\n",
      "Accuracy: 0.4159\n",
      "epoch: 380, training loss: 1.2066738605499268, testing loss: 1.9419389963150024\n",
      "Accuracy: 0.3729\n",
      "epoch: 380, training loss: 1.294170618057251, testing loss: 1.7557501792907715\n",
      "Accuracy: 0.4180\n",
      "epoch: 380, training loss: 1.2763510942459106, testing loss: 1.7533421516418457\n",
      "Accuracy: 0.4227\n",
      "epoch: 380, training loss: 1.2959235906600952, testing loss: 1.9101356267929077\n",
      "Accuracy: 0.3750\n",
      "epoch: 380, training loss: 1.2053451538085938, testing loss: 1.6354893445968628\n",
      "Accuracy: 0.4230\n",
      "epoch: 380, training loss: 1.3103439807891846, testing loss: 1.673802375793457\n",
      "Accuracy: 0.4373\n",
      "epoch: 380, training loss: 1.276483416557312, testing loss: 1.647341012954712\n",
      "Accuracy: 0.4512\n",
      "epoch: 380, training loss: 1.2764285802841187, testing loss: 1.6618025302886963\n",
      "Accuracy: 0.4451\n",
      "epoch: 380, training loss: 1.2117395401000977, testing loss: 1.7936617136001587\n",
      "Accuracy: 0.4407\n",
      "epoch: 380, training loss: 1.249556541442871, testing loss: 1.6719980239868164\n",
      "Accuracy: 0.4314\n",
      "epoch: 380, training loss: 1.2502491474151611, testing loss: 1.7371413707733154\n",
      "Accuracy: 0.4305\n",
      "epoch: 380, training loss: 1.2094330787658691, testing loss: 1.8206323385238647\n",
      "Accuracy: 0.4249\n",
      "epoch: 380, training loss: 1.1630041599273682, testing loss: 1.7697778940200806\n",
      "Accuracy: 0.3968\n",
      "epoch: 380, training loss: 1.305335283279419, testing loss: 1.8083083629608154\n",
      "Accuracy: 0.4465\n",
      "epoch: 380, training loss: 1.3009896278381348, testing loss: 1.7405110597610474\n",
      "Accuracy: 0.4272\n",
      "epoch: 380, training loss: 1.2532917261123657, testing loss: 1.750756859779358\n",
      "Accuracy: 0.3903\n",
      "epoch: 380, training loss: 1.2562487125396729, testing loss: 1.7033061981201172\n",
      "Accuracy: 0.4055\n",
      "epoch: 380, training loss: 1.2307240962982178, testing loss: 1.6605474948883057\n",
      "Accuracy: 0.4311\n",
      "epoch: 380, training loss: 1.2724523544311523, testing loss: 1.689455509185791\n",
      "Accuracy: 0.3904\n",
      "epoch: 380, training loss: 1.3015167713165283, testing loss: 1.6857290267944336\n",
      "Accuracy: 0.3805\n",
      "epoch: 380, training loss: 1.3092098236083984, testing loss: 1.7717938423156738\n",
      "Accuracy: 0.4201\n",
      "epoch: 380, training loss: 1.3545746803283691, testing loss: 1.6040436029434204\n",
      "Accuracy: 0.4027\n",
      "epoch: 380, training loss: 1.2729064226150513, testing loss: 1.5635570287704468\n",
      "Accuracy: 0.4540\n",
      "epoch: 380, training loss: 1.239119529724121, testing loss: 1.6387064456939697\n",
      "Accuracy: 0.4241\n",
      "epoch: 380, training loss: 1.2590681314468384, testing loss: 1.6301202774047852\n",
      "Accuracy: 0.4085\n",
      "epoch: 390, training loss: 1.2934293746948242, testing loss: 1.6397625207901\n",
      "Accuracy: 0.4455\n",
      "epoch: 390, training loss: 1.2801460027694702, testing loss: 1.6064457893371582\n",
      "Accuracy: 0.4291\n",
      "epoch: 390, training loss: 1.2715131044387817, testing loss: 1.7083286046981812\n",
      "Accuracy: 0.3878\n",
      "epoch: 390, training loss: 1.238044023513794, testing loss: 1.8079551458358765\n",
      "Accuracy: 0.3803\n",
      "epoch: 390, training loss: 1.2953890562057495, testing loss: 1.8107913732528687\n",
      "Accuracy: 0.3770\n",
      "epoch: 390, training loss: 1.206234335899353, testing loss: 1.787792682647705\n",
      "Accuracy: 0.4084\n",
      "epoch: 390, training loss: 1.2409241199493408, testing loss: 1.7292912006378174\n",
      "Accuracy: 0.3947\n",
      "epoch: 390, training loss: 1.361194372177124, testing loss: 1.7740733623504639\n",
      "Accuracy: 0.3675\n",
      "epoch: 390, training loss: 1.1990851163864136, testing loss: 1.599147915840149\n",
      "Accuracy: 0.4604\n",
      "epoch: 390, training loss: 1.1995395421981812, testing loss: 1.6984689235687256\n",
      "Accuracy: 0.4184\n",
      "epoch: 390, training loss: 1.2506643533706665, testing loss: 1.7232502698898315\n",
      "Accuracy: 0.3803\n",
      "epoch: 390, training loss: 1.2842841148376465, testing loss: 1.7426657676696777\n",
      "Accuracy: 0.4066\n",
      "epoch: 390, training loss: 1.2151633501052856, testing loss: 1.6669254302978516\n",
      "Accuracy: 0.4188\n",
      "epoch: 390, training loss: 1.2710617780685425, testing loss: 1.6559581756591797\n",
      "Accuracy: 0.3891\n",
      "epoch: 390, training loss: 1.2042752504348755, testing loss: 1.6883786916732788\n",
      "Accuracy: 0.4618\n",
      "epoch: 390, training loss: 1.2880725860595703, testing loss: 1.7836594581604004\n",
      "Accuracy: 0.4043\n",
      "epoch: 390, training loss: 1.2104648351669312, testing loss: 1.6036862134933472\n",
      "Accuracy: 0.4363\n",
      "epoch: 390, training loss: 1.285751223564148, testing loss: 1.6799545288085938\n",
      "Accuracy: 0.4027\n",
      "epoch: 390, training loss: 1.2373312711715698, testing loss: 1.6447169780731201\n",
      "Accuracy: 0.3946\n",
      "epoch: 390, training loss: 1.3160892724990845, testing loss: 1.6617509126663208\n",
      "Accuracy: 0.4393\n",
      "epoch: 390, training loss: 1.2877472639083862, testing loss: 1.7340993881225586\n",
      "Accuracy: 0.3645\n",
      "epoch: 390, training loss: 1.2405856847763062, testing loss: 1.6146165132522583\n",
      "Accuracy: 0.4039\n",
      "epoch: 390, training loss: 1.2136505842208862, testing loss: 1.7089530229568481\n",
      "Accuracy: 0.3812\n",
      "epoch: 390, training loss: 1.2834315299987793, testing loss: 1.6065970659255981\n",
      "Accuracy: 0.4245\n",
      "epoch: 390, training loss: 1.1958329677581787, testing loss: 1.7612282037734985\n",
      "Accuracy: 0.4161\n",
      "epoch: 390, training loss: 1.229215145111084, testing loss: 1.6559710502624512\n",
      "Accuracy: 0.4078\n",
      "epoch: 390, training loss: 1.2666977643966675, testing loss: 1.6204795837402344\n",
      "Accuracy: 0.4667\n",
      "epoch: 390, training loss: 1.2754757404327393, testing loss: 1.7913322448730469\n",
      "Accuracy: 0.3924\n",
      "epoch: 390, training loss: 1.2603825330734253, testing loss: 1.7095677852630615\n",
      "Accuracy: 0.4079\n",
      "epoch: 390, training loss: 1.2797625064849854, testing loss: 1.8107709884643555\n",
      "Accuracy: 0.3849\n",
      "epoch: 390, training loss: 1.2793140411376953, testing loss: 1.7345832586288452\n",
      "Accuracy: 0.3893\n",
      "epoch: 390, training loss: 1.244290828704834, testing loss: 1.76392662525177\n",
      "Accuracy: 0.4013\n",
      "epoch: 390, training loss: 1.2983460426330566, testing loss: 1.7340688705444336\n",
      "Accuracy: 0.4277\n",
      "epoch: 390, training loss: 1.2473376989364624, testing loss: 1.721779227256775\n",
      "Accuracy: 0.4344\n",
      "epoch: 390, training loss: 1.2361783981323242, testing loss: 1.81743323802948\n",
      "Accuracy: 0.4227\n",
      "epoch: 390, training loss: 1.296526551246643, testing loss: 1.7250169515609741\n",
      "Accuracy: 0.4227\n",
      "epoch: 390, training loss: 1.2665367126464844, testing loss: 1.61540949344635\n",
      "Accuracy: 0.4230\n",
      "epoch: 390, training loss: 1.2712515592575073, testing loss: 1.746199369430542\n",
      "Accuracy: 0.4255\n",
      "epoch: 390, training loss: 1.313966989517212, testing loss: 1.6302815675735474\n",
      "Accuracy: 0.4075\n",
      "epoch: 390, training loss: 1.2446926832199097, testing loss: 1.6673475503921509\n",
      "Accuracy: 0.4082\n",
      "epoch: 390, training loss: 1.1910074949264526, testing loss: 1.6184537410736084\n",
      "Accuracy: 0.4658\n",
      "epoch: 390, training loss: 1.2652419805526733, testing loss: 1.740309476852417\n",
      "Accuracy: 0.4055\n",
      "epoch: 390, training loss: 1.2467186450958252, testing loss: 1.669316053390503\n",
      "Accuracy: 0.4037\n",
      "epoch: 390, training loss: 1.3277530670166016, testing loss: 1.7920774221420288\n",
      "Accuracy: 0.4560\n",
      "epoch: 390, training loss: 1.222928762435913, testing loss: 1.620996117591858\n",
      "Accuracy: 0.4316\n",
      "epoch: 390, training loss: 1.3131061792373657, testing loss: 1.6546303033828735\n",
      "Accuracy: 0.4214\n",
      "epoch: 390, training loss: 1.2512799501419067, testing loss: 1.838257074356079\n",
      "Accuracy: 0.3974\n",
      "epoch: 390, training loss: 1.3087115287780762, testing loss: 1.7444006204605103\n",
      "Accuracy: 0.3828\n",
      "epoch: 390, training loss: 1.2365485429763794, testing loss: 1.6711164712905884\n",
      "Accuracy: 0.4153\n",
      "epoch: 390, training loss: 1.2593492269515991, testing loss: 1.712654948234558\n",
      "Accuracy: 0.3943\n",
      "epoch: 390, training loss: 1.2199907302856445, testing loss: 1.7610411643981934\n",
      "Accuracy: 0.4335\n",
      "epoch: 390, training loss: 1.2937304973602295, testing loss: 1.7332607507705688\n",
      "Accuracy: 0.4425\n",
      "epoch: 390, training loss: 1.2986575365066528, testing loss: 1.7270101308822632\n",
      "Accuracy: 0.4437\n",
      "epoch: 390, training loss: 1.2370109558105469, testing loss: 1.7560913562774658\n",
      "Accuracy: 0.4175\n",
      "epoch: 390, training loss: 1.2146074771881104, testing loss: 1.8054472208023071\n",
      "Accuracy: 0.3986\n",
      "epoch: 390, training loss: 1.1941007375717163, testing loss: 1.7249208688735962\n",
      "Accuracy: 0.4570\n",
      "epoch: 390, training loss: 1.2299081087112427, testing loss: 1.7086786031723022\n",
      "Accuracy: 0.3842\n",
      "epoch: 390, training loss: 1.282658576965332, testing loss: 1.7348781824111938\n",
      "Accuracy: 0.4094\n",
      "epoch: 390, training loss: 1.3258881568908691, testing loss: 1.5933915376663208\n",
      "Accuracy: 0.4066\n",
      "epoch: 390, training loss: 1.2757198810577393, testing loss: 1.7612911462783813\n",
      "Accuracy: 0.4314\n",
      "epoch: 390, training loss: 1.2162353992462158, testing loss: 1.794281244277954\n",
      "Accuracy: 0.3801\n",
      "epoch: 390, training loss: 1.2484564781188965, testing loss: 1.6115057468414307\n",
      "Accuracy: 0.4096\n",
      "epoch: 390, training loss: 1.2668238878250122, testing loss: 1.692647933959961\n",
      "Accuracy: 0.4286\n",
      "epoch: 390, training loss: 1.2676585912704468, testing loss: 1.6614750623703003\n",
      "Accuracy: 0.4497\n",
      "epoch: 390, training loss: 1.235476016998291, testing loss: 1.6395398378372192\n",
      "Accuracy: 0.4140\n",
      "epoch: 390, training loss: 1.2712630033493042, testing loss: 1.6693247556686401\n",
      "Accuracy: 0.4298\n",
      "epoch: 390, training loss: 1.2542332410812378, testing loss: 1.5680266618728638\n",
      "Accuracy: 0.4702\n",
      "epoch: 390, training loss: 1.2117588520050049, testing loss: 1.6758211851119995\n",
      "Accuracy: 0.4301\n",
      "epoch: 390, training loss: 1.2402147054672241, testing loss: 1.7696563005447388\n",
      "Accuracy: 0.4007\n",
      "epoch: 390, training loss: 1.2082616090774536, testing loss: 1.6127214431762695\n",
      "Accuracy: 0.4207\n",
      "epoch: 390, training loss: 1.2046680450439453, testing loss: 1.640866756439209\n",
      "Accuracy: 0.4455\n",
      "epoch: 390, training loss: 1.2521706819534302, testing loss: 1.6522527933120728\n",
      "Accuracy: 0.4615\n",
      "epoch: 390, training loss: 1.3119922876358032, testing loss: 1.54212486743927\n",
      "Accuracy: 0.4748\n",
      "epoch: 390, training loss: 1.2308979034423828, testing loss: 1.6969540119171143\n",
      "Accuracy: 0.4089\n",
      "epoch: 390, training loss: 1.2289131879806519, testing loss: 1.6867589950561523\n",
      "Accuracy: 0.4007\n",
      "epoch: 390, training loss: 1.2500488758087158, testing loss: 1.7370437383651733\n",
      "Accuracy: 0.3227\n",
      "epoch: 390, training loss: 1.2523258924484253, testing loss: 1.6762186288833618\n",
      "Accuracy: 0.3851\n",
      "epoch: 390, training loss: 1.1938505172729492, testing loss: 1.5202478170394897\n",
      "Accuracy: 0.4488\n",
      "epoch: 390, training loss: 1.298110842704773, testing loss: 1.675901174545288\n",
      "Accuracy: 0.4667\n",
      "epoch: 390, training loss: 1.3472820520401, testing loss: 1.6576204299926758\n",
      "Accuracy: 0.4231\n",
      "epoch: 390, training loss: 1.2355587482452393, testing loss: 1.6700184345245361\n",
      "Accuracy: 0.4291\n",
      "epoch: 390, training loss: 1.21142578125, testing loss: 1.7525068521499634\n",
      "Accuracy: 0.3725\n",
      "epoch: 390, training loss: 1.3200812339782715, testing loss: 1.8010287284851074\n",
      "Accuracy: 0.3563\n",
      "epoch: 390, training loss: 1.232992172241211, testing loss: 1.6030033826828003\n",
      "Accuracy: 0.4371\n",
      "epoch: 390, training loss: 1.2716174125671387, testing loss: 1.649497628211975\n",
      "Accuracy: 0.3958\n",
      "epoch: 390, training loss: 1.2837417125701904, testing loss: 1.5158737897872925\n",
      "Accuracy: 0.4516\n",
      "epoch: 390, training loss: 1.2452189922332764, testing loss: 1.6614023447036743\n",
      "Accuracy: 0.4932\n",
      "epoch: 390, training loss: 1.2658547163009644, testing loss: 1.5864149332046509\n",
      "Accuracy: 0.4184\n",
      "epoch: 390, training loss: 1.2541851997375488, testing loss: 1.699808120727539\n",
      "Accuracy: 0.3987\n",
      "epoch: 390, training loss: 1.2849010229110718, testing loss: 1.848730206489563\n",
      "Accuracy: 0.3611\n",
      "epoch: 390, training loss: 1.226359486579895, testing loss: 1.6532198190689087\n",
      "Accuracy: 0.4387\n",
      "epoch: 390, training loss: 1.2740777730941772, testing loss: 1.6050419807434082\n",
      "Accuracy: 0.4329\n",
      "epoch: 390, training loss: 1.2454854249954224, testing loss: 1.6807829141616821\n",
      "Accuracy: 0.4000\n",
      "epoch: 390, training loss: 1.2237331867218018, testing loss: 1.69852614402771\n",
      "Accuracy: 0.4318\n",
      "epoch: 390, training loss: 1.293797492980957, testing loss: 1.5507748126983643\n",
      "Accuracy: 0.3854\n",
      "epoch: 390, training loss: 1.2539703845977783, testing loss: 1.6140013933181763\n",
      "Accuracy: 0.4847\n",
      "epoch: 390, training loss: 1.3187044858932495, testing loss: 1.6559643745422363\n",
      "Accuracy: 0.4383\n",
      "epoch: 390, training loss: 1.2260140180587769, testing loss: 1.5959655046463013\n",
      "Accuracy: 0.4652\n",
      "epoch: 390, training loss: 1.315497875213623, testing loss: 1.6742987632751465\n",
      "Accuracy: 0.4075\n",
      "epoch: 390, training loss: 1.2289350032806396, testing loss: 1.652666449546814\n",
      "Accuracy: 0.3974\n",
      "epoch: 400, training loss: 1.2692797183990479, testing loss: 1.659722089767456\n",
      "Accuracy: 0.4407\n",
      "epoch: 400, training loss: 1.2267568111419678, testing loss: 1.7605979442596436\n",
      "Accuracy: 0.4523\n",
      "epoch: 400, training loss: 1.2728289365768433, testing loss: 1.7880785465240479\n",
      "Accuracy: 0.4157\n",
      "epoch: 400, training loss: 1.2912663221359253, testing loss: 1.7229644060134888\n",
      "Accuracy: 0.4041\n",
      "epoch: 400, training loss: 1.2707593441009521, testing loss: 1.6678458452224731\n",
      "Accuracy: 0.4326\n",
      "epoch: 400, training loss: 1.272153377532959, testing loss: 1.6531105041503906\n",
      "Accuracy: 0.4722\n",
      "epoch: 400, training loss: 1.2070821523666382, testing loss: 1.7080316543579102\n",
      "Accuracy: 0.4031\n",
      "epoch: 400, training loss: 1.213125228881836, testing loss: 1.625386357307434\n",
      "Accuracy: 0.4183\n",
      "epoch: 400, training loss: 1.319846272468567, testing loss: 1.7321839332580566\n",
      "Accuracy: 0.4363\n",
      "epoch: 400, training loss: 1.253206729888916, testing loss: 1.701512098312378\n",
      "Accuracy: 0.4194\n",
      "epoch: 400, training loss: 1.2235913276672363, testing loss: 1.6040347814559937\n",
      "Accuracy: 0.4768\n",
      "epoch: 400, training loss: 1.2479273080825806, testing loss: 1.7112205028533936\n",
      "Accuracy: 0.4085\n",
      "epoch: 400, training loss: 1.23245370388031, testing loss: 1.6365147829055786\n",
      "Accuracy: 0.3636\n",
      "epoch: 400, training loss: 1.2533259391784668, testing loss: 1.5634653568267822\n",
      "Accuracy: 0.4647\n",
      "epoch: 400, training loss: 1.2874282598495483, testing loss: 1.6481316089630127\n",
      "Accuracy: 0.4199\n",
      "epoch: 400, training loss: 1.2140848636627197, testing loss: 1.7423439025878906\n",
      "Accuracy: 0.4153\n",
      "epoch: 400, training loss: 1.2330348491668701, testing loss: 1.6223927736282349\n",
      "Accuracy: 0.4366\n",
      "epoch: 400, training loss: 1.2647120952606201, testing loss: 1.5849542617797852\n",
      "Accuracy: 0.4320\n",
      "epoch: 400, training loss: 1.3031243085861206, testing loss: 1.647361397743225\n",
      "Accuracy: 0.4040\n",
      "epoch: 400, training loss: 1.2093982696533203, testing loss: 1.7951189279556274\n",
      "Accuracy: 0.4267\n",
      "epoch: 400, training loss: 1.2573004961013794, testing loss: 1.648913025856018\n",
      "Accuracy: 0.4523\n",
      "epoch: 400, training loss: 1.2257463932037354, testing loss: 1.69777512550354\n",
      "Accuracy: 0.4051\n",
      "epoch: 400, training loss: 1.2596849203109741, testing loss: 1.6330112218856812\n",
      "Accuracy: 0.4490\n",
      "epoch: 400, training loss: 1.2781013250350952, testing loss: 1.8588613271713257\n",
      "Accuracy: 0.3867\n",
      "epoch: 400, training loss: 1.2235281467437744, testing loss: 1.599692940711975\n",
      "Accuracy: 0.4268\n",
      "epoch: 400, training loss: 1.2099186182022095, testing loss: 1.5524617433547974\n",
      "Accuracy: 0.4524\n",
      "epoch: 400, training loss: 1.2565574645996094, testing loss: 1.6474803686141968\n",
      "Accuracy: 0.4623\n",
      "epoch: 400, training loss: 1.249985694885254, testing loss: 1.7890024185180664\n",
      "Accuracy: 0.4116\n",
      "epoch: 400, training loss: 1.2136205434799194, testing loss: 1.5934324264526367\n",
      "Accuracy: 0.4577\n",
      "epoch: 400, training loss: 1.2151187658309937, testing loss: 1.7691534757614136\n",
      "Accuracy: 0.4248\n",
      "epoch: 400, training loss: 1.228613018989563, testing loss: 1.8822543621063232\n",
      "Accuracy: 0.3478\n",
      "epoch: 400, training loss: 1.2135586738586426, testing loss: 1.6877541542053223\n",
      "Accuracy: 0.4355\n",
      "epoch: 400, training loss: 1.2192624807357788, testing loss: 1.7560241222381592\n",
      "Accuracy: 0.4194\n",
      "epoch: 400, training loss: 1.234081506729126, testing loss: 1.67556631565094\n",
      "Accuracy: 0.4448\n",
      "epoch: 400, training loss: 1.2544023990631104, testing loss: 1.7214529514312744\n",
      "Accuracy: 0.3639\n",
      "epoch: 400, training loss: 1.2246928215026855, testing loss: 1.6387826204299927\n",
      "Accuracy: 0.4422\n",
      "epoch: 400, training loss: 1.2201637029647827, testing loss: 1.7864984273910522\n",
      "Accuracy: 0.3859\n",
      "epoch: 400, training loss: 1.274735689163208, testing loss: 1.8052934408187866\n",
      "Accuracy: 0.4286\n",
      "epoch: 400, training loss: 1.3106558322906494, testing loss: 1.5938303470611572\n",
      "Accuracy: 0.4263\n",
      "epoch: 400, training loss: 1.2737088203430176, testing loss: 1.7792918682098389\n",
      "Accuracy: 0.3934\n",
      "epoch: 400, training loss: 1.3069143295288086, testing loss: 1.7319989204406738\n",
      "Accuracy: 0.4164\n",
      "epoch: 400, training loss: 1.3160762786865234, testing loss: 1.7100493907928467\n",
      "Accuracy: 0.4143\n",
      "epoch: 400, training loss: 1.325866460800171, testing loss: 1.743133306503296\n",
      "Accuracy: 0.4393\n",
      "epoch: 400, training loss: 1.2232471704483032, testing loss: 1.6858272552490234\n",
      "Accuracy: 0.4129\n",
      "epoch: 400, training loss: 1.3188310861587524, testing loss: 1.564666509628296\n",
      "Accuracy: 0.4135\n",
      "epoch: 400, training loss: 1.3101270198822021, testing loss: 1.7753416299819946\n",
      "Accuracy: 0.4197\n",
      "epoch: 400, training loss: 1.2824981212615967, testing loss: 1.8125193119049072\n",
      "Accuracy: 0.3731\n",
      "epoch: 400, training loss: 1.339758276939392, testing loss: 1.5743153095245361\n",
      "Accuracy: 0.4469\n",
      "epoch: 400, training loss: 1.2779603004455566, testing loss: 1.5242608785629272\n",
      "Accuracy: 0.4074\n",
      "epoch: 400, training loss: 1.2390244007110596, testing loss: 1.6475685834884644\n",
      "Accuracy: 0.4266\n",
      "epoch: 400, training loss: 1.2400299310684204, testing loss: 1.6424309015274048\n",
      "Accuracy: 0.4377\n",
      "epoch: 400, training loss: 1.2526487112045288, testing loss: 1.7869268655776978\n",
      "Accuracy: 0.4072\n",
      "epoch: 400, training loss: 1.251578450202942, testing loss: 1.7409238815307617\n",
      "Accuracy: 0.4202\n",
      "epoch: 400, training loss: 1.2109237909317017, testing loss: 1.797036051750183\n",
      "Accuracy: 0.4253\n",
      "epoch: 400, training loss: 1.2683632373809814, testing loss: 1.710762619972229\n",
      "Accuracy: 0.4167\n",
      "epoch: 400, training loss: 1.2260088920593262, testing loss: 1.6505824327468872\n",
      "Accuracy: 0.3903\n",
      "epoch: 400, training loss: 1.1856874227523804, testing loss: 1.7559964656829834\n",
      "Accuracy: 0.4430\n",
      "epoch: 400, training loss: 1.233333945274353, testing loss: 1.7800441980361938\n",
      "Accuracy: 0.3940\n",
      "epoch: 400, training loss: 1.2653251886367798, testing loss: 1.594932198524475\n",
      "Accuracy: 0.4462\n",
      "epoch: 400, training loss: 1.2376742362976074, testing loss: 1.7737057209014893\n",
      "Accuracy: 0.4000\n",
      "epoch: 400, training loss: 1.311721920967102, testing loss: 1.7877405881881714\n",
      "Accuracy: 0.4178\n",
      "epoch: 400, training loss: 1.3069993257522583, testing loss: 1.696939468383789\n",
      "Accuracy: 0.4113\n",
      "epoch: 400, training loss: 1.2659275531768799, testing loss: 1.7142565250396729\n",
      "Accuracy: 0.4364\n",
      "epoch: 400, training loss: 1.2687175273895264, testing loss: 1.8682101964950562\n",
      "Accuracy: 0.4080\n",
      "epoch: 400, training loss: 1.2111599445343018, testing loss: 1.6923694610595703\n",
      "Accuracy: 0.4067\n",
      "epoch: 400, training loss: 1.260032057762146, testing loss: 1.7603881359100342\n",
      "Accuracy: 0.4051\n",
      "epoch: 400, training loss: 1.2703758478164673, testing loss: 1.6217191219329834\n",
      "Accuracy: 0.3878\n",
      "epoch: 400, training loss: 1.2658731937408447, testing loss: 1.7990281581878662\n",
      "Accuracy: 0.3525\n",
      "epoch: 400, training loss: 1.2651143074035645, testing loss: 1.5537545680999756\n",
      "Accuracy: 0.4153\n",
      "epoch: 400, training loss: 1.1386302709579468, testing loss: 1.6677361726760864\n",
      "Accuracy: 0.4120\n",
      "epoch: 400, training loss: 1.2911922931671143, testing loss: 1.659245252609253\n",
      "Accuracy: 0.4423\n",
      "epoch: 400, training loss: 1.253018856048584, testing loss: 1.6721770763397217\n",
      "Accuracy: 0.4065\n",
      "epoch: 400, training loss: 1.2875142097473145, testing loss: 1.719468116760254\n",
      "Accuracy: 0.3994\n",
      "epoch: 400, training loss: 1.2322030067443848, testing loss: 1.7611056566238403\n",
      "Accuracy: 0.4290\n",
      "epoch: 400, training loss: 1.2928520441055298, testing loss: 1.7241188287734985\n",
      "Accuracy: 0.3872\n",
      "epoch: 400, training loss: 1.2869038581848145, testing loss: 1.762015700340271\n",
      "Accuracy: 0.4145\n",
      "epoch: 400, training loss: 1.3066508769989014, testing loss: 1.6898083686828613\n",
      "Accuracy: 0.4609\n",
      "epoch: 400, training loss: 1.2768090963363647, testing loss: 1.6348826885223389\n",
      "Accuracy: 0.4057\n",
      "epoch: 400, training loss: 1.2540130615234375, testing loss: 1.6849485635757446\n",
      "Accuracy: 0.4630\n",
      "epoch: 400, training loss: 1.2453333139419556, testing loss: 1.7380250692367554\n",
      "Accuracy: 0.3875\n",
      "epoch: 400, training loss: 1.2311112880706787, testing loss: 1.766369342803955\n",
      "Accuracy: 0.4101\n",
      "epoch: 400, training loss: 1.2645729780197144, testing loss: 1.7148960828781128\n",
      "Accuracy: 0.4258\n",
      "epoch: 400, training loss: 1.1755188703536987, testing loss: 1.7048999071121216\n",
      "Accuracy: 0.4295\n",
      "epoch: 400, training loss: 1.2344220876693726, testing loss: 1.7040832042694092\n",
      "Accuracy: 0.4310\n",
      "epoch: 400, training loss: 1.3296401500701904, testing loss: 1.6566367149353027\n",
      "Accuracy: 0.4281\n",
      "epoch: 400, training loss: 1.2262433767318726, testing loss: 1.6548324823379517\n",
      "Accuracy: 0.4153\n",
      "epoch: 400, training loss: 1.364641547203064, testing loss: 1.7533087730407715\n",
      "Accuracy: 0.4489\n",
      "epoch: 400, training loss: 1.2625761032104492, testing loss: 1.8165524005889893\n",
      "Accuracy: 0.3917\n",
      "epoch: 400, training loss: 1.2604198455810547, testing loss: 1.7519798278808594\n",
      "Accuracy: 0.3887\n",
      "epoch: 400, training loss: 1.2394015789031982, testing loss: 1.6808656454086304\n",
      "Accuracy: 0.4129\n",
      "epoch: 400, training loss: 1.2404890060424805, testing loss: 1.7648766040802002\n",
      "Accuracy: 0.4141\n",
      "epoch: 400, training loss: 1.3176478147506714, testing loss: 1.766784429550171\n",
      "Accuracy: 0.3910\n",
      "epoch: 400, training loss: 1.2472951412200928, testing loss: 1.7162644863128662\n",
      "Accuracy: 0.4343\n",
      "epoch: 400, training loss: 1.2978531122207642, testing loss: 1.5177336931228638\n",
      "Accuracy: 0.4825\n",
      "epoch: 400, training loss: 1.309917688369751, testing loss: 1.7644190788269043\n",
      "Accuracy: 0.4441\n",
      "epoch: 400, training loss: 1.1761196851730347, testing loss: 1.6913769245147705\n",
      "Accuracy: 0.3947\n",
      "epoch: 400, training loss: 1.280289888381958, testing loss: 1.7451387643814087\n",
      "Accuracy: 0.4006\n",
      "epoch: 400, training loss: 1.2977683544158936, testing loss: 1.8175197839736938\n",
      "Accuracy: 0.3600\n",
      "epoch: 400, training loss: 1.2956326007843018, testing loss: 1.5530496835708618\n",
      "Accuracy: 0.4789\n",
      "epoch: 400, training loss: 1.2551463842391968, testing loss: 1.6968051195144653\n",
      "Accuracy: 0.4054\n",
      "epoch: 410, training loss: 1.28432035446167, testing loss: 1.7688465118408203\n",
      "Accuracy: 0.4585\n",
      "epoch: 410, training loss: 1.27936851978302, testing loss: 1.6284624338150024\n",
      "Accuracy: 0.4411\n",
      "epoch: 410, training loss: 1.2522052526474, testing loss: 1.6385643482208252\n",
      "Accuracy: 0.4511\n",
      "epoch: 410, training loss: 1.2627075910568237, testing loss: 1.6038658618927002\n",
      "Accuracy: 0.4620\n",
      "epoch: 410, training loss: 1.23801851272583, testing loss: 1.5516104698181152\n",
      "Accuracy: 0.4783\n",
      "epoch: 410, training loss: 1.2095205783843994, testing loss: 1.6306073665618896\n",
      "Accuracy: 0.4196\n",
      "epoch: 410, training loss: 1.2941566705703735, testing loss: 1.5255800485610962\n",
      "Accuracy: 0.4863\n",
      "epoch: 410, training loss: 1.232804775238037, testing loss: 1.6029298305511475\n",
      "Accuracy: 0.4551\n",
      "epoch: 410, training loss: 1.2987126111984253, testing loss: 1.7555077075958252\n",
      "Accuracy: 0.4118\n",
      "epoch: 410, training loss: 1.3123817443847656, testing loss: 1.8145381212234497\n",
      "Accuracy: 0.3611\n",
      "epoch: 410, training loss: 1.2309355735778809, testing loss: 1.7613027095794678\n",
      "Accuracy: 0.4272\n",
      "epoch: 410, training loss: 1.283730149269104, testing loss: 1.600395917892456\n",
      "Accuracy: 0.4530\n",
      "epoch: 410, training loss: 1.2253590822219849, testing loss: 1.6329729557037354\n",
      "Accuracy: 0.4354\n",
      "epoch: 410, training loss: 1.3141275644302368, testing loss: 1.6477612257003784\n",
      "Accuracy: 0.4422\n",
      "epoch: 410, training loss: 1.2737946510314941, testing loss: 1.5982155799865723\n",
      "Accuracy: 0.4500\n",
      "epoch: 410, training loss: 1.3657244443893433, testing loss: 1.8514056205749512\n",
      "Accuracy: 0.3710\n",
      "epoch: 410, training loss: 1.2387081384658813, testing loss: 1.6680718660354614\n",
      "Accuracy: 0.3967\n",
      "epoch: 410, training loss: 1.2536025047302246, testing loss: 1.6613651514053345\n",
      "Accuracy: 0.4557\n",
      "epoch: 410, training loss: 1.2140686511993408, testing loss: 1.6323614120483398\n",
      "Accuracy: 0.4189\n",
      "epoch: 410, training loss: 1.2751818895339966, testing loss: 1.6745108366012573\n",
      "Accuracy: 0.4172\n",
      "epoch: 410, training loss: 1.2408086061477661, testing loss: 1.658468246459961\n",
      "Accuracy: 0.4123\n",
      "epoch: 410, training loss: 1.259924292564392, testing loss: 1.6000032424926758\n",
      "Accuracy: 0.4654\n",
      "epoch: 410, training loss: 1.2294844388961792, testing loss: 1.7519445419311523\n",
      "Accuracy: 0.3771\n",
      "epoch: 410, training loss: 1.188046932220459, testing loss: 1.7387739419937134\n",
      "Accuracy: 0.4043\n",
      "epoch: 410, training loss: 1.2498846054077148, testing loss: 1.6909468173980713\n",
      "Accuracy: 0.4043\n",
      "epoch: 410, training loss: 1.2602163553237915, testing loss: 1.7922635078430176\n",
      "Accuracy: 0.4082\n",
      "epoch: 410, training loss: 1.2779061794281006, testing loss: 1.674513578414917\n",
      "Accuracy: 0.4473\n",
      "epoch: 410, training loss: 1.276326298713684, testing loss: 1.7469373941421509\n",
      "Accuracy: 0.4098\n",
      "epoch: 410, training loss: 1.2207067012786865, testing loss: 1.6745046377182007\n",
      "Accuracy: 0.4643\n",
      "epoch: 410, training loss: 1.201350450515747, testing loss: 1.6017783880233765\n",
      "Accuracy: 0.4395\n",
      "epoch: 410, training loss: 1.2995104789733887, testing loss: 1.7521376609802246\n",
      "Accuracy: 0.4224\n",
      "epoch: 410, training loss: 1.2223221063613892, testing loss: 1.6290539503097534\n",
      "Accuracy: 0.4531\n",
      "epoch: 410, training loss: 1.233571171760559, testing loss: 1.7899560928344727\n",
      "Accuracy: 0.4088\n",
      "epoch: 410, training loss: 1.2397898435592651, testing loss: 1.6470733880996704\n",
      "Accuracy: 0.4164\n",
      "epoch: 410, training loss: 1.221295714378357, testing loss: 1.5944082736968994\n",
      "Accuracy: 0.4290\n",
      "epoch: 410, training loss: 1.2490267753601074, testing loss: 1.673250436782837\n",
      "Accuracy: 0.3529\n",
      "epoch: 410, training loss: 1.2218722105026245, testing loss: 1.6306179761886597\n",
      "Accuracy: 0.4175\n",
      "epoch: 410, training loss: 1.2553412914276123, testing loss: 1.783556580543518\n",
      "Accuracy: 0.4564\n",
      "epoch: 410, training loss: 1.2143844366073608, testing loss: 1.6908345222473145\n",
      "Accuracy: 0.3929\n",
      "epoch: 410, training loss: 1.2370554208755493, testing loss: 1.7981257438659668\n",
      "Accuracy: 0.3754\n",
      "epoch: 410, training loss: 1.2060297727584839, testing loss: 1.6337260007858276\n",
      "Accuracy: 0.4072\n",
      "epoch: 410, training loss: 1.2156856060028076, testing loss: 1.854034662246704\n",
      "Accuracy: 0.3958\n",
      "epoch: 410, training loss: 1.219102144241333, testing loss: 1.7169157266616821\n",
      "Accuracy: 0.3902\n",
      "epoch: 410, training loss: 1.1622965335845947, testing loss: 1.5965427160263062\n",
      "Accuracy: 0.4251\n",
      "epoch: 410, training loss: 1.2640031576156616, testing loss: 1.6841398477554321\n",
      "Accuracy: 0.4345\n",
      "epoch: 410, training loss: 1.2339826822280884, testing loss: 1.5556442737579346\n",
      "Accuracy: 0.4557\n",
      "epoch: 410, training loss: 1.2303475141525269, testing loss: 1.7182564735412598\n",
      "Accuracy: 0.4418\n",
      "epoch: 410, training loss: 1.2283365726470947, testing loss: 1.8214887380599976\n",
      "Accuracy: 0.3776\n",
      "epoch: 410, training loss: 1.2330260276794434, testing loss: 1.6581825017929077\n",
      "Accuracy: 0.4768\n",
      "epoch: 410, training loss: 1.2570466995239258, testing loss: 1.7302180528640747\n",
      "Accuracy: 0.4228\n",
      "epoch: 410, training loss: 1.2717207670211792, testing loss: 1.724518060684204\n",
      "Accuracy: 0.4057\n",
      "epoch: 410, training loss: 1.2028937339782715, testing loss: 1.5699825286865234\n",
      "Accuracy: 0.4808\n",
      "epoch: 410, training loss: 1.231882095336914, testing loss: 1.543820858001709\n",
      "Accuracy: 0.4485\n",
      "epoch: 410, training loss: 1.2520661354064941, testing loss: 1.772178053855896\n",
      "Accuracy: 0.3981\n",
      "epoch: 410, training loss: 1.2421540021896362, testing loss: 1.6645973920822144\n",
      "Accuracy: 0.4377\n",
      "epoch: 410, training loss: 1.3101152181625366, testing loss: 1.7306307554244995\n",
      "Accuracy: 0.3404\n",
      "epoch: 410, training loss: 1.2421550750732422, testing loss: 1.8164982795715332\n",
      "Accuracy: 0.3891\n",
      "epoch: 410, training loss: 1.3017582893371582, testing loss: 1.6801421642303467\n",
      "Accuracy: 0.4062\n",
      "epoch: 410, training loss: 1.1993926763534546, testing loss: 1.6747705936431885\n",
      "Accuracy: 0.4214\n",
      "epoch: 410, training loss: 1.277018666267395, testing loss: 1.6689701080322266\n",
      "Accuracy: 0.3836\n",
      "epoch: 410, training loss: 1.2632116079330444, testing loss: 1.7521390914916992\n",
      "Accuracy: 0.3949\n",
      "epoch: 410, training loss: 1.2578703165054321, testing loss: 1.6389975547790527\n",
      "Accuracy: 0.4476\n",
      "epoch: 410, training loss: 1.2149674892425537, testing loss: 1.6661306619644165\n",
      "Accuracy: 0.4330\n",
      "epoch: 410, training loss: 1.266618013381958, testing loss: 1.5954639911651611\n",
      "Accuracy: 0.4346\n",
      "epoch: 410, training loss: 1.2029166221618652, testing loss: 1.648485779762268\n",
      "Accuracy: 0.4503\n",
      "epoch: 410, training loss: 1.243799090385437, testing loss: 1.6468806266784668\n",
      "Accuracy: 0.4402\n",
      "epoch: 410, training loss: 1.2901661396026611, testing loss: 1.7762559652328491\n",
      "Accuracy: 0.4164\n",
      "epoch: 410, training loss: 1.261565089225769, testing loss: 1.6028234958648682\n",
      "Accuracy: 0.4062\n",
      "epoch: 410, training loss: 1.2018516063690186, testing loss: 1.8250584602355957\n",
      "Accuracy: 0.4138\n",
      "epoch: 410, training loss: 1.2153483629226685, testing loss: 1.7110536098480225\n",
      "Accuracy: 0.4431\n",
      "epoch: 410, training loss: 1.1842349767684937, testing loss: 1.7245560884475708\n",
      "Accuracy: 0.4169\n",
      "epoch: 410, training loss: 1.2784944772720337, testing loss: 1.5758579969406128\n",
      "Accuracy: 0.4653\n",
      "epoch: 410, training loss: 1.2353345155715942, testing loss: 1.7806684970855713\n",
      "Accuracy: 0.3542\n",
      "epoch: 410, training loss: 1.304150938987732, testing loss: 1.6351217031478882\n",
      "Accuracy: 0.4531\n",
      "epoch: 410, training loss: 1.2928364276885986, testing loss: 1.7124651670455933\n",
      "Accuracy: 0.4675\n",
      "epoch: 410, training loss: 1.210018515586853, testing loss: 1.736255168914795\n",
      "Accuracy: 0.3776\n",
      "epoch: 410, training loss: 1.2605621814727783, testing loss: 1.6914758682250977\n",
      "Accuracy: 0.4498\n",
      "epoch: 410, training loss: 1.1687666177749634, testing loss: 1.7269331216812134\n",
      "Accuracy: 0.3636\n",
      "epoch: 410, training loss: 1.3051749467849731, testing loss: 1.715946912765503\n",
      "Accuracy: 0.4679\n",
      "epoch: 410, training loss: 1.3371917009353638, testing loss: 1.6641101837158203\n",
      "Accuracy: 0.4468\n",
      "epoch: 410, training loss: 1.2048496007919312, testing loss: 1.7666864395141602\n",
      "Accuracy: 0.4243\n",
      "epoch: 410, training loss: 1.3016897439956665, testing loss: 1.6968681812286377\n",
      "Accuracy: 0.4277\n",
      "epoch: 410, training loss: 1.221942663192749, testing loss: 1.7071871757507324\n",
      "Accuracy: 0.4615\n",
      "epoch: 410, training loss: 1.2811940908432007, testing loss: 1.6869275569915771\n",
      "Accuracy: 0.4356\n",
      "epoch: 410, training loss: 1.2762751579284668, testing loss: 1.6581543684005737\n",
      "Accuracy: 0.4074\n",
      "epoch: 410, training loss: 1.2189255952835083, testing loss: 1.61830472946167\n",
      "Accuracy: 0.4518\n",
      "epoch: 410, training loss: 1.265981912612915, testing loss: 1.711248517036438\n",
      "Accuracy: 0.4059\n",
      "epoch: 410, training loss: 1.2560303211212158, testing loss: 1.6969521045684814\n",
      "Accuracy: 0.4375\n",
      "epoch: 410, training loss: 1.2544687986373901, testing loss: 1.7418394088745117\n",
      "Accuracy: 0.4081\n",
      "epoch: 410, training loss: 1.2716015577316284, testing loss: 1.6317225694656372\n",
      "Accuracy: 0.4514\n",
      "epoch: 410, training loss: 1.2628357410430908, testing loss: 1.5202569961547852\n",
      "Accuracy: 0.4526\n",
      "epoch: 410, training loss: 1.2805123329162598, testing loss: 1.7628079652786255\n",
      "Accuracy: 0.3815\n",
      "epoch: 410, training loss: 1.2960541248321533, testing loss: 1.6792340278625488\n",
      "Accuracy: 0.4299\n",
      "epoch: 410, training loss: 1.2619740962982178, testing loss: 1.726793885231018\n",
      "Accuracy: 0.4485\n",
      "epoch: 410, training loss: 1.3233157396316528, testing loss: 1.738957166671753\n",
      "Accuracy: 0.4271\n",
      "epoch: 410, training loss: 1.3036911487579346, testing loss: 1.775694489479065\n",
      "Accuracy: 0.3834\n",
      "epoch: 410, training loss: 1.3302463293075562, testing loss: 1.6381897926330566\n",
      "Accuracy: 0.4236\n",
      "epoch: 410, training loss: 1.2596004009246826, testing loss: 1.6324572563171387\n",
      "Accuracy: 0.4164\n",
      "epoch: 410, training loss: 1.303411602973938, testing loss: 1.7287269830703735\n",
      "Accuracy: 0.4313\n",
      "epoch: 410, training loss: 1.2993419170379639, testing loss: 1.758376955986023\n",
      "Accuracy: 0.3841\n",
      "epoch: 420, training loss: 1.308549165725708, testing loss: 1.706849455833435\n",
      "Accuracy: 0.4417\n",
      "epoch: 420, training loss: 1.2475091218948364, testing loss: 1.8239179849624634\n",
      "Accuracy: 0.4261\n",
      "epoch: 420, training loss: 1.2692086696624756, testing loss: 1.571683645248413\n",
      "Accuracy: 0.4603\n",
      "epoch: 420, training loss: 1.275527000427246, testing loss: 1.6598880290985107\n",
      "Accuracy: 0.4407\n",
      "epoch: 420, training loss: 1.2526609897613525, testing loss: 1.610425591468811\n",
      "Accuracy: 0.4321\n",
      "epoch: 420, training loss: 1.2184194326400757, testing loss: 1.5949363708496094\n",
      "Accuracy: 0.4413\n",
      "epoch: 420, training loss: 1.2532838582992554, testing loss: 1.648963212966919\n",
      "Accuracy: 0.3861\n",
      "epoch: 420, training loss: 1.2183427810668945, testing loss: 1.771108865737915\n",
      "Accuracy: 0.4094\n",
      "epoch: 420, training loss: 1.2406589984893799, testing loss: 1.7303520441055298\n",
      "Accuracy: 0.4123\n",
      "epoch: 420, training loss: 1.3149069547653198, testing loss: 1.7090826034545898\n",
      "Accuracy: 0.4245\n",
      "epoch: 420, training loss: 1.2214184999465942, testing loss: 1.7398922443389893\n",
      "Accuracy: 0.4415\n",
      "epoch: 420, training loss: 1.2834272384643555, testing loss: 1.69605553150177\n",
      "Accuracy: 0.4329\n",
      "epoch: 420, training loss: 1.2727704048156738, testing loss: 1.7886980772018433\n",
      "Accuracy: 0.4510\n",
      "epoch: 420, training loss: 1.2624865770339966, testing loss: 1.8335713148117065\n",
      "Accuracy: 0.3993\n",
      "epoch: 420, training loss: 1.241974115371704, testing loss: 1.68599271774292\n",
      "Accuracy: 0.4497\n",
      "epoch: 420, training loss: 1.1912606954574585, testing loss: 1.7590910196304321\n",
      "Accuracy: 0.4385\n",
      "epoch: 420, training loss: 1.2552335262298584, testing loss: 1.7902852296829224\n",
      "Accuracy: 0.4343\n",
      "epoch: 420, training loss: 1.2850127220153809, testing loss: 1.7996107339859009\n",
      "Accuracy: 0.4337\n",
      "epoch: 420, training loss: 1.2794281244277954, testing loss: 1.6709654331207275\n",
      "Accuracy: 0.4200\n",
      "epoch: 420, training loss: 1.2681304216384888, testing loss: 1.804007887840271\n",
      "Accuracy: 0.4019\n",
      "epoch: 420, training loss: 1.2469664812088013, testing loss: 1.7857576608657837\n",
      "Accuracy: 0.3941\n",
      "epoch: 420, training loss: 1.3086155652999878, testing loss: 1.6247236728668213\n",
      "Accuracy: 0.4426\n",
      "epoch: 420, training loss: 1.268442153930664, testing loss: 1.8327863216400146\n",
      "Accuracy: 0.4072\n",
      "epoch: 420, training loss: 1.313096523284912, testing loss: 1.6032545566558838\n",
      "Accuracy: 0.4434\n",
      "epoch: 420, training loss: 1.2266954183578491, testing loss: 1.7792145013809204\n",
      "Accuracy: 0.4027\n",
      "epoch: 420, training loss: 1.2779475450515747, testing loss: 1.7358050346374512\n",
      "Accuracy: 0.3729\n",
      "epoch: 420, training loss: 1.3056926727294922, testing loss: 1.8248111009597778\n",
      "Accuracy: 0.4209\n",
      "epoch: 420, training loss: 1.2709910869598389, testing loss: 1.7919453382492065\n",
      "Accuracy: 0.3946\n",
      "epoch: 420, training loss: 1.1876442432403564, testing loss: 1.687061071395874\n",
      "Accuracy: 0.4007\n",
      "epoch: 420, training loss: 1.16728937625885, testing loss: 1.7447015047073364\n",
      "Accuracy: 0.3824\n",
      "epoch: 420, training loss: 1.2564101219177246, testing loss: 1.5894098281860352\n",
      "Accuracy: 0.4448\n",
      "epoch: 420, training loss: 1.182649850845337, testing loss: 1.754883885383606\n",
      "Accuracy: 0.3902\n",
      "epoch: 420, training loss: 1.2323708534240723, testing loss: 1.753931999206543\n",
      "Accuracy: 0.4217\n",
      "epoch: 420, training loss: 1.169350504875183, testing loss: 1.8165667057037354\n",
      "Accuracy: 0.4167\n",
      "epoch: 420, training loss: 1.2858283519744873, testing loss: 1.6777446269989014\n",
      "Accuracy: 0.4195\n",
      "epoch: 420, training loss: 1.2770456075668335, testing loss: 1.6511826515197754\n",
      "Accuracy: 0.4352\n",
      "epoch: 420, training loss: 1.2495625019073486, testing loss: 1.784124493598938\n",
      "Accuracy: 0.3988\n",
      "epoch: 420, training loss: 1.2506670951843262, testing loss: 1.7378144264221191\n",
      "Accuracy: 0.4273\n",
      "epoch: 420, training loss: 1.3791656494140625, testing loss: 1.628957748413086\n",
      "Accuracy: 0.4381\n",
      "epoch: 420, training loss: 1.2136865854263306, testing loss: 1.7178877592086792\n",
      "Accuracy: 0.4383\n",
      "epoch: 420, training loss: 1.2435706853866577, testing loss: 1.6551287174224854\n",
      "Accuracy: 0.4335\n",
      "epoch: 420, training loss: 1.3235770463943481, testing loss: 1.696386694908142\n",
      "Accuracy: 0.4349\n",
      "epoch: 420, training loss: 1.2326058149337769, testing loss: 1.6396840810775757\n",
      "Accuracy: 0.4199\n",
      "epoch: 420, training loss: 1.2693383693695068, testing loss: 1.7089322805404663\n",
      "Accuracy: 0.4078\n",
      "epoch: 420, training loss: 1.3052071332931519, testing loss: 1.6485234498977661\n",
      "Accuracy: 0.4019\n",
      "epoch: 420, training loss: 1.1783639192581177, testing loss: 1.6257096529006958\n",
      "Accuracy: 0.4304\n",
      "epoch: 420, training loss: 1.2514783143997192, testing loss: 1.7074410915374756\n",
      "Accuracy: 0.3988\n",
      "epoch: 420, training loss: 1.2697898149490356, testing loss: 1.5395019054412842\n",
      "Accuracy: 0.4881\n",
      "epoch: 420, training loss: 1.213545322418213, testing loss: 1.822126030921936\n",
      "Accuracy: 0.4139\n",
      "epoch: 420, training loss: 1.2518396377563477, testing loss: 1.7806336879730225\n",
      "Accuracy: 0.4047\n",
      "epoch: 420, training loss: 1.2206363677978516, testing loss: 1.7255257368087769\n",
      "Accuracy: 0.3814\n",
      "epoch: 420, training loss: 1.2315189838409424, testing loss: 1.7085208892822266\n",
      "Accuracy: 0.4087\n",
      "epoch: 420, training loss: 1.2787785530090332, testing loss: 1.8304435014724731\n",
      "Accuracy: 0.3880\n",
      "epoch: 420, training loss: 1.2521861791610718, testing loss: 1.6449832916259766\n",
      "Accuracy: 0.4256\n",
      "epoch: 420, training loss: 1.2114700078964233, testing loss: 1.696721076965332\n",
      "Accuracy: 0.4385\n",
      "epoch: 420, training loss: 1.277872085571289, testing loss: 1.7729614973068237\n",
      "Accuracy: 0.3646\n",
      "epoch: 420, training loss: 1.2914305925369263, testing loss: 1.6968106031417847\n",
      "Accuracy: 0.4052\n",
      "epoch: 420, training loss: 1.222418189048767, testing loss: 1.7039629220962524\n",
      "Accuracy: 0.3662\n",
      "epoch: 420, training loss: 1.2327595949172974, testing loss: 1.6570194959640503\n",
      "Accuracy: 0.3870\n",
      "epoch: 420, training loss: 1.2315690517425537, testing loss: 1.6573781967163086\n",
      "Accuracy: 0.4601\n",
      "epoch: 420, training loss: 1.2595294713974, testing loss: 1.7006211280822754\n",
      "Accuracy: 0.3794\n",
      "epoch: 420, training loss: 1.2773686647415161, testing loss: 1.5566542148590088\n",
      "Accuracy: 0.4455\n",
      "epoch: 420, training loss: 1.2383259534835815, testing loss: 1.7403638362884521\n",
      "Accuracy: 0.4396\n",
      "epoch: 420, training loss: 1.219576120376587, testing loss: 1.691037654876709\n",
      "Accuracy: 0.4414\n",
      "epoch: 420, training loss: 1.290013313293457, testing loss: 1.7519601583480835\n",
      "Accuracy: 0.3969\n",
      "epoch: 420, training loss: 1.2049225568771362, testing loss: 1.725264549255371\n",
      "Accuracy: 0.4272\n",
      "epoch: 420, training loss: 1.3341995477676392, testing loss: 1.6026537418365479\n",
      "Accuracy: 0.4399\n",
      "epoch: 420, training loss: 1.280072808265686, testing loss: 1.8473889827728271\n",
      "Accuracy: 0.4078\n",
      "epoch: 420, training loss: 1.309956669807434, testing loss: 1.597320795059204\n",
      "Accuracy: 0.4429\n",
      "epoch: 420, training loss: 1.3231899738311768, testing loss: 1.6984686851501465\n",
      "Accuracy: 0.4211\n",
      "epoch: 420, training loss: 1.2468304634094238, testing loss: 1.624424695968628\n",
      "Accuracy: 0.4648\n",
      "epoch: 420, training loss: 1.2119778394699097, testing loss: 1.622624397277832\n",
      "Accuracy: 0.4296\n",
      "epoch: 420, training loss: 1.301350474357605, testing loss: 1.6120107173919678\n",
      "Accuracy: 0.4300\n",
      "epoch: 420, training loss: 1.298358678817749, testing loss: 1.6641517877578735\n",
      "Accuracy: 0.4272\n",
      "epoch: 420, training loss: 1.2193529605865479, testing loss: 1.7209773063659668\n",
      "Accuracy: 0.4277\n",
      "epoch: 420, training loss: 1.283403754234314, testing loss: 1.6991642713546753\n",
      "Accuracy: 0.4497\n",
      "epoch: 420, training loss: 1.322507619857788, testing loss: 1.606971025466919\n",
      "Accuracy: 0.4353\n",
      "epoch: 420, training loss: 1.289795160293579, testing loss: 1.6169779300689697\n",
      "Accuracy: 0.4066\n",
      "epoch: 420, training loss: 1.340248703956604, testing loss: 1.713183879852295\n",
      "Accuracy: 0.4295\n",
      "epoch: 420, training loss: 1.2378009557724, testing loss: 1.6999937295913696\n",
      "Accuracy: 0.4086\n",
      "epoch: 420, training loss: 1.3064439296722412, testing loss: 1.5679614543914795\n",
      "Accuracy: 0.4295\n",
      "epoch: 420, training loss: 1.2366153001785278, testing loss: 1.5270590782165527\n",
      "Accuracy: 0.4646\n",
      "epoch: 420, training loss: 1.2839010953903198, testing loss: 1.6259464025497437\n",
      "Accuracy: 0.4249\n",
      "epoch: 420, training loss: 1.2650728225708008, testing loss: 1.839859127998352\n",
      "Accuracy: 0.3968\n",
      "epoch: 420, training loss: 1.3202018737792969, testing loss: 1.7145814895629883\n",
      "Accuracy: 0.4108\n",
      "epoch: 420, training loss: 1.2382090091705322, testing loss: 1.7005432844161987\n",
      "Accuracy: 0.4188\n",
      "epoch: 420, training loss: 1.3401298522949219, testing loss: 1.6569775342941284\n",
      "Accuracy: 0.4055\n",
      "epoch: 420, training loss: 1.2607077360153198, testing loss: 1.5641679763793945\n",
      "Accuracy: 0.3816\n",
      "epoch: 420, training loss: 1.293456792831421, testing loss: 1.812561273574829\n",
      "Accuracy: 0.3947\n",
      "epoch: 420, training loss: 1.2931734323501587, testing loss: 1.6823674440383911\n",
      "Accuracy: 0.4142\n",
      "epoch: 420, training loss: 1.267752766609192, testing loss: 1.712873101234436\n",
      "Accuracy: 0.3746\n",
      "epoch: 420, training loss: 1.228725790977478, testing loss: 1.6515353918075562\n",
      "Accuracy: 0.3714\n",
      "epoch: 420, training loss: 1.2344697713851929, testing loss: 1.772161602973938\n",
      "Accuracy: 0.4079\n",
      "epoch: 420, training loss: 1.2423932552337646, testing loss: 1.7337404489517212\n",
      "Accuracy: 0.4123\n",
      "epoch: 420, training loss: 1.3499435186386108, testing loss: 1.6786556243896484\n",
      "Accuracy: 0.3912\n",
      "epoch: 420, training loss: 1.35365891456604, testing loss: 1.7088154554367065\n",
      "Accuracy: 0.3821\n",
      "epoch: 420, training loss: 1.2177331447601318, testing loss: 1.582131028175354\n",
      "Accuracy: 0.4127\n",
      "epoch: 420, training loss: 1.3266924619674683, testing loss: 1.6379516124725342\n",
      "Accuracy: 0.4603\n",
      "epoch: 420, training loss: 1.2453367710113525, testing loss: 1.6967427730560303\n",
      "Accuracy: 0.4125\n",
      "epoch: 420, training loss: 1.2364552021026611, testing loss: 1.5552997589111328\n",
      "Accuracy: 0.4369\n",
      "epoch: 430, training loss: 1.2791297435760498, testing loss: 1.7691848278045654\n",
      "Accuracy: 0.3762\n",
      "epoch: 430, training loss: 1.2320038080215454, testing loss: 1.7286604642868042\n",
      "Accuracy: 0.3758\n",
      "epoch: 430, training loss: 1.2654106616973877, testing loss: 1.6930408477783203\n",
      "Accuracy: 0.4201\n",
      "epoch: 430, training loss: 1.2709726095199585, testing loss: 1.6693919897079468\n",
      "Accuracy: 0.4007\n",
      "epoch: 430, training loss: 1.325467824935913, testing loss: 1.7107762098312378\n",
      "Accuracy: 0.4232\n",
      "epoch: 430, training loss: 1.23964524269104, testing loss: 1.7490204572677612\n",
      "Accuracy: 0.4110\n",
      "epoch: 430, training loss: 1.2720210552215576, testing loss: 1.714226484298706\n",
      "Accuracy: 0.3994\n",
      "epoch: 430, training loss: 1.3046345710754395, testing loss: 1.842220425605774\n",
      "Accuracy: 0.4007\n",
      "epoch: 430, training loss: 1.250857949256897, testing loss: 1.7093318700790405\n",
      "Accuracy: 0.3968\n",
      "epoch: 430, training loss: 1.2074512243270874, testing loss: 1.7279233932495117\n",
      "Accuracy: 0.3849\n",
      "epoch: 430, training loss: 1.2796918153762817, testing loss: 1.5301731824874878\n",
      "Accuracy: 0.4567\n",
      "epoch: 430, training loss: 1.212567925453186, testing loss: 1.7255350351333618\n",
      "Accuracy: 0.4299\n",
      "epoch: 430, training loss: 1.3029563426971436, testing loss: 1.6401581764221191\n",
      "Accuracy: 0.4194\n",
      "epoch: 430, training loss: 1.271924376487732, testing loss: 1.7653558254241943\n",
      "Accuracy: 0.3871\n",
      "epoch: 430, training loss: 1.2522897720336914, testing loss: 1.461517095565796\n",
      "Accuracy: 0.4608\n",
      "epoch: 430, training loss: 1.2205392122268677, testing loss: 1.676896095275879\n",
      "Accuracy: 0.4033\n",
      "epoch: 430, training loss: 1.2125811576843262, testing loss: 1.6831655502319336\n",
      "Accuracy: 0.4495\n",
      "epoch: 430, training loss: 1.227784276008606, testing loss: 1.607980489730835\n",
      "Accuracy: 0.4667\n",
      "epoch: 430, training loss: 1.2498937845230103, testing loss: 1.718703269958496\n",
      "Accuracy: 0.3974\n",
      "epoch: 430, training loss: 1.3193923234939575, testing loss: 1.673317551612854\n",
      "Accuracy: 0.4625\n",
      "epoch: 430, training loss: 1.2018511295318604, testing loss: 1.6743870973587036\n",
      "Accuracy: 0.4299\n",
      "epoch: 430, training loss: 1.278963327407837, testing loss: 1.771690011024475\n",
      "Accuracy: 0.4030\n",
      "epoch: 430, training loss: 1.233545184135437, testing loss: 1.7318511009216309\n",
      "Accuracy: 0.4123\n",
      "epoch: 430, training loss: 1.2179495096206665, testing loss: 1.766116976737976\n",
      "Accuracy: 0.4401\n",
      "epoch: 430, training loss: 1.2680624723434448, testing loss: 1.7383756637573242\n",
      "Accuracy: 0.4137\n",
      "epoch: 430, training loss: 1.2368687391281128, testing loss: 1.547817587852478\n",
      "Accuracy: 0.4441\n",
      "epoch: 430, training loss: 1.202102780342102, testing loss: 1.7082575559616089\n",
      "Accuracy: 0.4492\n",
      "epoch: 430, training loss: 1.2648473978042603, testing loss: 1.7248032093048096\n",
      "Accuracy: 0.4087\n",
      "epoch: 430, training loss: 1.2012664079666138, testing loss: 1.7220206260681152\n",
      "Accuracy: 0.4085\n",
      "epoch: 430, training loss: 1.2404017448425293, testing loss: 1.7341994047164917\n",
      "Accuracy: 0.4048\n",
      "epoch: 430, training loss: 1.3354859352111816, testing loss: 1.6489245891571045\n",
      "Accuracy: 0.4519\n",
      "epoch: 430, training loss: 1.2811928987503052, testing loss: 1.68267023563385\n",
      "Accuracy: 0.4551\n",
      "epoch: 430, training loss: 1.2267792224884033, testing loss: 1.6853727102279663\n",
      "Accuracy: 0.4177\n",
      "epoch: 430, training loss: 1.2113044261932373, testing loss: 1.6975311040878296\n",
      "Accuracy: 0.4497\n",
      "epoch: 430, training loss: 1.293045997619629, testing loss: 1.7455897331237793\n",
      "Accuracy: 0.3858\n",
      "epoch: 430, training loss: 1.2311346530914307, testing loss: 1.7203831672668457\n",
      "Accuracy: 0.4396\n",
      "epoch: 430, training loss: 1.2699013948440552, testing loss: 1.8255876302719116\n",
      "Accuracy: 0.3750\n",
      "epoch: 430, training loss: 1.299153208732605, testing loss: 1.6359692811965942\n",
      "Accuracy: 0.4385\n",
      "epoch: 430, training loss: 1.2405081987380981, testing loss: 1.5767496824264526\n",
      "Accuracy: 0.4591\n",
      "epoch: 430, training loss: 1.2601104974746704, testing loss: 1.629574179649353\n",
      "Accuracy: 0.4582\n",
      "epoch: 430, training loss: 1.2848719358444214, testing loss: 1.8567912578582764\n",
      "Accuracy: 0.4000\n",
      "epoch: 430, training loss: 1.2043455839157104, testing loss: 1.660294532775879\n",
      "Accuracy: 0.4524\n",
      "epoch: 430, training loss: 1.2589048147201538, testing loss: 1.6313965320587158\n",
      "Accuracy: 0.4387\n",
      "epoch: 430, training loss: 1.296448826789856, testing loss: 1.7236131429672241\n",
      "Accuracy: 0.4258\n",
      "epoch: 430, training loss: 1.2907634973526, testing loss: 1.5946683883666992\n",
      "Accuracy: 0.4319\n",
      "epoch: 430, training loss: 1.2316956520080566, testing loss: 1.7013771533966064\n",
      "Accuracy: 0.4677\n",
      "epoch: 430, training loss: 1.194937825202942, testing loss: 1.7044936418533325\n",
      "Accuracy: 0.4087\n",
      "epoch: 430, training loss: 1.2609888315200806, testing loss: 1.774287223815918\n",
      "Accuracy: 0.3932\n",
      "epoch: 430, training loss: 1.2376898527145386, testing loss: 1.537877082824707\n",
      "Accuracy: 0.4757\n",
      "epoch: 430, training loss: 1.2328964471817017, testing loss: 1.775774598121643\n",
      "Accuracy: 0.3903\n",
      "epoch: 430, training loss: 1.2639672756195068, testing loss: 1.7609888315200806\n",
      "Accuracy: 0.3944\n",
      "epoch: 430, training loss: 1.2353724241256714, testing loss: 1.7461274862289429\n",
      "Accuracy: 0.3806\n",
      "epoch: 430, training loss: 1.2951918840408325, testing loss: 1.6773018836975098\n",
      "Accuracy: 0.4408\n",
      "epoch: 430, training loss: 1.2307411432266235, testing loss: 1.8072936534881592\n",
      "Accuracy: 0.3934\n",
      "epoch: 430, training loss: 1.2978790998458862, testing loss: 1.6546088457107544\n",
      "Accuracy: 0.4487\n",
      "epoch: 430, training loss: 1.2235918045043945, testing loss: 1.7770973443984985\n",
      "Accuracy: 0.4639\n",
      "epoch: 430, training loss: 1.2901719808578491, testing loss: 1.6287094354629517\n",
      "Accuracy: 0.4548\n",
      "epoch: 430, training loss: 1.2139335870742798, testing loss: 1.6889244318008423\n",
      "Accuracy: 0.4290\n",
      "epoch: 430, training loss: 1.2164621353149414, testing loss: 1.6735968589782715\n",
      "Accuracy: 0.4351\n",
      "epoch: 430, training loss: 1.3027870655059814, testing loss: 1.67575204372406\n",
      "Accuracy: 0.4300\n",
      "epoch: 430, training loss: 1.248247504234314, testing loss: 1.6257107257843018\n",
      "Accuracy: 0.4249\n",
      "epoch: 430, training loss: 1.2424308061599731, testing loss: 1.6866196393966675\n",
      "Accuracy: 0.4333\n",
      "epoch: 430, training loss: 1.3026243448257446, testing loss: 1.6401762962341309\n",
      "Accuracy: 0.4051\n",
      "epoch: 430, training loss: 1.220886468887329, testing loss: 1.6597037315368652\n",
      "Accuracy: 0.4034\n",
      "epoch: 430, training loss: 1.2544039487838745, testing loss: 1.6361236572265625\n",
      "Accuracy: 0.4530\n",
      "epoch: 430, training loss: 1.2294074296951294, testing loss: 1.722337245941162\n",
      "Accuracy: 0.4252\n",
      "epoch: 430, training loss: 1.2375640869140625, testing loss: 1.7164371013641357\n",
      "Accuracy: 0.4195\n",
      "epoch: 430, training loss: 1.195534110069275, testing loss: 1.7790709733963013\n",
      "Accuracy: 0.4219\n",
      "epoch: 430, training loss: 1.2350772619247437, testing loss: 1.616601586341858\n",
      "Accuracy: 0.4006\n",
      "epoch: 430, training loss: 1.3084131479263306, testing loss: 1.7061570882797241\n",
      "Accuracy: 0.3885\n",
      "epoch: 430, training loss: 1.205834984779358, testing loss: 1.7649136781692505\n",
      "Accuracy: 0.3892\n",
      "epoch: 430, training loss: 1.310947299003601, testing loss: 1.7007626295089722\n",
      "Accuracy: 0.4272\n",
      "epoch: 430, training loss: 1.240186333656311, testing loss: 1.6138513088226318\n",
      "Accuracy: 0.4710\n",
      "epoch: 430, training loss: 1.2840231657028198, testing loss: 1.6856290102005005\n",
      "Accuracy: 0.4375\n",
      "epoch: 430, training loss: 1.320418119430542, testing loss: 1.7004984617233276\n",
      "Accuracy: 0.4597\n",
      "epoch: 430, training loss: 1.2194381952285767, testing loss: 1.7564351558685303\n",
      "Accuracy: 0.4084\n",
      "epoch: 430, training loss: 1.2448499202728271, testing loss: 1.8229787349700928\n",
      "Accuracy: 0.3588\n",
      "epoch: 430, training loss: 1.2115962505340576, testing loss: 1.6356972455978394\n",
      "Accuracy: 0.4693\n",
      "epoch: 430, training loss: 1.2712500095367432, testing loss: 1.6468877792358398\n",
      "Accuracy: 0.4147\n",
      "epoch: 430, training loss: 1.2062673568725586, testing loss: 1.7080556154251099\n",
      "Accuracy: 0.4863\n",
      "epoch: 430, training loss: 1.2195794582366943, testing loss: 1.7535570859909058\n",
      "Accuracy: 0.4072\n",
      "epoch: 430, training loss: 1.2223427295684814, testing loss: 1.6314557790756226\n",
      "Accuracy: 0.4887\n",
      "epoch: 430, training loss: 1.2382001876831055, testing loss: 1.7588224411010742\n",
      "Accuracy: 0.3993\n",
      "epoch: 430, training loss: 1.2908047437667847, testing loss: 1.7231827974319458\n",
      "Accuracy: 0.4175\n",
      "epoch: 430, training loss: 1.2221052646636963, testing loss: 1.6716010570526123\n",
      "Accuracy: 0.4329\n",
      "epoch: 430, training loss: 1.2941585779190063, testing loss: 1.7447831630706787\n",
      "Accuracy: 0.4214\n",
      "epoch: 430, training loss: 1.2714195251464844, testing loss: 1.7215811014175415\n",
      "Accuracy: 0.4072\n",
      "epoch: 430, training loss: 1.2930171489715576, testing loss: 1.7334612607955933\n",
      "Accuracy: 0.4212\n",
      "epoch: 430, training loss: 1.187619686126709, testing loss: 1.703832745552063\n",
      "Accuracy: 0.4600\n",
      "epoch: 430, training loss: 1.247387409210205, testing loss: 1.7500816583633423\n",
      "Accuracy: 0.3917\n",
      "epoch: 430, training loss: 1.2830055952072144, testing loss: 1.6693369150161743\n",
      "Accuracy: 0.4226\n",
      "epoch: 430, training loss: 1.293278455734253, testing loss: 1.5566166639328003\n",
      "Accuracy: 0.4211\n",
      "epoch: 430, training loss: 1.2033113241195679, testing loss: 1.67061185836792\n",
      "Accuracy: 0.4537\n",
      "epoch: 430, training loss: 1.282821774482727, testing loss: 1.654759168624878\n",
      "Accuracy: 0.3880\n",
      "epoch: 430, training loss: 1.2094764709472656, testing loss: 1.8067102432250977\n",
      "Accuracy: 0.4140\n",
      "epoch: 430, training loss: 1.2428158521652222, testing loss: 1.762987732887268\n",
      "Accuracy: 0.4062\n",
      "epoch: 430, training loss: 1.2804032564163208, testing loss: 1.67835533618927\n",
      "Accuracy: 0.4110\n",
      "epoch: 430, training loss: 1.240877389907837, testing loss: 1.7849751710891724\n",
      "Accuracy: 0.3769\n",
      "epoch: 430, training loss: 1.1809736490249634, testing loss: 1.6724467277526855\n",
      "Accuracy: 0.4214\n",
      "epoch: 430, training loss: 1.2621971368789673, testing loss: 1.7405604124069214\n",
      "Accuracy: 0.4113\n",
      "epoch: 440, training loss: 1.331604242324829, testing loss: 1.7070289850234985\n",
      "Accuracy: 0.4106\n",
      "epoch: 440, training loss: 1.3266737461090088, testing loss: 1.699700117111206\n",
      "Accuracy: 0.3722\n",
      "epoch: 440, training loss: 1.2822785377502441, testing loss: 1.7394030094146729\n",
      "Accuracy: 0.3994\n",
      "epoch: 440, training loss: 1.2642682790756226, testing loss: 1.7300888299942017\n",
      "Accuracy: 0.4362\n",
      "epoch: 440, training loss: 1.2364997863769531, testing loss: 1.584524393081665\n",
      "Accuracy: 0.4161\n",
      "epoch: 440, training loss: 1.238285779953003, testing loss: 1.6135334968566895\n",
      "Accuracy: 0.4252\n",
      "epoch: 440, training loss: 1.2677110433578491, testing loss: 1.6642838716506958\n",
      "Accuracy: 0.4463\n",
      "epoch: 440, training loss: 1.292806625366211, testing loss: 1.6286735534667969\n",
      "Accuracy: 0.4000\n",
      "epoch: 440, training loss: 1.2562968730926514, testing loss: 1.657861590385437\n",
      "Accuracy: 0.4391\n",
      "epoch: 440, training loss: 1.2555837631225586, testing loss: 1.517978549003601\n",
      "Accuracy: 0.4361\n",
      "epoch: 440, training loss: 1.240929126739502, testing loss: 1.7863963842391968\n",
      "Accuracy: 0.4373\n",
      "epoch: 440, training loss: 1.282292127609253, testing loss: 1.680967926979065\n",
      "Accuracy: 0.4281\n",
      "epoch: 440, training loss: 1.242228388786316, testing loss: 1.7358043193817139\n",
      "Accuracy: 0.3947\n",
      "epoch: 440, training loss: 1.228616714477539, testing loss: 1.710379719734192\n",
      "Accuracy: 0.4083\n",
      "epoch: 440, training loss: 1.2076895236968994, testing loss: 1.7255793809890747\n",
      "Accuracy: 0.4233\n",
      "epoch: 440, training loss: 1.2321182489395142, testing loss: 1.7348541021347046\n",
      "Accuracy: 0.4328\n",
      "epoch: 440, training loss: 1.298716425895691, testing loss: 1.7478841543197632\n",
      "Accuracy: 0.3622\n",
      "epoch: 440, training loss: 1.2548913955688477, testing loss: 1.7410392761230469\n",
      "Accuracy: 0.4161\n",
      "epoch: 440, training loss: 1.2578307390213013, testing loss: 1.5923562049865723\n",
      "Accuracy: 0.4610\n",
      "epoch: 440, training loss: 1.282631516456604, testing loss: 1.7869895696640015\n",
      "Accuracy: 0.4000\n",
      "epoch: 440, training loss: 1.2434608936309814, testing loss: 1.753273606300354\n",
      "Accuracy: 0.4548\n",
      "epoch: 440, training loss: 1.1940885782241821, testing loss: 1.5709601640701294\n",
      "Accuracy: 0.4737\n",
      "epoch: 440, training loss: 1.2439215183258057, testing loss: 1.6550605297088623\n",
      "Accuracy: 0.3880\n",
      "epoch: 440, training loss: 1.2672982215881348, testing loss: 1.6877238750457764\n",
      "Accuracy: 0.3947\n",
      "epoch: 440, training loss: 1.2487343549728394, testing loss: 1.7009681463241577\n",
      "Accuracy: 0.4243\n",
      "epoch: 440, training loss: 1.2536686658859253, testing loss: 1.63309907913208\n",
      "Accuracy: 0.3864\n",
      "epoch: 440, training loss: 1.3265255689620972, testing loss: 1.6418434381484985\n",
      "Accuracy: 0.4548\n",
      "epoch: 440, training loss: 1.292409062385559, testing loss: 1.7529014348983765\n",
      "Accuracy: 0.4089\n",
      "epoch: 440, training loss: 1.1861447095870972, testing loss: 1.7993619441986084\n",
      "Accuracy: 0.4110\n",
      "epoch: 440, training loss: 1.3967514038085938, testing loss: 1.7704932689666748\n",
      "Accuracy: 0.3993\n",
      "epoch: 440, training loss: 1.2253003120422363, testing loss: 1.7613470554351807\n",
      "Accuracy: 0.4190\n",
      "epoch: 440, training loss: 1.2428698539733887, testing loss: 1.67547607421875\n",
      "Accuracy: 0.4290\n",
      "epoch: 440, training loss: 1.2533092498779297, testing loss: 1.728722095489502\n",
      "Accuracy: 0.3698\n",
      "epoch: 440, training loss: 1.2362819910049438, testing loss: 1.7474199533462524\n",
      "Accuracy: 0.3712\n",
      "epoch: 440, training loss: 1.1998175382614136, testing loss: 1.7067949771881104\n",
      "Accuracy: 0.4086\n",
      "epoch: 440, training loss: 1.249684453010559, testing loss: 1.6979643106460571\n",
      "Accuracy: 0.4329\n",
      "epoch: 440, training loss: 1.257907509803772, testing loss: 1.7399988174438477\n",
      "Accuracy: 0.3969\n",
      "epoch: 440, training loss: 1.290164589881897, testing loss: 1.6884567737579346\n",
      "Accuracy: 0.4147\n",
      "epoch: 440, training loss: 1.2114453315734863, testing loss: 1.7061810493469238\n",
      "Accuracy: 0.4097\n",
      "epoch: 440, training loss: 1.2214815616607666, testing loss: 1.6529626846313477\n",
      "Accuracy: 0.4470\n",
      "epoch: 440, training loss: 1.2303365468978882, testing loss: 1.732079267501831\n",
      "Accuracy: 0.3955\n",
      "epoch: 440, training loss: 1.224637508392334, testing loss: 1.6192528009414673\n",
      "Accuracy: 0.4121\n",
      "epoch: 440, training loss: 1.1794499158859253, testing loss: 1.704911470413208\n",
      "Accuracy: 0.4320\n",
      "epoch: 440, training loss: 1.3303455114364624, testing loss: 1.6942877769470215\n",
      "Accuracy: 0.4188\n",
      "epoch: 440, training loss: 1.1832008361816406, testing loss: 1.5387232303619385\n",
      "Accuracy: 0.4531\n",
      "epoch: 440, training loss: 1.2270113229751587, testing loss: 1.597829818725586\n",
      "Accuracy: 0.4231\n",
      "epoch: 440, training loss: 1.283077359199524, testing loss: 1.6453299522399902\n",
      "Accuracy: 0.4479\n",
      "epoch: 440, training loss: 1.2615699768066406, testing loss: 1.7737054824829102\n",
      "Accuracy: 0.3776\n",
      "epoch: 440, training loss: 1.2318122386932373, testing loss: 1.6054174900054932\n",
      "Accuracy: 0.4110\n",
      "epoch: 440, training loss: 1.2741765975952148, testing loss: 1.8218531608581543\n",
      "Accuracy: 0.3839\n",
      "epoch: 440, training loss: 1.2844791412353516, testing loss: 1.5061900615692139\n",
      "Accuracy: 0.4656\n",
      "epoch: 440, training loss: 1.2424637079238892, testing loss: 1.6173732280731201\n",
      "Accuracy: 0.4625\n",
      "epoch: 440, training loss: 1.2099885940551758, testing loss: 1.5365281105041504\n",
      "Accuracy: 0.4726\n",
      "epoch: 440, training loss: 1.2758104801177979, testing loss: 1.6119495630264282\n",
      "Accuracy: 0.4571\n",
      "epoch: 440, training loss: 1.2454410791397095, testing loss: 1.8251339197158813\n",
      "Accuracy: 0.4266\n",
      "epoch: 440, training loss: 1.2431081533432007, testing loss: 1.654320478439331\n",
      "Accuracy: 0.4371\n",
      "epoch: 440, training loss: 1.1603729724884033, testing loss: 1.634277582168579\n",
      "Accuracy: 0.4484\n",
      "epoch: 440, training loss: 1.2648411989212036, testing loss: 1.8236333131790161\n",
      "Accuracy: 0.3988\n",
      "epoch: 440, training loss: 1.2546495199203491, testing loss: 1.6061351299285889\n",
      "Accuracy: 0.3883\n",
      "epoch: 440, training loss: 1.2328755855560303, testing loss: 1.5935628414154053\n",
      "Accuracy: 0.4595\n",
      "epoch: 440, training loss: 1.2290459871292114, testing loss: 1.6446032524108887\n",
      "Accuracy: 0.4316\n",
      "epoch: 440, training loss: 1.2296525239944458, testing loss: 1.749464511871338\n",
      "Accuracy: 0.4110\n",
      "epoch: 440, training loss: 1.2869900465011597, testing loss: 1.703903317451477\n",
      "Accuracy: 0.3524\n",
      "epoch: 440, training loss: 1.3201192617416382, testing loss: 1.8747882843017578\n",
      "Accuracy: 0.3436\n",
      "epoch: 440, training loss: 1.2194634675979614, testing loss: 1.6855638027191162\n",
      "Accuracy: 0.4523\n",
      "epoch: 440, training loss: 1.3222692012786865, testing loss: 1.8039950132369995\n",
      "Accuracy: 0.4345\n",
      "epoch: 440, training loss: 1.338050365447998, testing loss: 1.6899245977401733\n",
      "Accuracy: 0.4174\n",
      "epoch: 440, training loss: 1.2598899602890015, testing loss: 1.82965087890625\n",
      "Accuracy: 0.4305\n",
      "epoch: 440, training loss: 1.229372501373291, testing loss: 1.7696210145950317\n",
      "Accuracy: 0.4006\n",
      "epoch: 440, training loss: 1.3570671081542969, testing loss: 1.6499754190444946\n",
      "Accuracy: 0.4329\n",
      "epoch: 440, training loss: 1.1647651195526123, testing loss: 1.7537553310394287\n",
      "Accuracy: 0.4182\n",
      "epoch: 440, training loss: 1.2851991653442383, testing loss: 1.7032698392868042\n",
      "Accuracy: 0.4119\n",
      "epoch: 440, training loss: 1.2306503057479858, testing loss: 1.5764641761779785\n",
      "Accuracy: 0.4822\n",
      "epoch: 440, training loss: 1.2842364311218262, testing loss: 1.782231092453003\n",
      "Accuracy: 0.3805\n",
      "epoch: 440, training loss: 1.318351149559021, testing loss: 1.5850383043289185\n",
      "Accuracy: 0.4334\n",
      "epoch: 440, training loss: 1.2091628313064575, testing loss: 1.6701018810272217\n",
      "Accuracy: 0.3958\n",
      "epoch: 440, training loss: 1.2736802101135254, testing loss: 1.5618689060211182\n",
      "Accuracy: 0.4621\n",
      "epoch: 440, training loss: 1.2522770166397095, testing loss: 1.5756070613861084\n",
      "Accuracy: 0.4427\n",
      "epoch: 440, training loss: 1.2549022436141968, testing loss: 1.902406930923462\n",
      "Accuracy: 0.4169\n",
      "epoch: 440, training loss: 1.2117537260055542, testing loss: 1.7175952196121216\n",
      "Accuracy: 0.4206\n",
      "epoch: 440, training loss: 1.2616623640060425, testing loss: 1.7877004146575928\n",
      "Accuracy: 0.3828\n",
      "epoch: 440, training loss: 1.3212363719940186, testing loss: 1.8094291687011719\n",
      "Accuracy: 0.3953\n",
      "epoch: 440, training loss: 1.185977578163147, testing loss: 1.658434271812439\n",
      "Accuracy: 0.4065\n",
      "epoch: 440, training loss: 1.2082940340042114, testing loss: 1.6374080181121826\n",
      "Accuracy: 0.4377\n",
      "epoch: 440, training loss: 1.3210901021957397, testing loss: 1.6678507328033447\n",
      "Accuracy: 0.4405\n",
      "epoch: 440, training loss: 1.242283582687378, testing loss: 1.6816987991333008\n",
      "Accuracy: 0.4650\n",
      "epoch: 440, training loss: 1.3084105253219604, testing loss: 1.670458436012268\n",
      "Accuracy: 0.4338\n",
      "epoch: 440, training loss: 1.234610915184021, testing loss: 1.867109775543213\n",
      "Accuracy: 0.3958\n",
      "epoch: 440, training loss: 1.2353744506835938, testing loss: 1.8487040996551514\n",
      "Accuracy: 0.3819\n",
      "epoch: 440, training loss: 1.1962804794311523, testing loss: 1.7287007570266724\n",
      "Accuracy: 0.4073\n",
      "epoch: 440, training loss: 1.253706932067871, testing loss: 1.7498794794082642\n",
      "Accuracy: 0.4057\n",
      "epoch: 440, training loss: 1.270071268081665, testing loss: 1.6724594831466675\n",
      "Accuracy: 0.4139\n",
      "epoch: 440, training loss: 1.2806875705718994, testing loss: 1.6945598125457764\n",
      "Accuracy: 0.4209\n",
      "epoch: 440, training loss: 1.2676416635513306, testing loss: 1.5794897079467773\n",
      "Accuracy: 0.4455\n",
      "epoch: 440, training loss: 1.2426856756210327, testing loss: 1.8432536125183105\n",
      "Accuracy: 0.4257\n",
      "epoch: 440, training loss: 1.2555334568023682, testing loss: 1.8179125785827637\n",
      "Accuracy: 0.4286\n",
      "epoch: 440, training loss: 1.1935545206069946, testing loss: 1.7764174938201904\n",
      "Accuracy: 0.4483\n",
      "epoch: 440, training loss: 1.193415880203247, testing loss: 1.8133203983306885\n",
      "Accuracy: 0.3988\n",
      "epoch: 440, training loss: 1.2557544708251953, testing loss: 1.6430025100708008\n",
      "Accuracy: 0.4525\n",
      "epoch: 440, training loss: 1.271685242652893, testing loss: 1.6636613607406616\n",
      "Accuracy: 0.4537\n",
      "epoch: 450, training loss: 1.253509283065796, testing loss: 1.6844419240951538\n",
      "Accuracy: 0.4054\n",
      "epoch: 450, training loss: 1.215692400932312, testing loss: 1.6493340730667114\n",
      "Accuracy: 0.4222\n",
      "epoch: 450, training loss: 1.2210214138031006, testing loss: 1.7247552871704102\n",
      "Accuracy: 0.4137\n",
      "epoch: 450, training loss: 1.2934235334396362, testing loss: 1.5885792970657349\n",
      "Accuracy: 0.4118\n",
      "epoch: 450, training loss: 1.156840205192566, testing loss: 1.6656256914138794\n",
      "Accuracy: 0.4497\n",
      "epoch: 450, training loss: 1.2731095552444458, testing loss: 1.7394660711288452\n",
      "Accuracy: 0.4314\n",
      "epoch: 450, training loss: 1.2983551025390625, testing loss: 1.759462833404541\n",
      "Accuracy: 0.4131\n",
      "epoch: 450, training loss: 1.2610043287277222, testing loss: 1.6682831048965454\n",
      "Accuracy: 0.3887\n",
      "epoch: 450, training loss: 1.2546981573104858, testing loss: 1.7593774795532227\n",
      "Accuracy: 0.4068\n",
      "epoch: 450, training loss: 1.1653562784194946, testing loss: 1.6681994199752808\n",
      "Accuracy: 0.4155\n",
      "epoch: 450, training loss: 1.2519326210021973, testing loss: 1.4896769523620605\n",
      "Accuracy: 0.5229\n",
      "epoch: 450, training loss: 1.2365398406982422, testing loss: 1.7388185262680054\n",
      "Accuracy: 0.4623\n",
      "epoch: 450, training loss: 1.2798064947128296, testing loss: 1.735215425491333\n",
      "Accuracy: 0.3785\n",
      "epoch: 450, training loss: 1.216953158378601, testing loss: 1.7616389989852905\n",
      "Accuracy: 0.3578\n",
      "epoch: 450, training loss: 1.2517266273498535, testing loss: 1.7119113206863403\n",
      "Accuracy: 0.4026\n",
      "epoch: 450, training loss: 1.1918203830718994, testing loss: 1.768729329109192\n",
      "Accuracy: 0.4105\n",
      "epoch: 450, training loss: 1.219892144203186, testing loss: 1.779733419418335\n",
      "Accuracy: 0.4104\n",
      "epoch: 450, training loss: 1.2096449136734009, testing loss: 1.7286057472229004\n",
      "Accuracy: 0.4326\n",
      "epoch: 450, training loss: 1.2008692026138306, testing loss: 1.7574453353881836\n",
      "Accuracy: 0.4222\n",
      "epoch: 450, training loss: 1.2483779191970825, testing loss: 1.6923974752426147\n",
      "Accuracy: 0.4079\n",
      "epoch: 450, training loss: 1.3562486171722412, testing loss: 1.6114096641540527\n",
      "Accuracy: 0.4128\n",
      "epoch: 450, training loss: 1.1977531909942627, testing loss: 1.6232407093048096\n",
      "Accuracy: 0.4178\n",
      "epoch: 450, training loss: 1.2165172100067139, testing loss: 1.6137099266052246\n",
      "Accuracy: 0.4224\n",
      "epoch: 450, training loss: 1.2493138313293457, testing loss: 1.7096755504608154\n",
      "Accuracy: 0.3734\n",
      "epoch: 450, training loss: 1.3237403631210327, testing loss: 1.68156898021698\n",
      "Accuracy: 0.3982\n",
      "epoch: 450, training loss: 1.2766878604888916, testing loss: 1.6363937854766846\n",
      "Accuracy: 0.4830\n",
      "epoch: 450, training loss: 1.2710384130477905, testing loss: 1.63154137134552\n",
      "Accuracy: 0.3981\n",
      "epoch: 450, training loss: 1.2421953678131104, testing loss: 1.7580195665359497\n",
      "Accuracy: 0.3929\n",
      "epoch: 450, training loss: 1.2145029306411743, testing loss: 1.6101921796798706\n",
      "Accuracy: 0.4543\n",
      "epoch: 450, training loss: 1.2100458145141602, testing loss: 1.7500303983688354\n",
      "Accuracy: 0.4143\n",
      "epoch: 450, training loss: 1.1805344820022583, testing loss: 1.6326771974563599\n",
      "Accuracy: 0.4057\n",
      "epoch: 450, training loss: 1.245774745941162, testing loss: 1.688376545906067\n",
      "Accuracy: 0.4058\n",
      "epoch: 450, training loss: 1.2802011966705322, testing loss: 1.808148980140686\n",
      "Accuracy: 0.3854\n",
      "epoch: 450, training loss: 1.3019956350326538, testing loss: 1.768442153930664\n",
      "Accuracy: 0.3775\n",
      "epoch: 450, training loss: 1.193709373474121, testing loss: 1.6883360147476196\n",
      "Accuracy: 0.4033\n",
      "epoch: 450, training loss: 1.2423367500305176, testing loss: 1.7727364301681519\n",
      "Accuracy: 0.4257\n",
      "epoch: 450, training loss: 1.3789453506469727, testing loss: 1.6579889059066772\n",
      "Accuracy: 0.4304\n",
      "epoch: 450, training loss: 1.2250579595565796, testing loss: 1.7805094718933105\n",
      "Accuracy: 0.4387\n",
      "epoch: 450, training loss: 1.2941055297851562, testing loss: 1.565363883972168\n",
      "Accuracy: 0.4212\n",
      "epoch: 450, training loss: 1.2214828729629517, testing loss: 1.6650937795639038\n",
      "Accuracy: 0.4221\n",
      "epoch: 450, training loss: 1.2742053270339966, testing loss: 1.80130136013031\n",
      "Accuracy: 0.3496\n",
      "epoch: 450, training loss: 1.284240961074829, testing loss: 1.763930082321167\n",
      "Accuracy: 0.4590\n",
      "epoch: 450, training loss: 1.2934305667877197, testing loss: 1.7751507759094238\n",
      "Accuracy: 0.4068\n",
      "epoch: 450, training loss: 1.2848361730575562, testing loss: 1.830726981163025\n",
      "Accuracy: 0.3520\n",
      "epoch: 450, training loss: 1.3232243061065674, testing loss: 1.737398624420166\n",
      "Accuracy: 0.4075\n",
      "epoch: 450, training loss: 1.258133888244629, testing loss: 1.6879078149795532\n",
      "Accuracy: 0.4486\n",
      "epoch: 450, training loss: 1.2568492889404297, testing loss: 1.60829496383667\n",
      "Accuracy: 0.4198\n",
      "epoch: 450, training loss: 1.2632862329483032, testing loss: 1.7259726524353027\n",
      "Accuracy: 0.3956\n",
      "epoch: 450, training loss: 1.2499960660934448, testing loss: 1.6898739337921143\n",
      "Accuracy: 0.4076\n",
      "epoch: 450, training loss: 1.2167983055114746, testing loss: 1.6592836380004883\n",
      "Accuracy: 0.4082\n",
      "epoch: 450, training loss: 1.219569444656372, testing loss: 1.716972827911377\n",
      "Accuracy: 0.4014\n",
      "epoch: 450, training loss: 1.282044768333435, testing loss: 1.6895685195922852\n",
      "Accuracy: 0.3920\n",
      "epoch: 450, training loss: 1.3280928134918213, testing loss: 1.7419859170913696\n",
      "Accuracy: 0.4143\n",
      "epoch: 450, training loss: 1.2156165838241577, testing loss: 1.7829691171646118\n",
      "Accuracy: 0.4379\n",
      "epoch: 450, training loss: 1.2328377962112427, testing loss: 1.8381929397583008\n",
      "Accuracy: 0.4000\n",
      "epoch: 450, training loss: 1.2613279819488525, testing loss: 1.9424384832382202\n",
      "Accuracy: 0.3825\n",
      "epoch: 450, training loss: 1.3594083786010742, testing loss: 1.7195584774017334\n",
      "Accuracy: 0.3864\n",
      "epoch: 450, training loss: 1.2666327953338623, testing loss: 1.6450706720352173\n",
      "Accuracy: 0.4406\n",
      "epoch: 450, training loss: 1.2394235134124756, testing loss: 1.6925562620162964\n",
      "Accuracy: 0.4121\n",
      "epoch: 450, training loss: 1.3251185417175293, testing loss: 1.7359824180603027\n",
      "Accuracy: 0.4400\n",
      "epoch: 450, training loss: 1.2666288614273071, testing loss: 1.6802890300750732\n",
      "Accuracy: 0.3836\n",
      "epoch: 450, training loss: 1.2644093036651611, testing loss: 1.607422113418579\n",
      "Accuracy: 0.4408\n",
      "epoch: 450, training loss: 1.233987808227539, testing loss: 1.6592133045196533\n",
      "Accuracy: 0.4503\n",
      "epoch: 450, training loss: 1.1978973150253296, testing loss: 1.7958043813705444\n",
      "Accuracy: 0.4022\n",
      "epoch: 450, training loss: 1.21114981174469, testing loss: 1.8123646974563599\n",
      "Accuracy: 0.4194\n",
      "epoch: 450, training loss: 1.2841663360595703, testing loss: 1.731467604637146\n",
      "Accuracy: 0.4018\n",
      "epoch: 450, training loss: 1.1890789270401, testing loss: 1.6627949476242065\n",
      "Accuracy: 0.4311\n",
      "epoch: 450, training loss: 1.2168211936950684, testing loss: 1.5450479984283447\n",
      "Accuracy: 0.4323\n",
      "epoch: 450, training loss: 1.2523231506347656, testing loss: 1.7359116077423096\n",
      "Accuracy: 0.4508\n",
      "epoch: 450, training loss: 1.2620445489883423, testing loss: 1.788615345954895\n",
      "Accuracy: 0.4343\n",
      "epoch: 450, training loss: 1.1601028442382812, testing loss: 1.7934378385543823\n",
      "Accuracy: 0.4140\n",
      "epoch: 450, training loss: 1.2179419994354248, testing loss: 1.503530502319336\n",
      "Accuracy: 0.4415\n",
      "epoch: 450, training loss: 1.1777303218841553, testing loss: 1.7496408224105835\n",
      "Accuracy: 0.3949\n",
      "epoch: 450, training loss: 1.2132636308670044, testing loss: 1.6394678354263306\n",
      "Accuracy: 0.4434\n",
      "epoch: 450, training loss: 1.2204487323760986, testing loss: 1.6545323133468628\n",
      "Accuracy: 0.4430\n",
      "epoch: 450, training loss: 1.248725175857544, testing loss: 1.7297968864440918\n",
      "Accuracy: 0.3599\n",
      "epoch: 450, training loss: 1.340522289276123, testing loss: 1.621118426322937\n",
      "Accuracy: 0.4156\n",
      "epoch: 450, training loss: 1.2211065292358398, testing loss: 1.6813315153121948\n",
      "Accuracy: 0.4677\n",
      "epoch: 450, training loss: 1.3221911191940308, testing loss: 1.7321351766586304\n",
      "Accuracy: 0.3993\n",
      "epoch: 450, training loss: 1.3142342567443848, testing loss: 1.6624066829681396\n",
      "Accuracy: 0.4231\n",
      "epoch: 450, training loss: 1.233654499053955, testing loss: 1.7341134548187256\n",
      "Accuracy: 0.4294\n",
      "epoch: 450, training loss: 1.262149453163147, testing loss: 1.6085622310638428\n",
      "Accuracy: 0.4319\n",
      "epoch: 450, training loss: 1.2342779636383057, testing loss: 1.6647557020187378\n",
      "Accuracy: 0.4221\n",
      "epoch: 450, training loss: 1.291411280632019, testing loss: 1.5951353311538696\n",
      "Accuracy: 0.4100\n",
      "epoch: 450, training loss: 1.3177241086959839, testing loss: 1.806355357170105\n",
      "Accuracy: 0.3944\n",
      "epoch: 450, training loss: 1.2800774574279785, testing loss: 1.7640275955200195\n",
      "Accuracy: 0.4083\n",
      "epoch: 450, training loss: 1.2283645868301392, testing loss: 1.7492913007736206\n",
      "Accuracy: 0.3867\n",
      "epoch: 450, training loss: 1.2704612016677856, testing loss: 1.5563877820968628\n",
      "Accuracy: 0.4767\n",
      "epoch: 450, training loss: 1.24596107006073, testing loss: 1.6685030460357666\n",
      "Accuracy: 0.4737\n",
      "epoch: 450, training loss: 1.2520023584365845, testing loss: 1.6684900522232056\n",
      "Accuracy: 0.4272\n",
      "epoch: 450, training loss: 1.2728906869888306, testing loss: 1.7353928089141846\n",
      "Accuracy: 0.4125\n",
      "epoch: 450, training loss: 1.2483659982681274, testing loss: 1.7677406072616577\n",
      "Accuracy: 0.4118\n",
      "epoch: 450, training loss: 1.2731574773788452, testing loss: 1.6489362716674805\n",
      "Accuracy: 0.4318\n",
      "epoch: 450, training loss: 1.185056209564209, testing loss: 1.543070673942566\n",
      "Accuracy: 0.4660\n",
      "epoch: 450, training loss: 1.2390079498291016, testing loss: 1.7041953802108765\n",
      "Accuracy: 0.4353\n",
      "epoch: 450, training loss: 1.2889389991760254, testing loss: 1.7213140726089478\n",
      "Accuracy: 0.3987\n",
      "epoch: 450, training loss: 1.251292109489441, testing loss: 1.755435824394226\n",
      "Accuracy: 0.4076\n",
      "epoch: 450, training loss: 1.2589585781097412, testing loss: 1.78225576877594\n",
      "Accuracy: 0.4125\n",
      "epoch: 450, training loss: 1.23310124874115, testing loss: 1.6419280767440796\n",
      "Accuracy: 0.4031\n",
      "epoch: 450, training loss: 1.216755747795105, testing loss: 1.558969259262085\n",
      "Accuracy: 0.4434\n",
      "epoch: 460, training loss: 1.283839464187622, testing loss: 1.6715340614318848\n",
      "Accuracy: 0.3863\n",
      "epoch: 460, training loss: 1.2225590944290161, testing loss: 1.7124793529510498\n",
      "Accuracy: 0.4085\n",
      "epoch: 460, training loss: 1.3363444805145264, testing loss: 1.6947075128555298\n",
      "Accuracy: 0.4172\n",
      "epoch: 460, training loss: 1.2339860200881958, testing loss: 1.7620823383331299\n",
      "Accuracy: 0.4067\n",
      "epoch: 460, training loss: 1.2374166250228882, testing loss: 1.8851547241210938\n",
      "Accuracy: 0.4047\n",
      "epoch: 460, training loss: 1.2857104539871216, testing loss: 1.7286490201950073\n",
      "Accuracy: 0.4338\n",
      "epoch: 460, training loss: 1.2886062860488892, testing loss: 1.8059508800506592\n",
      "Accuracy: 0.3559\n",
      "epoch: 460, training loss: 1.229744791984558, testing loss: 1.7974797487258911\n",
      "Accuracy: 0.3791\n",
      "epoch: 460, training loss: 1.221718430519104, testing loss: 1.6829415559768677\n",
      "Accuracy: 0.4198\n",
      "epoch: 460, training loss: 1.2306867837905884, testing loss: 1.6328281164169312\n",
      "Accuracy: 0.4494\n",
      "epoch: 460, training loss: 1.2324360609054565, testing loss: 1.7568304538726807\n",
      "Accuracy: 0.3942\n",
      "epoch: 460, training loss: 1.2538622617721558, testing loss: 1.7748013734817505\n",
      "Accuracy: 0.3737\n",
      "epoch: 460, training loss: 1.230944037437439, testing loss: 1.7590709924697876\n",
      "Accuracy: 0.4360\n",
      "epoch: 460, training loss: 1.2215782403945923, testing loss: 1.5396755933761597\n",
      "Accuracy: 0.5055\n",
      "epoch: 460, training loss: 1.2883790731430054, testing loss: 1.7649799585342407\n",
      "Accuracy: 0.3862\n",
      "epoch: 460, training loss: 1.3045719861984253, testing loss: 1.7750097513198853\n",
      "Accuracy: 0.4342\n",
      "epoch: 460, training loss: 1.229934573173523, testing loss: 1.7593740224838257\n",
      "Accuracy: 0.4429\n",
      "epoch: 460, training loss: 1.2564746141433716, testing loss: 1.8114898204803467\n",
      "Accuracy: 0.3832\n",
      "epoch: 460, training loss: 1.2976157665252686, testing loss: 1.8020678758621216\n",
      "Accuracy: 0.4267\n",
      "epoch: 460, training loss: 1.192744493484497, testing loss: 1.6896255016326904\n",
      "Accuracy: 0.4107\n",
      "epoch: 460, training loss: 1.230350136756897, testing loss: 1.6689329147338867\n",
      "Accuracy: 0.4050\n",
      "epoch: 460, training loss: 1.229227900505066, testing loss: 1.7772403955459595\n",
      "Accuracy: 0.4132\n",
      "epoch: 460, training loss: 1.3078312873840332, testing loss: 1.8384826183319092\n",
      "Accuracy: 0.3966\n",
      "epoch: 460, training loss: 1.1850773096084595, testing loss: 1.6976243257522583\n",
      "Accuracy: 0.3545\n",
      "epoch: 460, training loss: 1.3204416036605835, testing loss: 1.709271788597107\n",
      "Accuracy: 0.4257\n",
      "epoch: 460, training loss: 1.3723866939544678, testing loss: 1.7475144863128662\n",
      "Accuracy: 0.4300\n",
      "epoch: 460, training loss: 1.2290884256362915, testing loss: 1.8354156017303467\n",
      "Accuracy: 0.4337\n",
      "epoch: 460, training loss: 1.2496998310089111, testing loss: 1.74344003200531\n",
      "Accuracy: 0.3746\n",
      "epoch: 460, training loss: 1.2070351839065552, testing loss: 1.6668771505355835\n",
      "Accuracy: 0.4018\n",
      "epoch: 460, training loss: 1.2880427837371826, testing loss: 1.6894208192825317\n",
      "Accuracy: 0.4307\n",
      "epoch: 460, training loss: 1.1912071704864502, testing loss: 1.6476112604141235\n",
      "Accuracy: 0.4234\n",
      "epoch: 460, training loss: 1.2603793144226074, testing loss: 1.6555989980697632\n",
      "Accuracy: 0.4413\n",
      "epoch: 460, training loss: 1.2019784450531006, testing loss: 1.6072019338607788\n",
      "Accuracy: 0.4479\n",
      "epoch: 460, training loss: 1.2578076124191284, testing loss: 1.6861289739608765\n",
      "Accuracy: 0.4718\n",
      "epoch: 460, training loss: 1.20148766040802, testing loss: 1.6502658128738403\n",
      "Accuracy: 0.4475\n",
      "epoch: 460, training loss: 1.226252555847168, testing loss: 1.6422780752182007\n",
      "Accuracy: 0.4086\n",
      "epoch: 460, training loss: 1.2357697486877441, testing loss: 1.7004668712615967\n",
      "Accuracy: 0.4197\n",
      "epoch: 460, training loss: 1.2441900968551636, testing loss: 1.781281590461731\n",
      "Accuracy: 0.4121\n",
      "epoch: 460, training loss: 1.2452727556228638, testing loss: 1.7819764614105225\n",
      "Accuracy: 0.3994\n",
      "epoch: 460, training loss: 1.2043002843856812, testing loss: 1.7518680095672607\n",
      "Accuracy: 0.4000\n",
      "epoch: 460, training loss: 1.2760717868804932, testing loss: 1.8552310466766357\n",
      "Accuracy: 0.4062\n",
      "epoch: 460, training loss: 1.3231704235076904, testing loss: 1.6204721927642822\n",
      "Accuracy: 0.4533\n",
      "epoch: 460, training loss: 1.2045420408248901, testing loss: 1.7011162042617798\n",
      "Accuracy: 0.4595\n",
      "epoch: 460, training loss: 1.2103594541549683, testing loss: 1.810046911239624\n",
      "Accuracy: 0.3558\n",
      "epoch: 460, training loss: 1.2266473770141602, testing loss: 1.7666221857070923\n",
      "Accuracy: 0.4112\n",
      "epoch: 460, training loss: 1.219237208366394, testing loss: 1.6187465190887451\n",
      "Accuracy: 0.3754\n",
      "epoch: 460, training loss: 1.1621477603912354, testing loss: 1.687977910041809\n",
      "Accuracy: 0.3562\n",
      "epoch: 460, training loss: 1.236206293106079, testing loss: 1.8230562210083008\n",
      "Accuracy: 0.4064\n",
      "epoch: 460, training loss: 1.212262511253357, testing loss: 1.620674967765808\n",
      "Accuracy: 0.4383\n",
      "epoch: 460, training loss: 1.2724237442016602, testing loss: 1.5760120153427124\n",
      "Accuracy: 0.4671\n",
      "epoch: 460, training loss: 1.2552937269210815, testing loss: 1.6563092470169067\n",
      "Accuracy: 0.4133\n",
      "epoch: 460, training loss: 1.2674344778060913, testing loss: 1.5642051696777344\n",
      "Accuracy: 0.3663\n",
      "epoch: 460, training loss: 1.1850225925445557, testing loss: 1.7509713172912598\n",
      "Accuracy: 0.3741\n",
      "epoch: 460, training loss: 1.192465901374817, testing loss: 1.683073878288269\n",
      "Accuracy: 0.4441\n",
      "epoch: 460, training loss: 1.2530044317245483, testing loss: 1.7127211093902588\n",
      "Accuracy: 0.4129\n",
      "epoch: 460, training loss: 1.273415446281433, testing loss: 1.7017478942871094\n",
      "Accuracy: 0.4167\n",
      "epoch: 460, training loss: 1.2683501243591309, testing loss: 1.7032288312911987\n",
      "Accuracy: 0.4091\n",
      "epoch: 460, training loss: 1.1984055042266846, testing loss: 1.7096304893493652\n",
      "Accuracy: 0.3905\n",
      "epoch: 460, training loss: 1.316929578781128, testing loss: 1.777289628982544\n",
      "Accuracy: 0.3643\n",
      "epoch: 460, training loss: 1.2402762174606323, testing loss: 1.7568730115890503\n",
      "Accuracy: 0.4253\n",
      "epoch: 460, training loss: 1.203839898109436, testing loss: 1.7024977207183838\n",
      "Accuracy: 0.3982\n",
      "epoch: 460, training loss: 1.1805713176727295, testing loss: 1.7356444597244263\n",
      "Accuracy: 0.4136\n",
      "epoch: 460, training loss: 1.1876349449157715, testing loss: 1.7021996974945068\n",
      "Accuracy: 0.4132\n",
      "epoch: 460, training loss: 1.2777045965194702, testing loss: 1.8685684204101562\n",
      "Accuracy: 0.4277\n",
      "epoch: 460, training loss: 1.241929054260254, testing loss: 1.8445966243743896\n",
      "Accuracy: 0.3953\n",
      "epoch: 460, training loss: 1.1818543672561646, testing loss: 1.6894246339797974\n",
      "Accuracy: 0.4639\n",
      "epoch: 460, training loss: 1.302485466003418, testing loss: 1.7782282829284668\n",
      "Accuracy: 0.3831\n",
      "epoch: 460, training loss: 1.2295624017715454, testing loss: 1.7570186853408813\n",
      "Accuracy: 0.4017\n",
      "epoch: 460, training loss: 1.2081913948059082, testing loss: 1.7080812454223633\n",
      "Accuracy: 0.4557\n",
      "epoch: 460, training loss: 1.2094182968139648, testing loss: 1.8418155908584595\n",
      "Accuracy: 0.3810\n",
      "epoch: 460, training loss: 1.2842241525650024, testing loss: 1.7272140979766846\n",
      "Accuracy: 0.4075\n",
      "epoch: 460, training loss: 1.2860798835754395, testing loss: 1.7202228307724\n",
      "Accuracy: 0.3746\n",
      "epoch: 460, training loss: 1.2780852317810059, testing loss: 1.6670829057693481\n",
      "Accuracy: 0.4214\n",
      "epoch: 460, training loss: 1.317025899887085, testing loss: 1.6824655532836914\n",
      "Accuracy: 0.4423\n",
      "epoch: 460, training loss: 1.207139492034912, testing loss: 1.62178635597229\n",
      "Accuracy: 0.3846\n",
      "epoch: 460, training loss: 1.3309004306793213, testing loss: 1.7697927951812744\n",
      "Accuracy: 0.4241\n",
      "epoch: 460, training loss: 1.2505016326904297, testing loss: 1.7230767011642456\n",
      "Accuracy: 0.3994\n",
      "epoch: 460, training loss: 1.2456125020980835, testing loss: 1.7599282264709473\n",
      "Accuracy: 0.4224\n",
      "epoch: 460, training loss: 1.1872234344482422, testing loss: 1.965694785118103\n",
      "Accuracy: 0.3841\n",
      "epoch: 460, training loss: 1.2975139617919922, testing loss: 1.6706912517547607\n",
      "Accuracy: 0.4536\n",
      "epoch: 460, training loss: 1.2583268880844116, testing loss: 1.7843217849731445\n",
      "Accuracy: 0.4066\n",
      "epoch: 460, training loss: 1.1606632471084595, testing loss: 1.758662223815918\n",
      "Accuracy: 0.3852\n",
      "epoch: 460, training loss: 1.257916808128357, testing loss: 1.6321438550949097\n",
      "Accuracy: 0.4349\n",
      "epoch: 460, training loss: 1.211013674736023, testing loss: 1.6759854555130005\n",
      "Accuracy: 0.4545\n",
      "epoch: 460, training loss: 1.3065204620361328, testing loss: 1.6909369230270386\n",
      "Accuracy: 0.4177\n",
      "epoch: 460, training loss: 1.2245073318481445, testing loss: 1.6930524110794067\n",
      "Accuracy: 0.3663\n",
      "epoch: 460, training loss: 1.2526462078094482, testing loss: 1.7600202560424805\n",
      "Accuracy: 0.4020\n",
      "epoch: 460, training loss: 1.282759428024292, testing loss: 1.763877511024475\n",
      "Accuracy: 0.4118\n",
      "epoch: 460, training loss: 1.307154893875122, testing loss: 1.5984888076782227\n",
      "Accuracy: 0.4250\n",
      "epoch: 460, training loss: 1.253854513168335, testing loss: 1.7600656747817993\n",
      "Accuracy: 0.4286\n",
      "epoch: 460, training loss: 1.324677586555481, testing loss: 1.8291821479797363\n",
      "Accuracy: 0.4345\n",
      "epoch: 460, training loss: 1.2760486602783203, testing loss: 1.804405927658081\n",
      "Accuracy: 0.4205\n",
      "epoch: 460, training loss: 1.3329282999038696, testing loss: 1.8035324811935425\n",
      "Accuracy: 0.4314\n",
      "epoch: 460, training loss: 1.3122038841247559, testing loss: 1.577641487121582\n",
      "Accuracy: 0.4396\n",
      "epoch: 460, training loss: 1.2529569864273071, testing loss: 1.7012686729431152\n",
      "Accuracy: 0.4204\n",
      "epoch: 460, training loss: 1.2276581525802612, testing loss: 1.632667064666748\n",
      "Accuracy: 0.4123\n",
      "epoch: 460, training loss: 1.3076932430267334, testing loss: 1.8302409648895264\n",
      "Accuracy: 0.3974\n",
      "epoch: 460, training loss: 1.3019428253173828, testing loss: 1.7214932441711426\n",
      "Accuracy: 0.4540\n",
      "epoch: 460, training loss: 1.2477127313613892, testing loss: 1.6924492120742798\n",
      "Accuracy: 0.3935\n",
      "epoch: 460, training loss: 1.2386763095855713, testing loss: 1.768036127090454\n",
      "Accuracy: 0.3968\n",
      "epoch: 470, training loss: 1.3353713750839233, testing loss: 1.7091056108474731\n",
      "Accuracy: 0.4455\n",
      "epoch: 470, training loss: 1.2452501058578491, testing loss: 1.587005853652954\n",
      "Accuracy: 0.4708\n",
      "epoch: 470, training loss: 1.2266225814819336, testing loss: 1.721457600593567\n",
      "Accuracy: 0.3986\n",
      "epoch: 470, training loss: 1.2666038274765015, testing loss: 1.615025281906128\n",
      "Accuracy: 0.4592\n",
      "epoch: 470, training loss: 1.266432285308838, testing loss: 1.8164324760437012\n",
      "Accuracy: 0.4479\n",
      "epoch: 470, training loss: 1.2589271068572998, testing loss: 1.8152062892913818\n",
      "Accuracy: 0.3812\n",
      "epoch: 470, training loss: 1.1876713037490845, testing loss: 1.7299299240112305\n",
      "Accuracy: 0.4043\n",
      "epoch: 470, training loss: 1.261826992034912, testing loss: 1.6535189151763916\n",
      "Accuracy: 0.4188\n",
      "epoch: 470, training loss: 1.242889642715454, testing loss: 1.7398408651351929\n",
      "Accuracy: 0.3810\n",
      "epoch: 470, training loss: 1.262637734413147, testing loss: 1.7179230451583862\n",
      "Accuracy: 0.4058\n",
      "epoch: 470, training loss: 1.3249942064285278, testing loss: 1.6651962995529175\n",
      "Accuracy: 0.3916\n",
      "epoch: 470, training loss: 1.2624411582946777, testing loss: 1.638430118560791\n",
      "Accuracy: 0.4095\n",
      "epoch: 470, training loss: 1.1836856603622437, testing loss: 1.8438467979431152\n",
      "Accuracy: 0.4098\n",
      "epoch: 470, training loss: 1.2452143430709839, testing loss: 1.8087517023086548\n",
      "Accuracy: 0.4245\n",
      "epoch: 470, training loss: 1.219360113143921, testing loss: 1.691554069519043\n",
      "Accuracy: 0.4281\n",
      "epoch: 470, training loss: 1.2847256660461426, testing loss: 1.7699625492095947\n",
      "Accuracy: 0.4277\n",
      "epoch: 470, training loss: 1.244011402130127, testing loss: 1.7792965173721313\n",
      "Accuracy: 0.4448\n",
      "epoch: 470, training loss: 1.2796566486358643, testing loss: 1.8161962032318115\n",
      "Accuracy: 0.4031\n",
      "epoch: 470, training loss: 1.2646833658218384, testing loss: 1.7385936975479126\n",
      "Accuracy: 0.4227\n",
      "epoch: 470, training loss: 1.232014775276184, testing loss: 1.7181812524795532\n",
      "Accuracy: 0.3887\n",
      "epoch: 470, training loss: 1.2442773580551147, testing loss: 1.7089648246765137\n",
      "Accuracy: 0.4423\n",
      "epoch: 470, training loss: 1.2764902114868164, testing loss: 1.651165246963501\n",
      "Accuracy: 0.4071\n",
      "epoch: 470, training loss: 1.3504114151000977, testing loss: 1.6032798290252686\n",
      "Accuracy: 0.4249\n",
      "epoch: 470, training loss: 1.1437889337539673, testing loss: 1.6754579544067383\n",
      "Accuracy: 0.3758\n",
      "epoch: 470, training loss: 1.2649673223495483, testing loss: 1.555673360824585\n",
      "Accuracy: 0.4482\n",
      "epoch: 470, training loss: 1.2520004510879517, testing loss: 1.5486505031585693\n",
      "Accuracy: 0.4969\n",
      "epoch: 470, training loss: 1.2111541032791138, testing loss: 1.6413129568099976\n",
      "Accuracy: 0.4441\n",
      "epoch: 470, training loss: 1.267935037612915, testing loss: 1.6824636459350586\n",
      "Accuracy: 0.4295\n",
      "epoch: 470, training loss: 1.2097468376159668, testing loss: 1.6733527183532715\n",
      "Accuracy: 0.4465\n",
      "epoch: 470, training loss: 1.2278923988342285, testing loss: 1.5486172437667847\n",
      "Accuracy: 0.4554\n",
      "epoch: 470, training loss: 1.3188010454177856, testing loss: 1.6867958307266235\n",
      "Accuracy: 0.3937\n",
      "epoch: 470, training loss: 1.2411707639694214, testing loss: 1.5740631818771362\n",
      "Accuracy: 0.4271\n",
      "epoch: 470, training loss: 1.2466412782669067, testing loss: 1.6322555541992188\n",
      "Accuracy: 0.4408\n",
      "epoch: 470, training loss: 1.2171270847320557, testing loss: 1.604241967201233\n",
      "Accuracy: 0.4444\n",
      "epoch: 470, training loss: 1.248437762260437, testing loss: 1.7543288469314575\n",
      "Accuracy: 0.3933\n",
      "epoch: 470, training loss: 1.2569913864135742, testing loss: 1.7283800840377808\n",
      "Accuracy: 0.4034\n",
      "epoch: 470, training loss: 1.1892802715301514, testing loss: 1.7753726243972778\n",
      "Accuracy: 0.3974\n",
      "epoch: 470, training loss: 1.2695372104644775, testing loss: 1.819866418838501\n",
      "Accuracy: 0.4545\n",
      "epoch: 470, training loss: 1.2646888494491577, testing loss: 1.7179391384124756\n",
      "Accuracy: 0.4020\n",
      "epoch: 470, training loss: 1.279176115989685, testing loss: 1.6844531297683716\n",
      "Accuracy: 0.4228\n",
      "epoch: 470, training loss: 1.2618452310562134, testing loss: 1.7462999820709229\n",
      "Accuracy: 0.3667\n",
      "epoch: 470, training loss: 1.274797797203064, testing loss: 1.7754065990447998\n",
      "Accuracy: 0.4156\n",
      "epoch: 470, training loss: 1.2281076908111572, testing loss: 1.6910483837127686\n",
      "Accuracy: 0.4534\n",
      "epoch: 470, training loss: 1.3733632564544678, testing loss: 1.6435226202011108\n",
      "Accuracy: 0.4490\n",
      "epoch: 470, training loss: 1.280465841293335, testing loss: 1.6491436958312988\n",
      "Accuracy: 0.4551\n",
      "epoch: 470, training loss: 1.1931227445602417, testing loss: 1.7506779432296753\n",
      "Accuracy: 0.4492\n",
      "epoch: 470, training loss: 1.2334624528884888, testing loss: 1.746052861213684\n",
      "Accuracy: 0.3966\n",
      "epoch: 470, training loss: 1.3077465295791626, testing loss: 1.73684561252594\n",
      "Accuracy: 0.4066\n",
      "epoch: 470, training loss: 1.2351574897766113, testing loss: 1.6130847930908203\n",
      "Accuracy: 0.4482\n",
      "epoch: 470, training loss: 1.2380242347717285, testing loss: 1.6448495388031006\n",
      "Accuracy: 0.3939\n",
      "epoch: 470, training loss: 1.2427794933319092, testing loss: 1.6213608980178833\n",
      "Accuracy: 0.4448\n",
      "epoch: 470, training loss: 1.1905570030212402, testing loss: 1.6027822494506836\n",
      "Accuracy: 0.4406\n",
      "epoch: 470, training loss: 1.3187021017074585, testing loss: 1.770557165145874\n",
      "Accuracy: 0.3758\n",
      "epoch: 470, training loss: 1.3211781978607178, testing loss: 1.6983808279037476\n",
      "Accuracy: 0.4122\n",
      "epoch: 470, training loss: 1.2666144371032715, testing loss: 1.6490226984024048\n",
      "Accuracy: 0.4534\n",
      "epoch: 470, training loss: 1.2762866020202637, testing loss: 1.7588942050933838\n",
      "Accuracy: 0.4470\n",
      "epoch: 470, training loss: 1.2083525657653809, testing loss: 1.7580664157867432\n",
      "Accuracy: 0.3906\n",
      "epoch: 470, training loss: 1.246946930885315, testing loss: 1.6892610788345337\n",
      "Accuracy: 0.3844\n",
      "epoch: 470, training loss: 1.2804795503616333, testing loss: 1.8988884687423706\n",
      "Accuracy: 0.3555\n",
      "epoch: 470, training loss: 1.2805359363555908, testing loss: 1.60074782371521\n",
      "Accuracy: 0.4366\n",
      "epoch: 470, training loss: 1.2930482625961304, testing loss: 1.7399561405181885\n",
      "Accuracy: 0.4127\n",
      "epoch: 470, training loss: 1.243585467338562, testing loss: 1.6645578145980835\n",
      "Accuracy: 0.4157\n",
      "epoch: 470, training loss: 1.253003478050232, testing loss: 1.7536451816558838\n",
      "Accuracy: 0.3779\n",
      "epoch: 470, training loss: 1.2713427543640137, testing loss: 1.5643824338912964\n",
      "Accuracy: 0.4596\n",
      "epoch: 470, training loss: 1.3053936958312988, testing loss: 1.648324966430664\n",
      "Accuracy: 0.4221\n",
      "epoch: 470, training loss: 1.2885109186172485, testing loss: 1.7449970245361328\n",
      "Accuracy: 0.3839\n",
      "epoch: 470, training loss: 1.2633857727050781, testing loss: 1.6097966432571411\n",
      "Accuracy: 0.4304\n",
      "epoch: 470, training loss: 1.3018423318862915, testing loss: 1.7536377906799316\n",
      "Accuracy: 0.4329\n",
      "epoch: 470, training loss: 1.355502963066101, testing loss: 1.6889504194259644\n",
      "Accuracy: 0.4324\n",
      "epoch: 470, training loss: 1.2751668691635132, testing loss: 1.558417797088623\n",
      "Accuracy: 0.4396\n",
      "epoch: 470, training loss: 1.2083901166915894, testing loss: 1.603520154953003\n",
      "Accuracy: 0.4601\n",
      "epoch: 470, training loss: 1.2303595542907715, testing loss: 1.653599739074707\n",
      "Accuracy: 0.4007\n",
      "epoch: 470, training loss: 1.2494889497756958, testing loss: 1.7852425575256348\n",
      "Accuracy: 0.3881\n",
      "epoch: 470, training loss: 1.297402262687683, testing loss: 1.605392575263977\n",
      "Accuracy: 0.4352\n",
      "epoch: 470, training loss: 1.2460960149765015, testing loss: 1.5845354795455933\n",
      "Accuracy: 0.4427\n",
      "epoch: 470, training loss: 1.2463475465774536, testing loss: 1.7014976739883423\n",
      "Accuracy: 0.4161\n",
      "epoch: 470, training loss: 1.2760746479034424, testing loss: 1.7253814935684204\n",
      "Accuracy: 0.4218\n",
      "epoch: 470, training loss: 1.1968510150909424, testing loss: 1.587571144104004\n",
      "Accuracy: 0.4542\n",
      "epoch: 470, training loss: 1.205454707145691, testing loss: 1.6570727825164795\n",
      "Accuracy: 0.4209\n",
      "epoch: 470, training loss: 1.1561167240142822, testing loss: 1.678536057472229\n",
      "Accuracy: 0.4050\n",
      "epoch: 470, training loss: 1.2792612314224243, testing loss: 1.7180867195129395\n",
      "Accuracy: 0.4308\n",
      "epoch: 470, training loss: 1.281808853149414, testing loss: 1.8030976057052612\n",
      "Accuracy: 0.4198\n",
      "epoch: 470, training loss: 1.2585117816925049, testing loss: 1.7091963291168213\n",
      "Accuracy: 0.4032\n",
      "epoch: 470, training loss: 1.2320170402526855, testing loss: 1.8122893571853638\n",
      "Accuracy: 0.4470\n",
      "epoch: 470, training loss: 1.2177644968032837, testing loss: 1.8069428205490112\n",
      "Accuracy: 0.4161\n",
      "epoch: 470, training loss: 1.282004714012146, testing loss: 1.770362377166748\n",
      "Accuracy: 0.3894\n",
      "epoch: 470, training loss: 1.2035173177719116, testing loss: 1.789264440536499\n",
      "Accuracy: 0.4203\n",
      "epoch: 470, training loss: 1.2869436740875244, testing loss: 1.9391416311264038\n",
      "Accuracy: 0.3955\n",
      "epoch: 470, training loss: 1.3127450942993164, testing loss: 1.677630066871643\n",
      "Accuracy: 0.4075\n",
      "epoch: 470, training loss: 1.3294951915740967, testing loss: 1.6528470516204834\n",
      "Accuracy: 0.4467\n",
      "epoch: 470, training loss: 1.272010087966919, testing loss: 1.674708366394043\n",
      "Accuracy: 0.4214\n",
      "epoch: 470, training loss: 1.273281216621399, testing loss: 1.7847331762313843\n",
      "Accuracy: 0.3746\n",
      "epoch: 470, training loss: 1.209580898284912, testing loss: 1.6043033599853516\n",
      "Accuracy: 0.4527\n",
      "epoch: 470, training loss: 1.2380428314208984, testing loss: 1.5034394264221191\n",
      "Accuracy: 0.4642\n",
      "epoch: 470, training loss: 1.232071042060852, testing loss: 1.6558761596679688\n",
      "Accuracy: 0.4172\n",
      "epoch: 470, training loss: 1.207069754600525, testing loss: 1.8252933025360107\n",
      "Accuracy: 0.4158\n",
      "epoch: 470, training loss: 1.2689872980117798, testing loss: 1.8536021709442139\n",
      "Accuracy: 0.3704\n",
      "epoch: 470, training loss: 1.1913665533065796, testing loss: 1.6648435592651367\n",
      "Accuracy: 0.4050\n",
      "epoch: 470, training loss: 1.2471846342086792, testing loss: 1.5890885591506958\n",
      "Accuracy: 0.4551\n",
      "epoch: 470, training loss: 1.2564258575439453, testing loss: 1.5484145879745483\n",
      "Accuracy: 0.4585\n",
      "epoch: 480, training loss: 1.3106220960617065, testing loss: 1.7924221754074097\n",
      "Accuracy: 0.4158\n",
      "epoch: 480, training loss: 1.2610039710998535, testing loss: 1.736228585243225\n",
      "Accuracy: 0.4199\n",
      "epoch: 480, training loss: 1.2897300720214844, testing loss: 1.7034419775009155\n",
      "Accuracy: 0.3936\n",
      "epoch: 480, training loss: 1.227259874343872, testing loss: 1.6831978559494019\n",
      "Accuracy: 0.4609\n",
      "epoch: 480, training loss: 1.2213304042816162, testing loss: 1.6856451034545898\n",
      "Accuracy: 0.4121\n",
      "epoch: 480, training loss: 1.2189128398895264, testing loss: 1.748260736465454\n",
      "Accuracy: 0.3974\n",
      "epoch: 480, training loss: 1.303773045539856, testing loss: 1.7494105100631714\n",
      "Accuracy: 0.4202\n",
      "epoch: 480, training loss: 1.2655282020568848, testing loss: 1.765872836112976\n",
      "Accuracy: 0.3732\n",
      "epoch: 480, training loss: 1.245427131652832, testing loss: 1.6618061065673828\n",
      "Accuracy: 0.4304\n",
      "epoch: 480, training loss: 1.3085355758666992, testing loss: 1.7349740266799927\n",
      "Accuracy: 0.3831\n",
      "epoch: 480, training loss: 1.3023607730865479, testing loss: 1.695144772529602\n",
      "Accuracy: 0.4602\n",
      "epoch: 480, training loss: 1.2741104364395142, testing loss: 1.8401188850402832\n",
      "Accuracy: 0.3973\n",
      "epoch: 480, training loss: 1.29502272605896, testing loss: 1.7176405191421509\n",
      "Accuracy: 0.4190\n",
      "epoch: 480, training loss: 1.249772548675537, testing loss: 1.8541183471679688\n",
      "Accuracy: 0.4477\n",
      "epoch: 480, training loss: 1.1803605556488037, testing loss: 1.6333963871002197\n",
      "Accuracy: 0.4409\n",
      "epoch: 480, training loss: 1.2468044757843018, testing loss: 1.5738455057144165\n",
      "Accuracy: 0.4500\n",
      "epoch: 480, training loss: 1.2609690427780151, testing loss: 1.6817080974578857\n",
      "Accuracy: 0.3758\n",
      "epoch: 480, training loss: 1.2394336462020874, testing loss: 1.7063642740249634\n",
      "Accuracy: 0.4313\n",
      "epoch: 480, training loss: 1.2409839630126953, testing loss: 1.6760168075561523\n",
      "Accuracy: 0.4340\n",
      "epoch: 480, training loss: 1.2959133386611938, testing loss: 1.5735175609588623\n",
      "Accuracy: 0.4303\n",
      "epoch: 480, training loss: 1.277850866317749, testing loss: 1.5679395198822021\n",
      "Accuracy: 0.4362\n",
      "epoch: 480, training loss: 1.2228593826293945, testing loss: 1.6387637853622437\n",
      "Accuracy: 0.4189\n",
      "epoch: 480, training loss: 1.3141648769378662, testing loss: 1.720506191253662\n",
      "Accuracy: 0.3946\n",
      "epoch: 480, training loss: 1.2353442907333374, testing loss: 1.7562222480773926\n",
      "Accuracy: 0.3878\n",
      "epoch: 480, training loss: 1.2199771404266357, testing loss: 1.6665000915527344\n",
      "Accuracy: 0.4379\n",
      "epoch: 480, training loss: 1.2523808479309082, testing loss: 1.7120213508605957\n",
      "Accuracy: 0.4133\n",
      "epoch: 480, training loss: 1.2487770318984985, testing loss: 1.6268759965896606\n",
      "Accuracy: 0.4371\n",
      "epoch: 480, training loss: 1.2058120965957642, testing loss: 1.649725079536438\n",
      "Accuracy: 0.4639\n",
      "epoch: 480, training loss: 1.2580736875534058, testing loss: 1.7654564380645752\n",
      "Accuracy: 0.4024\n",
      "epoch: 480, training loss: 1.2643539905548096, testing loss: 1.7245577573776245\n",
      "Accuracy: 0.4273\n",
      "epoch: 480, training loss: 1.3790172338485718, testing loss: 1.7467012405395508\n",
      "Accuracy: 0.4000\n",
      "epoch: 480, training loss: 1.2771615982055664, testing loss: 1.6285988092422485\n",
      "Accuracy: 0.4585\n",
      "epoch: 480, training loss: 1.2629802227020264, testing loss: 1.7278653383255005\n",
      "Accuracy: 0.4479\n",
      "epoch: 480, training loss: 1.2338916063308716, testing loss: 1.6697750091552734\n",
      "Accuracy: 0.4407\n",
      "epoch: 480, training loss: 1.2331242561340332, testing loss: 1.8201284408569336\n",
      "Accuracy: 0.3698\n",
      "epoch: 480, training loss: 1.179086685180664, testing loss: 1.623910903930664\n",
      "Accuracy: 0.3941\n",
      "epoch: 480, training loss: 1.2829476594924927, testing loss: 1.5364534854888916\n",
      "Accuracy: 0.4580\n",
      "epoch: 480, training loss: 1.2666411399841309, testing loss: 1.8073089122772217\n",
      "Accuracy: 0.4000\n",
      "epoch: 480, training loss: 1.2834618091583252, testing loss: 1.6062583923339844\n",
      "Accuracy: 0.4317\n",
      "epoch: 480, training loss: 1.1898913383483887, testing loss: 1.5942829847335815\n",
      "Accuracy: 0.4635\n",
      "epoch: 480, training loss: 1.262932538986206, testing loss: 1.654414415359497\n",
      "Accuracy: 0.3880\n",
      "epoch: 480, training loss: 1.2478580474853516, testing loss: 1.8185619115829468\n",
      "Accuracy: 0.4272\n",
      "epoch: 480, training loss: 1.1960898637771606, testing loss: 1.6871485710144043\n",
      "Accuracy: 0.4379\n",
      "epoch: 480, training loss: 1.2672837972640991, testing loss: 1.8013434410095215\n",
      "Accuracy: 0.3963\n",
      "epoch: 480, training loss: 1.304218053817749, testing loss: 1.6382462978363037\n",
      "Accuracy: 0.4314\n",
      "epoch: 480, training loss: 1.2544963359832764, testing loss: 1.6396511793136597\n",
      "Accuracy: 0.4452\n",
      "epoch: 480, training loss: 1.2915585041046143, testing loss: 1.7088168859481812\n",
      "Accuracy: 0.3659\n",
      "epoch: 480, training loss: 1.2899116277694702, testing loss: 1.759009838104248\n",
      "Accuracy: 0.4128\n",
      "epoch: 480, training loss: 1.2715189456939697, testing loss: 1.6207927465438843\n",
      "Accuracy: 0.4595\n",
      "epoch: 480, training loss: 1.2174596786499023, testing loss: 1.7678333520889282\n",
      "Accuracy: 0.3882\n",
      "epoch: 480, training loss: 1.24965238571167, testing loss: 1.7559174299240112\n",
      "Accuracy: 0.3910\n",
      "epoch: 480, training loss: 1.3000068664550781, testing loss: 1.6707180738449097\n",
      "Accuracy: 0.4403\n",
      "epoch: 480, training loss: 1.1612783670425415, testing loss: 1.7931239604949951\n",
      "Accuracy: 0.3916\n",
      "epoch: 480, training loss: 1.218839168548584, testing loss: 1.7574502229690552\n",
      "Accuracy: 0.3801\n",
      "epoch: 480, training loss: 1.2587671279907227, testing loss: 1.7050997018814087\n",
      "Accuracy: 0.3938\n",
      "epoch: 480, training loss: 1.2941715717315674, testing loss: 1.7178488969802856\n",
      "Accuracy: 0.4084\n",
      "epoch: 480, training loss: 1.1845606565475464, testing loss: 1.756362795829773\n",
      "Accuracy: 0.4072\n",
      "epoch: 480, training loss: 1.343480110168457, testing loss: 1.7003694772720337\n",
      "Accuracy: 0.4000\n",
      "epoch: 480, training loss: 1.2581311464309692, testing loss: 1.7518413066864014\n",
      "Accuracy: 0.4286\n",
      "epoch: 480, training loss: 1.2153912782669067, testing loss: 1.5964851379394531\n",
      "Accuracy: 0.4198\n",
      "epoch: 480, training loss: 1.2443010807037354, testing loss: 1.8577840328216553\n",
      "Accuracy: 0.4260\n",
      "epoch: 480, training loss: 1.296389102935791, testing loss: 1.6833478212356567\n",
      "Accuracy: 0.4248\n",
      "epoch: 480, training loss: 1.232385516166687, testing loss: 1.7873737812042236\n",
      "Accuracy: 0.4090\n",
      "epoch: 480, training loss: 1.3122940063476562, testing loss: 1.7179515361785889\n",
      "Accuracy: 0.4257\n",
      "epoch: 480, training loss: 1.2538385391235352, testing loss: 1.6739521026611328\n",
      "Accuracy: 0.4103\n",
      "epoch: 480, training loss: 1.3032208681106567, testing loss: 1.6420639753341675\n",
      "Accuracy: 0.4340\n",
      "epoch: 480, training loss: 1.2012261152267456, testing loss: 1.6626688241958618\n",
      "Accuracy: 0.4411\n",
      "epoch: 480, training loss: 1.2706457376480103, testing loss: 1.7744239568710327\n",
      "Accuracy: 0.4026\n",
      "epoch: 480, training loss: 1.275993824005127, testing loss: 1.6336883306503296\n",
      "Accuracy: 0.4452\n",
      "epoch: 480, training loss: 1.2618157863616943, testing loss: 1.627629280090332\n",
      "Accuracy: 0.4513\n",
      "epoch: 480, training loss: 1.2911663055419922, testing loss: 1.657848834991455\n",
      "Accuracy: 0.4263\n",
      "epoch: 480, training loss: 1.1995174884796143, testing loss: 1.6663135290145874\n",
      "Accuracy: 0.4078\n",
      "epoch: 480, training loss: 1.2766107320785522, testing loss: 1.8134491443634033\n",
      "Accuracy: 0.4152\n",
      "epoch: 480, training loss: 1.234695553779602, testing loss: 1.7490123510360718\n",
      "Accuracy: 0.4228\n",
      "epoch: 480, training loss: 1.2686270475387573, testing loss: 1.9380052089691162\n",
      "Accuracy: 0.3885\n",
      "epoch: 480, training loss: 1.341171145439148, testing loss: 1.837581753730774\n",
      "Accuracy: 0.3758\n",
      "epoch: 480, training loss: 1.3159393072128296, testing loss: 1.680801510810852\n",
      "Accuracy: 0.3953\n",
      "epoch: 480, training loss: 1.3186216354370117, testing loss: 1.7251286506652832\n",
      "Accuracy: 0.3664\n",
      "epoch: 480, training loss: 1.2532323598861694, testing loss: 1.7084976434707642\n",
      "Accuracy: 0.4505\n",
      "epoch: 480, training loss: 1.2107487916946411, testing loss: 1.818969488143921\n",
      "Accuracy: 0.3873\n",
      "epoch: 480, training loss: 1.2662441730499268, testing loss: 1.736994981765747\n",
      "Accuracy: 0.3986\n",
      "epoch: 480, training loss: 1.2187670469284058, testing loss: 1.7598717212677002\n",
      "Accuracy: 0.4161\n",
      "epoch: 480, training loss: 1.2046393156051636, testing loss: 1.7547038793563843\n",
      "Accuracy: 0.4169\n",
      "epoch: 480, training loss: 1.3062223196029663, testing loss: 1.7404168844223022\n",
      "Accuracy: 0.4088\n",
      "epoch: 480, training loss: 1.3188220262527466, testing loss: 1.709386944770813\n",
      "Accuracy: 0.4019\n",
      "epoch: 480, training loss: 1.2613317966461182, testing loss: 1.7469929456710815\n",
      "Accuracy: 0.3962\n",
      "epoch: 480, training loss: 1.2756067514419556, testing loss: 1.6568732261657715\n",
      "Accuracy: 0.4006\n",
      "epoch: 480, training loss: 1.2304250001907349, testing loss: 1.695745587348938\n",
      "Accuracy: 0.4130\n",
      "epoch: 480, training loss: 1.227229118347168, testing loss: 1.7465733289718628\n",
      "Accuracy: 0.4114\n",
      "epoch: 480, training loss: 1.1942517757415771, testing loss: 1.7088264226913452\n",
      "Accuracy: 0.4079\n",
      "epoch: 480, training loss: 1.222645878791809, testing loss: 1.6422021389007568\n",
      "Accuracy: 0.4150\n",
      "epoch: 480, training loss: 1.2080377340316772, testing loss: 1.6833499670028687\n",
      "Accuracy: 0.4153\n",
      "epoch: 480, training loss: 1.2219985723495483, testing loss: 1.7967485189437866\n",
      "Accuracy: 0.3887\n",
      "epoch: 480, training loss: 1.3177350759506226, testing loss: 1.7470098733901978\n",
      "Accuracy: 0.3792\n",
      "epoch: 480, training loss: 1.2745815515518188, testing loss: 1.6470040082931519\n",
      "Accuracy: 0.4345\n",
      "epoch: 480, training loss: 1.247070550918579, testing loss: 1.6227140426635742\n",
      "Accuracy: 0.4291\n",
      "epoch: 480, training loss: 1.2833406925201416, testing loss: 1.7180570363998413\n",
      "Accuracy: 0.4590\n",
      "epoch: 480, training loss: 1.1919541358947754, testing loss: 1.573712706565857\n",
      "Accuracy: 0.4675\n",
      "epoch: 480, training loss: 1.2850226163864136, testing loss: 1.7117999792099\n",
      "Accuracy: 0.4062\n",
      "epoch: 480, training loss: 1.2473735809326172, testing loss: 1.740723967552185\n",
      "Accuracy: 0.3981\n",
      "epoch: 490, training loss: 1.275267481803894, testing loss: 1.757596731185913\n",
      "Accuracy: 0.4153\n",
      "epoch: 490, training loss: 1.220808744430542, testing loss: 1.618011236190796\n",
      "Accuracy: 0.4494\n",
      "epoch: 490, training loss: 1.2017039060592651, testing loss: 1.8211519718170166\n",
      "Accuracy: 0.4130\n",
      "epoch: 490, training loss: 1.2665411233901978, testing loss: 1.7686976194381714\n",
      "Accuracy: 0.4258\n",
      "epoch: 490, training loss: 1.2374763488769531, testing loss: 1.5573551654815674\n",
      "Accuracy: 0.4638\n",
      "epoch: 490, training loss: 1.2408620119094849, testing loss: 1.6163750886917114\n",
      "Accuracy: 0.4369\n",
      "epoch: 490, training loss: 1.2739052772521973, testing loss: 1.6610106229782104\n",
      "Accuracy: 0.4026\n",
      "epoch: 490, training loss: 1.2441613674163818, testing loss: 1.7226301431655884\n",
      "Accuracy: 0.4032\n",
      "epoch: 490, training loss: 1.25014066696167, testing loss: 1.6748504638671875\n",
      "Accuracy: 0.4548\n",
      "epoch: 490, training loss: 1.1949578523635864, testing loss: 1.6384676694869995\n",
      "Accuracy: 0.4317\n",
      "epoch: 490, training loss: 1.2464183568954468, testing loss: 1.8803809881210327\n",
      "Accuracy: 0.4026\n",
      "epoch: 490, training loss: 1.215948224067688, testing loss: 1.6918385028839111\n",
      "Accuracy: 0.4328\n",
      "epoch: 490, training loss: 1.2833751440048218, testing loss: 1.7710787057876587\n",
      "Accuracy: 0.3766\n",
      "epoch: 490, training loss: 1.3054752349853516, testing loss: 1.7238150835037231\n",
      "Accuracy: 0.4098\n",
      "epoch: 490, training loss: 1.2217129468917847, testing loss: 1.7581652402877808\n",
      "Accuracy: 0.4151\n",
      "epoch: 490, training loss: 1.2749080657958984, testing loss: 1.6332885026931763\n",
      "Accuracy: 0.4103\n",
      "epoch: 490, training loss: 1.3280121088027954, testing loss: 1.5771644115447998\n",
      "Accuracy: 0.4615\n",
      "epoch: 490, training loss: 1.2090200185775757, testing loss: 1.7614023685455322\n",
      "Accuracy: 0.4314\n",
      "epoch: 490, training loss: 1.2660419940948486, testing loss: 1.7293139696121216\n",
      "Accuracy: 0.4161\n",
      "epoch: 490, training loss: 1.2994365692138672, testing loss: 1.7484537363052368\n",
      "Accuracy: 0.4254\n",
      "epoch: 490, training loss: 1.2115155458450317, testing loss: 1.785714030265808\n",
      "Accuracy: 0.4051\n",
      "epoch: 490, training loss: 1.206063985824585, testing loss: 1.623928427696228\n",
      "Accuracy: 0.4426\n",
      "epoch: 490, training loss: 1.2787933349609375, testing loss: 1.716821551322937\n",
      "Accuracy: 0.4050\n",
      "epoch: 490, training loss: 1.2578747272491455, testing loss: 1.7480512857437134\n",
      "Accuracy: 0.4330\n",
      "epoch: 490, training loss: 1.2348122596740723, testing loss: 1.7494068145751953\n",
      "Accuracy: 0.3903\n",
      "epoch: 490, training loss: 1.1782170534133911, testing loss: 1.7634960412979126\n",
      "Accuracy: 0.3918\n",
      "epoch: 490, training loss: 1.2211445569992065, testing loss: 1.6253502368927002\n",
      "Accuracy: 0.4238\n",
      "epoch: 490, training loss: 1.2383869886398315, testing loss: 1.7327684164047241\n",
      "Accuracy: 0.4219\n",
      "epoch: 490, training loss: 1.3087975978851318, testing loss: 1.8049644231796265\n",
      "Accuracy: 0.4125\n",
      "epoch: 490, training loss: 1.1763689517974854, testing loss: 1.6952240467071533\n",
      "Accuracy: 0.4092\n",
      "epoch: 490, training loss: 1.2841575145721436, testing loss: 1.777649164199829\n",
      "Accuracy: 0.3855\n",
      "epoch: 490, training loss: 1.3218882083892822, testing loss: 1.650795340538025\n",
      "Accuracy: 0.4116\n",
      "epoch: 490, training loss: 1.2750834226608276, testing loss: 1.659777045249939\n",
      "Accuracy: 0.4599\n",
      "epoch: 490, training loss: 1.1964718103408813, testing loss: 1.7700968980789185\n",
      "Accuracy: 0.4259\n",
      "epoch: 490, training loss: 1.2373872995376587, testing loss: 1.751083254814148\n",
      "Accuracy: 0.4215\n",
      "epoch: 490, training loss: 1.2880829572677612, testing loss: 1.6527442932128906\n",
      "Accuracy: 0.4441\n",
      "epoch: 490, training loss: 1.2684974670410156, testing loss: 1.7702951431274414\n",
      "Accuracy: 0.4606\n",
      "epoch: 490, training loss: 1.2896028757095337, testing loss: 1.6589934825897217\n",
      "Accuracy: 0.4384\n",
      "epoch: 490, training loss: 1.3441115617752075, testing loss: 1.6862876415252686\n",
      "Accuracy: 0.4118\n",
      "epoch: 490, training loss: 1.2809851169586182, testing loss: 1.79526686668396\n",
      "Accuracy: 0.3818\n",
      "epoch: 490, training loss: 1.2566698789596558, testing loss: 1.695673942565918\n",
      "Accuracy: 0.4562\n",
      "epoch: 490, training loss: 1.2269275188446045, testing loss: 1.669011116027832\n",
      "Accuracy: 0.4145\n",
      "epoch: 490, training loss: 1.300222396850586, testing loss: 1.6639811992645264\n",
      "Accuracy: 0.4158\n",
      "epoch: 490, training loss: 1.3059484958648682, testing loss: 1.6953595876693726\n",
      "Accuracy: 0.4587\n",
      "epoch: 490, training loss: 1.2575747966766357, testing loss: 1.8066376447677612\n",
      "Accuracy: 0.4304\n",
      "epoch: 490, training loss: 1.1844477653503418, testing loss: 1.6714202165603638\n",
      "Accuracy: 0.4236\n",
      "epoch: 490, training loss: 1.231534719467163, testing loss: 1.7600445747375488\n",
      "Accuracy: 0.4295\n",
      "epoch: 490, training loss: 1.1923973560333252, testing loss: 1.7002842426300049\n",
      "Accuracy: 0.4613\n",
      "epoch: 490, training loss: 1.2656816244125366, testing loss: 1.6047598123550415\n",
      "Accuracy: 0.4267\n",
      "epoch: 490, training loss: 1.27369225025177, testing loss: 1.6091023683547974\n",
      "Accuracy: 0.4395\n",
      "epoch: 490, training loss: 1.2590301036834717, testing loss: 1.6875190734863281\n",
      "Accuracy: 0.4083\n",
      "epoch: 490, training loss: 1.3607207536697388, testing loss: 1.6955511569976807\n",
      "Accuracy: 0.4334\n",
      "epoch: 490, training loss: 1.290722131729126, testing loss: 1.6944003105163574\n",
      "Accuracy: 0.4129\n",
      "epoch: 490, training loss: 1.2549327611923218, testing loss: 1.6153876781463623\n",
      "Accuracy: 0.3934\n",
      "epoch: 490, training loss: 1.2163430452346802, testing loss: 1.7652744054794312\n",
      "Accuracy: 0.3656\n",
      "epoch: 490, training loss: 1.2811729907989502, testing loss: 1.7657455205917358\n",
      "Accuracy: 0.3679\n",
      "epoch: 490, training loss: 1.3001184463500977, testing loss: 1.8106268644332886\n",
      "Accuracy: 0.3578\n",
      "epoch: 490, training loss: 1.243450403213501, testing loss: 1.6699979305267334\n",
      "Accuracy: 0.4401\n",
      "epoch: 490, training loss: 1.230185627937317, testing loss: 1.7020010948181152\n",
      "Accuracy: 0.4248\n",
      "epoch: 490, training loss: 1.3180584907531738, testing loss: 1.659711480140686\n",
      "Accuracy: 0.4180\n",
      "epoch: 490, training loss: 1.2653721570968628, testing loss: 1.712174415588379\n",
      "Accuracy: 0.4399\n",
      "epoch: 490, training loss: 1.1877905130386353, testing loss: 1.7583189010620117\n",
      "Accuracy: 0.3854\n",
      "epoch: 490, training loss: 1.217215895652771, testing loss: 1.6156898736953735\n",
      "Accuracy: 0.4196\n",
      "epoch: 490, training loss: 1.2044206857681274, testing loss: 1.7591036558151245\n",
      "Accuracy: 0.4161\n",
      "epoch: 490, training loss: 1.2674901485443115, testing loss: 1.7459635734558105\n",
      "Accuracy: 0.4221\n",
      "epoch: 490, training loss: 1.1979414224624634, testing loss: 1.7886048555374146\n",
      "Accuracy: 0.4295\n",
      "epoch: 490, training loss: 1.2288873195648193, testing loss: 1.6788967847824097\n",
      "Accuracy: 0.4100\n",
      "epoch: 490, training loss: 1.200238585472107, testing loss: 1.8193892240524292\n",
      "Accuracy: 0.4147\n",
      "epoch: 490, training loss: 1.2129647731781006, testing loss: 1.5172877311706543\n",
      "Accuracy: 0.3897\n",
      "epoch: 490, training loss: 1.2022922039031982, testing loss: 1.7368921041488647\n",
      "Accuracy: 0.3794\n",
      "epoch: 490, training loss: 1.218409538269043, testing loss: 1.742158055305481\n",
      "Accuracy: 0.4257\n",
      "epoch: 490, training loss: 1.2626734972000122, testing loss: 1.6729100942611694\n",
      "Accuracy: 0.4355\n",
      "epoch: 490, training loss: 1.229479432106018, testing loss: 1.8173211812973022\n",
      "Accuracy: 0.4030\n",
      "epoch: 490, training loss: 1.259966254234314, testing loss: 1.786976933479309\n",
      "Accuracy: 0.3720\n",
      "epoch: 490, training loss: 1.2572667598724365, testing loss: 1.7200982570648193\n",
      "Accuracy: 0.4045\n",
      "epoch: 490, training loss: 1.2164536714553833, testing loss: 1.7696245908737183\n",
      "Accuracy: 0.4183\n",
      "epoch: 490, training loss: 1.276903748512268, testing loss: 1.7296124696731567\n",
      "Accuracy: 0.4522\n",
      "epoch: 490, training loss: 1.253290057182312, testing loss: 1.7209142446517944\n",
      "Accuracy: 0.4098\n",
      "epoch: 490, training loss: 1.2848105430603027, testing loss: 1.7202380895614624\n",
      "Accuracy: 0.4156\n",
      "epoch: 490, training loss: 1.2492061853408813, testing loss: 1.649733304977417\n",
      "Accuracy: 0.4273\n",
      "epoch: 490, training loss: 1.2381130456924438, testing loss: 1.683489441871643\n",
      "Accuracy: 0.4441\n",
      "epoch: 490, training loss: 1.2501055002212524, testing loss: 1.7425819635391235\n",
      "Accuracy: 0.4033\n",
      "epoch: 490, training loss: 1.2889132499694824, testing loss: 1.7321499586105347\n",
      "Accuracy: 0.4037\n",
      "epoch: 490, training loss: 1.2818483114242554, testing loss: 1.7478951215744019\n",
      "Accuracy: 0.4400\n",
      "epoch: 490, training loss: 1.264127254486084, testing loss: 1.6498358249664307\n",
      "Accuracy: 0.3610\n",
      "epoch: 490, training loss: 1.2632007598876953, testing loss: 1.7828000783920288\n",
      "Accuracy: 0.3859\n",
      "epoch: 490, training loss: 1.2100894451141357, testing loss: 1.728492259979248\n",
      "Accuracy: 0.3981\n",
      "epoch: 490, training loss: 1.283612847328186, testing loss: 1.6137058734893799\n",
      "Accuracy: 0.4174\n",
      "epoch: 490, training loss: 1.2598750591278076, testing loss: 1.828241229057312\n",
      "Accuracy: 0.3839\n",
      "epoch: 490, training loss: 1.279921531677246, testing loss: 1.9067730903625488\n",
      "Accuracy: 0.3833\n",
      "epoch: 490, training loss: 1.2695683240890503, testing loss: 1.696277141571045\n",
      "Accuracy: 0.3974\n",
      "epoch: 490, training loss: 1.2434886693954468, testing loss: 1.613487720489502\n",
      "Accuracy: 0.4359\n",
      "epoch: 490, training loss: 1.26478910446167, testing loss: 1.676294207572937\n",
      "Accuracy: 0.4321\n",
      "epoch: 490, training loss: 1.2517985105514526, testing loss: 1.791422963142395\n",
      "Accuracy: 0.4085\n",
      "epoch: 490, training loss: 1.2110109329223633, testing loss: 1.764892578125\n",
      "Accuracy: 0.4026\n",
      "epoch: 490, training loss: 1.232969880104065, testing loss: 1.7977139949798584\n",
      "Accuracy: 0.4129\n",
      "epoch: 490, training loss: 1.262493371963501, testing loss: 1.4988906383514404\n",
      "Accuracy: 0.5034\n",
      "epoch: 490, training loss: 1.2426668405532837, testing loss: 1.7177870273590088\n",
      "Accuracy: 0.4434\n",
      "epoch: 490, training loss: 1.1949118375778198, testing loss: 1.8220411539077759\n",
      "Accuracy: 0.3469\n",
      "epoch: 490, training loss: 1.310651421546936, testing loss: 1.6795620918273926\n",
      "Accuracy: 0.4123\n",
      "epoch: 500, training loss: 1.2778918743133545, testing loss: 1.674112319946289\n",
      "Accuracy: 0.4212\n",
      "epoch: 500, training loss: 1.2759801149368286, testing loss: 1.8227468729019165\n",
      "Accuracy: 0.3914\n",
      "epoch: 500, training loss: 1.3301092386245728, testing loss: 1.6001112461090088\n",
      "Accuracy: 0.4392\n",
      "epoch: 500, training loss: 1.2627702951431274, testing loss: 1.8532748222351074\n",
      "Accuracy: 0.3525\n",
      "epoch: 500, training loss: 1.2421973943710327, testing loss: 1.7182245254516602\n",
      "Accuracy: 0.4132\n",
      "epoch: 500, training loss: 1.248855471611023, testing loss: 1.7238155603408813\n",
      "Accuracy: 0.4286\n",
      "epoch: 500, training loss: 1.2703490257263184, testing loss: 1.6735122203826904\n",
      "Accuracy: 0.4182\n",
      "epoch: 500, training loss: 1.3311598300933838, testing loss: 1.5949345827102661\n",
      "Accuracy: 0.4256\n",
      "epoch: 500, training loss: 1.2843276262283325, testing loss: 1.6650164127349854\n",
      "Accuracy: 0.4430\n",
      "epoch: 500, training loss: 1.226954698562622, testing loss: 1.7182809114456177\n",
      "Accuracy: 0.3887\n",
      "epoch: 500, training loss: 1.2755314111709595, testing loss: 1.7782245874404907\n",
      "Accuracy: 0.4046\n",
      "epoch: 500, training loss: 1.226860523223877, testing loss: 1.8760194778442383\n",
      "Accuracy: 0.3881\n",
      "epoch: 500, training loss: 1.211499810218811, testing loss: 1.7717186212539673\n",
      "Accuracy: 0.4137\n",
      "epoch: 500, training loss: 1.2280179262161255, testing loss: 1.6971672773361206\n",
      "Accuracy: 0.4596\n",
      "epoch: 500, training loss: 1.2438641786575317, testing loss: 1.6605275869369507\n",
      "Accuracy: 0.4078\n",
      "epoch: 500, training loss: 1.212145209312439, testing loss: 1.8461726903915405\n",
      "Accuracy: 0.3677\n",
      "epoch: 500, training loss: 1.2465406656265259, testing loss: 1.6778379678726196\n",
      "Accuracy: 0.3987\n",
      "epoch: 500, training loss: 1.268034815788269, testing loss: 1.829164981842041\n",
      "Accuracy: 0.4150\n",
      "epoch: 500, training loss: 1.2169259786605835, testing loss: 1.6520140171051025\n",
      "Accuracy: 0.4411\n",
      "epoch: 500, training loss: 1.2572757005691528, testing loss: 1.6744102239608765\n",
      "Accuracy: 0.4012\n",
      "epoch: 500, training loss: 1.2348755598068237, testing loss: 1.6328552961349487\n",
      "Accuracy: 0.4200\n",
      "epoch: 500, training loss: 1.2515629529953003, testing loss: 1.7059087753295898\n",
      "Accuracy: 0.4095\n",
      "epoch: 500, training loss: 1.1903901100158691, testing loss: 1.7808040380477905\n",
      "Accuracy: 0.4189\n",
      "epoch: 500, training loss: 1.2877308130264282, testing loss: 1.6061925888061523\n",
      "Accuracy: 0.4704\n",
      "epoch: 500, training loss: 1.2711228132247925, testing loss: 1.6949031352996826\n",
      "Accuracy: 0.4290\n",
      "epoch: 500, training loss: 1.1721787452697754, testing loss: 1.7057693004608154\n",
      "Accuracy: 0.4045\n",
      "epoch: 500, training loss: 1.2541483640670776, testing loss: 1.803104281425476\n",
      "Accuracy: 0.4054\n",
      "epoch: 500, training loss: 1.2707269191741943, testing loss: 1.717481017112732\n",
      "Accuracy: 0.3801\n",
      "epoch: 500, training loss: 1.2675049304962158, testing loss: 1.6631027460098267\n",
      "Accuracy: 0.4300\n",
      "epoch: 500, training loss: 1.2248082160949707, testing loss: 1.7047996520996094\n",
      "Accuracy: 0.4272\n",
      "epoch: 500, training loss: 1.292310118675232, testing loss: 1.6027847528457642\n",
      "Accuracy: 0.4034\n",
      "epoch: 500, training loss: 1.2682077884674072, testing loss: 1.7349823713302612\n",
      "Accuracy: 0.4281\n",
      "epoch: 500, training loss: 1.2140311002731323, testing loss: 1.6870934963226318\n",
      "Accuracy: 0.4085\n",
      "epoch: 500, training loss: 1.2337583303451538, testing loss: 1.639831304550171\n",
      "Accuracy: 0.4373\n",
      "epoch: 500, training loss: 1.2700172662734985, testing loss: 1.6452233791351318\n",
      "Accuracy: 0.4262\n",
      "epoch: 500, training loss: 1.2828153371810913, testing loss: 1.6500160694122314\n",
      "Accuracy: 0.3883\n",
      "epoch: 500, training loss: 1.303024172782898, testing loss: 1.6778144836425781\n",
      "Accuracy: 0.4248\n",
      "epoch: 500, training loss: 1.1863956451416016, testing loss: 1.7014672756195068\n",
      "Accuracy: 0.4377\n",
      "epoch: 500, training loss: 1.2922186851501465, testing loss: 1.6915446519851685\n",
      "Accuracy: 0.4325\n",
      "epoch: 500, training loss: 1.2142635583877563, testing loss: 1.7065199613571167\n",
      "Accuracy: 0.4548\n",
      "epoch: 500, training loss: 1.224244236946106, testing loss: 1.590648889541626\n",
      "Accuracy: 0.4185\n",
      "epoch: 500, training loss: 1.2462365627288818, testing loss: 1.6194580793380737\n",
      "Accuracy: 0.4675\n",
      "epoch: 500, training loss: 1.2225892543792725, testing loss: 1.7324564456939697\n",
      "Accuracy: 0.4341\n",
      "epoch: 500, training loss: 1.3042646646499634, testing loss: 1.7881717681884766\n",
      "Accuracy: 0.3867\n",
      "epoch: 500, training loss: 1.2845308780670166, testing loss: 1.741756558418274\n",
      "Accuracy: 0.4007\n",
      "epoch: 500, training loss: 1.2578333616256714, testing loss: 1.7322273254394531\n",
      "Accuracy: 0.3750\n",
      "epoch: 500, training loss: 1.2516915798187256, testing loss: 1.5847878456115723\n",
      "Accuracy: 0.4185\n",
      "epoch: 500, training loss: 1.2981276512145996, testing loss: 1.8129078149795532\n",
      "Accuracy: 0.3812\n",
      "epoch: 500, training loss: 1.2519997358322144, testing loss: 1.6905711889266968\n",
      "Accuracy: 0.4336\n",
      "epoch: 500, training loss: 1.2188457250595093, testing loss: 1.6505622863769531\n",
      "Accuracy: 0.4123\n",
      "epoch: 500, training loss: 1.239931583404541, testing loss: 1.6128010749816895\n",
      "Accuracy: 0.4039\n",
      "epoch: 500, training loss: 1.2439801692962646, testing loss: 1.7683780193328857\n",
      "Accuracy: 0.3912\n",
      "epoch: 500, training loss: 1.2700694799423218, testing loss: 1.7177741527557373\n",
      "Accuracy: 0.4673\n",
      "epoch: 500, training loss: 1.2709850072860718, testing loss: 1.71590256690979\n",
      "Accuracy: 0.4054\n",
      "epoch: 500, training loss: 1.2456296682357788, testing loss: 1.7154769897460938\n",
      "Accuracy: 0.4313\n",
      "epoch: 500, training loss: 1.1189359426498413, testing loss: 1.5789422988891602\n",
      "Accuracy: 0.4331\n",
      "epoch: 500, training loss: 1.2817391157150269, testing loss: 1.7263504266738892\n",
      "Accuracy: 0.4232\n",
      "epoch: 500, training loss: 1.2927725315093994, testing loss: 1.7787141799926758\n",
      "Accuracy: 0.4267\n",
      "epoch: 500, training loss: 1.2351281642913818, testing loss: 1.6704707145690918\n",
      "Accuracy: 0.4536\n",
      "epoch: 500, training loss: 1.2531592845916748, testing loss: 1.6755050420761108\n",
      "Accuracy: 0.4298\n",
      "epoch: 500, training loss: 1.2161051034927368, testing loss: 1.74786376953125\n",
      "Accuracy: 0.3799\n",
      "epoch: 500, training loss: 1.1692352294921875, testing loss: 1.8191908597946167\n",
      "Accuracy: 0.4505\n",
      "epoch: 500, training loss: 1.2776552438735962, testing loss: 1.7194178104400635\n",
      "Accuracy: 0.3875\n",
      "epoch: 500, training loss: 1.261635184288025, testing loss: 1.7071763277053833\n",
      "Accuracy: 0.4125\n",
      "epoch: 500, training loss: 1.2595821619033813, testing loss: 1.647234559059143\n",
      "Accuracy: 0.4125\n",
      "epoch: 500, training loss: 1.3651965856552124, testing loss: 1.5968847274780273\n",
      "Accuracy: 0.4286\n",
      "epoch: 500, training loss: 1.2768436670303345, testing loss: 1.783145785331726\n",
      "Accuracy: 0.3703\n",
      "epoch: 500, training loss: 1.254520058631897, testing loss: 1.8291923999786377\n",
      "Accuracy: 0.4187\n",
      "epoch: 500, training loss: 1.2478982210159302, testing loss: 1.8297511339187622\n",
      "Accuracy: 0.4047\n",
      "epoch: 500, training loss: 1.2530217170715332, testing loss: 1.7009822130203247\n",
      "Accuracy: 0.4411\n",
      "epoch: 500, training loss: 1.2472538948059082, testing loss: 1.7244415283203125\n",
      "Accuracy: 0.4458\n",
      "epoch: 500, training loss: 1.2004145383834839, testing loss: 1.527822494506836\n",
      "Accuracy: 0.4223\n",
      "epoch: 500, training loss: 1.207648515701294, testing loss: 1.6774169206619263\n",
      "Accuracy: 0.4207\n",
      "epoch: 500, training loss: 1.2818517684936523, testing loss: 1.5550581216812134\n",
      "Accuracy: 0.4300\n",
      "epoch: 500, training loss: 1.2534667253494263, testing loss: 1.708077073097229\n",
      "Accuracy: 0.3754\n",
      "epoch: 500, training loss: 1.2398866415023804, testing loss: 1.6734436750411987\n",
      "Accuracy: 0.4476\n",
      "epoch: 500, training loss: 1.2612346410751343, testing loss: 1.6858913898468018\n",
      "Accuracy: 0.3916\n",
      "epoch: 500, training loss: 1.2838661670684814, testing loss: 1.6163979768753052\n",
      "Accuracy: 0.4000\n",
      "epoch: 500, training loss: 1.2655402421951294, testing loss: 1.6306960582733154\n",
      "Accuracy: 0.4523\n",
      "epoch: 500, training loss: 1.2227880954742432, testing loss: 1.7707704305648804\n",
      "Accuracy: 0.3887\n",
      "epoch: 500, training loss: 1.2550631761550903, testing loss: 1.5949145555496216\n",
      "Accuracy: 0.4686\n",
      "epoch: 500, training loss: 1.2261486053466797, testing loss: 1.814534068107605\n",
      "Accuracy: 0.3591\n",
      "epoch: 500, training loss: 1.2380528450012207, testing loss: 1.7107322216033936\n",
      "Accuracy: 0.4451\n",
      "epoch: 500, training loss: 1.2008477449417114, testing loss: 1.6570521593093872\n",
      "Accuracy: 0.4527\n",
      "epoch: 500, training loss: 1.3058841228485107, testing loss: 1.870416283607483\n",
      "Accuracy: 0.4263\n",
      "epoch: 500, training loss: 1.2820409536361694, testing loss: 1.712427020072937\n",
      "Accuracy: 0.3950\n",
      "epoch: 500, training loss: 1.209975004196167, testing loss: 1.6721203327178955\n",
      "Accuracy: 0.4322\n",
      "epoch: 500, training loss: 1.26327645778656, testing loss: 1.7311856746673584\n",
      "Accuracy: 0.3896\n",
      "epoch: 500, training loss: 1.2579903602600098, testing loss: 1.8253238201141357\n",
      "Accuracy: 0.3808\n",
      "epoch: 500, training loss: 1.265338659286499, testing loss: 1.6091233491897583\n",
      "Accuracy: 0.4498\n",
      "epoch: 500, training loss: 1.2472162246704102, testing loss: 1.7784372568130493\n",
      "Accuracy: 0.4126\n",
      "epoch: 500, training loss: 1.2572453022003174, testing loss: 1.6818252801895142\n",
      "Accuracy: 0.4214\n",
      "epoch: 500, training loss: 1.2459683418273926, testing loss: 1.671749234199524\n",
      "Accuracy: 0.4198\n",
      "epoch: 500, training loss: 1.2735265493392944, testing loss: 1.6843515634536743\n",
      "Accuracy: 0.3729\n",
      "epoch: 500, training loss: 1.2266901731491089, testing loss: 1.703687071800232\n",
      "Accuracy: 0.4881\n",
      "epoch: 500, training loss: 1.2337361574172974, testing loss: 1.685354471206665\n",
      "Accuracy: 0.4812\n",
      "epoch: 500, training loss: 1.1477463245391846, testing loss: 1.719978928565979\n",
      "Accuracy: 0.4479\n",
      "epoch: 500, training loss: 1.2393954992294312, testing loss: 1.7191210985183716\n",
      "Accuracy: 0.4359\n",
      "epoch: 500, training loss: 1.3002777099609375, testing loss: 1.729801058769226\n",
      "Accuracy: 0.4114\n",
      "epoch: 500, training loss: 1.2234543561935425, testing loss: 1.7252984046936035\n",
      "Accuracy: 0.4377\n",
      "epoch: 510, training loss: 1.2124197483062744, testing loss: 1.6572569608688354\n",
      "Accuracy: 0.3946\n",
      "epoch: 510, training loss: 1.2382124662399292, testing loss: 1.6845084428787231\n",
      "Accuracy: 0.4063\n",
      "epoch: 510, training loss: 1.2772338390350342, testing loss: 1.5715656280517578\n",
      "Accuracy: 0.4343\n",
      "epoch: 510, training loss: 1.241972804069519, testing loss: 1.880582332611084\n",
      "Accuracy: 0.3802\n",
      "epoch: 510, training loss: 1.224678635597229, testing loss: 1.8499946594238281\n",
      "Accuracy: 0.4104\n",
      "epoch: 510, training loss: 1.221191167831421, testing loss: 1.6699421405792236\n",
      "Accuracy: 0.3953\n",
      "epoch: 510, training loss: 1.243607521057129, testing loss: 1.6812082529067993\n",
      "Accuracy: 0.4191\n",
      "epoch: 510, training loss: 1.1824151277542114, testing loss: 1.780643343925476\n",
      "Accuracy: 0.4095\n",
      "epoch: 510, training loss: 1.2961976528167725, testing loss: 1.7396284341812134\n",
      "Accuracy: 0.3988\n",
      "epoch: 510, training loss: 1.2867122888565063, testing loss: 1.6165313720703125\n",
      "Accuracy: 0.4434\n",
      "epoch: 510, training loss: 1.2682538032531738, testing loss: 1.7076034545898438\n",
      "Accuracy: 0.4211\n",
      "epoch: 510, training loss: 1.2877668142318726, testing loss: 1.700939655303955\n",
      "Accuracy: 0.4084\n",
      "epoch: 510, training loss: 1.2914000749588013, testing loss: 1.7177196741104126\n",
      "Accuracy: 0.3851\n",
      "epoch: 510, training loss: 1.2656456232070923, testing loss: 1.619978904724121\n",
      "Accuracy: 0.4585\n",
      "epoch: 510, training loss: 1.2459694147109985, testing loss: 1.7227777242660522\n",
      "Accuracy: 0.3887\n",
      "epoch: 510, training loss: 1.3372507095336914, testing loss: 1.657534122467041\n",
      "Accuracy: 0.4408\n",
      "epoch: 510, training loss: 1.2989250421524048, testing loss: 1.6600005626678467\n",
      "Accuracy: 0.4519\n",
      "epoch: 510, training loss: 1.25490140914917, testing loss: 1.6114635467529297\n",
      "Accuracy: 0.4623\n",
      "epoch: 510, training loss: 1.275997281074524, testing loss: 1.7585862874984741\n",
      "Accuracy: 0.4051\n",
      "epoch: 510, training loss: 1.269689917564392, testing loss: 1.68186354637146\n",
      "Accuracy: 0.4082\n",
      "epoch: 510, training loss: 1.2952073812484741, testing loss: 1.736194372177124\n",
      "Accuracy: 0.4498\n",
      "epoch: 510, training loss: 1.2831861972808838, testing loss: 1.650273084640503\n",
      "Accuracy: 0.4600\n",
      "epoch: 510, training loss: 1.2512180805206299, testing loss: 1.6581494808197021\n",
      "Accuracy: 0.3816\n",
      "epoch: 510, training loss: 1.2305420637130737, testing loss: 1.7248449325561523\n",
      "Accuracy: 0.4074\n",
      "epoch: 510, training loss: 1.243043065071106, testing loss: 1.7327537536621094\n",
      "Accuracy: 0.4643\n",
      "epoch: 510, training loss: 1.1755824089050293, testing loss: 1.735314965248108\n",
      "Accuracy: 0.4431\n",
      "epoch: 510, training loss: 1.2768268585205078, testing loss: 1.7308818101882935\n",
      "Accuracy: 0.3895\n",
      "epoch: 510, training loss: 1.2642426490783691, testing loss: 1.7760100364685059\n",
      "Accuracy: 0.4305\n",
      "epoch: 510, training loss: 1.1787220239639282, testing loss: 1.6629828214645386\n",
      "Accuracy: 0.4427\n",
      "epoch: 510, training loss: 1.2376075983047485, testing loss: 1.7193738222122192\n",
      "Accuracy: 0.3926\n",
      "epoch: 510, training loss: 1.250933051109314, testing loss: 1.7920105457305908\n",
      "Accuracy: 0.4014\n",
      "epoch: 510, training loss: 1.284488320350647, testing loss: 1.7483742237091064\n",
      "Accuracy: 0.4140\n",
      "epoch: 510, training loss: 1.2342276573181152, testing loss: 1.787632703781128\n",
      "Accuracy: 0.3602\n",
      "epoch: 510, training loss: 1.288327932357788, testing loss: 1.7548182010650635\n",
      "Accuracy: 0.4051\n",
      "epoch: 510, training loss: 1.2374545335769653, testing loss: 1.737993597984314\n",
      "Accuracy: 0.3967\n",
      "epoch: 510, training loss: 1.2148455381393433, testing loss: 1.6085143089294434\n",
      "Accuracy: 0.3922\n",
      "epoch: 510, training loss: 1.2927511930465698, testing loss: 1.7048126459121704\n",
      "Accuracy: 0.3953\n",
      "epoch: 510, training loss: 1.194723129272461, testing loss: 1.5806286334991455\n",
      "Accuracy: 0.4116\n",
      "epoch: 510, training loss: 1.263452172279358, testing loss: 1.705801248550415\n",
      "Accuracy: 0.4317\n",
      "epoch: 510, training loss: 1.1983858346939087, testing loss: 1.7117546796798706\n",
      "Accuracy: 0.4186\n",
      "epoch: 510, training loss: 1.2735819816589355, testing loss: 1.745598554611206\n",
      "Accuracy: 0.3924\n",
      "epoch: 510, training loss: 1.2016924619674683, testing loss: 1.5863615274429321\n",
      "Accuracy: 0.4563\n",
      "epoch: 510, training loss: 1.312374234199524, testing loss: 1.7488102912902832\n",
      "Accuracy: 0.3876\n",
      "epoch: 510, training loss: 1.3375738859176636, testing loss: 1.6434025764465332\n",
      "Accuracy: 0.4633\n",
      "epoch: 510, training loss: 1.2392951250076294, testing loss: 1.7534955739974976\n",
      "Accuracy: 0.4665\n",
      "epoch: 510, training loss: 1.2633023262023926, testing loss: 1.6795963048934937\n",
      "Accuracy: 0.4388\n",
      "epoch: 510, training loss: 1.302529215812683, testing loss: 1.756442904472351\n",
      "Accuracy: 0.4367\n",
      "epoch: 510, training loss: 1.2836942672729492, testing loss: 1.6902801990509033\n",
      "Accuracy: 0.3912\n",
      "epoch: 510, training loss: 1.2818571329116821, testing loss: 1.720927119255066\n",
      "Accuracy: 0.3956\n",
      "epoch: 510, training loss: 1.2666466236114502, testing loss: 1.7517812252044678\n",
      "Accuracy: 0.4286\n",
      "epoch: 510, training loss: 1.190453052520752, testing loss: 1.817053198814392\n",
      "Accuracy: 0.4310\n",
      "epoch: 510, training loss: 1.200479507446289, testing loss: 1.6460026502609253\n",
      "Accuracy: 0.4082\n",
      "epoch: 510, training loss: 1.3437223434448242, testing loss: 1.653464913368225\n",
      "Accuracy: 0.4500\n",
      "epoch: 510, training loss: 1.2412265539169312, testing loss: 1.7673609256744385\n",
      "Accuracy: 0.3865\n",
      "epoch: 510, training loss: 1.3037136793136597, testing loss: 1.722273349761963\n",
      "Accuracy: 0.3856\n",
      "epoch: 510, training loss: 1.2099897861480713, testing loss: 1.8741044998168945\n",
      "Accuracy: 0.4064\n",
      "epoch: 510, training loss: 1.218546748161316, testing loss: 1.8408515453338623\n",
      "Accuracy: 0.4057\n",
      "epoch: 510, training loss: 1.2706142663955688, testing loss: 1.6694611310958862\n",
      "Accuracy: 0.4234\n",
      "epoch: 510, training loss: 1.2987977266311646, testing loss: 1.5497548580169678\n",
      "Accuracy: 0.4110\n",
      "epoch: 510, training loss: 1.2688255310058594, testing loss: 1.621201515197754\n",
      "Accuracy: 0.4806\n",
      "epoch: 510, training loss: 1.276259183883667, testing loss: 1.6681078672409058\n",
      "Accuracy: 0.4310\n",
      "epoch: 510, training loss: 1.229189157485962, testing loss: 1.8620163202285767\n",
      "Accuracy: 0.3560\n",
      "epoch: 510, training loss: 1.2184840440750122, testing loss: 1.7901772260665894\n",
      "Accuracy: 0.3846\n",
      "epoch: 510, training loss: 1.2555369138717651, testing loss: 1.60455322265625\n",
      "Accuracy: 0.3973\n",
      "epoch: 510, training loss: 1.2192178964614868, testing loss: 1.7953904867172241\n",
      "Accuracy: 0.3880\n",
      "epoch: 510, training loss: 1.2321563959121704, testing loss: 1.7710466384887695\n",
      "Accuracy: 0.4356\n",
      "epoch: 510, training loss: 1.3943469524383545, testing loss: 1.7452590465545654\n",
      "Accuracy: 0.3905\n",
      "epoch: 510, training loss: 1.1967432498931885, testing loss: 1.639342188835144\n",
      "Accuracy: 0.4573\n",
      "epoch: 510, training loss: 1.2996188402175903, testing loss: 1.6853880882263184\n",
      "Accuracy: 0.3981\n",
      "epoch: 510, training loss: 1.2837120294570923, testing loss: 1.6477227210998535\n",
      "Accuracy: 0.4045\n",
      "epoch: 510, training loss: 1.2946699857711792, testing loss: 1.7411967515945435\n",
      "Accuracy: 0.4479\n",
      "epoch: 510, training loss: 1.2171833515167236, testing loss: 1.6549201011657715\n",
      "Accuracy: 0.4451\n",
      "epoch: 510, training loss: 1.2671294212341309, testing loss: 1.8382952213287354\n",
      "Accuracy: 0.3861\n",
      "epoch: 510, training loss: 1.2364745140075684, testing loss: 1.596056342124939\n",
      "Accuracy: 0.4613\n",
      "epoch: 510, training loss: 1.2794883251190186, testing loss: 1.8083347082138062\n",
      "Accuracy: 0.4156\n",
      "epoch: 510, training loss: 1.2231169939041138, testing loss: 1.7235827445983887\n",
      "Accuracy: 0.4406\n",
      "epoch: 510, training loss: 1.2222785949707031, testing loss: 1.8194530010223389\n",
      "Accuracy: 0.3975\n",
      "epoch: 510, training loss: 1.2843291759490967, testing loss: 1.6725282669067383\n",
      "Accuracy: 0.4354\n",
      "epoch: 510, training loss: 1.252163052558899, testing loss: 1.6856266260147095\n",
      "Accuracy: 0.4430\n",
      "epoch: 510, training loss: 1.2877154350280762, testing loss: 1.8176696300506592\n",
      "Accuracy: 0.4211\n",
      "epoch: 510, training loss: 1.2290008068084717, testing loss: 1.612866997718811\n",
      "Accuracy: 0.4381\n",
      "epoch: 510, training loss: 1.2285642623901367, testing loss: 1.689906358718872\n",
      "Accuracy: 0.4199\n",
      "epoch: 510, training loss: 1.223730206489563, testing loss: 1.6769025325775146\n",
      "Accuracy: 0.4303\n",
      "epoch: 510, training loss: 1.1819599866867065, testing loss: 1.655582308769226\n",
      "Accuracy: 0.4051\n",
      "epoch: 510, training loss: 1.2227263450622559, testing loss: 1.6185718774795532\n",
      "Accuracy: 0.4667\n",
      "epoch: 510, training loss: 1.1728575229644775, testing loss: 1.659443736076355\n",
      "Accuracy: 0.4048\n",
      "epoch: 510, training loss: 1.308523416519165, testing loss: 1.7552443742752075\n",
      "Accuracy: 0.4262\n",
      "epoch: 510, training loss: 1.1923444271087646, testing loss: 1.6990479230880737\n",
      "Accuracy: 0.3939\n",
      "epoch: 510, training loss: 1.2888050079345703, testing loss: 1.760993480682373\n",
      "Accuracy: 0.3893\n",
      "epoch: 510, training loss: 1.280207872390747, testing loss: 1.7955732345581055\n",
      "Accuracy: 0.3937\n",
      "epoch: 510, training loss: 1.2767196893692017, testing loss: 1.841894268989563\n",
      "Accuracy: 0.3854\n",
      "epoch: 510, training loss: 1.2775496244430542, testing loss: 1.6995877027511597\n",
      "Accuracy: 0.4469\n",
      "epoch: 510, training loss: 1.2523208856582642, testing loss: 1.7735241651535034\n",
      "Accuracy: 0.4089\n",
      "epoch: 510, training loss: 1.2443040609359741, testing loss: 1.7470697164535522\n",
      "Accuracy: 0.4080\n",
      "epoch: 510, training loss: 1.2914711236953735, testing loss: 1.777617335319519\n",
      "Accuracy: 0.3723\n",
      "epoch: 510, training loss: 1.22190260887146, testing loss: 1.6683608293533325\n",
      "Accuracy: 0.3916\n",
      "epoch: 510, training loss: 1.2387709617614746, testing loss: 1.7246947288513184\n",
      "Accuracy: 0.4224\n",
      "epoch: 510, training loss: 1.2425134181976318, testing loss: 1.743222951889038\n",
      "Accuracy: 0.4344\n",
      "epoch: 510, training loss: 1.2460957765579224, testing loss: 1.5743385553359985\n",
      "Accuracy: 0.4716\n",
      "epoch: 510, training loss: 1.2790676355361938, testing loss: 1.749590516090393\n",
      "Accuracy: 0.4261\n",
      "epoch: 520, training loss: 1.2680505514144897, testing loss: 1.7917524576187134\n",
      "Accuracy: 0.3931\n",
      "epoch: 520, training loss: 1.2083595991134644, testing loss: 1.662575602531433\n",
      "Accuracy: 0.4819\n",
      "epoch: 520, training loss: 1.3067420721054077, testing loss: 1.7137010097503662\n",
      "Accuracy: 0.3696\n",
      "epoch: 520, training loss: 1.2868529558181763, testing loss: 1.8314614295959473\n",
      "Accuracy: 0.3654\n",
      "epoch: 520, training loss: 1.2492338418960571, testing loss: 1.8040680885314941\n",
      "Accuracy: 0.4212\n",
      "epoch: 520, training loss: 1.292428970336914, testing loss: 1.5983927249908447\n",
      "Accuracy: 0.4413\n",
      "epoch: 520, training loss: 1.2440295219421387, testing loss: 1.8290772438049316\n",
      "Accuracy: 0.3633\n",
      "epoch: 520, training loss: 1.3045281171798706, testing loss: 1.7324861288070679\n",
      "Accuracy: 0.4098\n",
      "epoch: 520, training loss: 1.3050979375839233, testing loss: 1.7799072265625\n",
      "Accuracy: 0.4543\n",
      "epoch: 520, training loss: 1.3082478046417236, testing loss: 1.7083889245986938\n",
      "Accuracy: 0.4377\n",
      "epoch: 520, training loss: 1.2190687656402588, testing loss: 1.669980764389038\n",
      "Accuracy: 0.3954\n",
      "epoch: 520, training loss: 1.2900322675704956, testing loss: 1.8125157356262207\n",
      "Accuracy: 0.4072\n",
      "epoch: 520, training loss: 1.2618486881256104, testing loss: 1.6883115768432617\n",
      "Accuracy: 0.4006\n",
      "epoch: 520, training loss: 1.3133046627044678, testing loss: 1.7777742147445679\n",
      "Accuracy: 0.4092\n",
      "epoch: 520, training loss: 1.2939014434814453, testing loss: 1.6039220094680786\n",
      "Accuracy: 0.4181\n",
      "epoch: 520, training loss: 1.2481448650360107, testing loss: 1.850181221961975\n",
      "Accuracy: 0.3533\n",
      "epoch: 520, training loss: 1.2161047458648682, testing loss: 1.6415455341339111\n",
      "Accuracy: 0.3981\n",
      "epoch: 520, training loss: 1.2278999090194702, testing loss: 1.7595425844192505\n",
      "Accuracy: 0.4110\n",
      "epoch: 520, training loss: 1.2695794105529785, testing loss: 1.6143075227737427\n",
      "Accuracy: 0.4112\n",
      "epoch: 520, training loss: 1.2562012672424316, testing loss: 1.7513459920883179\n",
      "Accuracy: 0.4161\n",
      "epoch: 520, training loss: 1.2252318859100342, testing loss: 1.7920514345169067\n",
      "Accuracy: 0.3814\n",
      "epoch: 520, training loss: 1.186616063117981, testing loss: 1.8103457689285278\n",
      "Accuracy: 0.3946\n",
      "epoch: 520, training loss: 1.2079589366912842, testing loss: 1.6867870092391968\n",
      "Accuracy: 0.4102\n",
      "epoch: 520, training loss: 1.2155656814575195, testing loss: 1.6864522695541382\n",
      "Accuracy: 0.4423\n",
      "epoch: 520, training loss: 1.2700068950653076, testing loss: 1.7116678953170776\n",
      "Accuracy: 0.3869\n",
      "epoch: 520, training loss: 1.2637460231781006, testing loss: 1.6938676834106445\n",
      "Accuracy: 0.4253\n",
      "epoch: 520, training loss: 1.208775520324707, testing loss: 1.7007595300674438\n",
      "Accuracy: 0.4362\n",
      "epoch: 520, training loss: 1.2273049354553223, testing loss: 1.6726562976837158\n",
      "Accuracy: 0.4688\n",
      "epoch: 520, training loss: 1.2821158170700073, testing loss: 1.6462513208389282\n",
      "Accuracy: 0.4548\n",
      "epoch: 520, training loss: 1.1975502967834473, testing loss: 1.6738091707229614\n",
      "Accuracy: 0.4025\n",
      "epoch: 520, training loss: 1.2568607330322266, testing loss: 1.7277345657348633\n",
      "Accuracy: 0.4311\n",
      "epoch: 520, training loss: 1.245870590209961, testing loss: 1.7833287715911865\n",
      "Accuracy: 0.4202\n",
      "epoch: 520, training loss: 1.263830542564392, testing loss: 1.565672516822815\n",
      "Accuracy: 0.4630\n",
      "epoch: 520, training loss: 1.2922325134277344, testing loss: 1.7277610301971436\n",
      "Accuracy: 0.3962\n",
      "epoch: 520, training loss: 1.2034838199615479, testing loss: 1.746448040008545\n",
      "Accuracy: 0.3823\n",
      "epoch: 520, training loss: 1.1750890016555786, testing loss: 1.685796856880188\n",
      "Accuracy: 0.4051\n",
      "epoch: 520, training loss: 1.2337219715118408, testing loss: 1.5465433597564697\n",
      "Accuracy: 0.4300\n",
      "epoch: 520, training loss: 1.2823008298873901, testing loss: 1.604422688484192\n",
      "Accuracy: 0.4571\n",
      "epoch: 520, training loss: 1.2331262826919556, testing loss: 1.8190009593963623\n",
      "Accuracy: 0.3574\n",
      "epoch: 520, training loss: 1.2469048500061035, testing loss: 1.6131418943405151\n",
      "Accuracy: 0.4643\n",
      "epoch: 520, training loss: 1.3190585374832153, testing loss: 1.626259207725525\n",
      "Accuracy: 0.3938\n",
      "epoch: 520, training loss: 1.3219507932662964, testing loss: 1.6284505128860474\n",
      "Accuracy: 0.4451\n",
      "epoch: 520, training loss: 1.2073588371276855, testing loss: 1.6457098722457886\n",
      "Accuracy: 0.4098\n",
      "epoch: 520, training loss: 1.1978750228881836, testing loss: 1.7240703105926514\n",
      "Accuracy: 0.4244\n",
      "epoch: 520, training loss: 1.2685853242874146, testing loss: 1.6151354312896729\n",
      "Accuracy: 0.3981\n",
      "epoch: 520, training loss: 1.2162641286849976, testing loss: 1.6288906335830688\n",
      "Accuracy: 0.3894\n",
      "epoch: 520, training loss: 1.1634055376052856, testing loss: 1.6202173233032227\n",
      "Accuracy: 0.4437\n",
      "epoch: 520, training loss: 1.2536091804504395, testing loss: 1.592488169670105\n",
      "Accuracy: 0.4448\n",
      "epoch: 520, training loss: 1.1949247121810913, testing loss: 1.6594809293746948\n",
      "Accuracy: 0.4122\n",
      "epoch: 520, training loss: 1.2237136363983154, testing loss: 1.782926082611084\n",
      "Accuracy: 0.4412\n",
      "epoch: 520, training loss: 1.2246006727218628, testing loss: 1.6264082193374634\n",
      "Accuracy: 0.4152\n",
      "epoch: 520, training loss: 1.270450472831726, testing loss: 1.7399916648864746\n",
      "Accuracy: 0.4103\n",
      "epoch: 520, training loss: 1.2737871408462524, testing loss: 1.7056148052215576\n",
      "Accuracy: 0.4390\n",
      "epoch: 520, training loss: 1.19391930103302, testing loss: 1.8505043983459473\n",
      "Accuracy: 0.3323\n",
      "epoch: 520, training loss: 1.2698038816452026, testing loss: 1.8199023008346558\n",
      "Accuracy: 0.4006\n",
      "epoch: 520, training loss: 1.2932933568954468, testing loss: 1.6639693975448608\n",
      "Accuracy: 0.4336\n",
      "epoch: 520, training loss: 1.2522908449172974, testing loss: 1.6687064170837402\n",
      "Accuracy: 0.4046\n",
      "epoch: 520, training loss: 1.2033846378326416, testing loss: 1.6774120330810547\n",
      "Accuracy: 0.4013\n",
      "epoch: 520, training loss: 1.200366735458374, testing loss: 1.8251054286956787\n",
      "Accuracy: 0.3533\n",
      "epoch: 520, training loss: 1.2410788536071777, testing loss: 1.724789023399353\n",
      "Accuracy: 0.3967\n",
      "epoch: 520, training loss: 1.2522021532058716, testing loss: 1.7387009859085083\n",
      "Accuracy: 0.4196\n",
      "epoch: 520, training loss: 1.296571135520935, testing loss: 1.7660725116729736\n",
      "Accuracy: 0.3604\n",
      "epoch: 520, training loss: 1.2217862606048584, testing loss: 1.7809053659439087\n",
      "Accuracy: 0.3906\n",
      "epoch: 520, training loss: 1.282145619392395, testing loss: 1.7862213850021362\n",
      "Accuracy: 0.3802\n",
      "epoch: 520, training loss: 1.2156932353973389, testing loss: 1.7918040752410889\n",
      "Accuracy: 0.4069\n",
      "epoch: 520, training loss: 1.273181438446045, testing loss: 1.9111446142196655\n",
      "Accuracy: 0.3645\n",
      "epoch: 520, training loss: 1.1913405656814575, testing loss: 1.6390997171401978\n",
      "Accuracy: 0.4783\n",
      "epoch: 520, training loss: 1.294387936592102, testing loss: 1.7434287071228027\n",
      "Accuracy: 0.4401\n",
      "epoch: 520, training loss: 1.252692699432373, testing loss: 1.7940508127212524\n",
      "Accuracy: 0.4281\n",
      "epoch: 520, training loss: 1.2688935995101929, testing loss: 1.6291556358337402\n",
      "Accuracy: 0.4396\n",
      "epoch: 520, training loss: 1.3000973463058472, testing loss: 1.6449522972106934\n",
      "Accuracy: 0.3906\n",
      "epoch: 520, training loss: 1.2270183563232422, testing loss: 1.6754696369171143\n",
      "Accuracy: 0.3987\n",
      "epoch: 520, training loss: 1.2496883869171143, testing loss: 1.68390953540802\n",
      "Accuracy: 0.4393\n",
      "epoch: 520, training loss: 1.219627022743225, testing loss: 1.6032524108886719\n",
      "Accuracy: 0.4715\n",
      "epoch: 520, training loss: 1.2528676986694336, testing loss: 1.722572922706604\n",
      "Accuracy: 0.4270\n",
      "epoch: 520, training loss: 1.2910261154174805, testing loss: 1.5123964548110962\n",
      "Accuracy: 0.4336\n",
      "epoch: 520, training loss: 1.3064926862716675, testing loss: 1.7690725326538086\n",
      "Accuracy: 0.4437\n",
      "epoch: 520, training loss: 1.269142508506775, testing loss: 1.7922803163528442\n",
      "Accuracy: 0.4201\n",
      "epoch: 520, training loss: 1.2006349563598633, testing loss: 1.787004828453064\n",
      "Accuracy: 0.4118\n",
      "epoch: 520, training loss: 1.2652642726898193, testing loss: 1.7053829431533813\n",
      "Accuracy: 0.3961\n",
      "epoch: 520, training loss: 1.2887017726898193, testing loss: 1.6543231010437012\n",
      "Accuracy: 0.4164\n",
      "epoch: 520, training loss: 1.293111801147461, testing loss: 1.7388744354248047\n",
      "Accuracy: 0.4338\n",
      "epoch: 520, training loss: 1.3237990140914917, testing loss: 1.738100528717041\n",
      "Accuracy: 0.4089\n",
      "epoch: 520, training loss: 1.2435493469238281, testing loss: 1.6201468706130981\n",
      "Accuracy: 0.4586\n",
      "epoch: 520, training loss: 1.2494637966156006, testing loss: 1.7697442770004272\n",
      "Accuracy: 0.4233\n",
      "epoch: 520, training loss: 1.2643325328826904, testing loss: 1.6960381269454956\n",
      "Accuracy: 0.4133\n",
      "epoch: 520, training loss: 1.2300926446914673, testing loss: 1.8433947563171387\n",
      "Accuracy: 0.3897\n",
      "epoch: 520, training loss: 1.2854818105697632, testing loss: 1.707433819770813\n",
      "Accuracy: 0.4277\n",
      "epoch: 520, training loss: 1.2398622035980225, testing loss: 1.71969735622406\n",
      "Accuracy: 0.4515\n",
      "epoch: 520, training loss: 1.1806970834732056, testing loss: 1.7499825954437256\n",
      "Accuracy: 0.3529\n",
      "epoch: 520, training loss: 1.271238923072815, testing loss: 1.7425583600997925\n",
      "Accuracy: 0.3726\n",
      "epoch: 520, training loss: 1.3102102279663086, testing loss: 1.6335922479629517\n",
      "Accuracy: 0.4684\n",
      "epoch: 520, training loss: 1.1955251693725586, testing loss: 1.6449171304702759\n",
      "Accuracy: 0.4164\n",
      "epoch: 520, training loss: 1.2472249269485474, testing loss: 1.6719425916671753\n",
      "Accuracy: 0.4024\n",
      "epoch: 520, training loss: 1.258414626121521, testing loss: 1.7407547235488892\n",
      "Accuracy: 0.3705\n",
      "epoch: 520, training loss: 1.257806658744812, testing loss: 1.6781264543533325\n",
      "Accuracy: 0.4397\n",
      "epoch: 520, training loss: 1.2020816802978516, testing loss: 1.6519527435302734\n",
      "Accuracy: 0.4400\n",
      "epoch: 520, training loss: 1.1769424676895142, testing loss: 1.5467525720596313\n",
      "Accuracy: 0.4332\n",
      "epoch: 520, training loss: 1.2210274934768677, testing loss: 1.8395813703536987\n",
      "Accuracy: 0.3435\n",
      "epoch: 520, training loss: 1.250357985496521, testing loss: 1.6942001581192017\n",
      "Accuracy: 0.4026\n",
      "epoch: 530, training loss: 1.3156927824020386, testing loss: 1.6075611114501953\n",
      "Accuracy: 0.4367\n",
      "epoch: 530, training loss: 1.2763845920562744, testing loss: 1.6838068962097168\n",
      "Accuracy: 0.3960\n",
      "epoch: 530, training loss: 1.2293963432312012, testing loss: 1.6523534059524536\n",
      "Accuracy: 0.4224\n",
      "epoch: 530, training loss: 1.2070013284683228, testing loss: 1.662160038948059\n",
      "Accuracy: 0.4181\n",
      "epoch: 530, training loss: 1.2447166442871094, testing loss: 1.8274970054626465\n",
      "Accuracy: 0.3962\n",
      "epoch: 530, training loss: 1.330133080482483, testing loss: 1.695934772491455\n",
      "Accuracy: 0.4219\n",
      "epoch: 530, training loss: 1.2453722953796387, testing loss: 1.7363888025283813\n",
      "Accuracy: 0.4487\n",
      "epoch: 530, training loss: 1.28205406665802, testing loss: 1.5339220762252808\n",
      "Accuracy: 0.4500\n",
      "epoch: 530, training loss: 1.2274417877197266, testing loss: 1.7991478443145752\n",
      "Accuracy: 0.4354\n",
      "epoch: 530, training loss: 1.262521505355835, testing loss: 1.5864239931106567\n",
      "Accuracy: 0.4606\n",
      "epoch: 530, training loss: 1.2563645839691162, testing loss: 1.7418081760406494\n",
      "Accuracy: 0.4043\n",
      "epoch: 530, training loss: 1.191972017288208, testing loss: 1.6773344278335571\n",
      "Accuracy: 0.4448\n",
      "epoch: 530, training loss: 1.2693320512771606, testing loss: 1.6459310054779053\n",
      "Accuracy: 0.4272\n",
      "epoch: 530, training loss: 1.2107014656066895, testing loss: 1.6926943063735962\n",
      "Accuracy: 0.4401\n",
      "epoch: 530, training loss: 1.255651593208313, testing loss: 1.648877501487732\n",
      "Accuracy: 0.3763\n",
      "epoch: 530, training loss: 1.191281795501709, testing loss: 1.8449255228042603\n",
      "Accuracy: 0.3795\n",
      "epoch: 530, training loss: 1.2660354375839233, testing loss: 1.6569633483886719\n",
      "Accuracy: 0.4167\n",
      "epoch: 530, training loss: 1.3045246601104736, testing loss: 1.7334929704666138\n",
      "Accuracy: 0.4252\n",
      "epoch: 530, training loss: 1.2819464206695557, testing loss: 1.663237452507019\n",
      "Accuracy: 0.4464\n",
      "epoch: 530, training loss: 1.2101494073867798, testing loss: 1.697939395904541\n",
      "Accuracy: 0.4426\n",
      "epoch: 530, training loss: 1.2311047315597534, testing loss: 1.685278296470642\n",
      "Accuracy: 0.4261\n",
      "epoch: 530, training loss: 1.3115715980529785, testing loss: 1.8361854553222656\n",
      "Accuracy: 0.3889\n",
      "epoch: 530, training loss: 1.2248648405075073, testing loss: 1.8261579275131226\n",
      "Accuracy: 0.4084\n",
      "epoch: 530, training loss: 1.2338852882385254, testing loss: 1.6763851642608643\n",
      "Accuracy: 0.4330\n",
      "epoch: 530, training loss: 1.3085709810256958, testing loss: 1.7132887840270996\n",
      "Accuracy: 0.4272\n",
      "epoch: 530, training loss: 1.286001443862915, testing loss: 1.8092386722564697\n",
      "Accuracy: 0.3783\n",
      "epoch: 530, training loss: 1.2632684707641602, testing loss: 1.7423102855682373\n",
      "Accuracy: 0.4029\n",
      "epoch: 530, training loss: 1.1897962093353271, testing loss: 1.703597068786621\n",
      "Accuracy: 0.4175\n",
      "epoch: 530, training loss: 1.2760159969329834, testing loss: 1.6699270009994507\n",
      "Accuracy: 0.4198\n",
      "epoch: 530, training loss: 1.2761107683181763, testing loss: 1.6714088916778564\n",
      "Accuracy: 0.4246\n",
      "epoch: 530, training loss: 1.246726393699646, testing loss: 1.6669516563415527\n",
      "Accuracy: 0.4067\n",
      "epoch: 530, training loss: 1.1995887756347656, testing loss: 1.6442304849624634\n",
      "Accuracy: 0.4490\n",
      "epoch: 530, training loss: 1.2159899473190308, testing loss: 1.7061532735824585\n",
      "Accuracy: 0.3857\n",
      "epoch: 530, training loss: 1.2666490077972412, testing loss: 1.7836132049560547\n",
      "Accuracy: 0.4486\n",
      "epoch: 530, training loss: 1.2488572597503662, testing loss: 1.7437502145767212\n",
      "Accuracy: 0.3738\n",
      "epoch: 530, training loss: 1.2633963823318481, testing loss: 1.8203544616699219\n",
      "Accuracy: 0.4188\n",
      "epoch: 530, training loss: 1.1942846775054932, testing loss: 1.8022255897521973\n",
      "Accuracy: 0.3878\n",
      "epoch: 530, training loss: 1.1861205101013184, testing loss: 1.8266783952713013\n",
      "Accuracy: 0.4110\n",
      "epoch: 530, training loss: 1.2866952419281006, testing loss: 1.849799633026123\n",
      "Accuracy: 0.4049\n",
      "epoch: 530, training loss: 1.258010983467102, testing loss: 1.7961417436599731\n",
      "Accuracy: 0.3689\n",
      "epoch: 530, training loss: 1.262479543685913, testing loss: 1.6781373023986816\n",
      "Accuracy: 0.4116\n",
      "epoch: 530, training loss: 1.3004329204559326, testing loss: 1.7657272815704346\n",
      "Accuracy: 0.4078\n",
      "epoch: 530, training loss: 1.274351954460144, testing loss: 1.5442109107971191\n",
      "Accuracy: 0.4677\n",
      "epoch: 530, training loss: 1.18150794506073, testing loss: 1.774658441543579\n",
      "Accuracy: 0.4419\n",
      "epoch: 530, training loss: 1.2503188848495483, testing loss: 1.7135534286499023\n",
      "Accuracy: 0.4276\n",
      "epoch: 530, training loss: 1.2632757425308228, testing loss: 1.6189558506011963\n",
      "Accuracy: 0.3944\n",
      "epoch: 530, training loss: 1.2657256126403809, testing loss: 1.7581796646118164\n",
      "Accuracy: 0.3662\n",
      "epoch: 530, training loss: 1.2909637689590454, testing loss: 1.6369549036026\n",
      "Accuracy: 0.4019\n",
      "epoch: 530, training loss: 1.232494592666626, testing loss: 1.701324224472046\n",
      "Accuracy: 0.3987\n",
      "epoch: 530, training loss: 1.2156928777694702, testing loss: 1.8079025745391846\n",
      "Accuracy: 0.3962\n",
      "epoch: 530, training loss: 1.2474596500396729, testing loss: 1.7979339361190796\n",
      "Accuracy: 0.3822\n",
      "epoch: 530, training loss: 1.2102700471878052, testing loss: 1.7117950916290283\n",
      "Accuracy: 0.4080\n",
      "epoch: 530, training loss: 1.2296885251998901, testing loss: 1.6409661769866943\n",
      "Accuracy: 0.3982\n",
      "epoch: 530, training loss: 1.2272253036499023, testing loss: 1.764330506324768\n",
      "Accuracy: 0.4290\n",
      "epoch: 530, training loss: 1.2742756605148315, testing loss: 1.7006192207336426\n",
      "Accuracy: 0.4000\n",
      "epoch: 530, training loss: 1.2444562911987305, testing loss: 1.7366957664489746\n",
      "Accuracy: 0.4429\n",
      "epoch: 530, training loss: 1.2845664024353027, testing loss: 1.6738308668136597\n",
      "Accuracy: 0.4441\n",
      "epoch: 530, training loss: 1.247757911682129, testing loss: 1.7778689861297607\n",
      "Accuracy: 0.4319\n",
      "epoch: 530, training loss: 1.25503408908844, testing loss: 1.5295597314834595\n",
      "Accuracy: 0.4415\n",
      "epoch: 530, training loss: 1.3323007822036743, testing loss: 1.667161226272583\n",
      "Accuracy: 0.4348\n",
      "epoch: 530, training loss: 1.1669838428497314, testing loss: 1.6922008991241455\n",
      "Accuracy: 0.3810\n",
      "epoch: 530, training loss: 1.2591447830200195, testing loss: 1.6866544485092163\n",
      "Accuracy: 0.4448\n",
      "epoch: 530, training loss: 1.2838542461395264, testing loss: 1.7025138139724731\n",
      "Accuracy: 0.3828\n",
      "epoch: 530, training loss: 1.2104750871658325, testing loss: 1.655412197113037\n",
      "Accuracy: 0.4740\n",
      "epoch: 530, training loss: 1.2610087394714355, testing loss: 1.6456143856048584\n",
      "Accuracy: 0.4601\n",
      "epoch: 530, training loss: 1.329055666923523, testing loss: 1.6275925636291504\n",
      "Accuracy: 0.4427\n",
      "epoch: 530, training loss: 1.3464374542236328, testing loss: 1.8552340269088745\n",
      "Accuracy: 0.4146\n",
      "epoch: 530, training loss: 1.1933513879776, testing loss: 1.7624242305755615\n",
      "Accuracy: 0.4038\n",
      "epoch: 530, training loss: 1.2361842393875122, testing loss: 1.5940452814102173\n",
      "Accuracy: 0.4207\n",
      "epoch: 530, training loss: 1.3168325424194336, testing loss: 1.7591516971588135\n",
      "Accuracy: 0.4340\n",
      "epoch: 530, training loss: 1.2194830179214478, testing loss: 1.741992473602295\n",
      "Accuracy: 0.3796\n",
      "epoch: 530, training loss: 1.2728267908096313, testing loss: 1.7160556316375732\n",
      "Accuracy: 0.4182\n",
      "epoch: 530, training loss: 1.2177777290344238, testing loss: 1.6417402029037476\n",
      "Accuracy: 0.4684\n",
      "epoch: 530, training loss: 1.227333664894104, testing loss: 1.7157459259033203\n",
      "Accuracy: 0.4216\n",
      "epoch: 530, training loss: 1.272484540939331, testing loss: 1.5754084587097168\n",
      "Accuracy: 0.4290\n",
      "epoch: 530, training loss: 1.2418324947357178, testing loss: 1.7887250185012817\n",
      "Accuracy: 0.4188\n",
      "epoch: 530, training loss: 1.2663618326187134, testing loss: 1.7485119104385376\n",
      "Accuracy: 0.4355\n",
      "epoch: 530, training loss: 1.2277699708938599, testing loss: 1.737350583076477\n",
      "Accuracy: 0.3988\n",
      "epoch: 530, training loss: 1.3173531293869019, testing loss: 1.6763404607772827\n",
      "Accuracy: 0.4554\n",
      "epoch: 530, training loss: 1.2340593338012695, testing loss: 1.7218135595321655\n",
      "Accuracy: 0.4388\n",
      "epoch: 530, training loss: 1.2353153228759766, testing loss: 1.7160141468048096\n",
      "Accuracy: 0.3958\n",
      "epoch: 530, training loss: 1.2489296197891235, testing loss: 1.6650443077087402\n",
      "Accuracy: 0.4230\n",
      "epoch: 530, training loss: 1.2835302352905273, testing loss: 1.737594485282898\n",
      "Accuracy: 0.4032\n",
      "epoch: 530, training loss: 1.2518008947372437, testing loss: 1.62117600440979\n",
      "Accuracy: 0.4075\n",
      "epoch: 530, training loss: 1.270193099975586, testing loss: 1.6501413583755493\n",
      "Accuracy: 0.5066\n",
      "epoch: 530, training loss: 1.304612398147583, testing loss: 1.811794638633728\n",
      "Accuracy: 0.3700\n",
      "epoch: 530, training loss: 1.2422444820404053, testing loss: 1.8226759433746338\n",
      "Accuracy: 0.3894\n",
      "epoch: 530, training loss: 1.2591757774353027, testing loss: 1.625511646270752\n",
      "Accuracy: 0.4098\n",
      "epoch: 530, training loss: 1.2928991317749023, testing loss: 1.7223031520843506\n",
      "Accuracy: 0.4505\n",
      "epoch: 530, training loss: 1.1996253728866577, testing loss: 1.783238410949707\n",
      "Accuracy: 0.4341\n",
      "epoch: 530, training loss: 1.255977749824524, testing loss: 1.880193829536438\n",
      "Accuracy: 0.4020\n",
      "epoch: 530, training loss: 1.2347183227539062, testing loss: 1.5561686754226685\n",
      "Accuracy: 0.4393\n",
      "epoch: 530, training loss: 1.2763439416885376, testing loss: 1.6207679510116577\n",
      "Accuracy: 0.4311\n",
      "epoch: 530, training loss: 1.2163314819335938, testing loss: 1.7180187702178955\n",
      "Accuracy: 0.4069\n",
      "epoch: 530, training loss: 1.1722995042800903, testing loss: 1.66493821144104\n",
      "Accuracy: 0.4299\n",
      "epoch: 530, training loss: 1.2461888790130615, testing loss: 1.5843989849090576\n",
      "Accuracy: 0.4472\n",
      "epoch: 530, training loss: 1.2930346727371216, testing loss: 1.8407526016235352\n",
      "Accuracy: 0.3812\n",
      "epoch: 530, training loss: 1.273921012878418, testing loss: 1.6575496196746826\n",
      "Accuracy: 0.4091\n",
      "epoch: 530, training loss: 1.2780251502990723, testing loss: 1.6432021856307983\n",
      "Accuracy: 0.4452\n",
      "epoch: 530, training loss: 1.2264225482940674, testing loss: 1.6668310165405273\n",
      "Accuracy: 0.4571\n",
      "epoch: 540, training loss: 1.3079334497451782, testing loss: 1.7480273246765137\n",
      "Accuracy: 0.4263\n",
      "epoch: 540, training loss: 1.204464316368103, testing loss: 1.8355273008346558\n",
      "Accuracy: 0.3619\n",
      "epoch: 540, training loss: 1.2957583665847778, testing loss: 1.7883021831512451\n",
      "Accuracy: 0.3994\n",
      "epoch: 540, training loss: 1.2681782245635986, testing loss: 1.6955480575561523\n",
      "Accuracy: 0.3846\n",
      "epoch: 540, training loss: 1.261461615562439, testing loss: 1.6628135442733765\n",
      "Accuracy: 0.4385\n",
      "epoch: 540, training loss: 1.2454456090927124, testing loss: 1.7090672254562378\n",
      "Accuracy: 0.4467\n",
      "epoch: 540, training loss: 1.206976294517517, testing loss: 1.690423846244812\n",
      "Accuracy: 0.3889\n",
      "epoch: 540, training loss: 1.161826729774475, testing loss: 1.7018378973007202\n",
      "Accuracy: 0.4100\n",
      "epoch: 540, training loss: 1.1689516305923462, testing loss: 1.6405891180038452\n",
      "Accuracy: 0.3986\n",
      "epoch: 540, training loss: 1.2628867626190186, testing loss: 1.7061611413955688\n",
      "Accuracy: 0.4103\n",
      "epoch: 540, training loss: 1.185699701309204, testing loss: 1.725345253944397\n",
      "Accuracy: 0.4223\n",
      "epoch: 540, training loss: 1.2360988855361938, testing loss: 1.7645525932312012\n",
      "Accuracy: 0.4103\n",
      "epoch: 540, training loss: 1.2025208473205566, testing loss: 1.7715054750442505\n",
      "Accuracy: 0.4256\n",
      "epoch: 540, training loss: 1.1605600118637085, testing loss: 1.7284443378448486\n",
      "Accuracy: 0.4237\n",
      "epoch: 540, training loss: 1.2742416858673096, testing loss: 1.7667509317398071\n",
      "Accuracy: 0.4542\n",
      "epoch: 540, training loss: 1.2261043787002563, testing loss: 1.6889983415603638\n",
      "Accuracy: 0.4555\n",
      "epoch: 540, training loss: 1.2286005020141602, testing loss: 1.7019439935684204\n",
      "Accuracy: 0.4746\n",
      "epoch: 540, training loss: 1.2454560995101929, testing loss: 1.8023290634155273\n",
      "Accuracy: 0.4562\n",
      "epoch: 540, training loss: 1.2330251932144165, testing loss: 1.6643449068069458\n",
      "Accuracy: 0.4224\n",
      "epoch: 540, training loss: 1.2579878568649292, testing loss: 1.6249895095825195\n",
      "Accuracy: 0.4869\n",
      "epoch: 540, training loss: 1.2532975673675537, testing loss: 1.6940197944641113\n",
      "Accuracy: 0.4196\n",
      "epoch: 540, training loss: 1.2483059167861938, testing loss: 1.7957335710525513\n",
      "Accuracy: 0.4178\n",
      "epoch: 540, training loss: 1.336614966392517, testing loss: 1.715558648109436\n",
      "Accuracy: 0.3836\n",
      "epoch: 540, training loss: 1.2670412063598633, testing loss: 1.7915652990341187\n",
      "Accuracy: 0.4118\n",
      "epoch: 540, training loss: 1.35506272315979, testing loss: 1.6418932676315308\n",
      "Accuracy: 0.4437\n",
      "epoch: 540, training loss: 1.315906047821045, testing loss: 1.6106966733932495\n",
      "Accuracy: 0.3767\n",
      "epoch: 540, training loss: 1.1940996646881104, testing loss: 1.6663514375686646\n",
      "Accuracy: 0.4397\n",
      "epoch: 540, training loss: 1.231717824935913, testing loss: 1.670692801475525\n",
      "Accuracy: 0.3696\n",
      "epoch: 540, training loss: 1.2779887914657593, testing loss: 1.6276651620864868\n",
      "Accuracy: 0.4775\n",
      "epoch: 540, training loss: 1.2024370431900024, testing loss: 1.7410768270492554\n",
      "Accuracy: 0.4387\n",
      "epoch: 540, training loss: 1.2987695932388306, testing loss: 1.6832751035690308\n",
      "Accuracy: 0.4245\n",
      "epoch: 540, training loss: 1.3243979215621948, testing loss: 1.676540732383728\n",
      "Accuracy: 0.4255\n",
      "epoch: 540, training loss: 1.2320116758346558, testing loss: 1.7226406335830688\n",
      "Accuracy: 0.4337\n",
      "epoch: 540, training loss: 1.2921372652053833, testing loss: 1.619197130203247\n",
      "Accuracy: 0.4360\n",
      "epoch: 540, training loss: 1.2237831354141235, testing loss: 1.7643141746520996\n",
      "Accuracy: 0.4295\n",
      "epoch: 540, training loss: 1.267980933189392, testing loss: 1.6572028398513794\n",
      "Accuracy: 0.4507\n",
      "epoch: 540, training loss: 1.1999907493591309, testing loss: 1.726813554763794\n",
      "Accuracy: 0.4430\n",
      "epoch: 540, training loss: 1.2118768692016602, testing loss: 1.641042709350586\n",
      "Accuracy: 0.4129\n",
      "epoch: 540, training loss: 1.2392393350601196, testing loss: 1.7318378686904907\n",
      "Accuracy: 0.4211\n",
      "epoch: 540, training loss: 1.2760521173477173, testing loss: 1.8160390853881836\n",
      "Accuracy: 0.4101\n",
      "epoch: 540, training loss: 1.2393983602523804, testing loss: 1.6290340423583984\n",
      "Accuracy: 0.4658\n",
      "epoch: 540, training loss: 1.2415639162063599, testing loss: 1.6816332340240479\n",
      "Accuracy: 0.4062\n",
      "epoch: 540, training loss: 1.22011137008667, testing loss: 1.6788662672042847\n",
      "Accuracy: 0.4462\n",
      "epoch: 540, training loss: 1.229301929473877, testing loss: 1.7721253633499146\n",
      "Accuracy: 0.3939\n",
      "epoch: 540, training loss: 1.2266343832015991, testing loss: 1.8428593873977661\n",
      "Accuracy: 0.4078\n",
      "epoch: 540, training loss: 1.3006550073623657, testing loss: 1.8346450328826904\n",
      "Accuracy: 0.4049\n",
      "epoch: 540, training loss: 1.257529854774475, testing loss: 1.7583935260772705\n",
      "Accuracy: 0.3988\n",
      "epoch: 540, training loss: 1.312490463256836, testing loss: 1.5735467672348022\n",
      "Accuracy: 0.5030\n",
      "epoch: 540, training loss: 1.2616525888442993, testing loss: 1.622370719909668\n",
      "Accuracy: 0.4540\n",
      "epoch: 540, training loss: 1.2407234907150269, testing loss: 1.6487449407577515\n",
      "Accuracy: 0.4548\n",
      "epoch: 540, training loss: 1.2530325651168823, testing loss: 1.6173591613769531\n",
      "Accuracy: 0.4433\n",
      "epoch: 540, training loss: 1.2526226043701172, testing loss: 1.7939695119857788\n",
      "Accuracy: 0.4059\n",
      "epoch: 540, training loss: 1.2172333002090454, testing loss: 1.6941423416137695\n",
      "Accuracy: 0.4352\n",
      "epoch: 540, training loss: 1.2358975410461426, testing loss: 1.7578445672988892\n",
      "Accuracy: 0.4199\n",
      "epoch: 540, training loss: 1.2438395023345947, testing loss: 1.8232148885726929\n",
      "Accuracy: 0.4142\n",
      "epoch: 540, training loss: 1.2660232782363892, testing loss: 1.6604540348052979\n",
      "Accuracy: 0.4121\n",
      "epoch: 540, training loss: 1.2685601711273193, testing loss: 1.7576199769973755\n",
      "Accuracy: 0.3618\n",
      "epoch: 540, training loss: 1.2172130346298218, testing loss: 1.5793840885162354\n",
      "Accuracy: 0.4593\n",
      "epoch: 540, training loss: 1.24601149559021, testing loss: 1.7681758403778076\n",
      "Accuracy: 0.4500\n",
      "epoch: 540, training loss: 1.2202305793762207, testing loss: 1.702934980392456\n",
      "Accuracy: 0.4177\n",
      "epoch: 540, training loss: 1.2647556066513062, testing loss: 1.7040423154830933\n",
      "Accuracy: 0.4051\n",
      "epoch: 540, training loss: 1.2909739017486572, testing loss: 1.7640912532806396\n",
      "Accuracy: 0.4243\n",
      "epoch: 540, training loss: 1.1876943111419678, testing loss: 1.8071401119232178\n",
      "Accuracy: 0.3766\n",
      "epoch: 540, training loss: 1.3064110279083252, testing loss: 1.6577001810073853\n",
      "Accuracy: 0.4125\n",
      "epoch: 540, training loss: 1.2824678421020508, testing loss: 1.6908413171768188\n",
      "Accuracy: 0.3869\n",
      "epoch: 540, training loss: 1.1616920232772827, testing loss: 1.7316780090332031\n",
      "Accuracy: 0.4026\n",
      "epoch: 540, training loss: 1.2683738470077515, testing loss: 1.7472633123397827\n",
      "Accuracy: 0.4076\n",
      "epoch: 540, training loss: 1.2503089904785156, testing loss: 1.7902621030807495\n",
      "Accuracy: 0.4240\n",
      "epoch: 540, training loss: 1.2439558506011963, testing loss: 1.6762316226959229\n",
      "Accuracy: 0.4356\n",
      "epoch: 540, training loss: 1.3087713718414307, testing loss: 1.7395710945129395\n",
      "Accuracy: 0.4325\n",
      "epoch: 540, training loss: 1.2957592010498047, testing loss: 1.7445429563522339\n",
      "Accuracy: 0.4182\n",
      "epoch: 540, training loss: 1.2406576871871948, testing loss: 1.70574152469635\n",
      "Accuracy: 0.4107\n",
      "epoch: 540, training loss: 1.2895747423171997, testing loss: 1.8106094598770142\n",
      "Accuracy: 0.4000\n",
      "epoch: 540, training loss: 1.2222175598144531, testing loss: 1.7291945219039917\n",
      "Accuracy: 0.3950\n",
      "epoch: 540, training loss: 1.2647217512130737, testing loss: 1.6382642984390259\n",
      "Accuracy: 0.4663\n",
      "epoch: 540, training loss: 1.2887927293777466, testing loss: 1.754897117614746\n",
      "Accuracy: 0.4276\n",
      "epoch: 540, training loss: 1.2578816413879395, testing loss: 1.595997929573059\n",
      "Accuracy: 0.4309\n",
      "epoch: 540, training loss: 1.2531291246414185, testing loss: 1.664345145225525\n",
      "Accuracy: 0.4341\n",
      "epoch: 540, training loss: 1.2382506132125854, testing loss: 1.7616409063339233\n",
      "Accuracy: 0.4207\n",
      "epoch: 540, training loss: 1.235398292541504, testing loss: 1.710450530052185\n",
      "Accuracy: 0.4247\n",
      "epoch: 540, training loss: 1.2651325464248657, testing loss: 1.6523239612579346\n",
      "Accuracy: 0.3988\n",
      "epoch: 540, training loss: 1.1881260871887207, testing loss: 1.5770559310913086\n",
      "Accuracy: 0.4474\n",
      "epoch: 540, training loss: 1.2389426231384277, testing loss: 1.6849346160888672\n",
      "Accuracy: 0.4371\n",
      "epoch: 540, training loss: 1.2212294340133667, testing loss: 1.72430419921875\n",
      "Accuracy: 0.3974\n",
      "epoch: 540, training loss: 1.2800840139389038, testing loss: 1.6397212743759155\n",
      "Accuracy: 0.4394\n",
      "epoch: 540, training loss: 1.2328819036483765, testing loss: 1.7184854745864868\n",
      "Accuracy: 0.4340\n",
      "epoch: 540, training loss: 1.1985573768615723, testing loss: 1.7237138748168945\n",
      "Accuracy: 0.4458\n",
      "epoch: 540, training loss: 1.3477612733840942, testing loss: 1.6889393329620361\n",
      "Accuracy: 0.4554\n",
      "epoch: 540, training loss: 1.2811932563781738, testing loss: 1.6551388502120972\n",
      "Accuracy: 0.4299\n",
      "epoch: 540, training loss: 1.1572648286819458, testing loss: 1.6335527896881104\n",
      "Accuracy: 0.4263\n",
      "epoch: 540, training loss: 1.2322802543640137, testing loss: 1.6755746603012085\n",
      "Accuracy: 0.4048\n",
      "epoch: 540, training loss: 1.1820226907730103, testing loss: 1.7587958574295044\n",
      "Accuracy: 0.4261\n",
      "epoch: 540, training loss: 1.2225757837295532, testing loss: 1.6724977493286133\n",
      "Accuracy: 0.4399\n",
      "epoch: 540, training loss: 1.2363861799240112, testing loss: 1.6387730836868286\n",
      "Accuracy: 0.4361\n",
      "epoch: 540, training loss: 1.263209581375122, testing loss: 1.7153550386428833\n",
      "Accuracy: 0.3884\n",
      "epoch: 540, training loss: 1.2293314933776855, testing loss: 1.6900064945220947\n",
      "Accuracy: 0.4167\n",
      "epoch: 540, training loss: 1.3030444383621216, testing loss: 1.7805901765823364\n",
      "Accuracy: 0.4143\n",
      "epoch: 540, training loss: 1.2672358751296997, testing loss: 1.6582709550857544\n",
      "Accuracy: 0.4461\n",
      "epoch: 540, training loss: 1.2122975587844849, testing loss: 1.6983903646469116\n",
      "Accuracy: 0.4212\n",
      "epoch: 540, training loss: 1.243207573890686, testing loss: 1.739159107208252\n",
      "Accuracy: 0.3574\n",
      "epoch: 550, training loss: 1.20454740524292, testing loss: 1.6988565921783447\n",
      "Accuracy: 0.3931\n",
      "epoch: 550, training loss: 1.1879680156707764, testing loss: 1.7617030143737793\n",
      "Accuracy: 0.3648\n",
      "epoch: 550, training loss: 1.2286453247070312, testing loss: 1.787488341331482\n",
      "Accuracy: 0.4128\n",
      "epoch: 550, training loss: 1.2634934186935425, testing loss: 1.8454772233963013\n",
      "Accuracy: 0.4102\n",
      "epoch: 550, training loss: 1.245685338973999, testing loss: 1.6949734687805176\n",
      "Accuracy: 0.4341\n",
      "epoch: 550, training loss: 1.2229565382003784, testing loss: 1.7178997993469238\n",
      "Accuracy: 0.4175\n",
      "epoch: 550, training loss: 1.2853176593780518, testing loss: 1.837733268737793\n",
      "Accuracy: 0.3567\n",
      "epoch: 550, training loss: 1.1275663375854492, testing loss: 1.7349542379379272\n",
      "Accuracy: 0.4407\n",
      "epoch: 550, training loss: 1.2425588369369507, testing loss: 1.9280204772949219\n",
      "Accuracy: 0.3790\n",
      "epoch: 550, training loss: 1.272009015083313, testing loss: 1.789433240890503\n",
      "Accuracy: 0.4150\n",
      "epoch: 550, training loss: 1.260489821434021, testing loss: 1.6862744092941284\n",
      "Accuracy: 0.3902\n",
      "epoch: 550, training loss: 1.2173608541488647, testing loss: 1.7989810705184937\n",
      "Accuracy: 0.4013\n",
      "epoch: 550, training loss: 1.2142068147659302, testing loss: 1.8099493980407715\n",
      "Accuracy: 0.4196\n",
      "epoch: 550, training loss: 1.2201346158981323, testing loss: 1.656433343887329\n",
      "Accuracy: 0.4610\n",
      "epoch: 550, training loss: 1.2380459308624268, testing loss: 1.727035641670227\n",
      "Accuracy: 0.4212\n",
      "epoch: 550, training loss: 1.262204647064209, testing loss: 1.5925430059432983\n",
      "Accuracy: 0.4510\n",
      "epoch: 550, training loss: 1.223111629486084, testing loss: 1.6473453044891357\n",
      "Accuracy: 0.3987\n",
      "epoch: 550, training loss: 1.2635717391967773, testing loss: 1.7092463970184326\n",
      "Accuracy: 0.4026\n",
      "epoch: 550, training loss: 1.2407485246658325, testing loss: 1.8069714307785034\n",
      "Accuracy: 0.3988\n",
      "epoch: 550, training loss: 1.2562259435653687, testing loss: 1.7527092695236206\n",
      "Accuracy: 0.3655\n",
      "epoch: 550, training loss: 1.2409483194351196, testing loss: 1.7023077011108398\n",
      "Accuracy: 0.4039\n",
      "epoch: 550, training loss: 1.2254462242126465, testing loss: 1.7087372541427612\n",
      "Accuracy: 0.4169\n",
      "epoch: 550, training loss: 1.3116875886917114, testing loss: 1.6867899894714355\n",
      "Accuracy: 0.3859\n",
      "epoch: 550, training loss: 1.2011767625808716, testing loss: 1.5897712707519531\n",
      "Accuracy: 0.4517\n",
      "epoch: 550, training loss: 1.2230091094970703, testing loss: 1.674217939376831\n",
      "Accuracy: 0.4031\n",
      "epoch: 550, training loss: 1.2676767110824585, testing loss: 1.8626538515090942\n",
      "Accuracy: 0.3636\n",
      "epoch: 550, training loss: 1.2469909191131592, testing loss: 1.6984213590621948\n",
      "Accuracy: 0.4704\n",
      "epoch: 550, training loss: 1.2127926349639893, testing loss: 1.8137143850326538\n",
      "Accuracy: 0.4108\n",
      "epoch: 550, training loss: 1.2387721538543701, testing loss: 1.7744779586791992\n",
      "Accuracy: 0.3729\n",
      "epoch: 550, training loss: 1.235870599746704, testing loss: 1.7342982292175293\n",
      "Accuracy: 0.4322\n",
      "epoch: 550, training loss: 1.2852007150650024, testing loss: 1.569649338722229\n",
      "Accuracy: 0.4431\n",
      "epoch: 550, training loss: 1.2603074312210083, testing loss: 1.7968547344207764\n",
      "Accuracy: 0.4000\n",
      "epoch: 550, training loss: 1.2114711999893188, testing loss: 1.7052069902420044\n",
      "Accuracy: 0.4167\n",
      "epoch: 550, training loss: 1.34832763671875, testing loss: 1.6912003755569458\n",
      "Accuracy: 0.4236\n",
      "epoch: 550, training loss: 1.2828280925750732, testing loss: 1.6892681121826172\n",
      "Accuracy: 0.4074\n",
      "epoch: 550, training loss: 1.3127294778823853, testing loss: 1.7223985195159912\n",
      "Accuracy: 0.3825\n",
      "epoch: 550, training loss: 1.2363132238388062, testing loss: 1.7428444623947144\n",
      "Accuracy: 0.4310\n",
      "epoch: 550, training loss: 1.2585484981536865, testing loss: 1.8366060256958008\n",
      "Accuracy: 0.3939\n",
      "epoch: 550, training loss: 1.2439281940460205, testing loss: 1.7206134796142578\n",
      "Accuracy: 0.4286\n",
      "epoch: 550, training loss: 1.279550552368164, testing loss: 1.6950373649597168\n",
      "Accuracy: 0.4308\n",
      "epoch: 550, training loss: 1.2589653730392456, testing loss: 1.619691252708435\n",
      "Accuracy: 0.4391\n",
      "epoch: 550, training loss: 1.2539074420928955, testing loss: 1.7013376951217651\n",
      "Accuracy: 0.4387\n",
      "epoch: 550, training loss: 1.2397992610931396, testing loss: 1.6801927089691162\n",
      "Accuracy: 0.4227\n",
      "epoch: 550, training loss: 1.2586147785186768, testing loss: 1.8579260110855103\n",
      "Accuracy: 0.4146\n",
      "epoch: 550, training loss: 1.2301607131958008, testing loss: 1.5991783142089844\n",
      "Accuracy: 0.4643\n",
      "epoch: 550, training loss: 1.236953854560852, testing loss: 1.8050317764282227\n",
      "Accuracy: 0.4020\n",
      "epoch: 550, training loss: 1.225803017616272, testing loss: 1.8131455183029175\n",
      "Accuracy: 0.4112\n",
      "epoch: 550, training loss: 1.3278244733810425, testing loss: 1.829146146774292\n",
      "Accuracy: 0.3841\n",
      "epoch: 550, training loss: 1.2444027662277222, testing loss: 1.656873345375061\n",
      "Accuracy: 0.4204\n",
      "epoch: 550, training loss: 1.2446272373199463, testing loss: 1.6936959028244019\n",
      "Accuracy: 0.4387\n",
      "epoch: 550, training loss: 1.2887848615646362, testing loss: 1.7043170928955078\n",
      "Accuracy: 0.4092\n",
      "epoch: 550, training loss: 1.2265276908874512, testing loss: 1.7156111001968384\n",
      "Accuracy: 0.4241\n",
      "epoch: 550, training loss: 1.2938038110733032, testing loss: 1.729151725769043\n",
      "Accuracy: 0.4416\n",
      "epoch: 550, training loss: 1.2317951917648315, testing loss: 1.674019455909729\n",
      "Accuracy: 0.3878\n",
      "epoch: 550, training loss: 1.2403318881988525, testing loss: 1.7068095207214355\n",
      "Accuracy: 0.4156\n",
      "epoch: 550, training loss: 1.20909583568573, testing loss: 1.7044434547424316\n",
      "Accuracy: 0.4013\n",
      "epoch: 550, training loss: 1.2542201280593872, testing loss: 1.5400179624557495\n",
      "Accuracy: 0.4510\n",
      "epoch: 550, training loss: 1.2334657907485962, testing loss: 1.7157344818115234\n",
      "Accuracy: 0.4123\n",
      "epoch: 550, training loss: 1.2177904844284058, testing loss: 1.6770775318145752\n",
      "Accuracy: 0.3906\n",
      "epoch: 550, training loss: 1.1793999671936035, testing loss: 1.6438959836959839\n",
      "Accuracy: 0.4169\n",
      "epoch: 550, training loss: 1.256842851638794, testing loss: 1.7156392335891724\n",
      "Accuracy: 0.4118\n",
      "epoch: 550, training loss: 1.2015831470489502, testing loss: 1.5655790567398071\n",
      "Accuracy: 0.4604\n",
      "epoch: 550, training loss: 1.259183645248413, testing loss: 1.6701914072036743\n",
      "Accuracy: 0.4071\n",
      "epoch: 550, training loss: 1.2222517728805542, testing loss: 1.8087191581726074\n",
      "Accuracy: 0.3910\n",
      "epoch: 550, training loss: 1.238521695137024, testing loss: 1.6723754405975342\n",
      "Accuracy: 0.4235\n",
      "epoch: 550, training loss: 1.292845368385315, testing loss: 1.5831315517425537\n",
      "Accuracy: 0.4201\n",
      "epoch: 550, training loss: 1.2476274967193604, testing loss: 1.777485728263855\n",
      "Accuracy: 0.4048\n",
      "epoch: 550, training loss: 1.183764100074768, testing loss: 1.7871642112731934\n",
      "Accuracy: 0.3895\n",
      "epoch: 550, training loss: 1.1921089887619019, testing loss: 1.7003527879714966\n",
      "Accuracy: 0.4359\n",
      "epoch: 550, training loss: 1.1940860748291016, testing loss: 1.6877981424331665\n",
      "Accuracy: 0.4189\n",
      "epoch: 550, training loss: 1.3628933429718018, testing loss: 1.6933436393737793\n",
      "Accuracy: 0.4650\n",
      "epoch: 550, training loss: 1.312519907951355, testing loss: 1.7117258310317993\n",
      "Accuracy: 0.3794\n",
      "epoch: 550, training loss: 1.3014436960220337, testing loss: 1.697316288948059\n",
      "Accuracy: 0.4267\n",
      "epoch: 550, training loss: 1.172234058380127, testing loss: 1.9301031827926636\n",
      "Accuracy: 0.3375\n",
      "epoch: 550, training loss: 1.2193224430084229, testing loss: 1.7261064052581787\n",
      "Accuracy: 0.4026\n",
      "epoch: 550, training loss: 1.255318284034729, testing loss: 1.7983661890029907\n",
      "Accuracy: 0.3729\n",
      "epoch: 550, training loss: 1.2213177680969238, testing loss: 1.712843894958496\n",
      "Accuracy: 0.4141\n",
      "epoch: 550, training loss: 1.2538442611694336, testing loss: 1.7167706489562988\n",
      "Accuracy: 0.4019\n",
      "epoch: 550, training loss: 1.2418485879898071, testing loss: 1.7167840003967285\n",
      "Accuracy: 0.4281\n",
      "epoch: 550, training loss: 1.229933738708496, testing loss: 1.645728588104248\n",
      "Accuracy: 0.4125\n",
      "epoch: 550, training loss: 1.1351934671401978, testing loss: 1.732428789138794\n",
      "Accuracy: 0.4082\n",
      "epoch: 550, training loss: 1.2397818565368652, testing loss: 1.625349760055542\n",
      "Accuracy: 0.4430\n",
      "epoch: 550, training loss: 1.3369885683059692, testing loss: 1.7289667129516602\n",
      "Accuracy: 0.3722\n",
      "epoch: 550, training loss: 1.2634838819503784, testing loss: 1.712209939956665\n",
      "Accuracy: 0.3683\n",
      "epoch: 550, training loss: 1.270877718925476, testing loss: 1.7019836902618408\n",
      "Accuracy: 0.4264\n",
      "epoch: 550, training loss: 1.2644091844558716, testing loss: 1.7419782876968384\n",
      "Accuracy: 0.3846\n",
      "epoch: 550, training loss: 1.235595941543579, testing loss: 1.6801389455795288\n",
      "Accuracy: 0.3785\n",
      "epoch: 550, training loss: 1.2161091566085815, testing loss: 1.6916457414627075\n",
      "Accuracy: 0.3919\n",
      "epoch: 550, training loss: 1.2349086999893188, testing loss: 1.7008270025253296\n",
      "Accuracy: 0.4169\n",
      "epoch: 550, training loss: 1.2991843223571777, testing loss: 1.6815651655197144\n",
      "Accuracy: 0.4591\n",
      "epoch: 550, training loss: 1.1987509727478027, testing loss: 1.691389799118042\n",
      "Accuracy: 0.4324\n",
      "epoch: 550, training loss: 1.2315313816070557, testing loss: 1.7563868761062622\n",
      "Accuracy: 0.4047\n",
      "epoch: 550, training loss: 1.2545504570007324, testing loss: 1.7405930757522583\n",
      "Accuracy: 0.3901\n",
      "epoch: 550, training loss: 1.2313766479492188, testing loss: 1.6398425102233887\n",
      "Accuracy: 0.4479\n",
      "epoch: 550, training loss: 1.2216581106185913, testing loss: 1.7844767570495605\n",
      "Accuracy: 0.4177\n",
      "epoch: 550, training loss: 1.2690180540084839, testing loss: 1.6955671310424805\n",
      "Accuracy: 0.4294\n",
      "epoch: 550, training loss: 1.186055302619934, testing loss: 1.6685128211975098\n",
      "Accuracy: 0.4119\n",
      "epoch: 550, training loss: 1.24252188205719, testing loss: 1.80562162399292\n",
      "Accuracy: 0.4228\n",
      "epoch: 550, training loss: 1.2538328170776367, testing loss: 1.7449203729629517\n",
      "Accuracy: 0.4620\n",
      "epoch: 550, training loss: 1.2634997367858887, testing loss: 1.8108446598052979\n",
      "Accuracy: 0.3702\n",
      "epoch: 560, training loss: 1.2553396224975586, testing loss: 1.6699472665786743\n",
      "Accuracy: 0.4125\n",
      "epoch: 560, training loss: 1.23874032497406, testing loss: 1.8435064554214478\n",
      "Accuracy: 0.4206\n",
      "epoch: 560, training loss: 1.2385544776916504, testing loss: 1.7056697607040405\n",
      "Accuracy: 0.4219\n",
      "epoch: 560, training loss: 1.2900581359863281, testing loss: 1.662642478942871\n",
      "Accuracy: 0.4024\n",
      "epoch: 560, training loss: 1.2598702907562256, testing loss: 1.8516424894332886\n",
      "Accuracy: 0.3930\n",
      "epoch: 560, training loss: 1.2069131135940552, testing loss: 1.730987548828125\n",
      "Accuracy: 0.4035\n",
      "epoch: 560, training loss: 1.2461873292922974, testing loss: 1.789151668548584\n",
      "Accuracy: 0.4038\n",
      "epoch: 560, training loss: 1.2725434303283691, testing loss: 1.7742496728897095\n",
      "Accuracy: 0.4072\n",
      "epoch: 560, training loss: 1.2171554565429688, testing loss: 1.8070734739303589\n",
      "Accuracy: 0.3974\n",
      "epoch: 560, training loss: 1.21678626537323, testing loss: 1.7620970010757446\n",
      "Accuracy: 0.4256\n",
      "epoch: 560, training loss: 1.242446780204773, testing loss: 1.6330444812774658\n",
      "Accuracy: 0.4062\n",
      "epoch: 560, training loss: 1.2927669286727905, testing loss: 1.7494674921035767\n",
      "Accuracy: 0.4049\n",
      "epoch: 560, training loss: 1.2350671291351318, testing loss: 1.6744860410690308\n",
      "Accuracy: 0.4242\n",
      "epoch: 560, training loss: 1.286899209022522, testing loss: 1.8565545082092285\n",
      "Accuracy: 0.4092\n",
      "epoch: 560, training loss: 1.276265025138855, testing loss: 1.6589505672454834\n",
      "Accuracy: 0.3885\n",
      "epoch: 560, training loss: 1.2486088275909424, testing loss: 1.6748607158660889\n",
      "Accuracy: 0.4586\n",
      "epoch: 560, training loss: 1.2434147596359253, testing loss: 1.629679799079895\n",
      "Accuracy: 0.4300\n",
      "epoch: 560, training loss: 1.2304868698120117, testing loss: 1.687351942062378\n",
      "Accuracy: 0.4264\n",
      "epoch: 560, training loss: 1.1822994947433472, testing loss: 1.6992454528808594\n",
      "Accuracy: 0.4251\n",
      "epoch: 560, training loss: 1.2715330123901367, testing loss: 1.827750325202942\n",
      "Accuracy: 0.3821\n",
      "epoch: 560, training loss: 1.2588294744491577, testing loss: 1.644061803817749\n",
      "Accuracy: 0.4137\n",
      "epoch: 560, training loss: 1.2322410345077515, testing loss: 1.6841979026794434\n",
      "Accuracy: 0.4128\n",
      "epoch: 560, training loss: 1.2725030183792114, testing loss: 1.5985215902328491\n",
      "Accuracy: 0.4286\n",
      "epoch: 560, training loss: 1.2242428064346313, testing loss: 1.6579352617263794\n",
      "Accuracy: 0.4043\n",
      "epoch: 560, training loss: 1.25554621219635, testing loss: 1.7268346548080444\n",
      "Accuracy: 0.4185\n",
      "epoch: 560, training loss: 1.2287789583206177, testing loss: 1.6908034086227417\n",
      "Accuracy: 0.4110\n",
      "epoch: 560, training loss: 1.200508952140808, testing loss: 1.8315778970718384\n",
      "Accuracy: 0.3899\n",
      "epoch: 560, training loss: 1.2293626070022583, testing loss: 1.7831405401229858\n",
      "Accuracy: 0.4492\n",
      "epoch: 560, training loss: 1.2069894075393677, testing loss: 1.69252347946167\n",
      "Accuracy: 0.4343\n",
      "epoch: 560, training loss: 1.2659108638763428, testing loss: 1.583935260772705\n",
      "Accuracy: 0.4413\n",
      "epoch: 560, training loss: 1.2049765586853027, testing loss: 1.7086141109466553\n",
      "Accuracy: 0.4531\n",
      "epoch: 560, training loss: 1.2589625120162964, testing loss: 1.6169369220733643\n",
      "Accuracy: 0.4181\n",
      "epoch: 560, training loss: 1.2409324645996094, testing loss: 1.7135491371154785\n",
      "Accuracy: 0.3927\n",
      "epoch: 560, training loss: 1.302155613899231, testing loss: 1.7017194032669067\n",
      "Accuracy: 0.4078\n",
      "epoch: 560, training loss: 1.1913516521453857, testing loss: 1.6139692068099976\n",
      "Accuracy: 0.4329\n",
      "epoch: 560, training loss: 1.301173448562622, testing loss: 1.7814898490905762\n",
      "Accuracy: 0.4286\n",
      "epoch: 560, training loss: 1.2909775972366333, testing loss: 1.6626160144805908\n",
      "Accuracy: 0.4089\n",
      "epoch: 560, training loss: 1.1720162630081177, testing loss: 1.7456523180007935\n",
      "Accuracy: 0.4361\n",
      "epoch: 560, training loss: 1.3410072326660156, testing loss: 1.7714262008666992\n",
      "Accuracy: 0.3853\n",
      "epoch: 560, training loss: 1.25946044921875, testing loss: 1.8140301704406738\n",
      "Accuracy: 0.3920\n",
      "epoch: 560, training loss: 1.191582441329956, testing loss: 1.7078522443771362\n",
      "Accuracy: 0.4455\n",
      "epoch: 560, training loss: 1.2710596323013306, testing loss: 1.6680220365524292\n",
      "Accuracy: 0.4164\n",
      "epoch: 560, training loss: 1.297736406326294, testing loss: 1.7557603120803833\n",
      "Accuracy: 0.4361\n",
      "epoch: 560, training loss: 1.2828952074050903, testing loss: 1.7941378355026245\n",
      "Accuracy: 0.3672\n",
      "epoch: 560, training loss: 1.294837474822998, testing loss: 1.7252533435821533\n",
      "Accuracy: 0.4100\n",
      "epoch: 560, training loss: 1.2978777885437012, testing loss: 1.7039355039596558\n",
      "Accuracy: 0.4257\n",
      "epoch: 560, training loss: 1.174147129058838, testing loss: 1.8113940954208374\n",
      "Accuracy: 0.4315\n",
      "epoch: 560, training loss: 1.1670047044754028, testing loss: 1.7510764598846436\n",
      "Accuracy: 0.3917\n",
      "epoch: 560, training loss: 1.2220624685287476, testing loss: 1.7114431858062744\n",
      "Accuracy: 0.4532\n",
      "epoch: 560, training loss: 1.2271205186843872, testing loss: 1.8256791830062866\n",
      "Accuracy: 0.4026\n",
      "epoch: 560, training loss: 1.229427456855774, testing loss: 1.6898303031921387\n",
      "Accuracy: 0.4394\n",
      "epoch: 560, training loss: 1.289739966392517, testing loss: 1.7682514190673828\n",
      "Accuracy: 0.4394\n",
      "epoch: 560, training loss: 1.283481478691101, testing loss: 1.7937730550765991\n",
      "Accuracy: 0.4545\n",
      "epoch: 560, training loss: 1.1829885244369507, testing loss: 1.7452855110168457\n",
      "Accuracy: 0.4353\n",
      "epoch: 560, training loss: 1.2422200441360474, testing loss: 1.6655488014221191\n",
      "Accuracy: 0.3954\n",
      "epoch: 560, training loss: 1.2673029899597168, testing loss: 1.63495671749115\n",
      "Accuracy: 0.4224\n",
      "epoch: 560, training loss: 1.2560539245605469, testing loss: 1.6885528564453125\n",
      "Accuracy: 0.4051\n",
      "epoch: 560, training loss: 1.2955511808395386, testing loss: 1.617152452468872\n",
      "Accuracy: 0.4369\n",
      "epoch: 560, training loss: 1.2903826236724854, testing loss: 1.6374210119247437\n",
      "Accuracy: 0.4767\n",
      "epoch: 560, training loss: 1.342455506324768, testing loss: 1.7614121437072754\n",
      "Accuracy: 0.4026\n",
      "epoch: 560, training loss: 1.2954591512680054, testing loss: 1.7835276126861572\n",
      "Accuracy: 0.3690\n",
      "epoch: 560, training loss: 1.2113783359527588, testing loss: 1.7171889543533325\n",
      "Accuracy: 0.4006\n",
      "epoch: 560, training loss: 1.2103062868118286, testing loss: 1.6867551803588867\n",
      "Accuracy: 0.4919\n",
      "epoch: 560, training loss: 1.2215832471847534, testing loss: 1.6739380359649658\n",
      "Accuracy: 0.4349\n",
      "epoch: 560, training loss: 1.231259822845459, testing loss: 1.5863816738128662\n",
      "Accuracy: 0.4257\n",
      "epoch: 560, training loss: 1.2748363018035889, testing loss: 1.7592098712921143\n",
      "Accuracy: 0.4014\n",
      "epoch: 560, training loss: 1.2666220664978027, testing loss: 1.711135745048523\n",
      "Accuracy: 0.3826\n",
      "epoch: 560, training loss: 1.2409157752990723, testing loss: 1.7259446382522583\n",
      "Accuracy: 0.4063\n",
      "epoch: 560, training loss: 1.2168909311294556, testing loss: 1.6898318529129028\n",
      "Accuracy: 0.4591\n",
      "epoch: 560, training loss: 1.2476526498794556, testing loss: 1.7452512979507446\n",
      "Accuracy: 0.4448\n",
      "epoch: 560, training loss: 1.2604387998580933, testing loss: 1.6000064611434937\n",
      "Accuracy: 0.4385\n",
      "epoch: 560, training loss: 1.2618721723556519, testing loss: 1.7605226039886475\n",
      "Accuracy: 0.3851\n",
      "epoch: 560, training loss: 1.2688422203063965, testing loss: 1.6708165407180786\n",
      "Accuracy: 0.4740\n",
      "epoch: 560, training loss: 1.2182625532150269, testing loss: 1.8213458061218262\n",
      "Accuracy: 0.3808\n",
      "epoch: 560, training loss: 1.2433100938796997, testing loss: 1.6076345443725586\n",
      "Accuracy: 0.4219\n",
      "epoch: 560, training loss: 1.2889890670776367, testing loss: 1.6538366079330444\n",
      "Accuracy: 0.4382\n",
      "epoch: 560, training loss: 1.256773829460144, testing loss: 1.707572340965271\n",
      "Accuracy: 0.3884\n",
      "epoch: 560, training loss: 1.2466520071029663, testing loss: 1.7316606044769287\n",
      "Accuracy: 0.3730\n",
      "epoch: 560, training loss: 1.272958517074585, testing loss: 1.729506254196167\n",
      "Accuracy: 0.4344\n",
      "epoch: 560, training loss: 1.2188844680786133, testing loss: 1.609473705291748\n",
      "Accuracy: 0.4452\n",
      "epoch: 560, training loss: 1.2319023609161377, testing loss: 1.7160894870758057\n",
      "Accuracy: 0.4423\n",
      "epoch: 560, training loss: 1.3127509355545044, testing loss: 1.6928342580795288\n",
      "Accuracy: 0.4488\n",
      "epoch: 560, training loss: 1.3088358640670776, testing loss: 1.8484755754470825\n",
      "Accuracy: 0.3713\n",
      "epoch: 560, training loss: 1.3316566944122314, testing loss: 1.7922691106796265\n",
      "Accuracy: 0.4503\n",
      "epoch: 560, training loss: 1.2276087999343872, testing loss: 1.7987334728240967\n",
      "Accuracy: 0.3868\n",
      "epoch: 560, training loss: 1.2841567993164062, testing loss: 1.6825979948043823\n",
      "Accuracy: 0.3930\n",
      "epoch: 560, training loss: 1.2015900611877441, testing loss: 1.6914472579956055\n",
      "Accuracy: 0.4153\n",
      "epoch: 560, training loss: 1.2413867712020874, testing loss: 1.827750563621521\n",
      "Accuracy: 0.4201\n",
      "epoch: 560, training loss: 1.230264663696289, testing loss: 1.6684902906417847\n",
      "Accuracy: 0.4069\n",
      "epoch: 560, training loss: 1.245213270187378, testing loss: 1.7314375638961792\n",
      "Accuracy: 0.4014\n",
      "epoch: 560, training loss: 1.2888818979263306, testing loss: 1.6344107389450073\n",
      "Accuracy: 0.4180\n",
      "epoch: 560, training loss: 1.2463282346725464, testing loss: 1.6595613956451416\n",
      "Accuracy: 0.4202\n",
      "epoch: 560, training loss: 1.2481107711791992, testing loss: 1.7435181140899658\n",
      "Accuracy: 0.3986\n",
      "epoch: 560, training loss: 1.2030723094940186, testing loss: 1.773772120475769\n",
      "Accuracy: 0.4466\n",
      "epoch: 560, training loss: 1.2423467636108398, testing loss: 1.7358404397964478\n",
      "Accuracy: 0.4238\n",
      "epoch: 560, training loss: 1.2025870084762573, testing loss: 1.6908034086227417\n",
      "Accuracy: 0.4014\n",
      "epoch: 560, training loss: 1.2491137981414795, testing loss: 1.7353482246398926\n",
      "Accuracy: 0.4089\n",
      "epoch: 560, training loss: 1.1595516204833984, testing loss: 1.6643980741500854\n",
      "Accuracy: 0.4396\n",
      "epoch: 560, training loss: 1.2134348154067993, testing loss: 1.7359026670455933\n",
      "Accuracy: 0.4082\n",
      "epoch: 560, training loss: 1.2586525678634644, testing loss: 1.744986891746521\n",
      "Accuracy: 0.4031\n",
      "epoch: 570, training loss: 1.2727328538894653, testing loss: 1.6937999725341797\n",
      "Accuracy: 0.4158\n",
      "epoch: 570, training loss: 1.1936697959899902, testing loss: 1.769580602645874\n",
      "Accuracy: 0.4138\n",
      "epoch: 570, training loss: 1.2026134729385376, testing loss: 1.7755752801895142\n",
      "Accuracy: 0.4355\n",
      "epoch: 570, training loss: 1.1655222177505493, testing loss: 1.7030613422393799\n",
      "Accuracy: 0.3848\n",
      "epoch: 570, training loss: 1.3743590116500854, testing loss: 1.7327896356582642\n",
      "Accuracy: 0.4108\n",
      "epoch: 570, training loss: 1.2651914358139038, testing loss: 1.7547931671142578\n",
      "Accuracy: 0.4189\n",
      "epoch: 570, training loss: 1.2723239660263062, testing loss: 1.6291992664337158\n",
      "Accuracy: 0.3922\n",
      "epoch: 570, training loss: 1.3006690740585327, testing loss: 1.8520420789718628\n",
      "Accuracy: 0.3400\n",
      "epoch: 570, training loss: 1.1565548181533813, testing loss: 1.7144466638565063\n",
      "Accuracy: 0.4177\n",
      "epoch: 570, training loss: 1.2988166809082031, testing loss: 1.637769103050232\n",
      "Accuracy: 0.3968\n",
      "epoch: 570, training loss: 1.1740407943725586, testing loss: 1.6848952770233154\n",
      "Accuracy: 0.4245\n",
      "epoch: 570, training loss: 1.1843453645706177, testing loss: 1.6691862344741821\n",
      "Accuracy: 0.4404\n",
      "epoch: 570, training loss: 1.216094732284546, testing loss: 1.6988855600357056\n",
      "Accuracy: 0.3906\n",
      "epoch: 570, training loss: 1.1876137256622314, testing loss: 1.792855978012085\n",
      "Accuracy: 0.4155\n",
      "epoch: 570, training loss: 1.2216304540634155, testing loss: 1.826714038848877\n",
      "Accuracy: 0.4128\n",
      "epoch: 570, training loss: 1.1977369785308838, testing loss: 1.7070162296295166\n",
      "Accuracy: 0.4328\n",
      "epoch: 570, training loss: 1.279435157775879, testing loss: 1.8022106885910034\n",
      "Accuracy: 0.4349\n",
      "epoch: 570, training loss: 1.247663140296936, testing loss: 1.6583540439605713\n",
      "Accuracy: 0.4434\n",
      "epoch: 570, training loss: 1.229034662246704, testing loss: 1.6230436563491821\n",
      "Accuracy: 0.4379\n",
      "epoch: 570, training loss: 1.2231981754302979, testing loss: 1.7924244403839111\n",
      "Accuracy: 0.3828\n",
      "epoch: 570, training loss: 1.2235252857208252, testing loss: 1.6662369966506958\n",
      "Accuracy: 0.3658\n",
      "epoch: 570, training loss: 1.2526837587356567, testing loss: 1.7172123193740845\n",
      "Accuracy: 0.3666\n",
      "epoch: 570, training loss: 1.2291251420974731, testing loss: 1.6351377964019775\n",
      "Accuracy: 0.3881\n",
      "epoch: 570, training loss: 1.300474762916565, testing loss: 1.5651524066925049\n",
      "Accuracy: 0.4469\n",
      "epoch: 570, training loss: 1.2812271118164062, testing loss: 1.809004545211792\n",
      "Accuracy: 0.4358\n",
      "epoch: 570, training loss: 1.2513034343719482, testing loss: 1.6350857019424438\n",
      "Accuracy: 0.4177\n",
      "epoch: 570, training loss: 1.2460768222808838, testing loss: 1.7581641674041748\n",
      "Accuracy: 0.4044\n",
      "epoch: 570, training loss: 1.2359157800674438, testing loss: 1.6035457849502563\n",
      "Accuracy: 0.4212\n",
      "epoch: 570, training loss: 1.2594326734542847, testing loss: 1.8106669187545776\n",
      "Accuracy: 0.3754\n",
      "epoch: 570, training loss: 1.2674412727355957, testing loss: 1.6669927835464478\n",
      "Accuracy: 0.3806\n",
      "epoch: 570, training loss: 1.1790306568145752, testing loss: 1.6900655031204224\n",
      "Accuracy: 0.4132\n",
      "epoch: 570, training loss: 1.1652157306671143, testing loss: 1.6580705642700195\n",
      "Accuracy: 0.4175\n",
      "epoch: 570, training loss: 1.2723196744918823, testing loss: 1.7252111434936523\n",
      "Accuracy: 0.4596\n",
      "epoch: 570, training loss: 1.372420310974121, testing loss: 1.669170618057251\n",
      "Accuracy: 0.4220\n",
      "epoch: 570, training loss: 1.2318118810653687, testing loss: 1.7390124797821045\n",
      "Accuracy: 0.3729\n",
      "epoch: 570, training loss: 1.1764051914215088, testing loss: 1.7101287841796875\n",
      "Accuracy: 0.4431\n",
      "epoch: 570, training loss: 1.2745212316513062, testing loss: 1.6442784070968628\n",
      "Accuracy: 0.4286\n",
      "epoch: 570, training loss: 1.2253605127334595, testing loss: 1.7396292686462402\n",
      "Accuracy: 0.3938\n",
      "epoch: 570, training loss: 1.3342865705490112, testing loss: 1.6202982664108276\n",
      "Accuracy: 0.4463\n",
      "epoch: 570, training loss: 1.2610853910446167, testing loss: 1.802107334136963\n",
      "Accuracy: 0.4000\n",
      "epoch: 570, training loss: 1.2158355712890625, testing loss: 1.6833372116088867\n",
      "Accuracy: 0.4111\n",
      "epoch: 570, training loss: 1.1720435619354248, testing loss: 1.8797568082809448\n",
      "Accuracy: 0.3686\n",
      "epoch: 570, training loss: 1.2504017353057861, testing loss: 1.911252498626709\n",
      "Accuracy: 0.4125\n",
      "epoch: 570, training loss: 1.2152557373046875, testing loss: 1.7134549617767334\n",
      "Accuracy: 0.4335\n",
      "epoch: 570, training loss: 1.172447681427002, testing loss: 1.6189489364624023\n",
      "Accuracy: 0.4702\n",
      "epoch: 570, training loss: 1.2255334854125977, testing loss: 1.7401158809661865\n",
      "Accuracy: 0.3844\n",
      "epoch: 570, training loss: 1.2285255193710327, testing loss: 1.7450374364852905\n",
      "Accuracy: 0.4218\n",
      "epoch: 570, training loss: 1.2777507305145264, testing loss: 1.7726761102676392\n",
      "Accuracy: 0.4049\n",
      "epoch: 570, training loss: 1.2332149744033813, testing loss: 1.6739553213119507\n",
      "Accuracy: 0.4136\n",
      "epoch: 570, training loss: 1.2623724937438965, testing loss: 1.7670645713806152\n",
      "Accuracy: 0.4590\n",
      "epoch: 570, training loss: 1.2346669435501099, testing loss: 1.769349455833435\n",
      "Accuracy: 0.3774\n",
      "epoch: 570, training loss: 1.1838897466659546, testing loss: 1.6439532041549683\n",
      "Accuracy: 0.4060\n",
      "epoch: 570, training loss: 1.2063404321670532, testing loss: 1.6918458938598633\n",
      "Accuracy: 0.4154\n",
      "epoch: 570, training loss: 1.2077349424362183, testing loss: 1.640539288520813\n",
      "Accuracy: 0.3917\n",
      "epoch: 570, training loss: 1.2225160598754883, testing loss: 1.6325761079788208\n",
      "Accuracy: 0.4169\n",
      "epoch: 570, training loss: 1.3163955211639404, testing loss: 1.7618281841278076\n",
      "Accuracy: 0.4153\n",
      "epoch: 570, training loss: 1.27881920337677, testing loss: 1.8616048097610474\n",
      "Accuracy: 0.4038\n",
      "epoch: 570, training loss: 1.330405831336975, testing loss: 1.7345068454742432\n",
      "Accuracy: 0.4156\n",
      "epoch: 570, training loss: 1.2127081155776978, testing loss: 1.7758018970489502\n",
      "Accuracy: 0.3691\n",
      "epoch: 570, training loss: 1.238277554512024, testing loss: 1.7383712530136108\n",
      "Accuracy: 0.4099\n",
      "epoch: 570, training loss: 1.2736940383911133, testing loss: 1.7455894947052002\n",
      "Accuracy: 0.3648\n",
      "epoch: 570, training loss: 1.2813700437545776, testing loss: 1.6992706060409546\n",
      "Accuracy: 0.3885\n",
      "epoch: 570, training loss: 1.2989834547042847, testing loss: 1.7736458778381348\n",
      "Accuracy: 0.4262\n",
      "epoch: 570, training loss: 1.1931777000427246, testing loss: 1.8361228704452515\n",
      "Accuracy: 0.3853\n",
      "epoch: 570, training loss: 1.2342405319213867, testing loss: 1.6041356325149536\n",
      "Accuracy: 0.4735\n",
      "epoch: 570, training loss: 1.2857637405395508, testing loss: 1.8812923431396484\n",
      "Accuracy: 0.3648\n",
      "epoch: 570, training loss: 1.226554274559021, testing loss: 1.700812578201294\n",
      "Accuracy: 0.4465\n",
      "epoch: 570, training loss: 1.264697790145874, testing loss: 1.7425708770751953\n",
      "Accuracy: 0.4247\n",
      "epoch: 570, training loss: 1.2764039039611816, testing loss: 1.6963300704956055\n",
      "Accuracy: 0.4366\n",
      "epoch: 570, training loss: 1.2162714004516602, testing loss: 1.7323904037475586\n",
      "Accuracy: 0.3874\n",
      "epoch: 570, training loss: 1.2045748233795166, testing loss: 1.8047157526016235\n",
      "Accuracy: 0.4455\n",
      "epoch: 570, training loss: 1.2809549570083618, testing loss: 1.7238202095031738\n",
      "Accuracy: 0.4268\n",
      "epoch: 570, training loss: 1.2518054246902466, testing loss: 1.7539288997650146\n",
      "Accuracy: 0.3905\n",
      "epoch: 570, training loss: 1.2666341066360474, testing loss: 1.7220795154571533\n",
      "Accuracy: 0.4286\n",
      "epoch: 570, training loss: 1.274706482887268, testing loss: 1.6641255617141724\n",
      "Accuracy: 0.4317\n",
      "epoch: 570, training loss: 1.2253262996673584, testing loss: 1.6069872379302979\n",
      "Accuracy: 0.4365\n",
      "epoch: 570, training loss: 1.1990214586257935, testing loss: 1.7288024425506592\n",
      "Accuracy: 0.4130\n",
      "epoch: 570, training loss: 1.206809163093567, testing loss: 1.600187063217163\n",
      "Accuracy: 0.4638\n",
      "epoch: 570, training loss: 1.2019537687301636, testing loss: 1.6148207187652588\n",
      "Accuracy: 0.4734\n",
      "epoch: 570, training loss: 1.2737751007080078, testing loss: 1.8408242464065552\n",
      "Accuracy: 0.4347\n",
      "epoch: 570, training loss: 1.3040189743041992, testing loss: 1.6610872745513916\n",
      "Accuracy: 0.4437\n",
      "epoch: 570, training loss: 1.2628527879714966, testing loss: 1.5871187448501587\n",
      "Accuracy: 0.4331\n",
      "epoch: 570, training loss: 1.2328855991363525, testing loss: 1.6947791576385498\n",
      "Accuracy: 0.4078\n",
      "epoch: 570, training loss: 1.2758408784866333, testing loss: 1.7165557146072388\n",
      "Accuracy: 0.3769\n",
      "epoch: 570, training loss: 1.1716763973236084, testing loss: 1.6577225923538208\n",
      "Accuracy: 0.4426\n",
      "epoch: 570, training loss: 1.2690777778625488, testing loss: 1.713965654373169\n",
      "Accuracy: 0.4102\n",
      "epoch: 570, training loss: 1.2034059762954712, testing loss: 1.6000962257385254\n",
      "Accuracy: 0.4671\n",
      "epoch: 570, training loss: 1.248788833618164, testing loss: 1.7253843545913696\n",
      "Accuracy: 0.3917\n",
      "epoch: 570, training loss: 1.2171366214752197, testing loss: 1.6914790868759155\n",
      "Accuracy: 0.4075\n",
      "epoch: 570, training loss: 1.2683275938034058, testing loss: 1.703778624534607\n",
      "Accuracy: 0.4255\n",
      "epoch: 570, training loss: 1.228880524635315, testing loss: 1.5142052173614502\n",
      "Accuracy: 0.4548\n",
      "epoch: 570, training loss: 1.2455967664718628, testing loss: 1.7213988304138184\n",
      "Accuracy: 0.4341\n",
      "epoch: 570, training loss: 1.230920672416687, testing loss: 1.7092084884643555\n",
      "Accuracy: 0.4353\n",
      "epoch: 570, training loss: 1.2323201894760132, testing loss: 1.6999777555465698\n",
      "Accuracy: 0.4387\n",
      "epoch: 570, training loss: 1.2026236057281494, testing loss: 1.700937032699585\n",
      "Accuracy: 0.4013\n",
      "epoch: 570, training loss: 1.2251043319702148, testing loss: 1.7115490436553955\n",
      "Accuracy: 0.4426\n",
      "epoch: 570, training loss: 1.262479543685913, testing loss: 1.737481951713562\n",
      "Accuracy: 0.4051\n",
      "epoch: 570, training loss: 1.212817907333374, testing loss: 1.8903961181640625\n",
      "Accuracy: 0.3420\n",
      "epoch: 570, training loss: 1.2105742692947388, testing loss: 1.7645732164382935\n",
      "Accuracy: 0.4207\n",
      "epoch: 570, training loss: 1.2555444240570068, testing loss: 1.6712117195129395\n",
      "Accuracy: 0.3940\n",
      "epoch: 580, training loss: 1.263662576675415, testing loss: 1.6931793689727783\n",
      "Accuracy: 0.3856\n",
      "epoch: 580, training loss: 1.21702241897583, testing loss: 1.7556126117706299\n",
      "Accuracy: 0.3826\n",
      "epoch: 580, training loss: 1.2104520797729492, testing loss: 1.752812147140503\n",
      "Accuracy: 0.4214\n",
      "epoch: 580, training loss: 1.2875404357910156, testing loss: 1.6107492446899414\n",
      "Accuracy: 0.4153\n",
      "epoch: 580, training loss: 1.2572143077850342, testing loss: 1.764696717262268\n",
      "Accuracy: 0.4180\n",
      "epoch: 580, training loss: 1.3200386762619019, testing loss: 1.606512427330017\n",
      "Accuracy: 0.4599\n",
      "epoch: 580, training loss: 1.1972582340240479, testing loss: 1.8217617273330688\n",
      "Accuracy: 0.4048\n",
      "epoch: 580, training loss: 1.202573299407959, testing loss: 1.6274898052215576\n",
      "Accuracy: 0.4558\n",
      "epoch: 580, training loss: 1.231431245803833, testing loss: 1.664371371269226\n",
      "Accuracy: 0.4452\n",
      "epoch: 580, training loss: 1.295238733291626, testing loss: 1.6118310689926147\n",
      "Accuracy: 0.4787\n",
      "epoch: 580, training loss: 1.2562233209609985, testing loss: 1.6580564975738525\n",
      "Accuracy: 0.4389\n",
      "epoch: 580, training loss: 1.3269312381744385, testing loss: 1.7711488008499146\n",
      "Accuracy: 0.4137\n",
      "epoch: 580, training loss: 1.162366509437561, testing loss: 1.8070865869522095\n",
      "Accuracy: 0.4155\n",
      "epoch: 580, training loss: 1.2555643320083618, testing loss: 1.6644834280014038\n",
      "Accuracy: 0.4531\n",
      "epoch: 580, training loss: 1.2626196146011353, testing loss: 1.7577099800109863\n",
      "Accuracy: 0.4119\n",
      "epoch: 580, training loss: 1.1767722368240356, testing loss: 1.6679174900054932\n",
      "Accuracy: 0.3938\n",
      "epoch: 580, training loss: 1.249698519706726, testing loss: 1.7549426555633545\n",
      "Accuracy: 0.4063\n",
      "epoch: 580, training loss: 1.1753953695297241, testing loss: 1.8548989295959473\n",
      "Accuracy: 0.4373\n",
      "epoch: 580, training loss: 1.3344098329544067, testing loss: 1.698074460029602\n",
      "Accuracy: 0.4121\n",
      "epoch: 580, training loss: 1.2076354026794434, testing loss: 1.7389347553253174\n",
      "Accuracy: 0.3956\n",
      "epoch: 580, training loss: 1.2387363910675049, testing loss: 1.6426631212234497\n",
      "Accuracy: 0.4459\n",
      "epoch: 580, training loss: 1.2205418348312378, testing loss: 1.7279608249664307\n",
      "Accuracy: 0.4406\n",
      "epoch: 580, training loss: 1.240004062652588, testing loss: 1.5926769971847534\n",
      "Accuracy: 0.4228\n",
      "epoch: 580, training loss: 1.223532795906067, testing loss: 1.541338562965393\n",
      "Accuracy: 0.4636\n",
      "epoch: 580, training loss: 1.3075708150863647, testing loss: 1.7742797136306763\n",
      "Accuracy: 0.4065\n",
      "epoch: 580, training loss: 1.2277441024780273, testing loss: 1.7141844034194946\n",
      "Accuracy: 0.4030\n",
      "epoch: 580, training loss: 1.2631468772888184, testing loss: 1.7497166395187378\n",
      "Accuracy: 0.3961\n",
      "epoch: 580, training loss: 1.2522556781768799, testing loss: 1.8152724504470825\n",
      "Accuracy: 0.4013\n",
      "epoch: 580, training loss: 1.298842191696167, testing loss: 1.7309271097183228\n",
      "Accuracy: 0.4198\n",
      "epoch: 580, training loss: 1.1765093803405762, testing loss: 1.738135814666748\n",
      "Accuracy: 0.4098\n",
      "epoch: 580, training loss: 1.263201355934143, testing loss: 1.6332602500915527\n",
      "Accuracy: 0.3669\n",
      "epoch: 580, training loss: 1.152691125869751, testing loss: 1.6528356075286865\n",
      "Accuracy: 0.4059\n",
      "epoch: 580, training loss: 1.239516019821167, testing loss: 1.6241296529769897\n",
      "Accuracy: 0.4267\n",
      "epoch: 580, training loss: 1.2672758102416992, testing loss: 1.6948821544647217\n",
      "Accuracy: 0.4637\n",
      "epoch: 580, training loss: 1.255335807800293, testing loss: 1.758745551109314\n",
      "Accuracy: 0.3956\n",
      "epoch: 580, training loss: 1.2176563739776611, testing loss: 1.719004511833191\n",
      "Accuracy: 0.3869\n",
      "epoch: 580, training loss: 1.3055832386016846, testing loss: 1.696908950805664\n",
      "Accuracy: 0.4635\n",
      "epoch: 580, training loss: 1.2269086837768555, testing loss: 1.6838570833206177\n",
      "Accuracy: 0.4145\n",
      "epoch: 580, training loss: 1.2994410991668701, testing loss: 1.7887325286865234\n",
      "Accuracy: 0.3849\n",
      "epoch: 580, training loss: 1.3017983436584473, testing loss: 1.699999451637268\n",
      "Accuracy: 0.4487\n",
      "epoch: 580, training loss: 1.191196084022522, testing loss: 1.6949423551559448\n",
      "Accuracy: 0.4032\n",
      "epoch: 580, training loss: 1.2873789072036743, testing loss: 1.7280874252319336\n",
      "Accuracy: 0.4412\n",
      "epoch: 580, training loss: 1.2264559268951416, testing loss: 1.737781047821045\n",
      "Accuracy: 0.4528\n",
      "epoch: 580, training loss: 1.259507656097412, testing loss: 1.657436490058899\n",
      "Accuracy: 0.4119\n",
      "epoch: 580, training loss: 1.2342654466629028, testing loss: 1.7166423797607422\n",
      "Accuracy: 0.4017\n",
      "epoch: 580, training loss: 1.290503978729248, testing loss: 1.7774673700332642\n",
      "Accuracy: 0.3961\n",
      "epoch: 580, training loss: 1.2459335327148438, testing loss: 1.636300802230835\n",
      "Accuracy: 0.4286\n",
      "epoch: 580, training loss: 1.2436459064483643, testing loss: 1.6152267456054688\n",
      "Accuracy: 0.4465\n",
      "epoch: 580, training loss: 1.2586698532104492, testing loss: 1.678125023841858\n",
      "Accuracy: 0.4333\n",
      "epoch: 580, training loss: 1.1643595695495605, testing loss: 1.7849854230880737\n",
      "Accuracy: 0.3909\n",
      "epoch: 580, training loss: 1.2868133783340454, testing loss: 1.738450527191162\n",
      "Accuracy: 0.4361\n",
      "epoch: 580, training loss: 1.2629191875457764, testing loss: 1.6333906650543213\n",
      "Accuracy: 0.4277\n",
      "epoch: 580, training loss: 1.2894246578216553, testing loss: 1.6270688772201538\n",
      "Accuracy: 0.4356\n",
      "epoch: 580, training loss: 1.2378883361816406, testing loss: 1.7626134157180786\n",
      "Accuracy: 0.4144\n",
      "epoch: 580, training loss: 1.2424217462539673, testing loss: 1.651310682296753\n",
      "Accuracy: 0.4137\n",
      "epoch: 580, training loss: 1.2604280710220337, testing loss: 1.662898063659668\n",
      "Accuracy: 0.4806\n",
      "epoch: 580, training loss: 1.261431336402893, testing loss: 1.7719606161117554\n",
      "Accuracy: 0.3844\n",
      "epoch: 580, training loss: 1.3016901016235352, testing loss: 1.5058553218841553\n",
      "Accuracy: 0.4304\n",
      "epoch: 580, training loss: 1.2277169227600098, testing loss: 1.914919376373291\n",
      "Accuracy: 0.3887\n",
      "epoch: 580, training loss: 1.1854792833328247, testing loss: 1.8898438215255737\n",
      "Accuracy: 0.4078\n",
      "epoch: 580, training loss: 1.2347546815872192, testing loss: 1.6704158782958984\n",
      "Accuracy: 0.4144\n",
      "epoch: 580, training loss: 1.248615026473999, testing loss: 1.8530170917510986\n",
      "Accuracy: 0.3874\n",
      "epoch: 580, training loss: 1.227573275566101, testing loss: 1.602807879447937\n",
      "Accuracy: 0.4233\n",
      "epoch: 580, training loss: 1.303275465965271, testing loss: 1.5066235065460205\n",
      "Accuracy: 0.5190\n",
      "epoch: 580, training loss: 1.2222477197647095, testing loss: 1.6034164428710938\n",
      "Accuracy: 0.4585\n",
      "epoch: 580, training loss: 1.2381857633590698, testing loss: 1.7686738967895508\n",
      "Accuracy: 0.4031\n",
      "epoch: 580, training loss: 1.2688125371932983, testing loss: 1.8129141330718994\n",
      "Accuracy: 0.3792\n",
      "epoch: 580, training loss: 1.2479908466339111, testing loss: 1.6903966665267944\n",
      "Accuracy: 0.3946\n",
      "epoch: 580, training loss: 1.199691891670227, testing loss: 1.6712292432785034\n",
      "Accuracy: 0.4340\n",
      "epoch: 580, training loss: 1.2232710123062134, testing loss: 1.6118930578231812\n",
      "Accuracy: 0.4399\n",
      "epoch: 580, training loss: 1.3057820796966553, testing loss: 1.7439240217208862\n",
      "Accuracy: 0.3880\n",
      "epoch: 580, training loss: 1.2059235572814941, testing loss: 1.6793535947799683\n",
      "Accuracy: 0.4218\n",
      "epoch: 580, training loss: 1.2698509693145752, testing loss: 1.6018072366714478\n",
      "Accuracy: 0.4444\n",
      "epoch: 580, training loss: 1.2286288738250732, testing loss: 1.8200181722640991\n",
      "Accuracy: 0.3892\n",
      "epoch: 580, training loss: 1.2438135147094727, testing loss: 1.718973994255066\n",
      "Accuracy: 0.4263\n",
      "epoch: 580, training loss: 1.3116968870162964, testing loss: 1.8193309307098389\n",
      "Accuracy: 0.4116\n",
      "epoch: 580, training loss: 1.2488731145858765, testing loss: 1.7339808940887451\n",
      "Accuracy: 0.4190\n",
      "epoch: 580, training loss: 1.2569680213928223, testing loss: 1.6637612581253052\n",
      "Accuracy: 0.4337\n",
      "epoch: 580, training loss: 1.282562494277954, testing loss: 1.697660207748413\n",
      "Accuracy: 0.4507\n",
      "epoch: 580, training loss: 1.2275025844573975, testing loss: 1.7703473567962646\n",
      "Accuracy: 0.3942\n",
      "epoch: 580, training loss: 1.244369626045227, testing loss: 1.6915594339370728\n",
      "Accuracy: 0.4296\n",
      "epoch: 580, training loss: 1.2782094478607178, testing loss: 1.703456997871399\n",
      "Accuracy: 0.4164\n",
      "epoch: 580, training loss: 1.2025114297866821, testing loss: 1.7820500135421753\n",
      "Accuracy: 0.3896\n",
      "epoch: 580, training loss: 1.1556295156478882, testing loss: 1.7134945392608643\n",
      "Accuracy: 0.4197\n",
      "epoch: 580, training loss: 1.2059558629989624, testing loss: 1.7682766914367676\n",
      "Accuracy: 0.4066\n",
      "epoch: 580, training loss: 1.2271591424942017, testing loss: 1.772389531135559\n",
      "Accuracy: 0.4107\n",
      "epoch: 580, training loss: 1.2513065338134766, testing loss: 1.717435359954834\n",
      "Accuracy: 0.4013\n",
      "epoch: 580, training loss: 1.2629201412200928, testing loss: 1.6502766609191895\n",
      "Accuracy: 0.4138\n",
      "epoch: 580, training loss: 1.2352577447891235, testing loss: 1.7471164464950562\n",
      "Accuracy: 0.3974\n",
      "epoch: 580, training loss: 1.2509957551956177, testing loss: 1.6688225269317627\n",
      "Accuracy: 0.4389\n",
      "epoch: 580, training loss: 1.2541395425796509, testing loss: 1.704609751701355\n",
      "Accuracy: 0.4811\n",
      "epoch: 580, training loss: 1.2831873893737793, testing loss: 1.736828327178955\n",
      "Accuracy: 0.4154\n",
      "epoch: 580, training loss: 1.212774634361267, testing loss: 1.5368345975875854\n",
      "Accuracy: 0.4744\n",
      "epoch: 580, training loss: 1.3098539113998413, testing loss: 1.7778301239013672\n",
      "Accuracy: 0.4281\n",
      "epoch: 580, training loss: 1.289052963256836, testing loss: 1.7091599702835083\n",
      "Accuracy: 0.4097\n",
      "epoch: 580, training loss: 1.2681907415390015, testing loss: 1.8098520040512085\n",
      "Accuracy: 0.4114\n",
      "epoch: 580, training loss: 1.2998651266098022, testing loss: 1.609718680381775\n",
      "Accuracy: 0.4320\n",
      "epoch: 580, training loss: 1.2704143524169922, testing loss: 1.6512131690979004\n",
      "Accuracy: 0.4190\n",
      "epoch: 580, training loss: 1.203290581703186, testing loss: 1.6522955894470215\n",
      "Accuracy: 0.4500\n",
      "epoch: 580, training loss: 1.2550491094589233, testing loss: 1.7077453136444092\n",
      "Accuracy: 0.4146\n",
      "epoch: 590, training loss: 1.2617212533950806, testing loss: 1.6209913492202759\n",
      "Accuracy: 0.4464\n",
      "epoch: 590, training loss: 1.2796036005020142, testing loss: 1.7522841691970825\n",
      "Accuracy: 0.4013\n",
      "epoch: 590, training loss: 1.260306715965271, testing loss: 1.7353006601333618\n",
      "Accuracy: 0.4013\n",
      "epoch: 590, training loss: 1.2366546392440796, testing loss: 1.6939445734024048\n",
      "Accuracy: 0.3930\n",
      "epoch: 590, training loss: 1.2203973531723022, testing loss: 1.797488808631897\n",
      "Accuracy: 0.3964\n",
      "epoch: 590, training loss: 1.227597951889038, testing loss: 1.6223371028900146\n",
      "Accuracy: 0.4257\n",
      "epoch: 590, training loss: 1.251688003540039, testing loss: 1.763200283050537\n",
      "Accuracy: 0.3726\n",
      "epoch: 590, training loss: 1.2869309186935425, testing loss: 1.7165602445602417\n",
      "Accuracy: 0.4082\n",
      "epoch: 590, training loss: 1.279563546180725, testing loss: 1.6366435289382935\n",
      "Accuracy: 0.4537\n",
      "epoch: 590, training loss: 1.2491178512573242, testing loss: 1.6671971082687378\n",
      "Accuracy: 0.4323\n",
      "epoch: 590, training loss: 1.2668825387954712, testing loss: 1.7528935670852661\n",
      "Accuracy: 0.4153\n",
      "epoch: 590, training loss: 1.1920827627182007, testing loss: 1.7964375019073486\n",
      "Accuracy: 0.4314\n",
      "epoch: 590, training loss: 1.2449798583984375, testing loss: 1.7429157495498657\n",
      "Accuracy: 0.3821\n",
      "epoch: 590, training loss: 1.2618955373764038, testing loss: 1.7275278568267822\n",
      "Accuracy: 0.4313\n",
      "epoch: 590, training loss: 1.2254875898361206, testing loss: 1.6713331937789917\n",
      "Accuracy: 0.4884\n",
      "epoch: 590, training loss: 1.2472575902938843, testing loss: 1.9388927221298218\n",
      "Accuracy: 0.4202\n",
      "epoch: 590, training loss: 1.2487940788269043, testing loss: 1.706162452697754\n",
      "Accuracy: 0.3937\n",
      "epoch: 590, training loss: 1.2516591548919678, testing loss: 1.6988540887832642\n",
      "Accuracy: 0.4108\n",
      "epoch: 590, training loss: 1.2403241395950317, testing loss: 1.73698091506958\n",
      "Accuracy: 0.4102\n",
      "epoch: 590, training loss: 1.1892386674880981, testing loss: 1.7562391757965088\n",
      "Accuracy: 0.4539\n",
      "epoch: 590, training loss: 1.194286584854126, testing loss: 1.687609076499939\n",
      "Accuracy: 0.4338\n",
      "epoch: 590, training loss: 1.3011277914047241, testing loss: 1.7816076278686523\n",
      "Accuracy: 0.4046\n",
      "epoch: 590, training loss: 1.2076793909072876, testing loss: 1.8258541822433472\n",
      "Accuracy: 0.4066\n",
      "epoch: 590, training loss: 1.273272156715393, testing loss: 1.7487096786499023\n",
      "Accuracy: 0.3604\n",
      "epoch: 590, training loss: 1.20089852809906, testing loss: 1.8541438579559326\n",
      "Accuracy: 0.3994\n",
      "epoch: 590, training loss: 1.1868853569030762, testing loss: 1.614667296409607\n",
      "Accuracy: 0.4049\n",
      "epoch: 590, training loss: 1.226435661315918, testing loss: 1.7389931678771973\n",
      "Accuracy: 0.4691\n",
      "epoch: 590, training loss: 1.2306673526763916, testing loss: 1.6267250776290894\n",
      "Accuracy: 0.3968\n",
      "epoch: 590, training loss: 1.2783927917480469, testing loss: 1.794057846069336\n",
      "Accuracy: 0.4152\n",
      "epoch: 590, training loss: 1.2989345788955688, testing loss: 1.683550238609314\n",
      "Accuracy: 0.4299\n",
      "epoch: 590, training loss: 1.3013989925384521, testing loss: 1.6338387727737427\n",
      "Accuracy: 0.4421\n",
      "epoch: 590, training loss: 1.2397408485412598, testing loss: 1.7240568399429321\n",
      "Accuracy: 0.4488\n",
      "epoch: 590, training loss: 1.2610173225402832, testing loss: 1.7399107217788696\n",
      "Accuracy: 0.4120\n",
      "epoch: 590, training loss: 1.2750262022018433, testing loss: 1.708932638168335\n",
      "Accuracy: 0.4367\n",
      "epoch: 590, training loss: 1.2028833627700806, testing loss: 1.7377920150756836\n",
      "Accuracy: 0.4503\n",
      "epoch: 590, training loss: 1.1824774742126465, testing loss: 1.5523842573165894\n",
      "Accuracy: 0.4740\n",
      "epoch: 590, training loss: 1.1820472478866577, testing loss: 1.647528052330017\n",
      "Accuracy: 0.4300\n",
      "epoch: 590, training loss: 1.3041571378707886, testing loss: 1.7191511392593384\n",
      "Accuracy: 0.4371\n",
      "epoch: 590, training loss: 1.2437026500701904, testing loss: 1.642502784729004\n",
      "Accuracy: 0.4508\n",
      "epoch: 590, training loss: 1.2316184043884277, testing loss: 1.7474355697631836\n",
      "Accuracy: 0.3783\n",
      "epoch: 590, training loss: 1.2358306646347046, testing loss: 1.7043451070785522\n",
      "Accuracy: 0.3737\n",
      "epoch: 590, training loss: 1.3481591939926147, testing loss: 1.7124042510986328\n",
      "Accuracy: 0.3676\n",
      "epoch: 590, training loss: 1.2701637744903564, testing loss: 1.7455953359603882\n",
      "Accuracy: 0.4221\n",
      "epoch: 590, training loss: 1.2549467086791992, testing loss: 1.7257159948349\n",
      "Accuracy: 0.4415\n",
      "epoch: 590, training loss: 1.2644751071929932, testing loss: 1.8004798889160156\n",
      "Accuracy: 0.4007\n",
      "epoch: 590, training loss: 1.3014817237854004, testing loss: 1.676719069480896\n",
      "Accuracy: 0.4399\n",
      "epoch: 590, training loss: 1.3062244653701782, testing loss: 1.7829952239990234\n",
      "Accuracy: 0.3720\n",
      "epoch: 590, training loss: 1.2546606063842773, testing loss: 1.6533082723617554\n",
      "Accuracy: 0.4630\n",
      "epoch: 590, training loss: 1.263541340827942, testing loss: 1.7800166606903076\n",
      "Accuracy: 0.4131\n",
      "epoch: 590, training loss: 1.2102283239364624, testing loss: 1.750645399093628\n",
      "Accuracy: 0.4110\n",
      "epoch: 590, training loss: 1.2724084854125977, testing loss: 1.7832351922988892\n",
      "Accuracy: 0.4262\n",
      "epoch: 590, training loss: 1.2935833930969238, testing loss: 1.7761162519454956\n",
      "Accuracy: 0.3763\n",
      "epoch: 590, training loss: 1.1951491832733154, testing loss: 1.7397528886795044\n",
      "Accuracy: 0.4572\n",
      "epoch: 590, training loss: 1.224629521369934, testing loss: 1.6769781112670898\n",
      "Accuracy: 0.4365\n",
      "epoch: 590, training loss: 1.323480486869812, testing loss: 1.7307463884353638\n",
      "Accuracy: 0.4423\n",
      "epoch: 590, training loss: 1.286120891571045, testing loss: 1.5279629230499268\n",
      "Accuracy: 0.4438\n",
      "epoch: 590, training loss: 1.178261160850525, testing loss: 1.675179123878479\n",
      "Accuracy: 0.4247\n",
      "epoch: 590, training loss: 1.2272647619247437, testing loss: 1.6564857959747314\n",
      "Accuracy: 0.4660\n",
      "epoch: 590, training loss: 1.1983931064605713, testing loss: 1.68241286277771\n",
      "Accuracy: 0.4400\n",
      "epoch: 590, training loss: 1.3460538387298584, testing loss: 1.6945222616195679\n",
      "Accuracy: 0.4022\n",
      "epoch: 590, training loss: 1.2257516384124756, testing loss: 1.603554368019104\n",
      "Accuracy: 0.4448\n",
      "epoch: 590, training loss: 1.2384917736053467, testing loss: 1.6083499193191528\n",
      "Accuracy: 0.4098\n",
      "epoch: 590, training loss: 1.1873403787612915, testing loss: 1.6662200689315796\n",
      "Accuracy: 0.4560\n",
      "epoch: 590, training loss: 1.3009190559387207, testing loss: 1.7692358493804932\n",
      "Accuracy: 0.4369\n",
      "epoch: 590, training loss: 1.2587965726852417, testing loss: 1.7754826545715332\n",
      "Accuracy: 0.4134\n",
      "epoch: 590, training loss: 1.2071231603622437, testing loss: 1.6720972061157227\n",
      "Accuracy: 0.3907\n",
      "epoch: 590, training loss: 1.3125702142715454, testing loss: 1.5613046884536743\n",
      "Accuracy: 0.4539\n",
      "epoch: 590, training loss: 1.237882375717163, testing loss: 1.6932004690170288\n",
      "Accuracy: 0.3814\n",
      "epoch: 590, training loss: 1.26951265335083, testing loss: 1.7717543840408325\n",
      "Accuracy: 0.3948\n",
      "epoch: 590, training loss: 1.2931534051895142, testing loss: 1.6887085437774658\n",
      "Accuracy: 0.4231\n",
      "epoch: 590, training loss: 1.2817803621292114, testing loss: 1.6205744743347168\n",
      "Accuracy: 0.4560\n",
      "epoch: 590, training loss: 1.2149333953857422, testing loss: 1.5692881345748901\n",
      "Accuracy: 0.4644\n",
      "epoch: 590, training loss: 1.2658398151397705, testing loss: 1.6180304288864136\n",
      "Accuracy: 0.4407\n",
      "epoch: 590, training loss: 1.329257845878601, testing loss: 1.6425971984863281\n",
      "Accuracy: 0.4148\n",
      "epoch: 590, training loss: 1.251393437385559, testing loss: 1.6468966007232666\n",
      "Accuracy: 0.4214\n",
      "epoch: 590, training loss: 1.2232486009597778, testing loss: 1.7986114025115967\n",
      "Accuracy: 0.4401\n",
      "epoch: 590, training loss: 1.2336617708206177, testing loss: 1.8120805025100708\n",
      "Accuracy: 0.4248\n",
      "epoch: 590, training loss: 1.198696255683899, testing loss: 1.7801731824874878\n",
      "Accuracy: 0.3956\n",
      "epoch: 590, training loss: 1.2676188945770264, testing loss: 1.741544246673584\n",
      "Accuracy: 0.4058\n",
      "epoch: 590, training loss: 1.2498177289962769, testing loss: 1.676883578300476\n",
      "Accuracy: 0.4114\n",
      "epoch: 590, training loss: 1.27386474609375, testing loss: 1.7257418632507324\n",
      "Accuracy: 0.4146\n",
      "epoch: 590, training loss: 1.2063829898834229, testing loss: 1.6691278219223022\n",
      "Accuracy: 0.4316\n",
      "epoch: 590, training loss: 1.2282408475875854, testing loss: 1.7440879344940186\n",
      "Accuracy: 0.4032\n",
      "epoch: 590, training loss: 1.2416340112686157, testing loss: 1.6507500410079956\n",
      "Accuracy: 0.4175\n",
      "epoch: 590, training loss: 1.2267005443572998, testing loss: 1.7334136962890625\n",
      "Accuracy: 0.4326\n",
      "epoch: 590, training loss: 1.2280058860778809, testing loss: 1.6506143808364868\n",
      "Accuracy: 0.4539\n",
      "epoch: 590, training loss: 1.2153960466384888, testing loss: 1.7547739744186401\n",
      "Accuracy: 0.4324\n",
      "epoch: 590, training loss: 1.25612473487854, testing loss: 1.6747249364852905\n",
      "Accuracy: 0.3591\n",
      "epoch: 590, training loss: 1.1831517219543457, testing loss: 1.74257230758667\n",
      "Accuracy: 0.4272\n",
      "epoch: 590, training loss: 1.2995805740356445, testing loss: 1.8728559017181396\n",
      "Accuracy: 0.3392\n",
      "epoch: 590, training loss: 1.2306386232376099, testing loss: 1.8502427339553833\n",
      "Accuracy: 0.3900\n",
      "epoch: 590, training loss: 1.1675634384155273, testing loss: 1.8259609937667847\n",
      "Accuracy: 0.3705\n",
      "epoch: 590, training loss: 1.1772905588150024, testing loss: 1.732506513595581\n",
      "Accuracy: 0.3792\n",
      "epoch: 590, training loss: 1.2482526302337646, testing loss: 1.6246904134750366\n",
      "Accuracy: 0.4397\n",
      "epoch: 590, training loss: 1.214226245880127, testing loss: 1.8550591468811035\n",
      "Accuracy: 0.3975\n",
      "epoch: 590, training loss: 1.2648977041244507, testing loss: 1.7210516929626465\n",
      "Accuracy: 0.4006\n",
      "epoch: 590, training loss: 1.291509747505188, testing loss: 1.6106210947036743\n",
      "Accuracy: 0.4224\n",
      "epoch: 590, training loss: 1.194806694984436, testing loss: 1.580952525138855\n",
      "Accuracy: 0.4048\n",
      "epoch: 590, training loss: 1.2049012184143066, testing loss: 1.8449299335479736\n",
      "Accuracy: 0.3849\n",
      "epoch: 590, training loss: 1.2325118780136108, testing loss: 1.7758986949920654\n",
      "Accuracy: 0.4480\n",
      "epoch: 600, training loss: 1.222308874130249, testing loss: 1.696372628211975\n",
      "Accuracy: 0.3790\n",
      "epoch: 600, training loss: 1.2434743642807007, testing loss: 1.6341352462768555\n",
      "Accuracy: 0.4090\n",
      "epoch: 600, training loss: 1.2189602851867676, testing loss: 1.7088687419891357\n",
      "Accuracy: 0.4019\n",
      "epoch: 600, training loss: 1.2756603956222534, testing loss: 1.7727259397506714\n",
      "Accuracy: 0.4169\n",
      "epoch: 600, training loss: 1.24891996383667, testing loss: 1.8190603256225586\n",
      "Accuracy: 0.3620\n",
      "epoch: 600, training loss: 1.3053257465362549, testing loss: 1.5879284143447876\n",
      "Accuracy: 0.4654\n",
      "epoch: 600, training loss: 1.260472059249878, testing loss: 1.6593313217163086\n",
      "Accuracy: 0.4332\n",
      "epoch: 600, training loss: 1.2270721197128296, testing loss: 1.6193197965621948\n",
      "Accuracy: 0.4613\n",
      "epoch: 600, training loss: 1.227151870727539, testing loss: 1.5714818239212036\n",
      "Accuracy: 0.4476\n",
      "epoch: 600, training loss: 1.2774512767791748, testing loss: 1.8187713623046875\n",
      "Accuracy: 0.4078\n",
      "epoch: 600, training loss: 1.2053956985473633, testing loss: 1.620050311088562\n",
      "Accuracy: 0.4855\n",
      "epoch: 600, training loss: 1.2160643339157104, testing loss: 1.667446255683899\n",
      "Accuracy: 0.4608\n",
      "epoch: 600, training loss: 1.2878766059875488, testing loss: 1.6176971197128296\n",
      "Accuracy: 0.4730\n",
      "epoch: 600, training loss: 1.2222540378570557, testing loss: 1.8539083003997803\n",
      "Accuracy: 0.4239\n",
      "epoch: 600, training loss: 1.2454862594604492, testing loss: 1.6979600191116333\n",
      "Accuracy: 0.3962\n",
      "epoch: 600, training loss: 1.2426807880401611, testing loss: 1.6733204126358032\n",
      "Accuracy: 0.4209\n",
      "epoch: 600, training loss: 1.3027485609054565, testing loss: 1.7156455516815186\n",
      "Accuracy: 0.4277\n",
      "epoch: 600, training loss: 1.240655541419983, testing loss: 1.758862853050232\n",
      "Accuracy: 0.3865\n",
      "epoch: 600, training loss: 1.2245328426361084, testing loss: 1.668127179145813\n",
      "Accuracy: 0.4455\n",
      "epoch: 600, training loss: 1.2005594968795776, testing loss: 1.87277352809906\n",
      "Accuracy: 0.3619\n",
      "epoch: 600, training loss: 1.316164493560791, testing loss: 1.7569636106491089\n",
      "Accuracy: 0.4190\n",
      "epoch: 600, training loss: 1.2098394632339478, testing loss: 1.6932259798049927\n",
      "Accuracy: 0.4482\n",
      "epoch: 600, training loss: 1.2659496068954468, testing loss: 1.7288941144943237\n",
      "Accuracy: 0.4214\n",
      "epoch: 600, training loss: 1.2095434665679932, testing loss: 1.7175730466842651\n",
      "Accuracy: 0.4112\n",
      "epoch: 600, training loss: 1.3416403532028198, testing loss: 1.5953521728515625\n",
      "Accuracy: 0.4633\n",
      "epoch: 600, training loss: 1.2396092414855957, testing loss: 1.8536654710769653\n",
      "Accuracy: 0.4202\n",
      "epoch: 600, training loss: 1.177777647972107, testing loss: 1.7846812009811401\n",
      "Accuracy: 0.4048\n",
      "epoch: 600, training loss: 1.1558904647827148, testing loss: 1.692733645439148\n",
      "Accuracy: 0.4196\n",
      "epoch: 600, training loss: 1.2664192914962769, testing loss: 1.7321327924728394\n",
      "Accuracy: 0.4448\n",
      "epoch: 600, training loss: 1.155333161354065, testing loss: 1.68978750705719\n",
      "Accuracy: 0.4494\n",
      "epoch: 600, training loss: 1.2844269275665283, testing loss: 1.7217389345169067\n",
      "Accuracy: 0.4418\n",
      "epoch: 600, training loss: 1.2089823484420776, testing loss: 1.795902132987976\n",
      "Accuracy: 0.4074\n",
      "epoch: 600, training loss: 1.19481360912323, testing loss: 1.7324206829071045\n",
      "Accuracy: 0.4027\n",
      "epoch: 600, training loss: 1.2471283674240112, testing loss: 1.7376039028167725\n",
      "Accuracy: 0.4141\n",
      "epoch: 600, training loss: 1.274704098701477, testing loss: 1.6297240257263184\n",
      "Accuracy: 0.3856\n",
      "epoch: 600, training loss: 1.2267194986343384, testing loss: 1.602117896080017\n",
      "Accuracy: 0.4701\n",
      "epoch: 600, training loss: 1.2247809171676636, testing loss: 1.6091898679733276\n",
      "Accuracy: 0.4400\n",
      "epoch: 600, training loss: 1.3085508346557617, testing loss: 1.688669204711914\n",
      "Accuracy: 0.4375\n",
      "epoch: 600, training loss: 1.2784167528152466, testing loss: 1.6253111362457275\n",
      "Accuracy: 0.4427\n",
      "epoch: 600, training loss: 1.2677292823791504, testing loss: 1.8085135221481323\n",
      "Accuracy: 0.4059\n",
      "epoch: 600, training loss: 1.2196896076202393, testing loss: 1.7304928302764893\n",
      "Accuracy: 0.4101\n",
      "epoch: 600, training loss: 1.2421762943267822, testing loss: 1.8095637559890747\n",
      "Accuracy: 0.4238\n",
      "epoch: 600, training loss: 1.2397425174713135, testing loss: 1.685226559638977\n",
      "Accuracy: 0.4235\n",
      "epoch: 600, training loss: 1.246492862701416, testing loss: 1.591124415397644\n",
      "Accuracy: 0.4305\n",
      "epoch: 600, training loss: 1.2699100971221924, testing loss: 1.650247573852539\n",
      "Accuracy: 0.4515\n",
      "epoch: 600, training loss: 1.2640987634658813, testing loss: 1.6383544206619263\n",
      "Accuracy: 0.4517\n",
      "epoch: 600, training loss: 1.246546745300293, testing loss: 1.793823480606079\n",
      "Accuracy: 0.3811\n",
      "epoch: 600, training loss: 1.3162308931350708, testing loss: 1.762904405593872\n",
      "Accuracy: 0.3722\n",
      "epoch: 600, training loss: 1.2113319635391235, testing loss: 1.765722393989563\n",
      "Accuracy: 0.3587\n",
      "epoch: 600, training loss: 1.2490651607513428, testing loss: 1.7351694107055664\n",
      "Accuracy: 0.3754\n",
      "epoch: 600, training loss: 1.1978881359100342, testing loss: 1.743591070175171\n",
      "Accuracy: 0.4399\n",
      "epoch: 600, training loss: 1.2368309497833252, testing loss: 1.6569840908050537\n",
      "Accuracy: 0.4112\n",
      "epoch: 600, training loss: 1.2569398880004883, testing loss: 1.7409530878067017\n",
      "Accuracy: 0.4177\n",
      "epoch: 600, training loss: 1.2888468503952026, testing loss: 1.7130300998687744\n",
      "Accuracy: 0.3979\n",
      "epoch: 600, training loss: 1.1852352619171143, testing loss: 1.660080909729004\n",
      "Accuracy: 0.4244\n",
      "epoch: 600, training loss: 1.21393883228302, testing loss: 1.6331321001052856\n",
      "Accuracy: 0.4164\n",
      "epoch: 600, training loss: 1.1910547018051147, testing loss: 1.6511110067367554\n",
      "Accuracy: 0.4232\n",
      "epoch: 600, training loss: 1.177438497543335, testing loss: 1.6512823104858398\n",
      "Accuracy: 0.4169\n",
      "epoch: 600, training loss: 1.185634732246399, testing loss: 1.7195590734481812\n",
      "Accuracy: 0.4174\n",
      "epoch: 600, training loss: 1.2664395570755005, testing loss: 1.6453921794891357\n",
      "Accuracy: 0.4216\n",
      "epoch: 600, training loss: 1.252403974533081, testing loss: 1.659226894378662\n",
      "Accuracy: 0.4536\n",
      "epoch: 600, training loss: 1.2681894302368164, testing loss: 1.7143893241882324\n",
      "Accuracy: 0.4411\n",
      "epoch: 600, training loss: 1.211746335029602, testing loss: 1.6848970651626587\n",
      "Accuracy: 0.4017\n",
      "epoch: 600, training loss: 1.2977511882781982, testing loss: 1.5888113975524902\n",
      "Accuracy: 0.4465\n",
      "epoch: 600, training loss: 1.341944694519043, testing loss: 1.688746690750122\n",
      "Accuracy: 0.4074\n",
      "epoch: 600, training loss: 1.2136826515197754, testing loss: 1.6267575025558472\n",
      "Accuracy: 0.4308\n",
      "epoch: 600, training loss: 1.2060564756393433, testing loss: 1.545589566230774\n",
      "Accuracy: 0.4194\n",
      "epoch: 600, training loss: 1.2460123300552368, testing loss: 1.7603471279144287\n",
      "Accuracy: 0.3943\n",
      "epoch: 600, training loss: 1.284730315208435, testing loss: 1.567811131477356\n",
      "Accuracy: 0.4452\n",
      "epoch: 600, training loss: 1.2467516660690308, testing loss: 1.7607852220535278\n",
      "Accuracy: 0.3729\n",
      "epoch: 600, training loss: 1.2359060049057007, testing loss: 1.7073315382003784\n",
      "Accuracy: 0.4295\n",
      "epoch: 600, training loss: 1.2048635482788086, testing loss: 1.7784748077392578\n",
      "Accuracy: 0.4180\n",
      "epoch: 600, training loss: 1.273575782775879, testing loss: 1.6893298625946045\n",
      "Accuracy: 0.4127\n",
      "epoch: 600, training loss: 1.191145420074463, testing loss: 1.8646557331085205\n",
      "Accuracy: 0.3710\n",
      "epoch: 600, training loss: 1.336237907409668, testing loss: 1.696042537689209\n",
      "Accuracy: 0.4167\n",
      "epoch: 600, training loss: 1.3278028964996338, testing loss: 1.7903600931167603\n",
      "Accuracy: 0.3951\n",
      "epoch: 600, training loss: 1.2497828006744385, testing loss: 1.583398461341858\n",
      "Accuracy: 0.4559\n",
      "epoch: 600, training loss: 1.2500386238098145, testing loss: 1.7381545305252075\n",
      "Accuracy: 0.4335\n",
      "epoch: 600, training loss: 1.225359320640564, testing loss: 1.650496244430542\n",
      "Accuracy: 0.4180\n",
      "epoch: 600, training loss: 1.324094533920288, testing loss: 1.678670883178711\n",
      "Accuracy: 0.3981\n",
      "epoch: 600, training loss: 1.2464406490325928, testing loss: 1.721867561340332\n",
      "Accuracy: 0.3836\n",
      "epoch: 600, training loss: 1.2028651237487793, testing loss: 1.691660761833191\n",
      "Accuracy: 0.4345\n",
      "epoch: 600, training loss: 1.2200894355773926, testing loss: 1.7978876829147339\n",
      "Accuracy: 0.3722\n",
      "epoch: 600, training loss: 1.241964340209961, testing loss: 1.8030771017074585\n",
      "Accuracy: 0.4225\n",
      "epoch: 600, training loss: 1.2628302574157715, testing loss: 1.7010412216186523\n",
      "Accuracy: 0.3904\n",
      "epoch: 600, training loss: 1.2820005416870117, testing loss: 1.7375481128692627\n",
      "Accuracy: 0.4534\n",
      "epoch: 600, training loss: 1.240546703338623, testing loss: 1.5477159023284912\n",
      "Accuracy: 0.4675\n",
      "epoch: 600, training loss: 1.2766156196594238, testing loss: 1.682439923286438\n",
      "Accuracy: 0.4712\n",
      "epoch: 600, training loss: 1.2043555974960327, testing loss: 1.657058835029602\n",
      "Accuracy: 0.4209\n",
      "epoch: 600, training loss: 1.2655400037765503, testing loss: 1.839295506477356\n",
      "Accuracy: 0.4327\n",
      "epoch: 600, training loss: 1.2747242450714111, testing loss: 1.7667055130004883\n",
      "Accuracy: 0.3898\n",
      "epoch: 600, training loss: 1.303800344467163, testing loss: 1.651427149772644\n",
      "Accuracy: 0.4361\n",
      "epoch: 600, training loss: 1.2423655986785889, testing loss: 1.7294374704360962\n",
      "Accuracy: 0.4175\n",
      "epoch: 600, training loss: 1.246782660484314, testing loss: 1.6641417741775513\n",
      "Accuracy: 0.3618\n",
      "epoch: 600, training loss: 1.2103840112686157, testing loss: 1.7126078605651855\n",
      "Accuracy: 0.3981\n",
      "epoch: 600, training loss: 1.1999671459197998, testing loss: 1.688382863998413\n",
      "Accuracy: 0.4455\n",
      "epoch: 600, training loss: 1.2278591394424438, testing loss: 1.5936622619628906\n",
      "Accuracy: 0.4660\n",
      "epoch: 600, training loss: 1.2539840936660767, testing loss: 1.709743857383728\n",
      "Accuracy: 0.3735\n",
      "epoch: 600, training loss: 1.2213664054870605, testing loss: 1.7476893663406372\n",
      "Accuracy: 0.3860\n",
      "epoch: 600, training loss: 1.2356219291687012, testing loss: 1.7177174091339111\n",
      "Accuracy: 0.4261\n",
      "epoch: 610, training loss: 1.2917933464050293, testing loss: 1.874504804611206\n",
      "Accuracy: 0.4018\n",
      "epoch: 610, training loss: 1.2941854000091553, testing loss: 1.6128721237182617\n",
      "Accuracy: 0.4241\n",
      "epoch: 610, training loss: 1.2397992610931396, testing loss: 1.7746717929840088\n",
      "Accuracy: 0.4714\n",
      "epoch: 610, training loss: 1.255075454711914, testing loss: 1.7266850471496582\n",
      "Accuracy: 0.3478\n",
      "epoch: 610, training loss: 1.2669970989227295, testing loss: 1.6487257480621338\n",
      "Accuracy: 0.4277\n",
      "epoch: 610, training loss: 1.2681812047958374, testing loss: 1.7318387031555176\n",
      "Accuracy: 0.4392\n",
      "epoch: 610, training loss: 1.2329628467559814, testing loss: 1.7220991849899292\n",
      "Accuracy: 0.4304\n",
      "epoch: 610, training loss: 1.2052271366119385, testing loss: 1.8372188806533813\n",
      "Accuracy: 0.3622\n",
      "epoch: 610, training loss: 1.2084214687347412, testing loss: 1.5957396030426025\n",
      "Accuracy: 0.3987\n",
      "epoch: 610, training loss: 1.2892215251922607, testing loss: 1.7494914531707764\n",
      "Accuracy: 0.4120\n",
      "epoch: 610, training loss: 1.2228413820266724, testing loss: 1.7961058616638184\n",
      "Accuracy: 0.3922\n",
      "epoch: 610, training loss: 1.2291046380996704, testing loss: 1.7082501649856567\n",
      "Accuracy: 0.4175\n",
      "epoch: 610, training loss: 1.253806471824646, testing loss: 1.801137089729309\n",
      "Accuracy: 0.4320\n",
      "epoch: 610, training loss: 1.213517189025879, testing loss: 1.7639256715774536\n",
      "Accuracy: 0.4095\n",
      "epoch: 610, training loss: 1.3415194749832153, testing loss: 1.7095259428024292\n",
      "Accuracy: 0.4393\n",
      "epoch: 610, training loss: 1.2487788200378418, testing loss: 1.7017896175384521\n",
      "Accuracy: 0.4307\n",
      "epoch: 610, training loss: 1.3592429161071777, testing loss: 1.737210750579834\n",
      "Accuracy: 0.4164\n",
      "epoch: 610, training loss: 1.3010432720184326, testing loss: 1.8616634607315063\n",
      "Accuracy: 0.4433\n",
      "epoch: 610, training loss: 1.2721425294876099, testing loss: 1.8773075342178345\n",
      "Accuracy: 0.3397\n",
      "epoch: 610, training loss: 1.295467495918274, testing loss: 1.5006698369979858\n",
      "Accuracy: 0.4903\n",
      "epoch: 610, training loss: 1.2305585145950317, testing loss: 1.6935356855392456\n",
      "Accuracy: 0.4067\n",
      "epoch: 610, training loss: 1.3253521919250488, testing loss: 1.6564453840255737\n",
      "Accuracy: 0.4175\n",
      "epoch: 610, training loss: 1.215238094329834, testing loss: 1.7134509086608887\n",
      "Accuracy: 0.4237\n",
      "epoch: 610, training loss: 1.28826904296875, testing loss: 1.6281870603561401\n",
      "Accuracy: 0.4426\n",
      "epoch: 610, training loss: 1.1798977851867676, testing loss: 1.6748908758163452\n",
      "Accuracy: 0.4085\n",
      "epoch: 610, training loss: 1.2794795036315918, testing loss: 1.6178823709487915\n",
      "Accuracy: 0.3969\n",
      "epoch: 610, training loss: 1.2712551355361938, testing loss: 1.7181951999664307\n",
      "Accuracy: 0.3962\n",
      "epoch: 610, training loss: 1.263477087020874, testing loss: 1.6849676370620728\n",
      "Accuracy: 0.4091\n",
      "epoch: 610, training loss: 1.211471676826477, testing loss: 1.677931308746338\n",
      "Accuracy: 0.3882\n",
      "epoch: 610, training loss: 1.2167894840240479, testing loss: 1.732774257659912\n",
      "Accuracy: 0.4367\n",
      "epoch: 610, training loss: 1.2715455293655396, testing loss: 1.5069901943206787\n",
      "Accuracy: 0.4816\n",
      "epoch: 610, training loss: 1.2287945747375488, testing loss: 1.719993233680725\n",
      "Accuracy: 0.4072\n",
      "epoch: 610, training loss: 1.156141757965088, testing loss: 1.7337751388549805\n",
      "Accuracy: 0.4092\n",
      "epoch: 610, training loss: 1.2499580383300781, testing loss: 1.7592723369598389\n",
      "Accuracy: 0.4031\n",
      "epoch: 610, training loss: 1.188425898551941, testing loss: 1.591316819190979\n",
      "Accuracy: 0.4318\n",
      "epoch: 610, training loss: 1.1900423765182495, testing loss: 1.6508485078811646\n",
      "Accuracy: 0.4381\n",
      "epoch: 610, training loss: 1.3635557889938354, testing loss: 1.7470810413360596\n",
      "Accuracy: 0.4455\n",
      "epoch: 610, training loss: 1.293538212776184, testing loss: 1.7082164287567139\n",
      "Accuracy: 0.4118\n",
      "epoch: 610, training loss: 1.2441284656524658, testing loss: 1.6226756572723389\n",
      "Accuracy: 0.4202\n",
      "epoch: 610, training loss: 1.3184000253677368, testing loss: 1.6266188621520996\n",
      "Accuracy: 0.4088\n",
      "epoch: 610, training loss: 1.2245973348617554, testing loss: 1.6769683361053467\n",
      "Accuracy: 0.4250\n",
      "epoch: 610, training loss: 1.2310739755630493, testing loss: 1.697166919708252\n",
      "Accuracy: 0.3889\n",
      "epoch: 610, training loss: 1.2813323736190796, testing loss: 1.7901034355163574\n",
      "Accuracy: 0.3646\n",
      "epoch: 610, training loss: 1.2532737255096436, testing loss: 1.7203136682510376\n",
      "Accuracy: 0.4211\n",
      "epoch: 610, training loss: 1.2464239597320557, testing loss: 1.7015527486801147\n",
      "Accuracy: 0.4518\n",
      "epoch: 610, training loss: 1.273945927619934, testing loss: 1.6876437664031982\n",
      "Accuracy: 0.4082\n",
      "epoch: 610, training loss: 1.2057507038116455, testing loss: 1.7222936153411865\n",
      "Accuracy: 0.4184\n",
      "epoch: 610, training loss: 1.225651741027832, testing loss: 1.8794174194335938\n",
      "Accuracy: 0.4231\n",
      "epoch: 610, training loss: 1.271425724029541, testing loss: 1.7055871486663818\n",
      "Accuracy: 0.4241\n",
      "epoch: 610, training loss: 1.2979971170425415, testing loss: 1.5686326026916504\n",
      "Accuracy: 0.4502\n",
      "epoch: 610, training loss: 1.205694317817688, testing loss: 1.6794180870056152\n",
      "Accuracy: 0.3730\n",
      "epoch: 610, training loss: 1.188849687576294, testing loss: 1.5718674659729004\n",
      "Accuracy: 0.4871\n",
      "epoch: 610, training loss: 1.2118146419525146, testing loss: 1.6358156204223633\n",
      "Accuracy: 0.4545\n",
      "epoch: 610, training loss: 1.2533124685287476, testing loss: 1.6557834148406982\n",
      "Accuracy: 0.4028\n",
      "epoch: 610, training loss: 1.2490437030792236, testing loss: 1.7746387720108032\n",
      "Accuracy: 0.3642\n",
      "epoch: 610, training loss: 1.1649314165115356, testing loss: 1.7457975149154663\n",
      "Accuracy: 0.4006\n",
      "epoch: 610, training loss: 1.2837891578674316, testing loss: 1.7602636814117432\n",
      "Accuracy: 0.4487\n",
      "epoch: 610, training loss: 1.1791605949401855, testing loss: 1.6331933736801147\n",
      "Accuracy: 0.4613\n",
      "epoch: 610, training loss: 1.280611276626587, testing loss: 1.7190042734146118\n",
      "Accuracy: 0.4532\n",
      "epoch: 610, training loss: 1.2855627536773682, testing loss: 1.7475770711898804\n",
      "Accuracy: 0.4046\n",
      "epoch: 610, training loss: 1.2124156951904297, testing loss: 1.7303507328033447\n",
      "Accuracy: 0.4295\n",
      "epoch: 610, training loss: 1.2247214317321777, testing loss: 1.667222499847412\n",
      "Accuracy: 0.4164\n",
      "epoch: 610, training loss: 1.2412142753601074, testing loss: 1.6897308826446533\n",
      "Accuracy: 0.4174\n",
      "epoch: 610, training loss: 1.2628883123397827, testing loss: 1.5578951835632324\n",
      "Accuracy: 0.4461\n",
      "epoch: 610, training loss: 1.1953195333480835, testing loss: 1.7004927396774292\n",
      "Accuracy: 0.3986\n",
      "epoch: 610, training loss: 1.2870676517486572, testing loss: 1.6239482164382935\n",
      "Accuracy: 0.3742\n",
      "epoch: 610, training loss: 1.1850223541259766, testing loss: 1.6767290830612183\n",
      "Accuracy: 0.4207\n",
      "epoch: 610, training loss: 1.2444584369659424, testing loss: 1.6600457429885864\n",
      "Accuracy: 0.4409\n",
      "epoch: 610, training loss: 1.2637014389038086, testing loss: 1.867445945739746\n",
      "Accuracy: 0.3918\n",
      "epoch: 610, training loss: 1.2545051574707031, testing loss: 1.7174369096755981\n",
      "Accuracy: 0.3955\n",
      "epoch: 610, training loss: 1.2430646419525146, testing loss: 1.655118703842163\n",
      "Accuracy: 0.4323\n",
      "epoch: 610, training loss: 1.2631256580352783, testing loss: 1.8438374996185303\n",
      "Accuracy: 0.3654\n",
      "epoch: 610, training loss: 1.2609856128692627, testing loss: 1.6427520513534546\n",
      "Accuracy: 0.4204\n",
      "epoch: 610, training loss: 1.21780526638031, testing loss: 1.6612824201583862\n",
      "Accuracy: 0.4056\n",
      "epoch: 610, training loss: 1.2365477085113525, testing loss: 1.6978744268417358\n",
      "Accuracy: 0.4185\n",
      "epoch: 610, training loss: 1.2774302959442139, testing loss: 1.70549738407135\n",
      "Accuracy: 0.4006\n",
      "epoch: 610, training loss: 1.2230998277664185, testing loss: 1.8593480587005615\n",
      "Accuracy: 0.4150\n",
      "epoch: 610, training loss: 1.233777642250061, testing loss: 1.579399585723877\n",
      "Accuracy: 0.4472\n",
      "epoch: 610, training loss: 1.1853797435760498, testing loss: 1.7111577987670898\n",
      "Accuracy: 0.3882\n",
      "epoch: 610, training loss: 1.2653532028198242, testing loss: 1.6433647871017456\n",
      "Accuracy: 0.4193\n",
      "epoch: 610, training loss: 1.2447024583816528, testing loss: 1.7147283554077148\n",
      "Accuracy: 0.3754\n",
      "epoch: 610, training loss: 1.222445011138916, testing loss: 1.6465977430343628\n",
      "Accuracy: 0.4426\n",
      "epoch: 610, training loss: 1.2416150569915771, testing loss: 1.7175437211990356\n",
      "Accuracy: 0.4123\n",
      "epoch: 610, training loss: 1.1777119636535645, testing loss: 1.7656906843185425\n",
      "Accuracy: 0.3790\n",
      "epoch: 610, training loss: 1.1836968660354614, testing loss: 1.8303735256195068\n",
      "Accuracy: 0.4133\n",
      "epoch: 610, training loss: 1.3001594543457031, testing loss: 1.680161714553833\n",
      "Accuracy: 0.4350\n",
      "epoch: 610, training loss: 1.2350294589996338, testing loss: 1.744457721710205\n",
      "Accuracy: 0.4227\n",
      "epoch: 610, training loss: 1.2643110752105713, testing loss: 1.6962249279022217\n",
      "Accuracy: 0.4071\n",
      "epoch: 610, training loss: 1.3061184883117676, testing loss: 1.6122331619262695\n",
      "Accuracy: 0.4690\n",
      "epoch: 610, training loss: 1.194808006286621, testing loss: 1.8086559772491455\n",
      "Accuracy: 0.4132\n",
      "epoch: 610, training loss: 1.3507647514343262, testing loss: 1.7720203399658203\n",
      "Accuracy: 0.4448\n",
      "epoch: 610, training loss: 1.2113817930221558, testing loss: 1.793273687362671\n",
      "Accuracy: 0.3893\n",
      "epoch: 610, training loss: 1.1922343969345093, testing loss: 1.6991913318634033\n",
      "Accuracy: 0.4169\n",
      "epoch: 610, training loss: 1.2904508113861084, testing loss: 1.7849398851394653\n",
      "Accuracy: 0.3909\n",
      "epoch: 610, training loss: 1.2723883390426636, testing loss: 1.708357810974121\n",
      "Accuracy: 0.4101\n",
      "epoch: 610, training loss: 1.198110818862915, testing loss: 1.750779390335083\n",
      "Accuracy: 0.4392\n",
      "epoch: 610, training loss: 1.1952712535858154, testing loss: 1.7462432384490967\n",
      "Accuracy: 0.4130\n",
      "epoch: 610, training loss: 1.2406256198883057, testing loss: 1.6457765102386475\n",
      "Accuracy: 0.4579\n",
      "epoch: 610, training loss: 1.2761763334274292, testing loss: 1.7436326742172241\n",
      "Accuracy: 0.3983\n",
      "epoch: 610, training loss: 1.2686753273010254, testing loss: 1.6725034713745117\n",
      "Accuracy: 0.4169\n",
      "epoch: 620, training loss: 1.2624777555465698, testing loss: 1.724655032157898\n",
      "Accuracy: 0.4121\n",
      "epoch: 620, training loss: 1.2467089891433716, testing loss: 1.7806396484375\n",
      "Accuracy: 0.4232\n",
      "epoch: 620, training loss: 1.2850427627563477, testing loss: 1.7975866794586182\n",
      "Accuracy: 0.4366\n",
      "epoch: 620, training loss: 1.2536301612854004, testing loss: 1.8461838960647583\n",
      "Accuracy: 0.3683\n",
      "epoch: 620, training loss: 1.2974252700805664, testing loss: 1.8794279098510742\n",
      "Accuracy: 0.3930\n",
      "epoch: 620, training loss: 1.2713476419448853, testing loss: 1.7509934902191162\n",
      "Accuracy: 0.3944\n",
      "epoch: 620, training loss: 1.2123583555221558, testing loss: 1.6099748611450195\n",
      "Accuracy: 0.4272\n",
      "epoch: 620, training loss: 1.244387149810791, testing loss: 1.7847073078155518\n",
      "Accuracy: 0.3920\n",
      "epoch: 620, training loss: 1.2658450603485107, testing loss: 1.6759648323059082\n",
      "Accuracy: 0.4028\n",
      "epoch: 620, training loss: 1.244752287864685, testing loss: 1.5995509624481201\n",
      "Accuracy: 0.4735\n",
      "epoch: 620, training loss: 1.2251769304275513, testing loss: 1.761043906211853\n",
      "Accuracy: 0.4069\n",
      "epoch: 620, training loss: 1.2730815410614014, testing loss: 1.7780269384384155\n",
      "Accuracy: 0.4028\n",
      "epoch: 620, training loss: 1.2713600397109985, testing loss: 1.5519193410873413\n",
      "Accuracy: 0.4577\n",
      "epoch: 620, training loss: 1.2810184955596924, testing loss: 1.6784664392471313\n",
      "Accuracy: 0.4290\n",
      "epoch: 620, training loss: 1.3002454042434692, testing loss: 1.7358653545379639\n",
      "Accuracy: 0.3798\n",
      "epoch: 620, training loss: 1.2564388513565063, testing loss: 1.6978998184204102\n",
      "Accuracy: 0.4199\n",
      "epoch: 620, training loss: 1.2689398527145386, testing loss: 1.7015918493270874\n",
      "Accuracy: 0.4327\n",
      "epoch: 620, training loss: 1.2556169033050537, testing loss: 1.6756384372711182\n",
      "Accuracy: 0.4209\n",
      "epoch: 620, training loss: 1.1869540214538574, testing loss: 1.6397993564605713\n",
      "Accuracy: 0.4524\n",
      "epoch: 620, training loss: 1.1946088075637817, testing loss: 1.6976815462112427\n",
      "Accuracy: 0.4019\n",
      "epoch: 620, training loss: 1.3323332071304321, testing loss: 1.638243317604065\n",
      "Accuracy: 0.4393\n",
      "epoch: 620, training loss: 1.2148139476776123, testing loss: 1.798756718635559\n",
      "Accuracy: 0.3889\n",
      "epoch: 620, training loss: 1.2499464750289917, testing loss: 1.6575881242752075\n",
      "Accuracy: 0.4554\n",
      "epoch: 620, training loss: 1.2703068256378174, testing loss: 1.764206886291504\n",
      "Accuracy: 0.4318\n",
      "epoch: 620, training loss: 1.25193452835083, testing loss: 1.6903928518295288\n",
      "Accuracy: 0.4343\n",
      "epoch: 620, training loss: 1.214707374572754, testing loss: 1.636107087135315\n",
      "Accuracy: 0.4170\n",
      "epoch: 620, training loss: 1.1672712564468384, testing loss: 1.7618870735168457\n",
      "Accuracy: 0.4301\n",
      "epoch: 620, training loss: 1.2311666011810303, testing loss: 1.722943663597107\n",
      "Accuracy: 0.4371\n",
      "epoch: 620, training loss: 1.321265459060669, testing loss: 1.7546439170837402\n",
      "Accuracy: 0.3836\n",
      "epoch: 620, training loss: 1.23307204246521, testing loss: 1.8086609840393066\n",
      "Accuracy: 0.4397\n",
      "epoch: 620, training loss: 1.218832015991211, testing loss: 1.6300796270370483\n",
      "Accuracy: 0.4180\n",
      "epoch: 620, training loss: 1.2367677688598633, testing loss: 1.6922060251235962\n",
      "Accuracy: 0.4324\n",
      "epoch: 620, training loss: 1.1907094717025757, testing loss: 1.6571768522262573\n",
      "Accuracy: 0.4227\n",
      "epoch: 620, training loss: 1.2159063816070557, testing loss: 1.6294035911560059\n",
      "Accuracy: 0.4062\n",
      "epoch: 620, training loss: 1.2434700727462769, testing loss: 1.809466004371643\n",
      "Accuracy: 0.4069\n",
      "epoch: 620, training loss: 1.2756606340408325, testing loss: 1.6628785133361816\n",
      "Accuracy: 0.4676\n",
      "epoch: 620, training loss: 1.2335000038146973, testing loss: 1.7987757921218872\n",
      "Accuracy: 0.4529\n",
      "epoch: 620, training loss: 1.247894048690796, testing loss: 1.6934466361999512\n",
      "Accuracy: 0.4226\n",
      "epoch: 620, training loss: 1.2462273836135864, testing loss: 1.6794344186782837\n",
      "Accuracy: 0.4062\n",
      "epoch: 620, training loss: 1.271760106086731, testing loss: 1.788446307182312\n",
      "Accuracy: 0.4281\n",
      "epoch: 620, training loss: 1.2120299339294434, testing loss: 1.822520136833191\n",
      "Accuracy: 0.3859\n",
      "epoch: 620, training loss: 1.2152079343795776, testing loss: 1.6295877695083618\n",
      "Accuracy: 0.4542\n",
      "epoch: 620, training loss: 1.2657699584960938, testing loss: 1.8343170881271362\n",
      "Accuracy: 0.4249\n",
      "epoch: 620, training loss: 1.2576181888580322, testing loss: 1.780321717262268\n",
      "Accuracy: 0.4233\n",
      "epoch: 620, training loss: 1.2671529054641724, testing loss: 1.7052873373031616\n",
      "Accuracy: 0.4375\n",
      "epoch: 620, training loss: 1.255255937576294, testing loss: 1.8164093494415283\n",
      "Accuracy: 0.3942\n",
      "epoch: 620, training loss: 1.2285656929016113, testing loss: 1.7407625913619995\n",
      "Accuracy: 0.3956\n",
      "epoch: 620, training loss: 1.204554796218872, testing loss: 1.754691481590271\n",
      "Accuracy: 0.4164\n",
      "epoch: 620, training loss: 1.2508314847946167, testing loss: 1.7131755352020264\n",
      "Accuracy: 0.3981\n",
      "epoch: 620, training loss: 1.2423349618911743, testing loss: 1.7891350984573364\n",
      "Accuracy: 0.4121\n",
      "epoch: 620, training loss: 1.231660008430481, testing loss: 1.7141779661178589\n",
      "Accuracy: 0.4202\n",
      "epoch: 620, training loss: 1.2619845867156982, testing loss: 1.8382000923156738\n",
      "Accuracy: 0.3834\n",
      "epoch: 620, training loss: 1.2787257432937622, testing loss: 1.7194030284881592\n",
      "Accuracy: 0.4020\n",
      "epoch: 620, training loss: 1.2349971532821655, testing loss: 1.8494666814804077\n",
      "Accuracy: 0.3876\n",
      "epoch: 620, training loss: 1.2093148231506348, testing loss: 1.672572135925293\n",
      "Accuracy: 0.4572\n",
      "epoch: 620, training loss: 1.210296630859375, testing loss: 1.7185101509094238\n",
      "Accuracy: 0.3973\n",
      "epoch: 620, training loss: 1.2229571342468262, testing loss: 1.7343571186065674\n",
      "Accuracy: 0.4169\n",
      "epoch: 620, training loss: 1.2746003866195679, testing loss: 1.6810487508773804\n",
      "Accuracy: 0.4053\n",
      "epoch: 620, training loss: 1.2193548679351807, testing loss: 1.8052135705947876\n",
      "Accuracy: 0.3849\n",
      "epoch: 620, training loss: 1.1989614963531494, testing loss: 1.5846502780914307\n",
      "Accuracy: 0.4429\n",
      "epoch: 620, training loss: 1.2344852685928345, testing loss: 1.841636300086975\n",
      "Accuracy: 0.3894\n",
      "epoch: 620, training loss: 1.3015395402908325, testing loss: 1.6155784130096436\n",
      "Accuracy: 0.4409\n",
      "epoch: 620, training loss: 1.2553085088729858, testing loss: 1.921678900718689\n",
      "Accuracy: 0.4136\n",
      "epoch: 620, training loss: 1.2570241689682007, testing loss: 1.7306369543075562\n",
      "Accuracy: 0.4000\n",
      "epoch: 620, training loss: 1.2011300325393677, testing loss: 1.7034428119659424\n",
      "Accuracy: 0.4521\n",
      "epoch: 620, training loss: 1.2223395109176636, testing loss: 1.682542085647583\n",
      "Accuracy: 0.4361\n",
      "epoch: 620, training loss: 1.2083827257156372, testing loss: 1.7613638639450073\n",
      "Accuracy: 0.4536\n",
      "epoch: 620, training loss: 1.2453478574752808, testing loss: 1.8204481601715088\n",
      "Accuracy: 0.4155\n",
      "epoch: 620, training loss: 1.2774649858474731, testing loss: 1.8109744787216187\n",
      "Accuracy: 0.4286\n",
      "epoch: 620, training loss: 1.265661358833313, testing loss: 1.826962947845459\n",
      "Accuracy: 0.4077\n",
      "epoch: 620, training loss: 1.2555229663848877, testing loss: 1.706493616104126\n",
      "Accuracy: 0.4304\n",
      "epoch: 620, training loss: 1.2610325813293457, testing loss: 1.8366910219192505\n",
      "Accuracy: 0.3915\n",
      "epoch: 620, training loss: 1.200819492340088, testing loss: 1.8836426734924316\n",
      "Accuracy: 0.3880\n",
      "epoch: 620, training loss: 1.2602276802062988, testing loss: 1.715039849281311\n",
      "Accuracy: 0.4087\n",
      "epoch: 620, training loss: 1.2536821365356445, testing loss: 1.7796657085418701\n",
      "Accuracy: 0.3877\n",
      "epoch: 620, training loss: 1.2812076807022095, testing loss: 1.743246078491211\n",
      "Accuracy: 0.3800\n",
      "epoch: 620, training loss: 1.2406548261642456, testing loss: 1.8496559858322144\n",
      "Accuracy: 0.3958\n",
      "epoch: 620, training loss: 1.2153037786483765, testing loss: 1.668391466140747\n",
      "Accuracy: 0.4206\n",
      "epoch: 620, training loss: 1.247933030128479, testing loss: 1.8204997777938843\n",
      "Accuracy: 0.3696\n",
      "epoch: 620, training loss: 1.342375636100769, testing loss: 1.7364459037780762\n",
      "Accuracy: 0.4419\n",
      "epoch: 620, training loss: 1.2687737941741943, testing loss: 1.7492831945419312\n",
      "Accuracy: 0.4185\n",
      "epoch: 620, training loss: 1.2601488828659058, testing loss: 1.7759575843811035\n",
      "Accuracy: 0.3882\n",
      "epoch: 620, training loss: 1.2316396236419678, testing loss: 1.6129539012908936\n",
      "Accuracy: 0.3945\n",
      "epoch: 620, training loss: 1.1839405298233032, testing loss: 1.7080790996551514\n",
      "Accuracy: 0.4116\n",
      "epoch: 620, training loss: 1.2220263481140137, testing loss: 1.770272135734558\n",
      "Accuracy: 0.4187\n",
      "epoch: 620, training loss: 1.2105560302734375, testing loss: 1.8066831827163696\n",
      "Accuracy: 0.4014\n",
      "epoch: 620, training loss: 1.319034218788147, testing loss: 1.6318187713623047\n",
      "Accuracy: 0.4886\n",
      "epoch: 620, training loss: 1.2939167022705078, testing loss: 1.7253391742706299\n",
      "Accuracy: 0.3916\n",
      "epoch: 620, training loss: 1.2472171783447266, testing loss: 1.8287192583084106\n",
      "Accuracy: 0.3792\n",
      "epoch: 620, training loss: 1.2296613454818726, testing loss: 1.480570673942566\n",
      "Accuracy: 0.4625\n",
      "epoch: 620, training loss: 1.2581521272659302, testing loss: 1.7932815551757812\n",
      "Accuracy: 0.3874\n",
      "epoch: 620, training loss: 1.3725799322128296, testing loss: 1.7176146507263184\n",
      "Accuracy: 0.4217\n",
      "epoch: 620, training loss: 1.2639871835708618, testing loss: 1.7944412231445312\n",
      "Accuracy: 0.3987\n",
      "epoch: 620, training loss: 1.2041618824005127, testing loss: 1.7052863836288452\n",
      "Accuracy: 0.4062\n",
      "epoch: 620, training loss: 1.25778329372406, testing loss: 1.6115363836288452\n",
      "Accuracy: 0.4281\n",
      "epoch: 620, training loss: 1.2242416143417358, testing loss: 1.7586207389831543\n",
      "Accuracy: 0.3873\n",
      "epoch: 620, training loss: 1.2595597505569458, testing loss: 1.7022628784179688\n",
      "Accuracy: 0.3987\n",
      "epoch: 620, training loss: 1.251695990562439, testing loss: 1.6742902994155884\n",
      "Accuracy: 0.4304\n",
      "epoch: 620, training loss: 1.2817672491073608, testing loss: 1.7034339904785156\n",
      "Accuracy: 0.3827\n",
      "epoch: 620, training loss: 1.2228822708129883, testing loss: 1.8460355997085571\n",
      "Accuracy: 0.3964\n",
      "epoch: 630, training loss: 1.1526471376419067, testing loss: 1.791276216506958\n",
      "Accuracy: 0.4112\n",
      "epoch: 630, training loss: 1.2725409269332886, testing loss: 1.759926438331604\n",
      "Accuracy: 0.4025\n",
      "epoch: 630, training loss: 1.2925145626068115, testing loss: 1.7326958179473877\n",
      "Accuracy: 0.4246\n",
      "epoch: 630, training loss: 1.1946638822555542, testing loss: 1.6398433446884155\n",
      "Accuracy: 0.4321\n",
      "epoch: 630, training loss: 1.2200697660446167, testing loss: 1.6297489404678345\n",
      "Accuracy: 0.4157\n",
      "epoch: 630, training loss: 1.2501633167266846, testing loss: 1.6805256605148315\n",
      "Accuracy: 0.4490\n",
      "epoch: 630, training loss: 1.2519298791885376, testing loss: 1.628584861755371\n",
      "Accuracy: 0.4286\n",
      "epoch: 630, training loss: 1.3313363790512085, testing loss: 1.7282525300979614\n",
      "Accuracy: 0.3805\n",
      "epoch: 630, training loss: 1.2271250486373901, testing loss: 1.7195965051651\n",
      "Accuracy: 0.4241\n",
      "epoch: 630, training loss: 1.2535698413848877, testing loss: 1.643790364265442\n",
      "Accuracy: 0.4095\n",
      "epoch: 630, training loss: 1.2699849605560303, testing loss: 1.885968804359436\n",
      "Accuracy: 0.3672\n",
      "epoch: 630, training loss: 1.336121678352356, testing loss: 1.6419355869293213\n",
      "Accuracy: 0.4233\n",
      "epoch: 630, training loss: 1.3494802713394165, testing loss: 1.7142771482467651\n",
      "Accuracy: 0.4340\n",
      "epoch: 630, training loss: 1.2900960445404053, testing loss: 1.7640776634216309\n",
      "Accuracy: 0.4307\n",
      "epoch: 630, training loss: 1.2155722379684448, testing loss: 1.6604061126708984\n",
      "Accuracy: 0.4277\n",
      "epoch: 630, training loss: 1.1645636558532715, testing loss: 1.6130571365356445\n",
      "Accuracy: 0.4057\n",
      "epoch: 630, training loss: 1.1632946729660034, testing loss: 1.7228984832763672\n",
      "Accuracy: 0.4202\n",
      "epoch: 630, training loss: 1.2362781763076782, testing loss: 1.8232433795928955\n",
      "Accuracy: 0.3942\n",
      "epoch: 630, training loss: 1.2169042825698853, testing loss: 1.6666561365127563\n",
      "Accuracy: 0.4188\n",
      "epoch: 630, training loss: 1.302560806274414, testing loss: 1.7005493640899658\n",
      "Accuracy: 0.3987\n",
      "epoch: 630, training loss: 1.2040787935256958, testing loss: 1.7326449155807495\n",
      "Accuracy: 0.4007\n",
      "epoch: 630, training loss: 1.200303316116333, testing loss: 1.7786290645599365\n",
      "Accuracy: 0.4007\n",
      "epoch: 630, training loss: 1.2313802242279053, testing loss: 1.8034162521362305\n",
      "Accuracy: 0.3658\n",
      "epoch: 630, training loss: 1.1821355819702148, testing loss: 1.7051278352737427\n",
      "Accuracy: 0.4088\n",
      "epoch: 630, training loss: 1.2402770519256592, testing loss: 1.7640048265457153\n",
      "Accuracy: 0.3758\n",
      "epoch: 630, training loss: 1.2192436456680298, testing loss: 1.7238370180130005\n",
      "Accuracy: 0.4422\n",
      "epoch: 630, training loss: 1.3323616981506348, testing loss: 1.6382253170013428\n",
      "Accuracy: 0.4357\n",
      "epoch: 630, training loss: 1.2405946254730225, testing loss: 1.7606087923049927\n",
      "Accuracy: 0.4028\n",
      "epoch: 630, training loss: 1.2694200277328491, testing loss: 1.6033084392547607\n",
      "Accuracy: 0.4756\n",
      "epoch: 630, training loss: 1.208519458770752, testing loss: 1.7765178680419922\n",
      "Accuracy: 0.3779\n",
      "epoch: 630, training loss: 1.3113582134246826, testing loss: 1.715604543685913\n",
      "Accuracy: 0.4074\n",
      "epoch: 630, training loss: 1.2587212324142456, testing loss: 1.722348690032959\n",
      "Accuracy: 0.4246\n",
      "epoch: 630, training loss: 1.1855717897415161, testing loss: 1.6937955617904663\n",
      "Accuracy: 0.4209\n",
      "epoch: 630, training loss: 1.2968661785125732, testing loss: 1.7837097644805908\n",
      "Accuracy: 0.4063\n",
      "epoch: 630, training loss: 1.2334563732147217, testing loss: 1.6556206941604614\n",
      "Accuracy: 0.4161\n",
      "epoch: 630, training loss: 1.249432921409607, testing loss: 1.8009053468704224\n",
      "Accuracy: 0.3687\n",
      "epoch: 630, training loss: 1.2996171712875366, testing loss: 1.6525919437408447\n",
      "Accuracy: 0.4728\n",
      "epoch: 630, training loss: 1.2439533472061157, testing loss: 1.7567166090011597\n",
      "Accuracy: 0.4334\n",
      "epoch: 630, training loss: 1.1797691583633423, testing loss: 1.6646515130996704\n",
      "Accuracy: 0.3980\n",
      "epoch: 630, training loss: 1.2681668996810913, testing loss: 1.6815588474273682\n",
      "Accuracy: 0.4113\n",
      "epoch: 630, training loss: 1.2649387121200562, testing loss: 1.6663153171539307\n",
      "Accuracy: 0.4121\n",
      "epoch: 630, training loss: 1.2038216590881348, testing loss: 1.5831923484802246\n",
      "Accuracy: 0.4671\n",
      "epoch: 630, training loss: 1.2245306968688965, testing loss: 1.6724493503570557\n",
      "Accuracy: 0.4048\n",
      "epoch: 630, training loss: 1.2173398733139038, testing loss: 1.6280418634414673\n",
      "Accuracy: 0.4079\n",
      "epoch: 630, training loss: 1.2414711713790894, testing loss: 1.7832560539245605\n",
      "Accuracy: 0.3765\n",
      "epoch: 630, training loss: 1.2578109502792358, testing loss: 1.760555386543274\n",
      "Accuracy: 0.4390\n",
      "epoch: 630, training loss: 1.2372190952301025, testing loss: 1.7731891870498657\n",
      "Accuracy: 0.3621\n",
      "epoch: 630, training loss: 1.2212656736373901, testing loss: 1.7788105010986328\n",
      "Accuracy: 0.3805\n",
      "epoch: 630, training loss: 1.1692726612091064, testing loss: 1.8639546632766724\n",
      "Accuracy: 0.3828\n",
      "epoch: 630, training loss: 1.2170724868774414, testing loss: 1.69916570186615\n",
      "Accuracy: 0.4379\n",
      "epoch: 630, training loss: 1.261366367340088, testing loss: 1.6559638977050781\n",
      "Accuracy: 0.4351\n",
      "epoch: 630, training loss: 1.2429662942886353, testing loss: 1.662871241569519\n",
      "Accuracy: 0.4638\n",
      "epoch: 630, training loss: 1.2436448335647583, testing loss: 1.7063406705856323\n",
      "Accuracy: 0.4308\n",
      "epoch: 630, training loss: 1.1702513694763184, testing loss: 1.7824145555496216\n",
      "Accuracy: 0.4317\n",
      "epoch: 630, training loss: 1.2035046815872192, testing loss: 1.6675260066986084\n",
      "Accuracy: 0.4172\n",
      "epoch: 630, training loss: 1.2327183485031128, testing loss: 1.7741193771362305\n",
      "Accuracy: 0.4272\n",
      "epoch: 630, training loss: 1.2405223846435547, testing loss: 1.7662711143493652\n",
      "Accuracy: 0.3811\n",
      "epoch: 630, training loss: 1.2831556797027588, testing loss: 1.8427469730377197\n",
      "Accuracy: 0.3738\n",
      "epoch: 630, training loss: 1.2114665508270264, testing loss: 1.8065195083618164\n",
      "Accuracy: 0.3688\n",
      "epoch: 630, training loss: 1.2919114828109741, testing loss: 1.5743532180786133\n",
      "Accuracy: 0.4356\n",
      "epoch: 630, training loss: 1.2508525848388672, testing loss: 1.7919976711273193\n",
      "Accuracy: 0.3600\n",
      "epoch: 630, training loss: 1.2030912637710571, testing loss: 1.6983976364135742\n",
      "Accuracy: 0.4051\n",
      "epoch: 630, training loss: 1.255667805671692, testing loss: 1.7254056930541992\n",
      "Accuracy: 0.4276\n",
      "epoch: 630, training loss: 1.2347052097320557, testing loss: 1.598170518875122\n",
      "Accuracy: 0.4132\n",
      "epoch: 630, training loss: 1.2772313356399536, testing loss: 1.6020426750183105\n",
      "Accuracy: 0.3938\n",
      "epoch: 630, training loss: 1.2163056135177612, testing loss: 1.7823963165283203\n",
      "Accuracy: 0.3993\n",
      "epoch: 630, training loss: 1.3223416805267334, testing loss: 1.7821133136749268\n",
      "Accuracy: 0.4150\n",
      "epoch: 630, training loss: 1.2050644159317017, testing loss: 1.7254196405410767\n",
      "Accuracy: 0.3881\n",
      "epoch: 630, training loss: 1.2531894445419312, testing loss: 1.6118675470352173\n",
      "Accuracy: 0.3894\n",
      "epoch: 630, training loss: 1.2409101724624634, testing loss: 1.6607341766357422\n",
      "Accuracy: 0.4518\n",
      "epoch: 630, training loss: 1.2483216524124146, testing loss: 1.7840126752853394\n",
      "Accuracy: 0.4156\n",
      "epoch: 630, training loss: 1.2798889875411987, testing loss: 1.680078387260437\n",
      "Accuracy: 0.4019\n",
      "epoch: 630, training loss: 1.3231794834136963, testing loss: 1.5703741312026978\n",
      "Accuracy: 0.4756\n",
      "epoch: 630, training loss: 1.2134674787521362, testing loss: 1.6268547773361206\n",
      "Accuracy: 0.4387\n",
      "epoch: 630, training loss: 1.2581382989883423, testing loss: 1.7596770524978638\n",
      "Accuracy: 0.3536\n",
      "epoch: 630, training loss: 1.3034194707870483, testing loss: 1.784424066543579\n",
      "Accuracy: 0.4053\n",
      "epoch: 630, training loss: 1.274502158164978, testing loss: 1.7331119775772095\n",
      "Accuracy: 0.4371\n",
      "epoch: 630, training loss: 1.2177022695541382, testing loss: 1.6234824657440186\n",
      "Accuracy: 0.4459\n",
      "epoch: 630, training loss: 1.2423001527786255, testing loss: 1.672546148300171\n",
      "Accuracy: 0.4574\n",
      "epoch: 630, training loss: 1.2511903047561646, testing loss: 1.7583726644515991\n",
      "Accuracy: 0.4244\n",
      "epoch: 630, training loss: 1.3018255233764648, testing loss: 1.6431701183319092\n",
      "Accuracy: 0.4290\n",
      "epoch: 630, training loss: 1.2515796422958374, testing loss: 1.677109956741333\n",
      "Accuracy: 0.3956\n",
      "epoch: 630, training loss: 1.2459642887115479, testing loss: 1.726750373840332\n",
      "Accuracy: 0.4326\n",
      "epoch: 630, training loss: 1.2096201181411743, testing loss: 1.8270376920700073\n",
      "Accuracy: 0.4319\n",
      "epoch: 630, training loss: 1.3044285774230957, testing loss: 1.715720772743225\n",
      "Accuracy: 0.4554\n",
      "epoch: 630, training loss: 1.2948907613754272, testing loss: 1.7266910076141357\n",
      "Accuracy: 0.4469\n",
      "epoch: 630, training loss: 1.1695528030395508, testing loss: 1.5493334531784058\n",
      "Accuracy: 0.4511\n",
      "epoch: 630, training loss: 1.3083570003509521, testing loss: 1.5042459964752197\n",
      "Accuracy: 0.4620\n",
      "epoch: 630, training loss: 1.2827285528182983, testing loss: 1.6145251989364624\n",
      "Accuracy: 0.4487\n",
      "epoch: 630, training loss: 1.2162667512893677, testing loss: 1.612957239151001\n",
      "Accuracy: 0.4154\n",
      "epoch: 630, training loss: 1.2491358518600464, testing loss: 1.656331181526184\n",
      "Accuracy: 0.4478\n",
      "epoch: 630, training loss: 1.2263141870498657, testing loss: 1.8224971294403076\n",
      "Accuracy: 0.3963\n",
      "epoch: 630, training loss: 1.213639259338379, testing loss: 1.8470966815948486\n",
      "Accuracy: 0.3903\n",
      "epoch: 630, training loss: 1.2212885618209839, testing loss: 1.6502958536148071\n",
      "Accuracy: 0.4240\n",
      "epoch: 630, training loss: 1.194618821144104, testing loss: 1.6325582265853882\n",
      "Accuracy: 0.4053\n",
      "epoch: 630, training loss: 1.2066646814346313, testing loss: 1.7262122631072998\n",
      "Accuracy: 0.4262\n",
      "epoch: 630, training loss: 1.320400595664978, testing loss: 1.8437349796295166\n",
      "Accuracy: 0.3682\n",
      "epoch: 630, training loss: 1.2505844831466675, testing loss: 1.556094765663147\n",
      "Accuracy: 0.4586\n",
      "epoch: 630, training loss: 1.17142915725708, testing loss: 1.7692620754241943\n",
      "Accuracy: 0.4194\n",
      "epoch: 630, training loss: 1.2640881538391113, testing loss: 1.7554044723510742\n",
      "Accuracy: 0.4590\n",
      "epoch: 640, training loss: 1.153054118156433, testing loss: 1.7664103507995605\n",
      "Accuracy: 0.3656\n",
      "epoch: 640, training loss: 1.2236433029174805, testing loss: 1.6625314950942993\n",
      "Accuracy: 0.4083\n",
      "epoch: 640, training loss: 1.316223382949829, testing loss: 1.6672518253326416\n",
      "Accuracy: 0.4161\n",
      "epoch: 640, training loss: 1.2752137184143066, testing loss: 1.7649424076080322\n",
      "Accuracy: 0.3947\n",
      "epoch: 640, training loss: 1.2172216176986694, testing loss: 1.8609472513198853\n",
      "Accuracy: 0.3955\n",
      "epoch: 640, training loss: 1.2282837629318237, testing loss: 1.7533222436904907\n",
      "Accuracy: 0.4422\n",
      "epoch: 640, training loss: 1.2307543754577637, testing loss: 1.7732393741607666\n",
      "Accuracy: 0.3754\n",
      "epoch: 640, training loss: 1.3685210943222046, testing loss: 1.7594177722930908\n",
      "Accuracy: 0.4587\n",
      "epoch: 640, training loss: 1.2317931652069092, testing loss: 1.6855263710021973\n",
      "Accuracy: 0.4583\n",
      "epoch: 640, training loss: 1.2389404773712158, testing loss: 1.617333173751831\n",
      "Accuracy: 0.4144\n",
      "epoch: 640, training loss: 1.2581779956817627, testing loss: 1.7228572368621826\n",
      "Accuracy: 0.4122\n",
      "epoch: 640, training loss: 1.169405221939087, testing loss: 1.7887898683547974\n",
      "Accuracy: 0.3614\n",
      "epoch: 640, training loss: 1.2434966564178467, testing loss: 1.7659450769424438\n",
      "Accuracy: 0.3880\n",
      "epoch: 640, training loss: 1.1677405834197998, testing loss: 1.7910467386245728\n",
      "Accuracy: 0.4018\n",
      "epoch: 640, training loss: 1.2250112295150757, testing loss: 1.7579867839813232\n",
      "Accuracy: 0.3960\n",
      "epoch: 640, training loss: 1.1549369096755981, testing loss: 1.7388068437576294\n",
      "Accuracy: 0.4248\n",
      "epoch: 640, training loss: 1.2956597805023193, testing loss: 1.6839853525161743\n",
      "Accuracy: 0.4452\n",
      "epoch: 640, training loss: 1.2591677904129028, testing loss: 1.7282823324203491\n",
      "Accuracy: 0.4323\n",
      "epoch: 640, training loss: 1.2110059261322021, testing loss: 1.817757248878479\n",
      "Accuracy: 0.3993\n",
      "epoch: 640, training loss: 1.297989845275879, testing loss: 1.7572757005691528\n",
      "Accuracy: 0.4257\n",
      "epoch: 640, training loss: 1.2228891849517822, testing loss: 1.8033888339996338\n",
      "Accuracy: 0.4239\n",
      "epoch: 640, training loss: 1.296180009841919, testing loss: 1.553362250328064\n",
      "Accuracy: 0.4473\n",
      "epoch: 640, training loss: 1.3252978324890137, testing loss: 1.7066470384597778\n",
      "Accuracy: 0.4381\n",
      "epoch: 640, training loss: 1.2461302280426025, testing loss: 1.702019214630127\n",
      "Accuracy: 0.4252\n",
      "epoch: 640, training loss: 1.2934397459030151, testing loss: 1.8676848411560059\n",
      "Accuracy: 0.3929\n",
      "epoch: 640, training loss: 1.2529717683792114, testing loss: 1.6792329549789429\n",
      "Accuracy: 0.4051\n",
      "epoch: 640, training loss: 1.2804254293441772, testing loss: 1.6355012655258179\n",
      "Accuracy: 0.4583\n",
      "epoch: 640, training loss: 1.2524102926254272, testing loss: 1.830879807472229\n",
      "Accuracy: 0.4182\n",
      "epoch: 640, training loss: 1.2712960243225098, testing loss: 1.7335386276245117\n",
      "Accuracy: 0.4734\n",
      "epoch: 640, training loss: 1.2561005353927612, testing loss: 1.7077196836471558\n",
      "Accuracy: 0.3932\n",
      "epoch: 640, training loss: 1.2229347229003906, testing loss: 1.7348605394363403\n",
      "Accuracy: 0.4495\n",
      "epoch: 640, training loss: 1.2012346982955933, testing loss: 1.778393030166626\n",
      "Accuracy: 0.3788\n",
      "epoch: 640, training loss: 1.2192515134811401, testing loss: 1.7741409540176392\n",
      "Accuracy: 0.4156\n",
      "epoch: 640, training loss: 1.2487190961837769, testing loss: 1.7690085172653198\n",
      "Accuracy: 0.4394\n",
      "epoch: 640, training loss: 1.3127665519714355, testing loss: 1.6543798446655273\n",
      "Accuracy: 0.4060\n",
      "epoch: 640, training loss: 1.2404608726501465, testing loss: 1.732370138168335\n",
      "Accuracy: 0.3498\n",
      "epoch: 640, training loss: 1.3311214447021484, testing loss: 1.7143335342407227\n",
      "Accuracy: 0.4369\n",
      "epoch: 640, training loss: 1.2629026174545288, testing loss: 1.74434232711792\n",
      "Accuracy: 0.4058\n",
      "epoch: 640, training loss: 1.1492886543273926, testing loss: 1.7557032108306885\n",
      "Accuracy: 0.3896\n",
      "epoch: 640, training loss: 1.2375538349151611, testing loss: 1.699644923210144\n",
      "Accuracy: 0.3939\n",
      "epoch: 640, training loss: 1.19487726688385, testing loss: 1.7090147733688354\n",
      "Accuracy: 0.4181\n",
      "epoch: 640, training loss: 1.3045090436935425, testing loss: 1.638985276222229\n",
      "Accuracy: 0.4299\n",
      "epoch: 640, training loss: 1.2513052225112915, testing loss: 1.5834784507751465\n",
      "Accuracy: 0.4300\n",
      "epoch: 640, training loss: 1.2145566940307617, testing loss: 1.6584798097610474\n",
      "Accuracy: 0.4615\n",
      "epoch: 640, training loss: 1.3094013929367065, testing loss: 1.7380794286727905\n",
      "Accuracy: 0.3677\n",
      "epoch: 640, training loss: 1.2384463548660278, testing loss: 1.646380066871643\n",
      "Accuracy: 0.4689\n",
      "epoch: 640, training loss: 1.1843116283416748, testing loss: 1.6538465023040771\n",
      "Accuracy: 0.4307\n",
      "epoch: 640, training loss: 1.2828848361968994, testing loss: 1.7475123405456543\n",
      "Accuracy: 0.3994\n",
      "epoch: 640, training loss: 1.2594294548034668, testing loss: 1.7194077968597412\n",
      "Accuracy: 0.4096\n",
      "epoch: 640, training loss: 1.1876802444458008, testing loss: 1.848104476928711\n",
      "Accuracy: 0.4098\n",
      "epoch: 640, training loss: 1.2378547191619873, testing loss: 1.7924778461456299\n",
      "Accuracy: 0.4233\n",
      "epoch: 640, training loss: 1.2097176313400269, testing loss: 1.650639295578003\n",
      "Accuracy: 0.4313\n",
      "epoch: 640, training loss: 1.341495156288147, testing loss: 1.8623276948928833\n",
      "Accuracy: 0.4034\n",
      "epoch: 640, training loss: 1.2150821685791016, testing loss: 1.7518633604049683\n",
      "Accuracy: 0.4419\n",
      "epoch: 640, training loss: 1.1910028457641602, testing loss: 1.7007777690887451\n",
      "Accuracy: 0.4500\n",
      "epoch: 640, training loss: 1.2009913921356201, testing loss: 1.8515915870666504\n",
      "Accuracy: 0.4230\n",
      "epoch: 640, training loss: 1.2590913772583008, testing loss: 1.7009176015853882\n",
      "Accuracy: 0.4172\n",
      "epoch: 640, training loss: 1.2649352550506592, testing loss: 1.8528231382369995\n",
      "Accuracy: 0.3904\n",
      "epoch: 640, training loss: 1.3297120332717896, testing loss: 1.6996698379516602\n",
      "Accuracy: 0.4159\n",
      "epoch: 640, training loss: 1.2329339981079102, testing loss: 1.6345568895339966\n",
      "Accuracy: 0.4159\n",
      "epoch: 640, training loss: 1.1786561012268066, testing loss: 1.7240715026855469\n",
      "Accuracy: 0.4388\n",
      "epoch: 640, training loss: 1.2196623086929321, testing loss: 1.6584941148757935\n",
      "Accuracy: 0.4195\n",
      "epoch: 640, training loss: 1.174742341041565, testing loss: 1.780260682106018\n",
      "Accuracy: 0.4091\n",
      "epoch: 640, training loss: 1.2552064657211304, testing loss: 1.73434579372406\n",
      "Accuracy: 0.3891\n",
      "epoch: 640, training loss: 1.2372664213180542, testing loss: 1.6950201988220215\n",
      "Accuracy: 0.4118\n",
      "epoch: 640, training loss: 1.2136907577514648, testing loss: 1.795196771621704\n",
      "Accuracy: 0.3719\n",
      "epoch: 640, training loss: 1.1518491506576538, testing loss: 1.6703935861587524\n",
      "Accuracy: 0.4071\n",
      "epoch: 640, training loss: 1.1996649503707886, testing loss: 1.6998742818832397\n",
      "Accuracy: 0.3902\n",
      "epoch: 640, training loss: 1.2726224660873413, testing loss: 1.8266388177871704\n",
      "Accuracy: 0.4268\n",
      "epoch: 640, training loss: 1.2460277080535889, testing loss: 1.6703675985336304\n",
      "Accuracy: 0.4710\n",
      "epoch: 640, training loss: 1.2491289377212524, testing loss: 1.7711509466171265\n",
      "Accuracy: 0.4066\n",
      "epoch: 640, training loss: 1.236229419708252, testing loss: 1.7077370882034302\n",
      "Accuracy: 0.4207\n",
      "epoch: 640, training loss: 1.2256561517715454, testing loss: 1.7174514532089233\n",
      "Accuracy: 0.3871\n",
      "epoch: 640, training loss: 1.2075141668319702, testing loss: 1.7624883651733398\n",
      "Accuracy: 0.4286\n",
      "epoch: 640, training loss: 1.2130061388015747, testing loss: 1.6987329721450806\n",
      "Accuracy: 0.4524\n",
      "epoch: 640, training loss: 1.25587797164917, testing loss: 1.7452430725097656\n",
      "Accuracy: 0.3788\n",
      "epoch: 640, training loss: 1.2150635719299316, testing loss: 1.7116389274597168\n",
      "Accuracy: 0.4186\n",
      "epoch: 640, training loss: 1.2594246864318848, testing loss: 1.8172974586486816\n",
      "Accuracy: 0.3916\n",
      "epoch: 640, training loss: 1.2187894582748413, testing loss: 1.695170521736145\n",
      "Accuracy: 0.4494\n",
      "epoch: 640, training loss: 1.2689439058303833, testing loss: 1.5542097091674805\n",
      "Accuracy: 0.4452\n",
      "epoch: 640, training loss: 1.2084945440292358, testing loss: 1.7246568202972412\n",
      "Accuracy: 0.4088\n",
      "epoch: 640, training loss: 1.2145154476165771, testing loss: 1.8060636520385742\n",
      "Accuracy: 0.3926\n",
      "epoch: 640, training loss: 1.2367445230484009, testing loss: 1.7387183904647827\n",
      "Accuracy: 0.4667\n",
      "epoch: 640, training loss: 1.2617508172988892, testing loss: 1.6611965894699097\n",
      "Accuracy: 0.3968\n",
      "epoch: 640, training loss: 1.2230565547943115, testing loss: 1.7921103239059448\n",
      "Accuracy: 0.4170\n",
      "epoch: 640, training loss: 1.2279963493347168, testing loss: 1.6631146669387817\n",
      "Accuracy: 0.3822\n",
      "epoch: 640, training loss: 1.1793218851089478, testing loss: 1.6754238605499268\n",
      "Accuracy: 0.4400\n",
      "epoch: 640, training loss: 1.2005988359451294, testing loss: 1.6792054176330566\n",
      "Accuracy: 0.4243\n",
      "epoch: 640, training loss: 1.1177175045013428, testing loss: 1.8475589752197266\n",
      "Accuracy: 0.3770\n",
      "epoch: 640, training loss: 1.272569179534912, testing loss: 1.900540828704834\n",
      "Accuracy: 0.4192\n",
      "epoch: 640, training loss: 1.2070895433425903, testing loss: 1.7758851051330566\n",
      "Accuracy: 0.4092\n",
      "epoch: 640, training loss: 1.291852593421936, testing loss: 1.7734187841415405\n",
      "Accuracy: 0.3885\n",
      "epoch: 640, training loss: 1.2489572763442993, testing loss: 1.5466349124908447\n",
      "Accuracy: 0.4346\n",
      "epoch: 640, training loss: 1.3149691820144653, testing loss: 1.8334382772445679\n",
      "Accuracy: 0.4303\n",
      "epoch: 640, training loss: 1.273652195930481, testing loss: 1.7019394636154175\n",
      "Accuracy: 0.3926\n",
      "epoch: 640, training loss: 1.2300119400024414, testing loss: 1.7656558752059937\n",
      "Accuracy: 0.4300\n",
      "epoch: 640, training loss: 1.2890669107437134, testing loss: 1.5666940212249756\n",
      "Accuracy: 0.4629\n",
      "epoch: 640, training loss: 1.3416584730148315, testing loss: 1.6405693292617798\n",
      "Accuracy: 0.4468\n",
      "epoch: 640, training loss: 1.285370945930481, testing loss: 1.804768443107605\n",
      "Accuracy: 0.3916\n",
      "epoch: 640, training loss: 1.1895248889923096, testing loss: 1.8015925884246826\n",
      "Accuracy: 0.3270\n",
      "epoch: 650, training loss: 1.2570687532424927, testing loss: 1.6851717233657837\n",
      "Accuracy: 0.4153\n",
      "epoch: 650, training loss: 1.2878979444503784, testing loss: 1.6393224000930786\n",
      "Accuracy: 0.4593\n",
      "epoch: 650, training loss: 1.238129734992981, testing loss: 1.8268671035766602\n",
      "Accuracy: 0.3674\n",
      "epoch: 650, training loss: 1.225629448890686, testing loss: 1.8204034566879272\n",
      "Accuracy: 0.3628\n",
      "epoch: 650, training loss: 1.1761683225631714, testing loss: 1.6368744373321533\n",
      "Accuracy: 0.4319\n",
      "epoch: 650, training loss: 1.2644145488739014, testing loss: 1.7496973276138306\n",
      "Accuracy: 0.4236\n",
      "epoch: 650, training loss: 1.2278441190719604, testing loss: 1.7569998502731323\n",
      "Accuracy: 0.4405\n",
      "epoch: 650, training loss: 1.214311957359314, testing loss: 1.718047857284546\n",
      "Accuracy: 0.4139\n",
      "epoch: 650, training loss: 1.2165699005126953, testing loss: 1.842521071434021\n",
      "Accuracy: 0.3976\n",
      "epoch: 650, training loss: 1.2542517185211182, testing loss: 1.8092650175094604\n",
      "Accuracy: 0.4353\n",
      "epoch: 650, training loss: 1.258451223373413, testing loss: 1.7572435140609741\n",
      "Accuracy: 0.3835\n",
      "epoch: 650, training loss: 1.2310354709625244, testing loss: 1.6113476753234863\n",
      "Accuracy: 0.3829\n",
      "epoch: 650, training loss: 1.2373051643371582, testing loss: 1.5895912647247314\n",
      "Accuracy: 0.4411\n",
      "epoch: 650, training loss: 1.3254756927490234, testing loss: 1.7177702188491821\n",
      "Accuracy: 0.4492\n",
      "epoch: 650, training loss: 1.1897163391113281, testing loss: 1.6272509098052979\n",
      "Accuracy: 0.4286\n",
      "epoch: 650, training loss: 1.3341233730316162, testing loss: 1.7026617527008057\n",
      "Accuracy: 0.4320\n",
      "epoch: 650, training loss: 1.2577892541885376, testing loss: 1.7979073524475098\n",
      "Accuracy: 0.3885\n",
      "epoch: 650, training loss: 1.2484074831008911, testing loss: 1.662323236465454\n",
      "Accuracy: 0.4276\n",
      "epoch: 650, training loss: 1.2895004749298096, testing loss: 1.7889107465744019\n",
      "Accuracy: 0.4466\n",
      "epoch: 650, training loss: 1.2399693727493286, testing loss: 1.7315971851348877\n",
      "Accuracy: 0.4110\n",
      "epoch: 650, training loss: 1.3337651491165161, testing loss: 1.8353327512741089\n",
      "Accuracy: 0.4063\n",
      "epoch: 650, training loss: 1.2226386070251465, testing loss: 1.7473198175430298\n",
      "Accuracy: 0.4081\n",
      "epoch: 650, training loss: 1.2708877325057983, testing loss: 1.632736325263977\n",
      "Accuracy: 0.4290\n",
      "epoch: 650, training loss: 1.222040057182312, testing loss: 1.634966254234314\n",
      "Accuracy: 0.4330\n",
      "epoch: 650, training loss: 1.20502507686615, testing loss: 1.7965270280838013\n",
      "Accuracy: 0.4281\n",
      "epoch: 650, training loss: 1.2471953630447388, testing loss: 1.741662621498108\n",
      "Accuracy: 0.3902\n",
      "epoch: 650, training loss: 1.2318404912948608, testing loss: 1.6671487092971802\n",
      "Accuracy: 0.4167\n",
      "epoch: 650, training loss: 1.277392029762268, testing loss: 1.7267295122146606\n",
      "Accuracy: 0.4221\n",
      "epoch: 650, training loss: 1.2842501401901245, testing loss: 1.7702375650405884\n",
      "Accuracy: 0.4413\n",
      "epoch: 650, training loss: 1.3035415410995483, testing loss: 1.7300511598587036\n",
      "Accuracy: 0.4179\n",
      "epoch: 650, training loss: 1.2123603820800781, testing loss: 1.630934238433838\n",
      "Accuracy: 0.4236\n",
      "epoch: 650, training loss: 1.2216490507125854, testing loss: 1.624123215675354\n",
      "Accuracy: 0.4268\n",
      "epoch: 650, training loss: 1.191564917564392, testing loss: 1.695739507675171\n",
      "Accuracy: 0.4500\n",
      "epoch: 650, training loss: 1.2707045078277588, testing loss: 1.708862543106079\n",
      "Accuracy: 0.4500\n",
      "epoch: 650, training loss: 1.293190598487854, testing loss: 1.7004631757736206\n",
      "Accuracy: 0.3791\n",
      "epoch: 650, training loss: 1.2286159992218018, testing loss: 1.7153902053833008\n",
      "Accuracy: 0.4340\n",
      "epoch: 650, training loss: 1.222204327583313, testing loss: 1.6558897495269775\n",
      "Accuracy: 0.4255\n",
      "epoch: 650, training loss: 1.2738701105117798, testing loss: 1.7955595254898071\n",
      "Accuracy: 0.3891\n",
      "epoch: 650, training loss: 1.2317850589752197, testing loss: 1.6362143754959106\n",
      "Accuracy: 0.4235\n",
      "epoch: 650, training loss: 1.1922630071640015, testing loss: 1.7042102813720703\n",
      "Accuracy: 0.4365\n",
      "epoch: 650, training loss: 1.2260596752166748, testing loss: 1.7397692203521729\n",
      "Accuracy: 0.4204\n",
      "epoch: 650, training loss: 1.2179949283599854, testing loss: 1.927464246749878\n",
      "Accuracy: 0.4199\n",
      "epoch: 650, training loss: 1.2492094039916992, testing loss: 1.8449714183807373\n",
      "Accuracy: 0.3886\n",
      "epoch: 650, training loss: 1.2713358402252197, testing loss: 1.7650530338287354\n",
      "Accuracy: 0.4536\n",
      "epoch: 650, training loss: 1.2908962965011597, testing loss: 1.7876626253128052\n",
      "Accuracy: 0.4277\n",
      "epoch: 650, training loss: 1.2251150608062744, testing loss: 1.6865386962890625\n",
      "Accuracy: 0.4251\n",
      "epoch: 650, training loss: 1.2454383373260498, testing loss: 1.7581208944320679\n",
      "Accuracy: 0.4158\n",
      "epoch: 650, training loss: 1.2405097484588623, testing loss: 1.723663568496704\n",
      "Accuracy: 0.4146\n",
      "epoch: 650, training loss: 1.2053236961364746, testing loss: 1.7516149282455444\n",
      "Accuracy: 0.4365\n",
      "epoch: 650, training loss: 1.2377214431762695, testing loss: 1.9162906408309937\n",
      "Accuracy: 0.3931\n",
      "epoch: 650, training loss: 1.2758407592773438, testing loss: 1.7828142642974854\n",
      "Accuracy: 0.4158\n",
      "epoch: 650, training loss: 1.2509174346923828, testing loss: 1.7638107538223267\n",
      "Accuracy: 0.4145\n",
      "epoch: 650, training loss: 1.2046235799789429, testing loss: 1.7437325716018677\n",
      "Accuracy: 0.4398\n",
      "epoch: 650, training loss: 1.186333179473877, testing loss: 1.7332059144973755\n",
      "Accuracy: 0.4415\n",
      "epoch: 650, training loss: 1.250278353691101, testing loss: 1.7709952592849731\n",
      "Accuracy: 0.3981\n",
      "epoch: 650, training loss: 1.2150654792785645, testing loss: 1.740582823753357\n",
      "Accuracy: 0.4150\n",
      "epoch: 650, training loss: 1.2117714881896973, testing loss: 1.7975730895996094\n",
      "Accuracy: 0.3554\n",
      "epoch: 650, training loss: 1.1941328048706055, testing loss: 1.7800623178482056\n",
      "Accuracy: 0.4592\n",
      "epoch: 650, training loss: 1.2824571132659912, testing loss: 1.7832661867141724\n",
      "Accuracy: 0.3913\n",
      "epoch: 650, training loss: 1.1918736696243286, testing loss: 1.8372853994369507\n",
      "Accuracy: 0.3958\n",
      "epoch: 650, training loss: 1.3304396867752075, testing loss: 1.6799917221069336\n",
      "Accuracy: 0.4272\n",
      "epoch: 650, training loss: 1.2721617221832275, testing loss: 1.6831538677215576\n",
      "Accuracy: 0.4007\n",
      "epoch: 650, training loss: 1.1958460807800293, testing loss: 1.7104521989822388\n",
      "Accuracy: 0.4157\n",
      "epoch: 650, training loss: 1.180827021598816, testing loss: 1.7562103271484375\n",
      "Accuracy: 0.4083\n",
      "epoch: 650, training loss: 1.2357221841812134, testing loss: 1.7661722898483276\n",
      "Accuracy: 0.4228\n",
      "epoch: 650, training loss: 1.2334848642349243, testing loss: 1.9310952425003052\n",
      "Accuracy: 0.3811\n",
      "epoch: 650, training loss: 1.2348264455795288, testing loss: 1.620718240737915\n",
      "Accuracy: 0.4475\n",
      "epoch: 650, training loss: 1.247792363166809, testing loss: 1.6128653287887573\n",
      "Accuracy: 0.4695\n",
      "epoch: 650, training loss: 1.25255286693573, testing loss: 1.7218236923217773\n",
      "Accuracy: 0.4389\n",
      "epoch: 650, training loss: 1.2421941757202148, testing loss: 1.7726494073867798\n",
      "Accuracy: 0.4031\n",
      "epoch: 650, training loss: 1.2729027271270752, testing loss: 1.738608717918396\n",
      "Accuracy: 0.4562\n",
      "epoch: 650, training loss: 1.2177534103393555, testing loss: 1.7535715103149414\n",
      "Accuracy: 0.3987\n",
      "epoch: 650, training loss: 1.292819619178772, testing loss: 1.6921478509902954\n",
      "Accuracy: 0.4178\n",
      "epoch: 650, training loss: 1.2174731492996216, testing loss: 1.6789945363998413\n",
      "Accuracy: 0.4351\n",
      "epoch: 650, training loss: 1.287164330482483, testing loss: 1.6934717893600464\n",
      "Accuracy: 0.4159\n",
      "epoch: 650, training loss: 1.234535813331604, testing loss: 1.7343212366104126\n",
      "Accuracy: 0.3882\n",
      "epoch: 650, training loss: 1.241593360900879, testing loss: 1.6353682279586792\n",
      "Accuracy: 0.4353\n",
      "epoch: 650, training loss: 1.257012963294983, testing loss: 1.8046714067459106\n",
      "Accuracy: 0.4237\n",
      "epoch: 650, training loss: 1.1950149536132812, testing loss: 1.831923484802246\n",
      "Accuracy: 0.4136\n",
      "epoch: 650, training loss: 1.2464488744735718, testing loss: 1.7024084329605103\n",
      "Accuracy: 0.3993\n",
      "epoch: 650, training loss: 1.2234500646591187, testing loss: 1.6887463331222534\n",
      "Accuracy: 0.3754\n",
      "epoch: 650, training loss: 1.2733242511749268, testing loss: 1.6390584707260132\n",
      "Accuracy: 0.4383\n",
      "epoch: 650, training loss: 1.289246916770935, testing loss: 1.6609339714050293\n",
      "Accuracy: 0.4338\n",
      "epoch: 650, training loss: 1.2629890441894531, testing loss: 1.8269827365875244\n",
      "Accuracy: 0.4172\n",
      "epoch: 650, training loss: 1.1895065307617188, testing loss: 1.796950101852417\n",
      "Accuracy: 0.3909\n",
      "epoch: 650, training loss: 1.2278934717178345, testing loss: 1.7122690677642822\n",
      "Accuracy: 0.3813\n",
      "epoch: 650, training loss: 1.2713497877120972, testing loss: 1.6395270824432373\n",
      "Accuracy: 0.4167\n",
      "epoch: 650, training loss: 1.249515414237976, testing loss: 1.813254714012146\n",
      "Accuracy: 0.3922\n",
      "epoch: 650, training loss: 1.3102893829345703, testing loss: 1.684311866760254\n",
      "Accuracy: 0.4156\n",
      "epoch: 650, training loss: 1.173156976699829, testing loss: 1.64922034740448\n",
      "Accuracy: 0.4660\n",
      "epoch: 650, training loss: 1.2554093599319458, testing loss: 1.5922490358352661\n",
      "Accuracy: 0.4483\n",
      "epoch: 650, training loss: 1.2383396625518799, testing loss: 1.7575348615646362\n",
      "Accuracy: 0.3993\n",
      "epoch: 650, training loss: 1.2538381814956665, testing loss: 1.687314748764038\n",
      "Accuracy: 0.4186\n",
      "epoch: 650, training loss: 1.2243270874023438, testing loss: 1.6939911842346191\n",
      "Accuracy: 0.4484\n",
      "epoch: 650, training loss: 1.1968902349472046, testing loss: 1.6318199634552002\n",
      "Accuracy: 0.3910\n",
      "epoch: 650, training loss: 1.2558135986328125, testing loss: 1.6406266689300537\n",
      "Accuracy: 0.4527\n",
      "epoch: 650, training loss: 1.168611764907837, testing loss: 1.6484533548355103\n",
      "Accuracy: 0.4241\n",
      "epoch: 650, training loss: 1.2361154556274414, testing loss: 1.7107527256011963\n",
      "Accuracy: 0.4377\n",
      "epoch: 650, training loss: 1.2350362539291382, testing loss: 1.7015347480773926\n",
      "Accuracy: 0.4157\n",
      "epoch: 650, training loss: 1.2867441177368164, testing loss: 1.7127176523208618\n",
      "Accuracy: 0.4626\n",
      "epoch: 660, training loss: 1.2183188199996948, testing loss: 1.658517837524414\n",
      "Accuracy: 0.4352\n",
      "epoch: 660, training loss: 1.2228761911392212, testing loss: 1.664724588394165\n",
      "Accuracy: 0.4838\n",
      "epoch: 660, training loss: 1.2651506662368774, testing loss: 1.7769372463226318\n",
      "Accuracy: 0.4281\n",
      "epoch: 660, training loss: 1.2241272926330566, testing loss: 1.7784695625305176\n",
      "Accuracy: 0.3967\n",
      "epoch: 660, training loss: 1.2037444114685059, testing loss: 1.742925763130188\n",
      "Accuracy: 0.3742\n",
      "epoch: 660, training loss: 1.2899826765060425, testing loss: 1.6964130401611328\n",
      "Accuracy: 0.4198\n",
      "epoch: 660, training loss: 1.2564494609832764, testing loss: 1.7417677640914917\n",
      "Accuracy: 0.3924\n",
      "epoch: 660, training loss: 1.1824278831481934, testing loss: 1.5437419414520264\n",
      "Accuracy: 0.4413\n",
      "epoch: 660, training loss: 1.2267602682113647, testing loss: 1.7635865211486816\n",
      "Accuracy: 0.4037\n",
      "epoch: 660, training loss: 1.3540444374084473, testing loss: 1.8742669820785522\n",
      "Accuracy: 0.3920\n",
      "epoch: 660, training loss: 1.2054084539413452, testing loss: 1.751437783241272\n",
      "Accuracy: 0.4140\n",
      "epoch: 660, training loss: 1.2877368927001953, testing loss: 1.8539870977401733\n",
      "Accuracy: 0.4045\n",
      "epoch: 660, training loss: 1.234784722328186, testing loss: 1.5099278688430786\n",
      "Accuracy: 0.4627\n",
      "epoch: 660, training loss: 1.2896541357040405, testing loss: 1.6610934734344482\n",
      "Accuracy: 0.4615\n",
      "epoch: 660, training loss: 1.2700780630111694, testing loss: 1.7561911344528198\n",
      "Accuracy: 0.4014\n",
      "epoch: 660, training loss: 1.254399299621582, testing loss: 1.7014172077178955\n",
      "Accuracy: 0.3916\n",
      "epoch: 660, training loss: 1.2284048795700073, testing loss: 1.7640111446380615\n",
      "Accuracy: 0.3980\n",
      "epoch: 660, training loss: 1.2421672344207764, testing loss: 1.6423962116241455\n",
      "Accuracy: 0.4226\n",
      "epoch: 660, training loss: 1.2049996852874756, testing loss: 1.656671166419983\n",
      "Accuracy: 0.4247\n",
      "epoch: 660, training loss: 1.216109275817871, testing loss: 1.6759814023971558\n",
      "Accuracy: 0.4254\n",
      "epoch: 660, training loss: 1.2242761850357056, testing loss: 1.8122408390045166\n",
      "Accuracy: 0.4031\n",
      "epoch: 660, training loss: 1.2457200288772583, testing loss: 1.7562626600265503\n",
      "Accuracy: 0.3684\n",
      "epoch: 660, training loss: 1.2492518424987793, testing loss: 1.6932743787765503\n",
      "Accuracy: 0.4167\n",
      "epoch: 660, training loss: 1.190931797027588, testing loss: 1.6783064603805542\n",
      "Accuracy: 0.4411\n",
      "epoch: 660, training loss: 1.2117968797683716, testing loss: 1.62504243850708\n",
      "Accuracy: 0.4272\n",
      "epoch: 660, training loss: 1.2123514413833618, testing loss: 1.7689799070358276\n",
      "Accuracy: 0.4185\n",
      "epoch: 660, training loss: 1.2479114532470703, testing loss: 1.6287983655929565\n",
      "Accuracy: 0.4169\n",
      "epoch: 660, training loss: 1.3053412437438965, testing loss: 1.7699686288833618\n",
      "Accuracy: 0.4281\n",
      "epoch: 660, training loss: 1.2196578979492188, testing loss: 1.689780354499817\n",
      "Accuracy: 0.4367\n",
      "epoch: 660, training loss: 1.207149624824524, testing loss: 1.6242082118988037\n",
      "Accuracy: 0.4359\n",
      "epoch: 660, training loss: 1.226485013961792, testing loss: 1.7253086566925049\n",
      "Accuracy: 0.4608\n",
      "epoch: 660, training loss: 1.2212861776351929, testing loss: 1.7420780658721924\n",
      "Accuracy: 0.4408\n",
      "epoch: 660, training loss: 1.3226053714752197, testing loss: 1.7365491390228271\n",
      "Accuracy: 0.4186\n",
      "epoch: 660, training loss: 1.283868432044983, testing loss: 1.6087080240249634\n",
      "Accuracy: 0.4726\n",
      "epoch: 660, training loss: 1.2462314367294312, testing loss: 1.7666006088256836\n",
      "Accuracy: 0.4216\n",
      "epoch: 660, training loss: 1.2386188507080078, testing loss: 1.580776572227478\n",
      "Accuracy: 0.4121\n",
      "epoch: 660, training loss: 1.252140760421753, testing loss: 1.637672781944275\n",
      "Accuracy: 0.4202\n",
      "epoch: 660, training loss: 1.288730263710022, testing loss: 1.8421391248703003\n",
      "Accuracy: 0.3987\n",
      "epoch: 660, training loss: 1.247572898864746, testing loss: 1.7315670251846313\n",
      "Accuracy: 0.3846\n",
      "epoch: 660, training loss: 1.2832424640655518, testing loss: 1.6864714622497559\n",
      "Accuracy: 0.4558\n",
      "epoch: 660, training loss: 1.2306302785873413, testing loss: 1.7223721742630005\n",
      "Accuracy: 0.4161\n",
      "epoch: 660, training loss: 1.2160155773162842, testing loss: 1.780016541481018\n",
      "Accuracy: 0.4104\n",
      "epoch: 660, training loss: 1.2499079704284668, testing loss: 1.7236320972442627\n",
      "Accuracy: 0.3851\n",
      "epoch: 660, training loss: 1.1996711492538452, testing loss: 1.774877667427063\n",
      "Accuracy: 0.4107\n",
      "epoch: 660, training loss: 1.2375669479370117, testing loss: 1.6499543190002441\n",
      "Accuracy: 0.4551\n",
      "epoch: 660, training loss: 1.2738322019577026, testing loss: 1.6672909259796143\n",
      "Accuracy: 0.4411\n",
      "epoch: 660, training loss: 1.233105182647705, testing loss: 1.7741990089416504\n",
      "Accuracy: 0.3841\n",
      "epoch: 660, training loss: 1.1789147853851318, testing loss: 1.626870036125183\n",
      "Accuracy: 0.4404\n",
      "epoch: 660, training loss: 1.2350857257843018, testing loss: 1.7895703315734863\n",
      "Accuracy: 0.4355\n",
      "epoch: 660, training loss: 1.1757593154907227, testing loss: 1.7144781351089478\n",
      "Accuracy: 0.4151\n",
      "epoch: 660, training loss: 1.218263030052185, testing loss: 1.7367838621139526\n",
      "Accuracy: 0.3859\n",
      "epoch: 660, training loss: 1.2420237064361572, testing loss: 1.690151333808899\n",
      "Accuracy: 0.4006\n",
      "epoch: 660, training loss: 1.291199803352356, testing loss: 1.7257497310638428\n",
      "Accuracy: 0.3689\n",
      "epoch: 660, training loss: 1.2024232149124146, testing loss: 1.7200517654418945\n",
      "Accuracy: 0.4247\n",
      "epoch: 660, training loss: 1.2710813283920288, testing loss: 1.7278188467025757\n",
      "Accuracy: 0.4503\n",
      "epoch: 660, training loss: 1.2274214029312134, testing loss: 1.7607487440109253\n",
      "Accuracy: 0.4104\n",
      "epoch: 660, training loss: 1.1928083896636963, testing loss: 1.5328633785247803\n",
      "Accuracy: 0.4799\n",
      "epoch: 660, training loss: 1.2576408386230469, testing loss: 1.6126837730407715\n",
      "Accuracy: 0.4413\n",
      "epoch: 660, training loss: 1.2199678421020508, testing loss: 1.6883876323699951\n",
      "Accuracy: 0.4330\n",
      "epoch: 660, training loss: 1.1940195560455322, testing loss: 1.700941801071167\n",
      "Accuracy: 0.3885\n",
      "epoch: 660, training loss: 1.1945449113845825, testing loss: 1.6201919317245483\n",
      "Accuracy: 0.4455\n",
      "epoch: 660, training loss: 1.2716758251190186, testing loss: 1.7263433933258057\n",
      "Accuracy: 0.3969\n",
      "epoch: 660, training loss: 1.2653001546859741, testing loss: 1.691391944885254\n",
      "Accuracy: 0.4272\n",
      "epoch: 660, training loss: 1.2540844678878784, testing loss: 1.8692349195480347\n",
      "Accuracy: 0.4174\n",
      "epoch: 660, training loss: 1.228278398513794, testing loss: 1.6838213205337524\n",
      "Accuracy: 0.4271\n",
      "epoch: 660, training loss: 1.2195791006088257, testing loss: 1.6589723825454712\n",
      "Accuracy: 0.4405\n",
      "epoch: 660, training loss: 1.2289018630981445, testing loss: 1.7626324892044067\n",
      "Accuracy: 0.4207\n",
      "epoch: 660, training loss: 1.194494366645813, testing loss: 1.6559346914291382\n",
      "Accuracy: 0.4639\n",
      "epoch: 660, training loss: 1.1886917352676392, testing loss: 1.7235817909240723\n",
      "Accuracy: 0.4227\n",
      "epoch: 660, training loss: 1.2477668523788452, testing loss: 1.6309643983840942\n",
      "Accuracy: 0.4420\n",
      "epoch: 660, training loss: 1.2118408679962158, testing loss: 1.7665743827819824\n",
      "Accuracy: 0.4557\n",
      "epoch: 660, training loss: 1.2506672143936157, testing loss: 1.7248765230178833\n",
      "Accuracy: 0.4059\n",
      "epoch: 660, training loss: 1.2690787315368652, testing loss: 1.6282707452774048\n",
      "Accuracy: 0.3951\n",
      "epoch: 660, training loss: 1.2135009765625, testing loss: 1.7671924829483032\n",
      "Accuracy: 0.3654\n",
      "epoch: 660, training loss: 1.203474998474121, testing loss: 1.720912218093872\n",
      "Accuracy: 0.4045\n",
      "epoch: 660, training loss: 1.229148268699646, testing loss: 1.6769907474517822\n",
      "Accuracy: 0.4312\n",
      "epoch: 660, training loss: 1.2268072366714478, testing loss: 1.7210079431533813\n",
      "Accuracy: 0.4475\n",
      "epoch: 660, training loss: 1.2286286354064941, testing loss: 1.6805838346481323\n",
      "Accuracy: 0.4708\n",
      "epoch: 660, training loss: 1.2176581621170044, testing loss: 1.709473729133606\n",
      "Accuracy: 0.4303\n",
      "epoch: 660, training loss: 1.2051892280578613, testing loss: 1.7153141498565674\n",
      "Accuracy: 0.4276\n",
      "epoch: 660, training loss: 1.1556577682495117, testing loss: 1.6297023296356201\n",
      "Accuracy: 0.4441\n",
      "epoch: 660, training loss: 1.2992515563964844, testing loss: 1.6331121921539307\n",
      "Accuracy: 0.4230\n",
      "epoch: 660, training loss: 1.2548738718032837, testing loss: 1.7304303646087646\n",
      "Accuracy: 0.4448\n",
      "epoch: 660, training loss: 1.2681467533111572, testing loss: 1.6662384271621704\n",
      "Accuracy: 0.4684\n",
      "epoch: 660, training loss: 1.2949652671813965, testing loss: 1.687516212463379\n",
      "Accuracy: 0.3851\n",
      "epoch: 660, training loss: 1.1905779838562012, testing loss: 1.6746050119400024\n",
      "Accuracy: 0.4137\n",
      "epoch: 660, training loss: 1.2012500762939453, testing loss: 1.5200339555740356\n",
      "Accuracy: 0.4545\n",
      "epoch: 660, training loss: 1.3114453554153442, testing loss: 1.7126635313034058\n",
      "Accuracy: 0.4063\n",
      "epoch: 660, training loss: 1.2419198751449585, testing loss: 1.6911745071411133\n",
      "Accuracy: 0.4106\n",
      "epoch: 660, training loss: 1.2122243642807007, testing loss: 1.7550474405288696\n",
      "Accuracy: 0.4521\n",
      "epoch: 660, training loss: 1.2517458200454712, testing loss: 1.6251760721206665\n",
      "Accuracy: 0.3799\n",
      "epoch: 660, training loss: 1.2313505411148071, testing loss: 1.610447645187378\n",
      "Accuracy: 0.4752\n",
      "epoch: 660, training loss: 1.1758252382278442, testing loss: 1.8470312356948853\n",
      "Accuracy: 0.4492\n",
      "epoch: 660, training loss: 1.2657320499420166, testing loss: 1.62885582447052\n",
      "Accuracy: 0.4331\n",
      "epoch: 660, training loss: 1.258643388748169, testing loss: 1.6958740949630737\n",
      "Accuracy: 0.3919\n",
      "epoch: 660, training loss: 1.17856764793396, testing loss: 1.7161310911178589\n",
      "Accuracy: 0.3987\n",
      "epoch: 660, training loss: 1.2808576822280884, testing loss: 1.6591588258743286\n",
      "Accuracy: 0.4323\n",
      "epoch: 660, training loss: 1.2624711990356445, testing loss: 1.8507707118988037\n",
      "Accuracy: 0.3924\n",
      "epoch: 660, training loss: 1.2516897916793823, testing loss: 1.706721544265747\n",
      "Accuracy: 0.4281\n",
      "epoch: 660, training loss: 1.239098310470581, testing loss: 1.594104290008545\n",
      "Accuracy: 0.4206\n",
      "epoch: 670, training loss: 1.2329928874969482, testing loss: 1.7573703527450562\n",
      "Accuracy: 0.3729\n",
      "epoch: 670, training loss: 1.1868294477462769, testing loss: 1.7952102422714233\n",
      "Accuracy: 0.3966\n",
      "epoch: 670, training loss: 1.2517788410186768, testing loss: 1.7079168558120728\n",
      "Accuracy: 0.4181\n",
      "epoch: 670, training loss: 1.193479299545288, testing loss: 1.713897466659546\n",
      "Accuracy: 0.3614\n",
      "epoch: 670, training loss: 1.200337529182434, testing loss: 1.7412809133529663\n",
      "Accuracy: 0.4268\n",
      "epoch: 670, training loss: 1.2990124225616455, testing loss: 1.794406533241272\n",
      "Accuracy: 0.4102\n",
      "epoch: 670, training loss: 1.2746896743774414, testing loss: 1.7381081581115723\n",
      "Accuracy: 0.4231\n",
      "epoch: 670, training loss: 1.16880464553833, testing loss: 1.768284559249878\n",
      "Accuracy: 0.3824\n",
      "epoch: 670, training loss: 1.2637755870819092, testing loss: 1.6954079866409302\n",
      "Accuracy: 0.4212\n",
      "epoch: 670, training loss: 1.2508125305175781, testing loss: 1.724273920059204\n",
      "Accuracy: 0.4337\n",
      "epoch: 670, training loss: 1.246108889579773, testing loss: 1.7114300727844238\n",
      "Accuracy: 0.4194\n",
      "epoch: 670, training loss: 1.2708920240402222, testing loss: 1.6855826377868652\n",
      "Accuracy: 0.4477\n",
      "epoch: 670, training loss: 1.3193541765213013, testing loss: 1.720244288444519\n",
      "Accuracy: 0.4319\n",
      "epoch: 670, training loss: 1.2654876708984375, testing loss: 1.6697759628295898\n",
      "Accuracy: 0.4769\n",
      "epoch: 670, training loss: 1.1953799724578857, testing loss: 1.7424553632736206\n",
      "Accuracy: 0.3398\n",
      "epoch: 670, training loss: 1.2106165885925293, testing loss: 1.72781503200531\n",
      "Accuracy: 0.3939\n",
      "epoch: 670, training loss: 1.2584322690963745, testing loss: 1.7433844804763794\n",
      "Accuracy: 0.3920\n",
      "epoch: 670, training loss: 1.274915099143982, testing loss: 1.773462176322937\n",
      "Accuracy: 0.4172\n",
      "epoch: 670, training loss: 1.271121621131897, testing loss: 1.7060505151748657\n",
      "Accuracy: 0.4090\n",
      "epoch: 670, training loss: 1.2946711778640747, testing loss: 1.7468788623809814\n",
      "Accuracy: 0.4164\n",
      "epoch: 670, training loss: 1.1763449907302856, testing loss: 1.7581976652145386\n",
      "Accuracy: 0.4080\n",
      "epoch: 670, training loss: 1.3284778594970703, testing loss: 1.6889827251434326\n",
      "Accuracy: 0.4178\n",
      "epoch: 670, training loss: 1.2885770797729492, testing loss: 1.6811847686767578\n",
      "Accuracy: 0.4057\n",
      "epoch: 670, training loss: 1.2691166400909424, testing loss: 1.6658583879470825\n",
      "Accuracy: 0.4745\n",
      "epoch: 670, training loss: 1.1556848287582397, testing loss: 1.8343071937561035\n",
      "Accuracy: 0.3974\n",
      "epoch: 670, training loss: 1.27083158493042, testing loss: 1.7071868181228638\n",
      "Accuracy: 0.4099\n",
      "epoch: 670, training loss: 1.2481671571731567, testing loss: 1.70430326461792\n",
      "Accuracy: 0.4088\n",
      "epoch: 670, training loss: 1.215188980102539, testing loss: 1.7271703481674194\n",
      "Accuracy: 0.3870\n",
      "epoch: 670, training loss: 1.2543199062347412, testing loss: 1.713320255279541\n",
      "Accuracy: 0.4247\n",
      "epoch: 670, training loss: 1.2487376928329468, testing loss: 1.6466468572616577\n",
      "Accuracy: 0.4468\n",
      "epoch: 670, training loss: 1.3207043409347534, testing loss: 1.7862852811813354\n",
      "Accuracy: 0.3643\n",
      "epoch: 670, training loss: 1.2601553201675415, testing loss: 1.8788548707962036\n",
      "Accuracy: 0.3870\n",
      "epoch: 670, training loss: 1.2313777208328247, testing loss: 1.684548020362854\n",
      "Accuracy: 0.4349\n",
      "epoch: 670, training loss: 1.2764089107513428, testing loss: 1.6321430206298828\n",
      "Accuracy: 0.3878\n",
      "epoch: 670, training loss: 1.2320990562438965, testing loss: 1.6963857412338257\n",
      "Accuracy: 0.4469\n",
      "epoch: 670, training loss: 1.2744357585906982, testing loss: 1.7780863046646118\n",
      "Accuracy: 0.4156\n",
      "epoch: 670, training loss: 1.2359622716903687, testing loss: 1.577980875968933\n",
      "Accuracy: 0.4492\n",
      "epoch: 670, training loss: 1.229871392250061, testing loss: 1.6728184223175049\n",
      "Accuracy: 0.4281\n",
      "epoch: 670, training loss: 1.2158232927322388, testing loss: 1.7852213382720947\n",
      "Accuracy: 0.4107\n",
      "epoch: 670, training loss: 1.2496618032455444, testing loss: 1.6918187141418457\n",
      "Accuracy: 0.4112\n",
      "epoch: 670, training loss: 1.2510689496994019, testing loss: 1.7120234966278076\n",
      "Accuracy: 0.4389\n",
      "epoch: 670, training loss: 1.203212857246399, testing loss: 1.7552615404129028\n",
      "Accuracy: 0.4174\n",
      "epoch: 670, training loss: 1.2925574779510498, testing loss: 1.7234176397323608\n",
      "Accuracy: 0.4108\n",
      "epoch: 670, training loss: 1.2315623760223389, testing loss: 1.7556005716323853\n",
      "Accuracy: 0.3648\n",
      "epoch: 670, training loss: 1.2261110544204712, testing loss: 1.6621958017349243\n",
      "Accuracy: 0.4645\n",
      "epoch: 670, training loss: 1.2710165977478027, testing loss: 1.8447872400283813\n",
      "Accuracy: 0.3964\n",
      "epoch: 670, training loss: 1.1940275430679321, testing loss: 1.8246915340423584\n",
      "Accuracy: 0.3624\n",
      "epoch: 670, training loss: 1.2282425165176392, testing loss: 1.578816294670105\n",
      "Accuracy: 0.4601\n",
      "epoch: 670, training loss: 1.2344305515289307, testing loss: 1.834066390991211\n",
      "Accuracy: 0.4076\n",
      "epoch: 670, training loss: 1.2569379806518555, testing loss: 1.8738980293273926\n",
      "Accuracy: 0.3714\n",
      "epoch: 670, training loss: 1.2632514238357544, testing loss: 1.5500999689102173\n",
      "Accuracy: 0.4500\n",
      "epoch: 670, training loss: 1.2465323209762573, testing loss: 1.902477741241455\n",
      "Accuracy: 0.4006\n",
      "epoch: 670, training loss: 1.2811344861984253, testing loss: 1.6207642555236816\n",
      "Accuracy: 0.4752\n",
      "epoch: 670, training loss: 1.2608098983764648, testing loss: 1.7235666513442993\n",
      "Accuracy: 0.3799\n",
      "epoch: 670, training loss: 1.2775850296020508, testing loss: 1.8309061527252197\n",
      "Accuracy: 0.3720\n",
      "epoch: 670, training loss: 1.2567336559295654, testing loss: 1.6643811464309692\n",
      "Accuracy: 0.4689\n",
      "epoch: 670, training loss: 1.2788112163543701, testing loss: 1.7390594482421875\n",
      "Accuracy: 0.4038\n",
      "epoch: 670, training loss: 1.2170026302337646, testing loss: 1.7742412090301514\n",
      "Accuracy: 0.4020\n",
      "epoch: 670, training loss: 1.2730861902236938, testing loss: 1.7218873500823975\n",
      "Accuracy: 0.4156\n",
      "epoch: 670, training loss: 1.199144959449768, testing loss: 1.4527170658111572\n",
      "Accuracy: 0.4768\n",
      "epoch: 670, training loss: 1.312906265258789, testing loss: 1.7018343210220337\n",
      "Accuracy: 0.4382\n",
      "epoch: 670, training loss: 1.2274624109268188, testing loss: 1.6903823614120483\n",
      "Accuracy: 0.4513\n",
      "epoch: 670, training loss: 1.2059733867645264, testing loss: 1.6937217712402344\n",
      "Accuracy: 0.4346\n",
      "epoch: 670, training loss: 1.2506271600723267, testing loss: 1.8099658489227295\n",
      "Accuracy: 0.3480\n",
      "epoch: 670, training loss: 1.2346165180206299, testing loss: 1.5564123392105103\n",
      "Accuracy: 0.4277\n",
      "epoch: 670, training loss: 1.2756352424621582, testing loss: 1.707139492034912\n",
      "Accuracy: 0.4303\n",
      "epoch: 670, training loss: 1.236099123954773, testing loss: 1.757745623588562\n",
      "Accuracy: 0.4257\n",
      "epoch: 670, training loss: 1.173467993736267, testing loss: 1.661745309829712\n",
      "Accuracy: 0.4403\n",
      "epoch: 670, training loss: 1.2331587076187134, testing loss: 1.6398265361785889\n",
      "Accuracy: 0.4172\n",
      "epoch: 670, training loss: 1.1871663331985474, testing loss: 1.7955567836761475\n",
      "Accuracy: 0.4089\n",
      "epoch: 670, training loss: 1.2326128482818604, testing loss: 1.6939616203308105\n",
      "Accuracy: 0.4545\n",
      "epoch: 670, training loss: 1.2530301809310913, testing loss: 1.7623608112335205\n",
      "Accuracy: 0.4183\n",
      "epoch: 670, training loss: 1.3077242374420166, testing loss: 1.7386962175369263\n",
      "Accuracy: 0.4415\n",
      "epoch: 670, training loss: 1.272774338722229, testing loss: 1.6278825998306274\n",
      "Accuracy: 0.3993\n",
      "epoch: 670, training loss: 1.2348203659057617, testing loss: 1.677844762802124\n",
      "Accuracy: 0.4063\n",
      "epoch: 670, training loss: 1.2923693656921387, testing loss: 1.7460130453109741\n",
      "Accuracy: 0.4146\n",
      "epoch: 670, training loss: 1.2141995429992676, testing loss: 1.7631795406341553\n",
      "Accuracy: 0.4013\n",
      "epoch: 670, training loss: 1.316526174545288, testing loss: 1.798842191696167\n",
      "Accuracy: 0.4276\n",
      "epoch: 670, training loss: 1.2051576375961304, testing loss: 1.6358096599578857\n",
      "Accuracy: 0.4385\n",
      "epoch: 670, training loss: 1.244504690170288, testing loss: 1.6648268699645996\n",
      "Accuracy: 0.4421\n",
      "epoch: 670, training loss: 1.2274484634399414, testing loss: 1.678607702255249\n",
      "Accuracy: 0.4082\n",
      "epoch: 670, training loss: 1.2687675952911377, testing loss: 1.714879035949707\n",
      "Accuracy: 0.3805\n",
      "epoch: 670, training loss: 1.252128005027771, testing loss: 1.6108437776565552\n",
      "Accuracy: 0.4448\n",
      "epoch: 670, training loss: 1.2617690563201904, testing loss: 1.7741129398345947\n",
      "Accuracy: 0.3800\n",
      "epoch: 670, training loss: 1.289512038230896, testing loss: 1.7473326921463013\n",
      "Accuracy: 0.4048\n",
      "epoch: 670, training loss: 1.196883201599121, testing loss: 1.5413564443588257\n",
      "Accuracy: 0.4630\n",
      "epoch: 670, training loss: 1.2202768325805664, testing loss: 1.6287964582443237\n",
      "Accuracy: 0.4473\n",
      "epoch: 670, training loss: 1.2241829633712769, testing loss: 1.851161003112793\n",
      "Accuracy: 0.4088\n",
      "epoch: 670, training loss: 1.3195099830627441, testing loss: 1.6235876083374023\n",
      "Accuracy: 0.4243\n",
      "epoch: 670, training loss: 1.1821186542510986, testing loss: 1.688429832458496\n",
      "Accuracy: 0.4469\n",
      "epoch: 670, training loss: 1.2069717645645142, testing loss: 1.7682651281356812\n",
      "Accuracy: 0.3739\n",
      "epoch: 670, training loss: 1.1807063817977905, testing loss: 1.674072265625\n",
      "Accuracy: 0.4061\n",
      "epoch: 670, training loss: 1.248566746711731, testing loss: 1.6064263582229614\n",
      "Accuracy: 0.4295\n",
      "epoch: 670, training loss: 1.2625956535339355, testing loss: 1.6355897188186646\n",
      "Accuracy: 0.4388\n",
      "epoch: 670, training loss: 1.2554038763046265, testing loss: 1.7176427841186523\n",
      "Accuracy: 0.3969\n",
      "epoch: 670, training loss: 1.2940373420715332, testing loss: 1.743039846420288\n",
      "Accuracy: 0.3906\n",
      "epoch: 670, training loss: 1.1699979305267334, testing loss: 1.6139131784439087\n",
      "Accuracy: 0.4713\n",
      "epoch: 670, training loss: 1.237086296081543, testing loss: 1.5860694646835327\n",
      "Accuracy: 0.4359\n",
      "epoch: 670, training loss: 1.2591888904571533, testing loss: 1.8544516563415527\n",
      "Accuracy: 0.3980\n",
      "epoch: 670, training loss: 1.179215431213379, testing loss: 1.7221940755844116\n",
      "Accuracy: 0.4196\n",
      "epoch: 680, training loss: 1.2307816743850708, testing loss: 1.7408666610717773\n",
      "Accuracy: 0.4464\n",
      "epoch: 680, training loss: 1.306310772895813, testing loss: 1.7435808181762695\n",
      "Accuracy: 0.4513\n",
      "epoch: 680, training loss: 1.1994295120239258, testing loss: 1.8924955129623413\n",
      "Accuracy: 0.3948\n",
      "epoch: 680, training loss: 1.2398591041564941, testing loss: 1.6875704526901245\n",
      "Accuracy: 0.4091\n",
      "epoch: 680, training loss: 1.229953646659851, testing loss: 1.6805086135864258\n",
      "Accuracy: 0.4357\n",
      "epoch: 680, training loss: 1.229677438735962, testing loss: 1.6860692501068115\n",
      "Accuracy: 0.4344\n",
      "epoch: 680, training loss: 1.2826613187789917, testing loss: 1.6877403259277344\n",
      "Accuracy: 0.4429\n",
      "epoch: 680, training loss: 1.2388509511947632, testing loss: 1.6785835027694702\n",
      "Accuracy: 0.3880\n",
      "epoch: 680, training loss: 1.23328697681427, testing loss: 1.7226189374923706\n",
      "Accuracy: 0.4242\n",
      "epoch: 680, training loss: 1.2545380592346191, testing loss: 1.7990710735321045\n",
      "Accuracy: 0.3734\n",
      "epoch: 680, training loss: 1.186033844947815, testing loss: 1.6786302328109741\n",
      "Accuracy: 0.4104\n",
      "epoch: 680, training loss: 1.1793752908706665, testing loss: 1.6807893514633179\n",
      "Accuracy: 0.3851\n",
      "epoch: 680, training loss: 1.2263984680175781, testing loss: 1.7022013664245605\n",
      "Accuracy: 0.4106\n",
      "epoch: 680, training loss: 1.2759736776351929, testing loss: 1.7595547437667847\n",
      "Accuracy: 0.3908\n",
      "epoch: 680, training loss: 1.2273011207580566, testing loss: 1.607258677482605\n",
      "Accuracy: 0.4516\n",
      "epoch: 680, training loss: 1.1856651306152344, testing loss: 1.874322772026062\n",
      "Accuracy: 0.4061\n",
      "epoch: 680, training loss: 1.234852910041809, testing loss: 1.829622507095337\n",
      "Accuracy: 0.4188\n",
      "epoch: 680, training loss: 1.1814626455307007, testing loss: 1.6220358610153198\n",
      "Accuracy: 0.4076\n",
      "epoch: 680, training loss: 1.2667142152786255, testing loss: 1.7210431098937988\n",
      "Accuracy: 0.4548\n",
      "epoch: 680, training loss: 1.2039124965667725, testing loss: 1.5898534059524536\n",
      "Accuracy: 0.4214\n",
      "epoch: 680, training loss: 1.3186922073364258, testing loss: 1.6252552270889282\n",
      "Accuracy: 0.4387\n",
      "epoch: 680, training loss: 1.205880045890808, testing loss: 1.643082857131958\n",
      "Accuracy: 0.4515\n",
      "epoch: 680, training loss: 1.3228275775909424, testing loss: 1.7018448114395142\n",
      "Accuracy: 0.4326\n",
      "epoch: 680, training loss: 1.2726778984069824, testing loss: 1.6496574878692627\n",
      "Accuracy: 0.4375\n",
      "epoch: 680, training loss: 1.2165035009384155, testing loss: 1.7462809085845947\n",
      "Accuracy: 0.4107\n",
      "epoch: 680, training loss: 1.2872549295425415, testing loss: 1.6625664234161377\n",
      "Accuracy: 0.4227\n",
      "epoch: 680, training loss: 1.2307084798812866, testing loss: 1.7189910411834717\n",
      "Accuracy: 0.4483\n",
      "epoch: 680, training loss: 1.2495495080947876, testing loss: 1.7043026685714722\n",
      "Accuracy: 0.3994\n",
      "epoch: 680, training loss: 1.239675760269165, testing loss: 1.5566890239715576\n",
      "Accuracy: 0.4662\n",
      "epoch: 680, training loss: 1.1968003511428833, testing loss: 1.7838363647460938\n",
      "Accuracy: 0.3878\n",
      "epoch: 680, training loss: 1.2732431888580322, testing loss: 1.7091647386550903\n",
      "Accuracy: 0.4346\n",
      "epoch: 680, training loss: 1.276761531829834, testing loss: 1.6737444400787354\n",
      "Accuracy: 0.4013\n",
      "epoch: 680, training loss: 1.2876906394958496, testing loss: 1.6674009561538696\n",
      "Accuracy: 0.3914\n",
      "epoch: 680, training loss: 1.2817237377166748, testing loss: 1.7530587911605835\n",
      "Accuracy: 0.3790\n",
      "epoch: 680, training loss: 1.2457778453826904, testing loss: 1.6753607988357544\n",
      "Accuracy: 0.4281\n",
      "epoch: 680, training loss: 1.2218276262283325, testing loss: 1.6443636417388916\n",
      "Accuracy: 0.4724\n",
      "epoch: 680, training loss: 1.2563588619232178, testing loss: 1.6778550148010254\n",
      "Accuracy: 0.4267\n",
      "epoch: 680, training loss: 1.247211217880249, testing loss: 1.7271616458892822\n",
      "Accuracy: 0.4094\n",
      "epoch: 680, training loss: 1.2313941717147827, testing loss: 1.6515814065933228\n",
      "Accuracy: 0.4521\n",
      "epoch: 680, training loss: 1.3029506206512451, testing loss: 1.7837181091308594\n",
      "Accuracy: 0.3925\n",
      "epoch: 680, training loss: 1.2278701066970825, testing loss: 1.7141413688659668\n",
      "Accuracy: 0.3993\n",
      "epoch: 680, training loss: 1.244185447692871, testing loss: 1.7310690879821777\n",
      "Accuracy: 0.4290\n",
      "epoch: 680, training loss: 1.1642093658447266, testing loss: 1.679260015487671\n",
      "Accuracy: 0.3925\n",
      "epoch: 680, training loss: 1.2445824146270752, testing loss: 1.7350752353668213\n",
      "Accuracy: 0.4286\n",
      "epoch: 680, training loss: 1.2882927656173706, testing loss: 1.7133699655532837\n",
      "Accuracy: 0.4448\n",
      "epoch: 680, training loss: 1.324066400527954, testing loss: 1.6799829006195068\n",
      "Accuracy: 0.3824\n",
      "epoch: 680, training loss: 1.2405805587768555, testing loss: 1.6459541320800781\n",
      "Accuracy: 0.4567\n",
      "epoch: 680, training loss: 1.243044137954712, testing loss: 1.752110481262207\n",
      "Accuracy: 0.4221\n",
      "epoch: 680, training loss: 1.2445002794265747, testing loss: 1.6166504621505737\n",
      "Accuracy: 0.4533\n",
      "epoch: 680, training loss: 1.2329461574554443, testing loss: 1.7391935586929321\n",
      "Accuracy: 0.4286\n",
      "epoch: 680, training loss: 1.1899336576461792, testing loss: 1.6265480518341064\n",
      "Accuracy: 0.4430\n",
      "epoch: 680, training loss: 1.2510801553726196, testing loss: 1.5968917608261108\n",
      "Accuracy: 0.4463\n",
      "epoch: 680, training loss: 1.2845830917358398, testing loss: 1.7836356163024902\n",
      "Accuracy: 0.3982\n",
      "epoch: 680, training loss: 1.3367747068405151, testing loss: 1.7733629941940308\n",
      "Accuracy: 0.3988\n",
      "epoch: 680, training loss: 1.2874342203140259, testing loss: 1.6593064069747925\n",
      "Accuracy: 0.4329\n",
      "epoch: 680, training loss: 1.261494755744934, testing loss: 1.6552116870880127\n",
      "Accuracy: 0.4330\n",
      "epoch: 680, training loss: 1.2782875299453735, testing loss: 1.767655849456787\n",
      "Accuracy: 0.3800\n",
      "epoch: 680, training loss: 1.2337058782577515, testing loss: 1.6929928064346313\n",
      "Accuracy: 0.4395\n",
      "epoch: 680, training loss: 1.2504358291625977, testing loss: 1.6610153913497925\n",
      "Accuracy: 0.4385\n",
      "epoch: 680, training loss: 1.2736233472824097, testing loss: 1.643876075744629\n",
      "Accuracy: 0.4320\n",
      "epoch: 680, training loss: 1.163966178894043, testing loss: 1.6268424987792969\n",
      "Accuracy: 0.4027\n",
      "epoch: 680, training loss: 1.2599892616271973, testing loss: 1.6544002294540405\n",
      "Accuracy: 0.4537\n",
      "epoch: 680, training loss: 1.2237428426742554, testing loss: 1.6505993604660034\n",
      "Accuracy: 0.4190\n",
      "epoch: 680, training loss: 1.2598111629486084, testing loss: 1.7194448709487915\n",
      "Accuracy: 0.4399\n",
      "epoch: 680, training loss: 1.276190161705017, testing loss: 1.7141852378845215\n",
      "Accuracy: 0.4401\n",
      "epoch: 680, training loss: 1.1956945657730103, testing loss: 1.7326791286468506\n",
      "Accuracy: 0.4264\n",
      "epoch: 680, training loss: 1.2031710147857666, testing loss: 1.7292429208755493\n",
      "Accuracy: 0.4286\n",
      "epoch: 680, training loss: 1.2896384000778198, testing loss: 1.670862078666687\n",
      "Accuracy: 0.4143\n",
      "epoch: 680, training loss: 1.2623050212860107, testing loss: 1.8206716775894165\n",
      "Accuracy: 0.4437\n",
      "epoch: 680, training loss: 1.2000975608825684, testing loss: 1.5624847412109375\n",
      "Accuracy: 0.4514\n",
      "epoch: 680, training loss: 1.2126249074935913, testing loss: 1.6160181760787964\n",
      "Accuracy: 0.4174\n",
      "epoch: 680, training loss: 1.2381854057312012, testing loss: 1.6802961826324463\n",
      "Accuracy: 0.4475\n",
      "epoch: 680, training loss: 1.2754316329956055, testing loss: 1.6521832942962646\n",
      "Accuracy: 0.4127\n",
      "epoch: 680, training loss: 1.161034345626831, testing loss: 1.7560235261917114\n",
      "Accuracy: 0.3829\n",
      "epoch: 680, training loss: 1.2445472478866577, testing loss: 1.745674967765808\n",
      "Accuracy: 0.4091\n",
      "epoch: 680, training loss: 1.194532871246338, testing loss: 1.7326383590698242\n",
      "Accuracy: 0.3881\n",
      "epoch: 680, training loss: 1.2465856075286865, testing loss: 1.7926852703094482\n",
      "Accuracy: 0.3857\n",
      "epoch: 680, training loss: 1.2846814393997192, testing loss: 1.602752447128296\n",
      "Accuracy: 0.4406\n",
      "epoch: 680, training loss: 1.250341773033142, testing loss: 1.6980738639831543\n",
      "Accuracy: 0.4389\n",
      "epoch: 680, training loss: 1.2334734201431274, testing loss: 1.6919853687286377\n",
      "Accuracy: 0.4224\n",
      "epoch: 680, training loss: 1.223552942276001, testing loss: 1.7521998882293701\n",
      "Accuracy: 0.4042\n",
      "epoch: 680, training loss: 1.2324786186218262, testing loss: 1.612009882926941\n",
      "Accuracy: 0.4240\n",
      "epoch: 680, training loss: 1.2993236780166626, testing loss: 1.781299114227295\n",
      "Accuracy: 0.4316\n",
      "epoch: 680, training loss: 1.2453505992889404, testing loss: 1.7308318614959717\n",
      "Accuracy: 0.3981\n",
      "epoch: 680, training loss: 1.192036747932434, testing loss: 1.8571126461029053\n",
      "Accuracy: 0.4000\n",
      "epoch: 680, training loss: 1.2539222240447998, testing loss: 1.819252610206604\n",
      "Accuracy: 0.3754\n",
      "epoch: 680, training loss: 1.2168694734573364, testing loss: 1.6209418773651123\n",
      "Accuracy: 0.4532\n",
      "epoch: 680, training loss: 1.2140456438064575, testing loss: 1.6038240194320679\n",
      "Accuracy: 0.4536\n",
      "epoch: 680, training loss: 1.1838256120681763, testing loss: 1.777695655822754\n",
      "Accuracy: 0.3684\n",
      "epoch: 680, training loss: 1.2178113460540771, testing loss: 1.7364634275436401\n",
      "Accuracy: 0.3912\n",
      "epoch: 680, training loss: 1.2713161706924438, testing loss: 1.8194859027862549\n",
      "Accuracy: 0.4470\n",
      "epoch: 680, training loss: 1.2620631456375122, testing loss: 1.819164514541626\n",
      "Accuracy: 0.4000\n",
      "epoch: 680, training loss: 1.2125009298324585, testing loss: 1.75371253490448\n",
      "Accuracy: 0.4437\n",
      "epoch: 680, training loss: 1.2282084226608276, testing loss: 1.6826497316360474\n",
      "Accuracy: 0.3912\n",
      "epoch: 680, training loss: 1.334383249282837, testing loss: 1.7362028360366821\n",
      "Accuracy: 0.4184\n",
      "epoch: 680, training loss: 1.283675193786621, testing loss: 1.7995147705078125\n",
      "Accuracy: 0.3656\n",
      "epoch: 680, training loss: 1.191648006439209, testing loss: 1.7080892324447632\n",
      "Accuracy: 0.4006\n",
      "epoch: 680, training loss: 1.129296064376831, testing loss: 1.7525384426116943\n",
      "Accuracy: 0.4264\n",
      "epoch: 680, training loss: 1.2648475170135498, testing loss: 1.6389622688293457\n",
      "Accuracy: 0.4112\n",
      "epoch: 680, training loss: 1.2053381204605103, testing loss: 1.7338240146636963\n",
      "Accuracy: 0.3868\n",
      "epoch: 690, training loss: 1.3815103769302368, testing loss: 1.671324372291565\n",
      "Accuracy: 0.4114\n",
      "epoch: 690, training loss: 1.2651666402816772, testing loss: 1.5937471389770508\n",
      "Accuracy: 0.4178\n",
      "epoch: 690, training loss: 1.2709851264953613, testing loss: 1.6848605871200562\n",
      "Accuracy: 0.4430\n",
      "epoch: 690, training loss: 1.3250035047531128, testing loss: 1.6516000032424927\n",
      "Accuracy: 0.3824\n",
      "epoch: 690, training loss: 1.2690106630325317, testing loss: 1.81862473487854\n",
      "Accuracy: 0.3692\n",
      "epoch: 690, training loss: 1.2380859851837158, testing loss: 1.673354148864746\n",
      "Accuracy: 0.4381\n",
      "epoch: 690, training loss: 1.230142593383789, testing loss: 1.5764857530593872\n",
      "Accuracy: 0.4229\n",
      "epoch: 690, training loss: 1.2569327354431152, testing loss: 1.8659533262252808\n",
      "Accuracy: 0.3663\n",
      "epoch: 690, training loss: 1.2981064319610596, testing loss: 1.6246155500411987\n",
      "Accuracy: 0.4247\n",
      "epoch: 690, training loss: 1.2554787397384644, testing loss: 1.5585211515426636\n",
      "Accuracy: 0.4416\n",
      "epoch: 690, training loss: 1.2922910451889038, testing loss: 1.642834186553955\n",
      "Accuracy: 0.4373\n",
      "epoch: 690, training loss: 1.288817048072815, testing loss: 1.727227807044983\n",
      "Accuracy: 0.3903\n",
      "epoch: 690, training loss: 1.2785354852676392, testing loss: 1.841287612915039\n",
      "Accuracy: 0.3838\n",
      "epoch: 690, training loss: 1.2954741716384888, testing loss: 1.7672401666641235\n",
      "Accuracy: 0.4413\n",
      "epoch: 690, training loss: 1.2979011535644531, testing loss: 1.694899082183838\n",
      "Accuracy: 0.4046\n",
      "epoch: 690, training loss: 1.2534912824630737, testing loss: 1.6520642042160034\n",
      "Accuracy: 0.4354\n",
      "epoch: 690, training loss: 1.1729460954666138, testing loss: 1.8064602613449097\n",
      "Accuracy: 0.3430\n",
      "epoch: 690, training loss: 1.2466802597045898, testing loss: 1.6968050003051758\n",
      "Accuracy: 0.3829\n",
      "epoch: 690, training loss: 1.2204716205596924, testing loss: 1.7580444812774658\n",
      "Accuracy: 0.3738\n",
      "epoch: 690, training loss: 1.2320035696029663, testing loss: 1.6650991439819336\n",
      "Accuracy: 0.4048\n",
      "epoch: 690, training loss: 1.3068486452102661, testing loss: 1.656482219696045\n",
      "Accuracy: 0.4566\n",
      "epoch: 690, training loss: 1.2221399545669556, testing loss: 1.7060459852218628\n",
      "Accuracy: 0.3844\n",
      "epoch: 690, training loss: 1.2106382846832275, testing loss: 1.693650245666504\n",
      "Accuracy: 0.4277\n",
      "epoch: 690, training loss: 1.200498104095459, testing loss: 1.8010427951812744\n",
      "Accuracy: 0.3718\n",
      "epoch: 690, training loss: 1.1729369163513184, testing loss: 1.751678228378296\n",
      "Accuracy: 0.3905\n",
      "epoch: 690, training loss: 1.2409429550170898, testing loss: 1.6915086507797241\n",
      "Accuracy: 0.4216\n",
      "epoch: 690, training loss: 1.2484993934631348, testing loss: 1.70802903175354\n",
      "Accuracy: 0.3968\n",
      "epoch: 690, training loss: 1.1921424865722656, testing loss: 1.7343355417251587\n",
      "Accuracy: 0.4103\n",
      "epoch: 690, training loss: 1.228045105934143, testing loss: 1.6557663679122925\n",
      "Accuracy: 0.4075\n",
      "epoch: 690, training loss: 1.2367905378341675, testing loss: 1.6908965110778809\n",
      "Accuracy: 0.4143\n",
      "epoch: 690, training loss: 1.2725461721420288, testing loss: 1.7175791263580322\n",
      "Accuracy: 0.4172\n",
      "epoch: 690, training loss: 1.291675090789795, testing loss: 1.7566261291503906\n",
      "Accuracy: 0.3906\n",
      "epoch: 690, training loss: 1.2287225723266602, testing loss: 1.8492695093154907\n",
      "Accuracy: 0.3938\n",
      "epoch: 690, training loss: 1.1802802085876465, testing loss: 1.8367919921875\n",
      "Accuracy: 0.4263\n",
      "epoch: 690, training loss: 1.258941650390625, testing loss: 1.6391092538833618\n",
      "Accuracy: 0.4415\n",
      "epoch: 690, training loss: 1.250182867050171, testing loss: 1.7369859218597412\n",
      "Accuracy: 0.4122\n",
      "epoch: 690, training loss: 1.2573158740997314, testing loss: 1.8056973218917847\n",
      "Accuracy: 0.3910\n",
      "epoch: 690, training loss: 1.2482882738113403, testing loss: 1.7244940996170044\n",
      "Accuracy: 0.3986\n",
      "epoch: 690, training loss: 1.2901054620742798, testing loss: 1.716354250907898\n",
      "Accuracy: 0.4101\n",
      "epoch: 690, training loss: 1.2766704559326172, testing loss: 1.7001640796661377\n",
      "Accuracy: 0.4220\n",
      "epoch: 690, training loss: 1.2490676641464233, testing loss: 1.7004313468933105\n",
      "Accuracy: 0.3721\n",
      "epoch: 690, training loss: 1.1957509517669678, testing loss: 1.7337865829467773\n",
      "Accuracy: 0.4322\n",
      "epoch: 690, training loss: 1.288625717163086, testing loss: 1.6603599786758423\n",
      "Accuracy: 0.4459\n",
      "epoch: 690, training loss: 1.200098991394043, testing loss: 1.6416138410568237\n",
      "Accuracy: 0.4172\n",
      "epoch: 690, training loss: 1.216895341873169, testing loss: 1.7671726942062378\n",
      "Accuracy: 0.3820\n",
      "epoch: 690, training loss: 1.1943190097808838, testing loss: 1.6467912197113037\n",
      "Accuracy: 0.4190\n",
      "epoch: 690, training loss: 1.2377253770828247, testing loss: 1.7206909656524658\n",
      "Accuracy: 0.4222\n",
      "epoch: 690, training loss: 1.245673656463623, testing loss: 1.559744119644165\n",
      "Accuracy: 0.4333\n",
      "epoch: 690, training loss: 1.266135811805725, testing loss: 1.851405143737793\n",
      "Accuracy: 0.3912\n",
      "epoch: 690, training loss: 1.2173347473144531, testing loss: 1.66372549533844\n",
      "Accuracy: 0.4625\n",
      "epoch: 690, training loss: 1.178051471710205, testing loss: 1.6871891021728516\n",
      "Accuracy: 0.3987\n",
      "epoch: 690, training loss: 1.341032862663269, testing loss: 1.7250251770019531\n",
      "Accuracy: 0.4000\n",
      "epoch: 690, training loss: 1.2962621450424194, testing loss: 1.714072346687317\n",
      "Accuracy: 0.4116\n",
      "epoch: 690, training loss: 1.2679485082626343, testing loss: 1.6401311159133911\n",
      "Accuracy: 0.4528\n",
      "epoch: 690, training loss: 1.1602369546890259, testing loss: 1.7881520986557007\n",
      "Accuracy: 0.4740\n",
      "epoch: 690, training loss: 1.2307058572769165, testing loss: 1.7284880876541138\n",
      "Accuracy: 0.4327\n",
      "epoch: 690, training loss: 1.247793436050415, testing loss: 1.7548075914382935\n",
      "Accuracy: 0.4138\n",
      "epoch: 690, training loss: 1.2659239768981934, testing loss: 1.7471654415130615\n",
      "Accuracy: 0.4373\n",
      "epoch: 690, training loss: 1.1917662620544434, testing loss: 1.586467981338501\n",
      "Accuracy: 0.4072\n",
      "epoch: 690, training loss: 1.1996898651123047, testing loss: 1.7357779741287231\n",
      "Accuracy: 0.4108\n",
      "epoch: 690, training loss: 1.2268378734588623, testing loss: 1.82112717628479\n",
      "Accuracy: 0.3691\n",
      "epoch: 690, training loss: 1.2019383907318115, testing loss: 1.767930030822754\n",
      "Accuracy: 0.4020\n",
      "epoch: 690, training loss: 1.2192473411560059, testing loss: 1.6488333940505981\n",
      "Accuracy: 0.4339\n",
      "epoch: 690, training loss: 1.1937856674194336, testing loss: 1.6565109491348267\n",
      "Accuracy: 0.4328\n",
      "epoch: 690, training loss: 1.2518115043640137, testing loss: 1.6327989101409912\n",
      "Accuracy: 0.4190\n",
      "epoch: 690, training loss: 1.2386088371276855, testing loss: 1.8058948516845703\n",
      "Accuracy: 0.3977\n",
      "epoch: 690, training loss: 1.2482486963272095, testing loss: 1.7963743209838867\n",
      "Accuracy: 0.4235\n",
      "epoch: 690, training loss: 1.2573933601379395, testing loss: 1.7752808332443237\n",
      "Accuracy: 0.3553\n",
      "epoch: 690, training loss: 1.2112617492675781, testing loss: 1.8177326917648315\n",
      "Accuracy: 0.3968\n",
      "epoch: 690, training loss: 1.1929115056991577, testing loss: 1.7337483167648315\n",
      "Accuracy: 0.3876\n",
      "epoch: 690, training loss: 1.2461419105529785, testing loss: 1.6509592533111572\n",
      "Accuracy: 0.4092\n",
      "epoch: 690, training loss: 1.1785993576049805, testing loss: 1.5709835290908813\n",
      "Accuracy: 0.5034\n",
      "epoch: 690, training loss: 1.2586030960083008, testing loss: 1.784096360206604\n",
      "Accuracy: 0.4043\n",
      "epoch: 690, training loss: 1.2644697427749634, testing loss: 1.6707569360733032\n",
      "Accuracy: 0.4145\n",
      "epoch: 690, training loss: 1.2038393020629883, testing loss: 1.7240334749221802\n",
      "Accuracy: 0.4054\n",
      "epoch: 690, training loss: 1.2823224067687988, testing loss: 1.6701276302337646\n",
      "Accuracy: 0.4613\n",
      "epoch: 690, training loss: 1.2141660451889038, testing loss: 1.7839325666427612\n",
      "Accuracy: 0.4046\n",
      "epoch: 690, training loss: 1.2797746658325195, testing loss: 1.6711702346801758\n",
      "Accuracy: 0.4155\n",
      "epoch: 690, training loss: 1.2140344381332397, testing loss: 1.860777735710144\n",
      "Accuracy: 0.4059\n",
      "epoch: 690, training loss: 1.2415895462036133, testing loss: 1.7707576751708984\n",
      "Accuracy: 0.4103\n",
      "epoch: 690, training loss: 1.2151005268096924, testing loss: 1.8352161645889282\n",
      "Accuracy: 0.3600\n",
      "epoch: 690, training loss: 1.27536940574646, testing loss: 1.6212704181671143\n",
      "Accuracy: 0.4212\n",
      "epoch: 690, training loss: 1.2268952131271362, testing loss: 1.7052539587020874\n",
      "Accuracy: 0.4563\n",
      "epoch: 690, training loss: 1.2348575592041016, testing loss: 1.705116629600525\n",
      "Accuracy: 0.4164\n",
      "epoch: 690, training loss: 1.2343441247940063, testing loss: 1.7486234903335571\n",
      "Accuracy: 0.3931\n",
      "epoch: 690, training loss: 1.1656466722488403, testing loss: 1.6475902795791626\n",
      "Accuracy: 0.4233\n",
      "epoch: 690, training loss: 1.2834235429763794, testing loss: 1.7232794761657715\n",
      "Accuracy: 0.4633\n",
      "epoch: 690, training loss: 1.3544329404830933, testing loss: 1.6347850561141968\n",
      "Accuracy: 0.4348\n",
      "epoch: 690, training loss: 1.2123090028762817, testing loss: 1.629388451576233\n",
      "Accuracy: 0.4820\n",
      "epoch: 690, training loss: 1.2596787214279175, testing loss: 1.734289526939392\n",
      "Accuracy: 0.3838\n",
      "epoch: 690, training loss: 1.2358002662658691, testing loss: 1.6553380489349365\n",
      "Accuracy: 0.3977\n",
      "epoch: 690, training loss: 1.2998827695846558, testing loss: 1.6452983617782593\n",
      "Accuracy: 0.3902\n",
      "epoch: 690, training loss: 1.2443445920944214, testing loss: 1.7643394470214844\n",
      "Accuracy: 0.4259\n",
      "epoch: 690, training loss: 1.1764895915985107, testing loss: 1.6966419219970703\n",
      "Accuracy: 0.3703\n",
      "epoch: 690, training loss: 1.2369194030761719, testing loss: 1.5544097423553467\n",
      "Accuracy: 0.4401\n",
      "epoch: 690, training loss: 1.2572706937789917, testing loss: 1.7640938758850098\n",
      "Accuracy: 0.4360\n",
      "epoch: 690, training loss: 1.2467831373214722, testing loss: 1.6351406574249268\n",
      "Accuracy: 0.4409\n",
      "epoch: 690, training loss: 1.2150033712387085, testing loss: 1.7527592182159424\n",
      "Accuracy: 0.3932\n",
      "epoch: 690, training loss: 1.2384912967681885, testing loss: 1.7414087057113647\n",
      "Accuracy: 0.4234\n",
      "epoch: 690, training loss: 1.267734169960022, testing loss: 1.6777838468551636\n",
      "Accuracy: 0.4235\n",
      "epoch: 700, training loss: 1.2707228660583496, testing loss: 1.7295048236846924\n",
      "Accuracy: 0.4273\n",
      "epoch: 700, training loss: 1.2692078351974487, testing loss: 1.604078769683838\n",
      "Accuracy: 0.4746\n",
      "epoch: 700, training loss: 1.2181700468063354, testing loss: 1.911377191543579\n",
      "Accuracy: 0.4448\n",
      "epoch: 700, training loss: 1.2427037954330444, testing loss: 1.6563113927841187\n",
      "Accuracy: 0.4381\n",
      "epoch: 700, training loss: 1.2046157121658325, testing loss: 1.7598474025726318\n",
      "Accuracy: 0.3986\n",
      "epoch: 700, training loss: 1.2634931802749634, testing loss: 1.687430739402771\n",
      "Accuracy: 0.3871\n",
      "epoch: 700, training loss: 1.2302101850509644, testing loss: 1.6522496938705444\n",
      "Accuracy: 0.4444\n",
      "epoch: 700, training loss: 1.2757329940795898, testing loss: 1.7809618711471558\n",
      "Accuracy: 0.4529\n",
      "epoch: 700, training loss: 1.2481331825256348, testing loss: 1.7862956523895264\n",
      "Accuracy: 0.4114\n",
      "epoch: 700, training loss: 1.3075813055038452, testing loss: 1.70566725730896\n",
      "Accuracy: 0.3923\n",
      "epoch: 700, training loss: 1.2852766513824463, testing loss: 1.667898178100586\n",
      "Accuracy: 0.4536\n",
      "epoch: 700, training loss: 1.2481939792633057, testing loss: 1.7675565481185913\n",
      "Accuracy: 0.4142\n",
      "epoch: 700, training loss: 1.2555006742477417, testing loss: 1.6784855127334595\n",
      "Accuracy: 0.4233\n",
      "epoch: 700, training loss: 1.239701271057129, testing loss: 1.7067039012908936\n",
      "Accuracy: 0.3840\n",
      "epoch: 700, training loss: 1.2607605457305908, testing loss: 1.7670835256576538\n",
      "Accuracy: 0.4128\n",
      "epoch: 700, training loss: 1.1885429620742798, testing loss: 1.7645336389541626\n",
      "Accuracy: 0.3801\n",
      "epoch: 700, training loss: 1.2480367422103882, testing loss: 1.5415900945663452\n",
      "Accuracy: 0.4759\n",
      "epoch: 700, training loss: 1.2082014083862305, testing loss: 1.6394329071044922\n",
      "Accuracy: 0.4652\n",
      "epoch: 700, training loss: 1.2454315423965454, testing loss: 1.84005606174469\n",
      "Accuracy: 0.3746\n",
      "epoch: 700, training loss: 1.2752588987350464, testing loss: 1.7229957580566406\n",
      "Accuracy: 0.4334\n",
      "epoch: 700, training loss: 1.1986427307128906, testing loss: 1.6007126569747925\n",
      "Accuracy: 0.4451\n",
      "epoch: 700, training loss: 1.278922200202942, testing loss: 1.722746729850769\n",
      "Accuracy: 0.4600\n",
      "epoch: 700, training loss: 1.2736365795135498, testing loss: 1.7660579681396484\n",
      "Accuracy: 0.4130\n",
      "epoch: 700, training loss: 1.2724562883377075, testing loss: 1.7480762004852295\n",
      "Accuracy: 0.3785\n",
      "epoch: 700, training loss: 1.2884058952331543, testing loss: 1.6989374160766602\n",
      "Accuracy: 0.4358\n",
      "epoch: 700, training loss: 1.2901846170425415, testing loss: 1.7955729961395264\n",
      "Accuracy: 0.3969\n",
      "epoch: 700, training loss: 1.3025037050247192, testing loss: 1.7155436277389526\n",
      "Accuracy: 0.3851\n",
      "epoch: 700, training loss: 1.1916028261184692, testing loss: 1.6628869771957397\n",
      "Accuracy: 0.4158\n",
      "epoch: 700, training loss: 1.2962195873260498, testing loss: 1.7182319164276123\n",
      "Accuracy: 0.3773\n",
      "epoch: 700, training loss: 1.2653743028640747, testing loss: 1.6614936590194702\n",
      "Accuracy: 0.4018\n",
      "epoch: 700, training loss: 1.209578514099121, testing loss: 1.8170286417007446\n",
      "Accuracy: 0.3651\n",
      "epoch: 700, training loss: 1.2708611488342285, testing loss: 1.7806627750396729\n",
      "Accuracy: 0.3920\n",
      "epoch: 700, training loss: 1.2169945240020752, testing loss: 1.746514916419983\n",
      "Accuracy: 0.3762\n",
      "epoch: 700, training loss: 1.1952483654022217, testing loss: 1.671160340309143\n",
      "Accuracy: 0.4102\n",
      "epoch: 700, training loss: 1.2266173362731934, testing loss: 1.6555792093276978\n",
      "Accuracy: 0.4268\n",
      "epoch: 700, training loss: 1.262642502784729, testing loss: 1.7763208150863647\n",
      "Accuracy: 0.4084\n",
      "epoch: 700, training loss: 1.3031829595565796, testing loss: 1.804792881011963\n",
      "Accuracy: 0.3816\n",
      "epoch: 700, training loss: 1.1955493688583374, testing loss: 1.7182000875473022\n",
      "Accuracy: 0.4241\n",
      "epoch: 700, training loss: 1.2736011743545532, testing loss: 1.8501890897750854\n",
      "Accuracy: 0.3849\n",
      "epoch: 700, training loss: 1.2415902614593506, testing loss: 1.673389196395874\n",
      "Accuracy: 0.4463\n",
      "epoch: 700, training loss: 1.2277069091796875, testing loss: 1.7788212299346924\n",
      "Accuracy: 0.4498\n",
      "epoch: 700, training loss: 1.2548643350601196, testing loss: 1.6736122369766235\n",
      "Accuracy: 0.4144\n",
      "epoch: 700, training loss: 1.2060933113098145, testing loss: 1.727754831314087\n",
      "Accuracy: 0.3910\n",
      "epoch: 700, training loss: 1.2030260562896729, testing loss: 1.7273775339126587\n",
      "Accuracy: 0.4563\n",
      "epoch: 700, training loss: 1.2239344120025635, testing loss: 1.6200473308563232\n",
      "Accuracy: 0.4507\n",
      "epoch: 700, training loss: 1.2658650875091553, testing loss: 1.760404348373413\n",
      "Accuracy: 0.4263\n",
      "epoch: 700, training loss: 1.3248076438903809, testing loss: 1.7444000244140625\n",
      "Accuracy: 0.4053\n",
      "epoch: 700, training loss: 1.2314317226409912, testing loss: 1.8926670551300049\n",
      "Accuracy: 0.3431\n",
      "epoch: 700, training loss: 1.2156109809875488, testing loss: 1.766808271408081\n",
      "Accuracy: 0.4345\n",
      "epoch: 700, training loss: 1.2683308124542236, testing loss: 1.6666772365570068\n",
      "Accuracy: 0.4753\n",
      "epoch: 700, training loss: 1.2126386165618896, testing loss: 1.7528910636901855\n",
      "Accuracy: 0.4153\n",
      "epoch: 700, training loss: 1.2016781568527222, testing loss: 1.6021194458007812\n",
      "Accuracy: 0.4701\n",
      "epoch: 700, training loss: 1.2679283618927002, testing loss: 1.6684356927871704\n",
      "Accuracy: 0.4317\n",
      "epoch: 700, training loss: 1.2046966552734375, testing loss: 1.7716165781021118\n",
      "Accuracy: 0.4305\n",
      "epoch: 700, training loss: 1.1906712055206299, testing loss: 1.722041130065918\n",
      "Accuracy: 0.4025\n",
      "epoch: 700, training loss: 1.2592005729675293, testing loss: 1.8783293962478638\n",
      "Accuracy: 0.3588\n",
      "epoch: 700, training loss: 1.3069877624511719, testing loss: 1.793119192123413\n",
      "Accuracy: 0.4000\n",
      "epoch: 700, training loss: 1.299103856086731, testing loss: 1.7755036354064941\n",
      "Accuracy: 0.4196\n",
      "epoch: 700, training loss: 1.2522631883621216, testing loss: 1.6923611164093018\n",
      "Accuracy: 0.4487\n",
      "epoch: 700, training loss: 1.2696951627731323, testing loss: 1.67070734500885\n",
      "Accuracy: 0.3682\n",
      "epoch: 700, training loss: 1.2288353443145752, testing loss: 1.6776586771011353\n",
      "Accuracy: 0.4369\n",
      "epoch: 700, training loss: 1.2135010957717896, testing loss: 1.7025071382522583\n",
      "Accuracy: 0.4515\n",
      "epoch: 700, training loss: 1.2459664344787598, testing loss: 1.70561945438385\n",
      "Accuracy: 0.4299\n",
      "epoch: 700, training loss: 1.2264024019241333, testing loss: 1.6566205024719238\n",
      "Accuracy: 0.4448\n",
      "epoch: 700, training loss: 1.234528660774231, testing loss: 1.6960147619247437\n",
      "Accuracy: 0.4448\n",
      "epoch: 700, training loss: 1.2231816053390503, testing loss: 1.7316160202026367\n",
      "Accuracy: 0.4276\n",
      "epoch: 700, training loss: 1.2170708179473877, testing loss: 1.8599061965942383\n",
      "Accuracy: 0.4177\n",
      "epoch: 700, training loss: 1.1629663705825806, testing loss: 1.714118242263794\n",
      "Accuracy: 0.4053\n",
      "epoch: 700, training loss: 1.2267463207244873, testing loss: 1.7423017024993896\n",
      "Accuracy: 0.4161\n",
      "epoch: 700, training loss: 1.2343404293060303, testing loss: 1.76951003074646\n",
      "Accuracy: 0.4504\n",
      "epoch: 700, training loss: 1.2213040590286255, testing loss: 1.7136303186416626\n",
      "Accuracy: 0.4238\n",
      "epoch: 700, training loss: 1.2394486665725708, testing loss: 1.7039544582366943\n",
      "Accuracy: 0.4656\n",
      "epoch: 700, training loss: 1.2431527376174927, testing loss: 1.7017732858657837\n",
      "Accuracy: 0.4103\n",
      "epoch: 700, training loss: 1.1673673391342163, testing loss: 1.7092714309692383\n",
      "Accuracy: 0.4641\n",
      "epoch: 700, training loss: 1.2977299690246582, testing loss: 1.7562955617904663\n",
      "Accuracy: 0.3426\n",
      "epoch: 700, training loss: 1.2806050777435303, testing loss: 1.8041669130325317\n",
      "Accuracy: 0.3691\n",
      "epoch: 700, training loss: 1.173651099205017, testing loss: 1.5946580171585083\n",
      "Accuracy: 0.4469\n",
      "epoch: 700, training loss: 1.2618720531463623, testing loss: 1.6944777965545654\n",
      "Accuracy: 0.4608\n",
      "epoch: 700, training loss: 1.2615649700164795, testing loss: 1.6825106143951416\n",
      "Accuracy: 0.4286\n",
      "epoch: 700, training loss: 1.263869047164917, testing loss: 1.8388265371322632\n",
      "Accuracy: 0.4091\n",
      "epoch: 700, training loss: 1.272422432899475, testing loss: 1.6967154741287231\n",
      "Accuracy: 0.4072\n",
      "epoch: 700, training loss: 1.257282018661499, testing loss: 1.675934910774231\n",
      "Accuracy: 0.4441\n",
      "epoch: 700, training loss: 1.2731324434280396, testing loss: 1.7592920064926147\n",
      "Accuracy: 0.3689\n",
      "epoch: 700, training loss: 1.2412208318710327, testing loss: 1.7349364757537842\n",
      "Accuracy: 0.3825\n",
      "epoch: 700, training loss: 1.233276128768921, testing loss: 1.7757371664047241\n",
      "Accuracy: 0.3717\n",
      "epoch: 700, training loss: 1.2471134662628174, testing loss: 1.6276344060897827\n",
      "Accuracy: 0.4327\n",
      "epoch: 700, training loss: 1.1849123239517212, testing loss: 1.802701473236084\n",
      "Accuracy: 0.3587\n",
      "epoch: 700, training loss: 1.288489818572998, testing loss: 1.615639090538025\n",
      "Accuracy: 0.4534\n",
      "epoch: 700, training loss: 1.247279405593872, testing loss: 1.8378982543945312\n",
      "Accuracy: 0.3686\n",
      "epoch: 700, training loss: 1.2686868906021118, testing loss: 1.7848950624465942\n",
      "Accuracy: 0.4087\n",
      "epoch: 700, training loss: 1.237114429473877, testing loss: 1.9109266996383667\n",
      "Accuracy: 0.4033\n",
      "epoch: 700, training loss: 1.2793550491333008, testing loss: 1.8223049640655518\n",
      "Accuracy: 0.4098\n",
      "epoch: 700, training loss: 1.2087739706039429, testing loss: 1.8140345811843872\n",
      "Accuracy: 0.3822\n",
      "epoch: 700, training loss: 1.2168937921524048, testing loss: 1.8289639949798584\n",
      "Accuracy: 0.3742\n",
      "epoch: 700, training loss: 1.271442174911499, testing loss: 1.755175232887268\n",
      "Accuracy: 0.3974\n",
      "epoch: 700, training loss: 1.2235172986984253, testing loss: 1.7253919839859009\n",
      "Accuracy: 0.4339\n",
      "epoch: 700, training loss: 1.269503116607666, testing loss: 1.8197643756866455\n",
      "Accuracy: 0.4338\n",
      "epoch: 700, training loss: 1.214651346206665, testing loss: 1.7101469039916992\n",
      "Accuracy: 0.3968\n",
      "epoch: 700, training loss: 1.1805732250213623, testing loss: 1.8265444040298462\n",
      "Accuracy: 0.3730\n",
      "epoch: 700, training loss: 1.2768356800079346, testing loss: 1.7731879949569702\n",
      "Accuracy: 0.4189\n",
      "epoch: 710, training loss: 1.2215852737426758, testing loss: 1.6773169040679932\n",
      "Accuracy: 0.4056\n",
      "epoch: 710, training loss: 1.1853638887405396, testing loss: 1.7457964420318604\n",
      "Accuracy: 0.4226\n",
      "epoch: 710, training loss: 1.24225914478302, testing loss: 1.7203007936477661\n",
      "Accuracy: 0.3887\n",
      "epoch: 710, training loss: 1.218711018562317, testing loss: 1.7288421392440796\n",
      "Accuracy: 0.4095\n",
      "epoch: 710, training loss: 1.2638664245605469, testing loss: 1.630463719367981\n",
      "Accuracy: 0.4406\n",
      "epoch: 710, training loss: 1.3514412641525269, testing loss: 1.6688913106918335\n",
      "Accuracy: 0.3894\n",
      "epoch: 710, training loss: 1.2799582481384277, testing loss: 1.6823076009750366\n",
      "Accuracy: 0.4533\n",
      "epoch: 710, training loss: 1.1405073404312134, testing loss: 1.684342622756958\n",
      "Accuracy: 0.4494\n",
      "epoch: 710, training loss: 1.2841777801513672, testing loss: 1.6911202669143677\n",
      "Accuracy: 0.4214\n",
      "epoch: 710, training loss: 1.248531460762024, testing loss: 1.6039625406265259\n",
      "Accuracy: 0.4064\n",
      "epoch: 710, training loss: 1.223993182182312, testing loss: 1.6090980768203735\n",
      "Accuracy: 0.4207\n",
      "epoch: 710, training loss: 1.2686818838119507, testing loss: 1.6712716817855835\n",
      "Accuracy: 0.4488\n",
      "epoch: 710, training loss: 1.244937777519226, testing loss: 1.56919527053833\n",
      "Accuracy: 0.4286\n",
      "epoch: 710, training loss: 1.2372043132781982, testing loss: 1.6993348598480225\n",
      "Accuracy: 0.4069\n",
      "epoch: 710, training loss: 1.2715696096420288, testing loss: 1.6519091129302979\n",
      "Accuracy: 0.4695\n",
      "epoch: 710, training loss: 1.2417418956756592, testing loss: 1.7785519361495972\n",
      "Accuracy: 0.3945\n",
      "epoch: 710, training loss: 1.2903228998184204, testing loss: 1.7937959432601929\n",
      "Accuracy: 0.3817\n",
      "epoch: 710, training loss: 1.2521817684173584, testing loss: 1.8681347370147705\n",
      "Accuracy: 0.4355\n",
      "epoch: 710, training loss: 1.2366619110107422, testing loss: 1.615957260131836\n",
      "Accuracy: 0.4460\n",
      "epoch: 710, training loss: 1.2390066385269165, testing loss: 1.7361692190170288\n",
      "Accuracy: 0.4102\n",
      "epoch: 710, training loss: 1.2434407472610474, testing loss: 1.6651332378387451\n",
      "Accuracy: 0.4238\n",
      "epoch: 710, training loss: 1.2822167873382568, testing loss: 1.747470498085022\n",
      "Accuracy: 0.3994\n",
      "epoch: 710, training loss: 1.2577742338180542, testing loss: 1.8476775884628296\n",
      "Accuracy: 0.4018\n",
      "epoch: 710, training loss: 1.2338839769363403, testing loss: 1.7976064682006836\n",
      "Accuracy: 0.4032\n",
      "epoch: 710, training loss: 1.224422574043274, testing loss: 1.7510215044021606\n",
      "Accuracy: 0.4135\n",
      "epoch: 710, training loss: 1.2093982696533203, testing loss: 1.7150899171829224\n",
      "Accuracy: 0.3818\n",
      "epoch: 710, training loss: 1.1998857259750366, testing loss: 1.7286815643310547\n",
      "Accuracy: 0.4350\n",
      "epoch: 710, training loss: 1.249330997467041, testing loss: 1.9627697467803955\n",
      "Accuracy: 0.3550\n",
      "epoch: 710, training loss: 1.2554603815078735, testing loss: 1.7378312349319458\n",
      "Accuracy: 0.4184\n",
      "epoch: 710, training loss: 1.1599541902542114, testing loss: 1.8428196907043457\n",
      "Accuracy: 0.3844\n",
      "epoch: 710, training loss: 1.2542527914047241, testing loss: 1.8296841382980347\n",
      "Accuracy: 0.3854\n",
      "epoch: 710, training loss: 1.1989623308181763, testing loss: 1.7243640422821045\n",
      "Accuracy: 0.4019\n",
      "epoch: 710, training loss: 1.2453172206878662, testing loss: 1.6676725149154663\n",
      "Accuracy: 0.4216\n",
      "epoch: 710, training loss: 1.2656527757644653, testing loss: 1.6697150468826294\n",
      "Accuracy: 0.4402\n",
      "epoch: 710, training loss: 1.1854054927825928, testing loss: 1.7227157354354858\n",
      "Accuracy: 0.4007\n",
      "epoch: 710, training loss: 1.2504292726516724, testing loss: 1.8157322406768799\n",
      "Accuracy: 0.3964\n",
      "epoch: 710, training loss: 1.2633243799209595, testing loss: 1.7267215251922607\n",
      "Accuracy: 0.4379\n",
      "epoch: 710, training loss: 1.2966636419296265, testing loss: 1.8024976253509521\n",
      "Accuracy: 0.4000\n",
      "epoch: 710, training loss: 1.291795253753662, testing loss: 1.7239729166030884\n",
      "Accuracy: 0.4000\n",
      "epoch: 710, training loss: 1.2250463962554932, testing loss: 1.8330820798873901\n",
      "Accuracy: 0.3967\n",
      "epoch: 710, training loss: 1.2766257524490356, testing loss: 1.6970363855361938\n",
      "Accuracy: 0.4248\n",
      "epoch: 710, training loss: 1.2758972644805908, testing loss: 1.7339448928833008\n",
      "Accuracy: 0.4358\n",
      "epoch: 710, training loss: 1.3115593194961548, testing loss: 1.7776542901992798\n",
      "Accuracy: 0.4027\n",
      "epoch: 710, training loss: 1.2000293731689453, testing loss: 1.7599073648452759\n",
      "Accuracy: 0.3664\n",
      "epoch: 710, training loss: 1.1785261631011963, testing loss: 1.8570160865783691\n",
      "Accuracy: 0.4030\n",
      "epoch: 710, training loss: 1.2136905193328857, testing loss: 1.792847752571106\n",
      "Accuracy: 0.4094\n",
      "epoch: 710, training loss: 1.2477811574935913, testing loss: 1.6882548332214355\n",
      "Accuracy: 0.4019\n",
      "epoch: 710, training loss: 1.1963204145431519, testing loss: 1.675183892250061\n",
      "Accuracy: 0.4577\n",
      "epoch: 710, training loss: 1.2671912908554077, testing loss: 1.7776985168457031\n",
      "Accuracy: 0.4215\n",
      "epoch: 710, training loss: 1.3208985328674316, testing loss: 1.6927201747894287\n",
      "Accuracy: 0.4038\n",
      "epoch: 710, training loss: 1.2556878328323364, testing loss: 1.7088297605514526\n",
      "Accuracy: 0.4258\n",
      "epoch: 710, training loss: 1.2685505151748657, testing loss: 1.7876131534576416\n",
      "Accuracy: 0.4417\n",
      "epoch: 710, training loss: 1.2900118827819824, testing loss: 1.6716057062149048\n",
      "Accuracy: 0.4884\n",
      "epoch: 710, training loss: 1.1711890697479248, testing loss: 1.5689502954483032\n",
      "Accuracy: 0.4497\n",
      "epoch: 710, training loss: 1.2987377643585205, testing loss: 1.7100830078125\n",
      "Accuracy: 0.4281\n",
      "epoch: 710, training loss: 1.2089077234268188, testing loss: 1.7978758811950684\n",
      "Accuracy: 0.4324\n",
      "epoch: 710, training loss: 1.2512247562408447, testing loss: 1.7332909107208252\n",
      "Accuracy: 0.3861\n",
      "epoch: 710, training loss: 1.188661813735962, testing loss: 1.7227108478546143\n",
      "Accuracy: 0.4363\n",
      "epoch: 710, training loss: 1.2581340074539185, testing loss: 1.6408231258392334\n",
      "Accuracy: 0.4433\n",
      "epoch: 710, training loss: 1.2663326263427734, testing loss: 1.721752405166626\n",
      "Accuracy: 0.4136\n",
      "epoch: 710, training loss: 1.2303102016448975, testing loss: 1.7451895475387573\n",
      "Accuracy: 0.3839\n",
      "epoch: 710, training loss: 1.232297420501709, testing loss: 1.780044436454773\n",
      "Accuracy: 0.4244\n",
      "epoch: 710, training loss: 1.2625126838684082, testing loss: 1.6999735832214355\n",
      "Accuracy: 0.4143\n",
      "epoch: 710, training loss: 1.1448026895523071, testing loss: 1.7146223783493042\n",
      "Accuracy: 0.3918\n",
      "epoch: 710, training loss: 1.1816579103469849, testing loss: 1.7177904844284058\n",
      "Accuracy: 0.4086\n",
      "epoch: 710, training loss: 1.203629732131958, testing loss: 1.6824901103973389\n",
      "Accuracy: 0.4305\n",
      "epoch: 710, training loss: 1.2686094045639038, testing loss: 1.6829720735549927\n",
      "Accuracy: 0.4410\n",
      "epoch: 710, training loss: 1.2668389081954956, testing loss: 1.6020640134811401\n",
      "Accuracy: 0.4351\n",
      "epoch: 710, training loss: 1.1868199110031128, testing loss: 1.7011072635650635\n",
      "Accuracy: 0.4169\n",
      "epoch: 710, training loss: 1.2926373481750488, testing loss: 1.6840364933013916\n",
      "Accuracy: 0.4251\n",
      "epoch: 710, training loss: 1.2265044450759888, testing loss: 1.707733154296875\n",
      "Accuracy: 0.4348\n",
      "epoch: 710, training loss: 1.2198063135147095, testing loss: 1.6694265604019165\n",
      "Accuracy: 0.4255\n",
      "epoch: 710, training loss: 1.283319115638733, testing loss: 1.8827743530273438\n",
      "Accuracy: 0.4369\n",
      "epoch: 710, training loss: 1.300139307975769, testing loss: 1.8472051620483398\n",
      "Accuracy: 0.4103\n",
      "epoch: 710, training loss: 1.233328104019165, testing loss: 1.6719826459884644\n",
      "Accuracy: 0.4318\n",
      "epoch: 710, training loss: 1.2622157335281372, testing loss: 1.6341586112976074\n",
      "Accuracy: 0.4329\n",
      "epoch: 710, training loss: 1.230376958847046, testing loss: 1.6703661680221558\n",
      "Accuracy: 0.4000\n",
      "epoch: 710, training loss: 1.269974946975708, testing loss: 1.7463767528533936\n",
      "Accuracy: 0.4308\n",
      "epoch: 710, training loss: 1.2431117296218872, testing loss: 1.7690098285675049\n",
      "Accuracy: 0.3930\n",
      "epoch: 710, training loss: 1.293584942817688, testing loss: 1.8205164670944214\n",
      "Accuracy: 0.4113\n",
      "epoch: 710, training loss: 1.222545862197876, testing loss: 1.6631985902786255\n",
      "Accuracy: 0.4309\n",
      "epoch: 710, training loss: 1.266956090927124, testing loss: 1.8644558191299438\n",
      "Accuracy: 0.3813\n",
      "epoch: 710, training loss: 1.205054759979248, testing loss: 1.8482391834259033\n",
      "Accuracy: 0.4188\n",
      "epoch: 710, training loss: 1.2101470232009888, testing loss: 1.7889432907104492\n",
      "Accuracy: 0.4416\n",
      "epoch: 710, training loss: 1.3045985698699951, testing loss: 1.643829584121704\n",
      "Accuracy: 0.4079\n",
      "epoch: 710, training loss: 1.2042591571807861, testing loss: 1.634509563446045\n",
      "Accuracy: 0.4601\n",
      "epoch: 710, training loss: 1.2285819053649902, testing loss: 1.7031447887420654\n",
      "Accuracy: 0.4290\n",
      "epoch: 710, training loss: 1.1898528337478638, testing loss: 1.678512454032898\n",
      "Accuracy: 0.3910\n",
      "epoch: 710, training loss: 1.263912320137024, testing loss: 1.63941490650177\n",
      "Accuracy: 0.4396\n",
      "epoch: 710, training loss: 1.2593789100646973, testing loss: 1.7228071689605713\n",
      "Accuracy: 0.4328\n",
      "epoch: 710, training loss: 1.1439337730407715, testing loss: 1.6324424743652344\n",
      "Accuracy: 0.4217\n",
      "epoch: 710, training loss: 1.1907340288162231, testing loss: 1.5932016372680664\n",
      "Accuracy: 0.4744\n",
      "epoch: 710, training loss: 1.2119426727294922, testing loss: 1.6283071041107178\n",
      "Accuracy: 0.4239\n",
      "epoch: 710, training loss: 1.2510896921157837, testing loss: 1.647481083869934\n",
      "Accuracy: 0.4444\n",
      "epoch: 710, training loss: 1.3000094890594482, testing loss: 1.5931594371795654\n",
      "Accuracy: 0.4451\n",
      "epoch: 710, training loss: 1.2751439809799194, testing loss: 1.616391897201538\n",
      "Accuracy: 0.4563\n",
      "epoch: 710, training loss: 1.2449556589126587, testing loss: 1.7432496547698975\n",
      "Accuracy: 0.4413\n",
      "epoch: 710, training loss: 1.1520875692367554, testing loss: 1.692521333694458\n",
      "Accuracy: 0.4908\n",
      "epoch: 710, training loss: 1.2586607933044434, testing loss: 1.660159945487976\n",
      "Accuracy: 0.4164\n",
      "epoch: 710, training loss: 1.2323445081710815, testing loss: 1.737452507019043\n",
      "Accuracy: 0.4479\n",
      "epoch: 720, training loss: 1.2625583410263062, testing loss: 1.6211518049240112\n",
      "Accuracy: 0.4319\n",
      "epoch: 720, training loss: 1.2350380420684814, testing loss: 1.617902398109436\n",
      "Accuracy: 0.4441\n",
      "epoch: 720, training loss: 1.2222968339920044, testing loss: 1.8398995399475098\n",
      "Accuracy: 0.3666\n",
      "epoch: 720, training loss: 1.237356185913086, testing loss: 1.6867836713790894\n",
      "Accuracy: 0.4157\n",
      "epoch: 720, training loss: 1.2524827718734741, testing loss: 1.7948529720306396\n",
      "Accuracy: 0.4371\n",
      "epoch: 720, training loss: 1.2619043588638306, testing loss: 1.7948274612426758\n",
      "Accuracy: 0.4128\n",
      "epoch: 720, training loss: 1.1726551055908203, testing loss: 1.7530765533447266\n",
      "Accuracy: 0.4185\n",
      "epoch: 720, training loss: 1.2409262657165527, testing loss: 1.688277244567871\n",
      "Accuracy: 0.4332\n",
      "epoch: 720, training loss: 1.2225934267044067, testing loss: 1.781464695930481\n",
      "Accuracy: 0.4419\n",
      "epoch: 720, training loss: 1.2320114374160767, testing loss: 1.6101069450378418\n",
      "Accuracy: 0.4710\n",
      "epoch: 720, training loss: 1.2149107456207275, testing loss: 1.7007110118865967\n",
      "Accuracy: 0.3893\n",
      "epoch: 720, training loss: 1.2491824626922607, testing loss: 1.6479815244674683\n",
      "Accuracy: 0.4212\n",
      "epoch: 720, training loss: 1.288527488708496, testing loss: 1.7586994171142578\n",
      "Accuracy: 0.4071\n",
      "epoch: 720, training loss: 1.2397183179855347, testing loss: 1.6753774881362915\n",
      "Accuracy: 0.4365\n",
      "epoch: 720, training loss: 1.2281153202056885, testing loss: 1.7462036609649658\n",
      "Accuracy: 0.3785\n",
      "epoch: 720, training loss: 1.2340914011001587, testing loss: 1.7855613231658936\n",
      "Accuracy: 0.3758\n",
      "epoch: 720, training loss: 1.2039313316345215, testing loss: 1.7797855138778687\n",
      "Accuracy: 0.3913\n",
      "epoch: 720, training loss: 1.262155532836914, testing loss: 1.6474170684814453\n",
      "Accuracy: 0.4075\n",
      "epoch: 720, training loss: 1.2495429515838623, testing loss: 1.6724101305007935\n",
      "Accuracy: 0.4398\n",
      "epoch: 720, training loss: 1.2534741163253784, testing loss: 1.6756707429885864\n",
      "Accuracy: 0.4116\n",
      "epoch: 720, training loss: 1.2198084592819214, testing loss: 1.6420363187789917\n",
      "Accuracy: 0.4340\n",
      "epoch: 720, training loss: 1.2753829956054688, testing loss: 1.7245815992355347\n",
      "Accuracy: 0.3925\n",
      "epoch: 720, training loss: 1.254573941230774, testing loss: 1.7857269048690796\n",
      "Accuracy: 0.4212\n",
      "epoch: 720, training loss: 1.238572359085083, testing loss: 1.6380361318588257\n",
      "Accuracy: 0.4437\n",
      "epoch: 720, training loss: 1.2307052612304688, testing loss: 1.7504416704177856\n",
      "Accuracy: 0.3716\n",
      "epoch: 720, training loss: 1.1968404054641724, testing loss: 1.6640392541885376\n",
      "Accuracy: 0.4303\n",
      "epoch: 720, training loss: 1.2805757522583008, testing loss: 1.8123477697372437\n",
      "Accuracy: 0.3780\n",
      "epoch: 720, training loss: 1.2466295957565308, testing loss: 1.9087116718292236\n",
      "Accuracy: 0.3668\n",
      "epoch: 720, training loss: 1.224212408065796, testing loss: 1.7137137651443481\n",
      "Accuracy: 0.4194\n",
      "epoch: 720, training loss: 1.1657966375350952, testing loss: 1.819043517112732\n",
      "Accuracy: 0.3803\n",
      "epoch: 720, training loss: 1.3328402042388916, testing loss: 1.6046556234359741\n",
      "Accuracy: 0.4601\n",
      "epoch: 720, training loss: 1.2102513313293457, testing loss: 1.7337634563446045\n",
      "Accuracy: 0.3788\n",
      "epoch: 720, training loss: 1.2200087308883667, testing loss: 1.6051476001739502\n",
      "Accuracy: 0.4238\n",
      "epoch: 720, training loss: 1.1716605424880981, testing loss: 1.6323446035385132\n",
      "Accuracy: 0.4686\n",
      "epoch: 720, training loss: 1.1972095966339111, testing loss: 1.6309973001480103\n",
      "Accuracy: 0.4357\n",
      "epoch: 720, training loss: 1.26060152053833, testing loss: 1.7429019212722778\n",
      "Accuracy: 0.4218\n",
      "epoch: 720, training loss: 1.2657612562179565, testing loss: 1.73055100440979\n",
      "Accuracy: 0.4164\n",
      "epoch: 720, training loss: 1.3117624521255493, testing loss: 1.7079795598983765\n",
      "Accuracy: 0.3880\n",
      "epoch: 720, training loss: 1.165671467781067, testing loss: 1.929603934288025\n",
      "Accuracy: 0.3877\n",
      "epoch: 720, training loss: 1.2838094234466553, testing loss: 1.8186655044555664\n",
      "Accuracy: 0.4244\n",
      "epoch: 720, training loss: 1.1982535123825073, testing loss: 1.645476222038269\n",
      "Accuracy: 0.4281\n",
      "epoch: 720, training loss: 1.2866424322128296, testing loss: 1.8295061588287354\n",
      "Accuracy: 0.4046\n",
      "epoch: 720, training loss: 1.3269996643066406, testing loss: 1.6770658493041992\n",
      "Accuracy: 0.4618\n",
      "epoch: 720, training loss: 1.2950732707977295, testing loss: 1.7974281311035156\n",
      "Accuracy: 0.4228\n",
      "epoch: 720, training loss: 1.2153010368347168, testing loss: 1.7279804944992065\n",
      "Accuracy: 0.4417\n",
      "epoch: 720, training loss: 1.2787020206451416, testing loss: 1.6928746700286865\n",
      "Accuracy: 0.4709\n",
      "epoch: 720, training loss: 1.3082853555679321, testing loss: 1.738142490386963\n",
      "Accuracy: 0.3831\n",
      "epoch: 720, training loss: 1.2976216077804565, testing loss: 1.6833473443984985\n",
      "Accuracy: 0.4415\n",
      "epoch: 720, training loss: 1.2595465183258057, testing loss: 1.6196317672729492\n",
      "Accuracy: 0.4048\n",
      "epoch: 720, training loss: 1.2534948587417603, testing loss: 1.782610297203064\n",
      "Accuracy: 0.4427\n",
      "epoch: 720, training loss: 1.2043561935424805, testing loss: 1.755318522453308\n",
      "Accuracy: 0.4352\n",
      "epoch: 720, training loss: 1.228755235671997, testing loss: 1.663312554359436\n",
      "Accuracy: 0.4078\n",
      "epoch: 720, training loss: 1.181557536125183, testing loss: 1.8961697816848755\n",
      "Accuracy: 0.3517\n",
      "epoch: 720, training loss: 1.1907309293746948, testing loss: 1.7180825471878052\n",
      "Accuracy: 0.4253\n",
      "epoch: 720, training loss: 1.245158314704895, testing loss: 1.6686466932296753\n",
      "Accuracy: 0.3926\n",
      "epoch: 720, training loss: 1.2176215648651123, testing loss: 1.7751245498657227\n",
      "Accuracy: 0.4207\n",
      "epoch: 720, training loss: 1.2678658962249756, testing loss: 1.6116158962249756\n",
      "Accuracy: 0.4304\n",
      "epoch: 720, training loss: 1.3462061882019043, testing loss: 1.686246633529663\n",
      "Accuracy: 0.3958\n",
      "epoch: 720, training loss: 1.2381160259246826, testing loss: 1.9311928749084473\n",
      "Accuracy: 0.3587\n",
      "epoch: 720, training loss: 1.2485852241516113, testing loss: 1.690887689590454\n",
      "Accuracy: 0.4369\n",
      "epoch: 720, training loss: 1.2643818855285645, testing loss: 1.6044577360153198\n",
      "Accuracy: 0.4695\n",
      "epoch: 720, training loss: 1.273882269859314, testing loss: 1.8190439939498901\n",
      "Accuracy: 0.3652\n",
      "epoch: 720, training loss: 1.2529847621917725, testing loss: 1.8636497259140015\n",
      "Accuracy: 0.3994\n",
      "epoch: 720, training loss: 1.244695782661438, testing loss: 2.0569326877593994\n",
      "Accuracy: 0.3516\n",
      "epoch: 720, training loss: 1.2204179763793945, testing loss: 1.6523199081420898\n",
      "Accuracy: 0.4337\n",
      "epoch: 720, training loss: 1.2997254133224487, testing loss: 1.6512300968170166\n",
      "Accuracy: 0.4219\n",
      "epoch: 720, training loss: 1.2227933406829834, testing loss: 1.759883165359497\n",
      "Accuracy: 0.4065\n",
      "epoch: 720, training loss: 1.2280125617980957, testing loss: 1.679463267326355\n",
      "Accuracy: 0.4495\n",
      "epoch: 720, training loss: 1.2731965780258179, testing loss: 1.6606781482696533\n",
      "Accuracy: 0.4437\n",
      "epoch: 720, training loss: 1.1708471775054932, testing loss: 1.6966922283172607\n",
      "Accuracy: 0.4146\n",
      "epoch: 720, training loss: 1.2802433967590332, testing loss: 1.7229371070861816\n",
      "Accuracy: 0.4192\n",
      "epoch: 720, training loss: 1.3396117687225342, testing loss: 1.6637258529663086\n",
      "Accuracy: 0.4247\n",
      "epoch: 720, training loss: 1.223732352256775, testing loss: 1.7122600078582764\n",
      "Accuracy: 0.4448\n",
      "epoch: 720, training loss: 1.2325854301452637, testing loss: 1.6862947940826416\n",
      "Accuracy: 0.3941\n",
      "epoch: 720, training loss: 1.2260280847549438, testing loss: 1.818151593208313\n",
      "Accuracy: 0.4230\n",
      "epoch: 720, training loss: 1.2544770240783691, testing loss: 1.7522027492523193\n",
      "Accuracy: 0.4215\n",
      "epoch: 720, training loss: 1.1690081357955933, testing loss: 1.783138394355774\n",
      "Accuracy: 0.4377\n",
      "epoch: 720, training loss: 1.244011640548706, testing loss: 1.6975091695785522\n",
      "Accuracy: 0.4857\n",
      "epoch: 720, training loss: 1.2445424795150757, testing loss: 1.719989538192749\n",
      "Accuracy: 0.4470\n",
      "epoch: 720, training loss: 1.2791030406951904, testing loss: 1.69115149974823\n",
      "Accuracy: 0.4190\n",
      "epoch: 720, training loss: 1.236580491065979, testing loss: 1.6870193481445312\n",
      "Accuracy: 0.4423\n",
      "epoch: 720, training loss: 1.228683352470398, testing loss: 1.6466542482376099\n",
      "Accuracy: 0.4157\n",
      "epoch: 720, training loss: 1.2874717712402344, testing loss: 1.6955755949020386\n",
      "Accuracy: 0.4000\n",
      "epoch: 720, training loss: 1.2757974863052368, testing loss: 1.8595300912857056\n",
      "Accuracy: 0.3871\n",
      "epoch: 720, training loss: 1.2416574954986572, testing loss: 1.716153860092163\n",
      "Accuracy: 0.4524\n",
      "epoch: 720, training loss: 1.2685626745224, testing loss: 1.6436998844146729\n",
      "Accuracy: 0.4460\n",
      "epoch: 720, training loss: 1.2442309856414795, testing loss: 1.6700563430786133\n",
      "Accuracy: 0.4063\n",
      "epoch: 720, training loss: 1.2093390226364136, testing loss: 1.6798864603042603\n",
      "Accuracy: 0.4066\n",
      "epoch: 720, training loss: 1.244215965270996, testing loss: 1.65204918384552\n",
      "Accuracy: 0.4317\n",
      "epoch: 720, training loss: 1.2604724168777466, testing loss: 1.6477645635604858\n",
      "Accuracy: 0.4295\n",
      "epoch: 720, training loss: 1.2237151861190796, testing loss: 1.8301934003829956\n",
      "Accuracy: 0.4146\n",
      "epoch: 720, training loss: 1.2183995246887207, testing loss: 1.5942471027374268\n",
      "Accuracy: 0.4686\n",
      "epoch: 720, training loss: 1.215893030166626, testing loss: 1.6745083332061768\n",
      "Accuracy: 0.3927\n",
      "epoch: 720, training loss: 1.2269564867019653, testing loss: 1.6826484203338623\n",
      "Accuracy: 0.4437\n",
      "epoch: 720, training loss: 1.220048189163208, testing loss: 1.6471903324127197\n",
      "Accuracy: 0.4084\n",
      "epoch: 720, training loss: 1.1958566904067993, testing loss: 1.6174408197402954\n",
      "Accuracy: 0.4373\n",
      "epoch: 720, training loss: 1.157988429069519, testing loss: 1.694136142730713\n",
      "Accuracy: 0.4000\n",
      "epoch: 720, training loss: 1.26308274269104, testing loss: 1.7952572107315063\n",
      "Accuracy: 0.4184\n",
      "epoch: 720, training loss: 1.1898800134658813, testing loss: 1.8299871683120728\n",
      "Accuracy: 0.4045\n",
      "epoch: 720, training loss: 1.2695481777191162, testing loss: 1.7115676403045654\n",
      "Accuracy: 0.4768\n",
      "epoch: 730, training loss: 1.1660891771316528, testing loss: 1.8308593034744263\n",
      "Accuracy: 0.3851\n",
      "epoch: 730, training loss: 1.261924147605896, testing loss: 1.6991393566131592\n",
      "Accuracy: 0.4121\n",
      "epoch: 730, training loss: 1.1844267845153809, testing loss: 1.6573108434677124\n",
      "Accuracy: 0.4551\n",
      "epoch: 730, training loss: 1.2945252656936646, testing loss: 1.7485212087631226\n",
      "Accuracy: 0.4175\n",
      "epoch: 730, training loss: 1.231261134147644, testing loss: 1.729023814201355\n",
      "Accuracy: 0.4007\n",
      "epoch: 730, training loss: 1.23678719997406, testing loss: 1.7740404605865479\n",
      "Accuracy: 0.4097\n",
      "epoch: 730, training loss: 1.1937742233276367, testing loss: 1.6701830625534058\n",
      "Accuracy: 0.4067\n",
      "epoch: 730, training loss: 1.2180677652359009, testing loss: 1.6902939081192017\n",
      "Accuracy: 0.4095\n",
      "epoch: 730, training loss: 1.176169991493225, testing loss: 1.655312418937683\n",
      "Accuracy: 0.3788\n",
      "epoch: 730, training loss: 1.2568564414978027, testing loss: 1.6257983446121216\n",
      "Accuracy: 0.4494\n",
      "epoch: 730, training loss: 1.2274729013442993, testing loss: 1.634620189666748\n",
      "Accuracy: 0.4389\n",
      "epoch: 730, training loss: 1.2742606401443481, testing loss: 1.7303255796432495\n",
      "Accuracy: 0.3866\n",
      "epoch: 730, training loss: 1.2412590980529785, testing loss: 1.583677887916565\n",
      "Accuracy: 0.4747\n",
      "epoch: 730, training loss: 1.2518301010131836, testing loss: 1.8356425762176514\n",
      "Accuracy: 0.3669\n",
      "epoch: 730, training loss: 1.248684287071228, testing loss: 1.7162303924560547\n",
      "Accuracy: 0.4263\n",
      "epoch: 730, training loss: 1.2622720003128052, testing loss: 1.791743278503418\n",
      "Accuracy: 0.4419\n",
      "epoch: 730, training loss: 1.2234126329421997, testing loss: 1.7967541217803955\n",
      "Accuracy: 0.3935\n",
      "epoch: 730, training loss: 1.2582229375839233, testing loss: 1.6852763891220093\n",
      "Accuracy: 0.3978\n",
      "epoch: 730, training loss: 1.198426604270935, testing loss: 1.6371601819992065\n",
      "Accuracy: 0.4528\n",
      "epoch: 730, training loss: 1.1893377304077148, testing loss: 1.5969797372817993\n",
      "Accuracy: 0.4787\n",
      "epoch: 730, training loss: 1.3063156604766846, testing loss: 1.759729027748108\n",
      "Accuracy: 0.4006\n",
      "epoch: 730, training loss: 1.3015570640563965, testing loss: 1.7317934036254883\n",
      "Accuracy: 0.4542\n",
      "epoch: 730, training loss: 1.2735202312469482, testing loss: 1.7987350225448608\n",
      "Accuracy: 0.3717\n",
      "epoch: 730, training loss: 1.2503468990325928, testing loss: 1.6996642351150513\n",
      "Accuracy: 0.4486\n",
      "epoch: 730, training loss: 1.2434966564178467, testing loss: 1.7918320894241333\n",
      "Accuracy: 0.4007\n",
      "epoch: 730, training loss: 1.267124891281128, testing loss: 1.7754689455032349\n",
      "Accuracy: 0.4056\n",
      "epoch: 730, training loss: 1.1941901445388794, testing loss: 1.8149313926696777\n",
      "Accuracy: 0.4012\n",
      "epoch: 730, training loss: 1.2234164476394653, testing loss: 1.7731744050979614\n",
      "Accuracy: 0.4272\n",
      "epoch: 730, training loss: 1.2829887866973877, testing loss: 1.8401507139205933\n",
      "Accuracy: 0.3725\n",
      "epoch: 730, training loss: 1.2136248350143433, testing loss: 1.756587266921997\n",
      "Accuracy: 0.3994\n",
      "epoch: 730, training loss: 1.2847282886505127, testing loss: 1.7352255582809448\n",
      "Accuracy: 0.4150\n",
      "epoch: 730, training loss: 1.1642334461212158, testing loss: 1.7222999334335327\n",
      "Accuracy: 0.4344\n",
      "epoch: 730, training loss: 1.2680490016937256, testing loss: 1.7318909168243408\n",
      "Accuracy: 0.4444\n",
      "epoch: 730, training loss: 1.2994611263275146, testing loss: 1.8251729011535645\n",
      "Accuracy: 0.4050\n",
      "epoch: 730, training loss: 1.297947883605957, testing loss: 1.7321432828903198\n",
      "Accuracy: 0.4352\n",
      "epoch: 730, training loss: 1.3091541528701782, testing loss: 1.7133526802062988\n",
      "Accuracy: 0.4371\n",
      "epoch: 730, training loss: 1.1900557279586792, testing loss: 1.614060401916504\n",
      "Accuracy: 0.4459\n",
      "epoch: 730, training loss: 1.3357505798339844, testing loss: 1.6993893384933472\n",
      "Accuracy: 0.3931\n",
      "epoch: 730, training loss: 1.2783273458480835, testing loss: 1.6616452932357788\n",
      "Accuracy: 0.4238\n",
      "epoch: 730, training loss: 1.2541508674621582, testing loss: 1.6676114797592163\n",
      "Accuracy: 0.4156\n",
      "epoch: 730, training loss: 1.246730923652649, testing loss: 1.8605279922485352\n",
      "Accuracy: 0.3742\n",
      "epoch: 730, training loss: 1.234510898590088, testing loss: 1.697189211845398\n",
      "Accuracy: 0.4060\n",
      "epoch: 730, training loss: 1.2179138660430908, testing loss: 1.807314157485962\n",
      "Accuracy: 0.4075\n",
      "epoch: 730, training loss: 1.2786436080932617, testing loss: 1.7343084812164307\n",
      "Accuracy: 0.4258\n",
      "epoch: 730, training loss: 1.2304939031600952, testing loss: 1.8898812532424927\n",
      "Accuracy: 0.3943\n",
      "epoch: 730, training loss: 1.2092983722686768, testing loss: 1.6764873266220093\n",
      "Accuracy: 0.4053\n",
      "epoch: 730, training loss: 1.2319973707199097, testing loss: 1.7229585647583008\n",
      "Accuracy: 0.4147\n",
      "epoch: 730, training loss: 1.2521921396255493, testing loss: 1.7393356561660767\n",
      "Accuracy: 0.4094\n",
      "epoch: 730, training loss: 1.2520334720611572, testing loss: 1.7577452659606934\n",
      "Accuracy: 0.4227\n",
      "epoch: 730, training loss: 1.2309094667434692, testing loss: 1.6674317121505737\n",
      "Accuracy: 0.3851\n",
      "epoch: 730, training loss: 1.259299635887146, testing loss: 1.665393352508545\n",
      "Accuracy: 0.4218\n",
      "epoch: 730, training loss: 1.203370213508606, testing loss: 1.67972993850708\n",
      "Accuracy: 0.3841\n",
      "epoch: 730, training loss: 1.2400249242782593, testing loss: 1.7271931171417236\n",
      "Accuracy: 0.4290\n",
      "epoch: 730, training loss: 1.1668200492858887, testing loss: 1.6958801746368408\n",
      "Accuracy: 0.4485\n",
      "epoch: 730, training loss: 1.302338719367981, testing loss: 1.6403093338012695\n",
      "Accuracy: 0.4124\n",
      "epoch: 730, training loss: 1.2096854448318481, testing loss: 1.6882507801055908\n",
      "Accuracy: 0.4200\n",
      "epoch: 730, training loss: 1.2559515237808228, testing loss: 1.77301824092865\n",
      "Accuracy: 0.4309\n",
      "epoch: 730, training loss: 1.2653144598007202, testing loss: 1.8169803619384766\n",
      "Accuracy: 0.4426\n",
      "epoch: 730, training loss: 1.2658289670944214, testing loss: 1.6453553438186646\n",
      "Accuracy: 0.4272\n",
      "epoch: 730, training loss: 1.2299331426620483, testing loss: 1.6972837448120117\n",
      "Accuracy: 0.4592\n",
      "epoch: 730, training loss: 1.290878415107727, testing loss: 1.708829402923584\n",
      "Accuracy: 0.4271\n",
      "epoch: 730, training loss: 1.244367003440857, testing loss: 1.727998971939087\n",
      "Accuracy: 0.4019\n",
      "epoch: 730, training loss: 1.169948935508728, testing loss: 1.7327959537506104\n",
      "Accuracy: 0.4245\n",
      "epoch: 730, training loss: 1.2443188428878784, testing loss: 1.734236717224121\n",
      "Accuracy: 0.3793\n",
      "epoch: 730, training loss: 1.2906192541122437, testing loss: 1.6576719284057617\n",
      "Accuracy: 0.4375\n",
      "epoch: 730, training loss: 1.247545599937439, testing loss: 1.7944481372833252\n",
      "Accuracy: 0.3633\n",
      "epoch: 730, training loss: 1.3128416538238525, testing loss: 1.8144077062606812\n",
      "Accuracy: 0.4227\n",
      "epoch: 730, training loss: 1.2859522104263306, testing loss: 1.696099042892456\n",
      "Accuracy: 0.4205\n",
      "epoch: 730, training loss: 1.2858806848526, testing loss: 1.7252726554870605\n",
      "Accuracy: 0.4342\n",
      "epoch: 730, training loss: 1.2386382818222046, testing loss: 1.8425654172897339\n",
      "Accuracy: 0.3830\n",
      "epoch: 730, training loss: 1.2017370462417603, testing loss: 1.7213313579559326\n",
      "Accuracy: 0.4506\n",
      "epoch: 730, training loss: 1.2202363014221191, testing loss: 1.778334617614746\n",
      "Accuracy: 0.4381\n",
      "epoch: 730, training loss: 1.2031430006027222, testing loss: 1.6573594808578491\n",
      "Accuracy: 0.4689\n",
      "epoch: 730, training loss: 1.1896551847457886, testing loss: 1.629522442817688\n",
      "Accuracy: 0.4197\n",
      "epoch: 730, training loss: 1.291101098060608, testing loss: 1.7425134181976318\n",
      "Accuracy: 0.4683\n",
      "epoch: 730, training loss: 1.2380131483078003, testing loss: 1.644329309463501\n",
      "Accuracy: 0.4542\n",
      "epoch: 730, training loss: 1.247580647468567, testing loss: 1.6871854066848755\n",
      "Accuracy: 0.4817\n",
      "epoch: 730, training loss: 1.2159870862960815, testing loss: 1.6274504661560059\n",
      "Accuracy: 0.4603\n",
      "epoch: 730, training loss: 1.2456673383712769, testing loss: 1.9632551670074463\n",
      "Accuracy: 0.3609\n",
      "epoch: 730, training loss: 1.2537672519683838, testing loss: 1.7770140171051025\n",
      "Accuracy: 0.3791\n",
      "epoch: 730, training loss: 1.284470558166504, testing loss: 1.800775408744812\n",
      "Accuracy: 0.4092\n",
      "epoch: 730, training loss: 1.193394422531128, testing loss: 1.8574414253234863\n",
      "Accuracy: 0.4026\n",
      "epoch: 730, training loss: 1.2593547105789185, testing loss: 1.9318608045578003\n",
      "Accuracy: 0.3684\n",
      "epoch: 730, training loss: 1.193037748336792, testing loss: 1.6857017278671265\n",
      "Accuracy: 0.4604\n",
      "epoch: 730, training loss: 1.2950265407562256, testing loss: 1.6638857126235962\n",
      "Accuracy: 0.4200\n",
      "epoch: 730, training loss: 1.2778929471969604, testing loss: 1.6050522327423096\n",
      "Accuracy: 0.4475\n",
      "epoch: 730, training loss: 1.3262256383895874, testing loss: 1.7203184366226196\n",
      "Accuracy: 0.4191\n",
      "epoch: 730, training loss: 1.2890195846557617, testing loss: 1.8249948024749756\n",
      "Accuracy: 0.3689\n",
      "epoch: 730, training loss: 1.257893443107605, testing loss: 1.7729976177215576\n",
      "Accuracy: 0.3883\n",
      "epoch: 730, training loss: 1.1992913484573364, testing loss: 1.647586464881897\n",
      "Accuracy: 0.4675\n",
      "epoch: 730, training loss: 1.2420467138290405, testing loss: 1.8476920127868652\n",
      "Accuracy: 0.3817\n",
      "epoch: 730, training loss: 1.2090545892715454, testing loss: 1.7353622913360596\n",
      "Accuracy: 0.4618\n",
      "epoch: 730, training loss: 1.2262530326843262, testing loss: 1.7852890491485596\n",
      "Accuracy: 0.4199\n",
      "epoch: 730, training loss: 1.2255529165267944, testing loss: 1.6667271852493286\n",
      "Accuracy: 0.4495\n",
      "epoch: 730, training loss: 1.2672406435012817, testing loss: 1.6235907077789307\n",
      "Accuracy: 0.4024\n",
      "epoch: 730, training loss: 1.206825613975525, testing loss: 1.5817259550094604\n",
      "Accuracy: 0.4691\n",
      "epoch: 730, training loss: 1.2337905168533325, testing loss: 1.7051702737808228\n",
      "Accuracy: 0.4182\n",
      "epoch: 730, training loss: 1.2712560892105103, testing loss: 1.7279196977615356\n",
      "Accuracy: 0.4067\n",
      "epoch: 730, training loss: 1.2445472478866577, testing loss: 1.7422223091125488\n",
      "Accuracy: 0.3653\n",
      "epoch: 730, training loss: 1.2546422481536865, testing loss: 1.5187069177627563\n",
      "Accuracy: 0.4487\n",
      "epoch: 740, training loss: 1.2034249305725098, testing loss: 1.7602778673171997\n",
      "Accuracy: 0.4199\n",
      "epoch: 740, training loss: 1.3259352445602417, testing loss: 1.5918103456497192\n",
      "Accuracy: 0.4503\n",
      "epoch: 740, training loss: 1.2195383310317993, testing loss: 1.6815789937973022\n",
      "Accuracy: 0.4448\n",
      "epoch: 740, training loss: 1.2576464414596558, testing loss: 1.7295353412628174\n",
      "Accuracy: 0.4218\n",
      "epoch: 740, training loss: 1.2972151041030884, testing loss: 1.8735734224319458\n",
      "Accuracy: 0.3656\n",
      "epoch: 740, training loss: 1.2173222303390503, testing loss: 1.7349032163619995\n",
      "Accuracy: 0.3950\n",
      "epoch: 740, training loss: 1.2273977994918823, testing loss: 1.6885141134262085\n",
      "Accuracy: 0.4203\n",
      "epoch: 740, training loss: 1.260333776473999, testing loss: 1.6060429811477661\n",
      "Accuracy: 0.4463\n",
      "epoch: 740, training loss: 1.2108641862869263, testing loss: 1.7025870084762573\n",
      "Accuracy: 0.4112\n",
      "epoch: 740, training loss: 1.2526414394378662, testing loss: 1.6075153350830078\n",
      "Accuracy: 0.4455\n",
      "epoch: 740, training loss: 1.2165488004684448, testing loss: 1.8394501209259033\n",
      "Accuracy: 0.3700\n",
      "epoch: 740, training loss: 1.2434065341949463, testing loss: 1.7417079210281372\n",
      "Accuracy: 0.3833\n",
      "epoch: 740, training loss: 1.22512686252594, testing loss: 1.7025173902511597\n",
      "Accuracy: 0.4248\n",
      "epoch: 740, training loss: 1.2282720804214478, testing loss: 1.6925266981124878\n",
      "Accuracy: 0.4526\n",
      "epoch: 740, training loss: 1.2705987691879272, testing loss: 1.7155696153640747\n",
      "Accuracy: 0.4128\n",
      "epoch: 740, training loss: 1.2680447101593018, testing loss: 1.6205986738204956\n",
      "Accuracy: 0.3949\n",
      "epoch: 740, training loss: 1.1931605339050293, testing loss: 1.772485375404358\n",
      "Accuracy: 0.4032\n",
      "epoch: 740, training loss: 1.2687618732452393, testing loss: 1.8054784536361694\n",
      "Accuracy: 0.4006\n",
      "epoch: 740, training loss: 1.263150691986084, testing loss: 1.6407105922698975\n",
      "Accuracy: 0.4487\n",
      "epoch: 740, training loss: 1.1849024295806885, testing loss: 1.7064299583435059\n",
      "Accuracy: 0.4682\n",
      "epoch: 740, training loss: 1.1879596710205078, testing loss: 1.702952265739441\n",
      "Accuracy: 0.4246\n",
      "epoch: 740, training loss: 1.1813679933547974, testing loss: 1.6880592107772827\n",
      "Accuracy: 0.4095\n",
      "epoch: 740, training loss: 1.2721959352493286, testing loss: 1.710278868675232\n",
      "Accuracy: 0.4353\n",
      "epoch: 740, training loss: 1.2518318891525269, testing loss: 1.7928415536880493\n",
      "Accuracy: 0.3746\n",
      "epoch: 740, training loss: 1.1984115839004517, testing loss: 1.6352450847625732\n",
      "Accuracy: 0.4328\n",
      "epoch: 740, training loss: 1.2936286926269531, testing loss: 1.5621510744094849\n",
      "Accuracy: 0.4567\n",
      "epoch: 740, training loss: 1.293504238128662, testing loss: 1.8188650608062744\n",
      "Accuracy: 0.3951\n",
      "epoch: 740, training loss: 1.2642215490341187, testing loss: 1.596113681793213\n",
      "Accuracy: 0.4795\n",
      "epoch: 740, training loss: 1.2559418678283691, testing loss: 1.5456558465957642\n",
      "Accuracy: 0.4420\n",
      "epoch: 740, training loss: 1.2291444540023804, testing loss: 1.8802695274353027\n",
      "Accuracy: 0.4201\n",
      "epoch: 740, training loss: 1.2697991132736206, testing loss: 1.7631267309188843\n",
      "Accuracy: 0.4188\n",
      "epoch: 740, training loss: 1.2064568996429443, testing loss: 1.7527399063110352\n",
      "Accuracy: 0.4434\n",
      "epoch: 740, training loss: 1.2408713102340698, testing loss: 1.716705083847046\n",
      "Accuracy: 0.3907\n",
      "epoch: 740, training loss: 1.3066362142562866, testing loss: 1.8754974603652954\n",
      "Accuracy: 0.3660\n",
      "epoch: 740, training loss: 1.2235952615737915, testing loss: 1.8977464437484741\n",
      "Accuracy: 0.3261\n",
      "epoch: 740, training loss: 1.2667908668518066, testing loss: 1.6727488040924072\n",
      "Accuracy: 0.4313\n",
      "epoch: 740, training loss: 1.1676043272018433, testing loss: 1.7239634990692139\n",
      "Accuracy: 0.4085\n",
      "epoch: 740, training loss: 1.2987204790115356, testing loss: 1.8425495624542236\n",
      "Accuracy: 0.3553\n",
      "epoch: 740, training loss: 1.2235842943191528, testing loss: 1.5919461250305176\n",
      "Accuracy: 0.4150\n",
      "epoch: 740, training loss: 1.236019492149353, testing loss: 1.5823497772216797\n",
      "Accuracy: 0.4557\n",
      "epoch: 740, training loss: 1.2778970003128052, testing loss: 1.645094633102417\n",
      "Accuracy: 0.4385\n",
      "epoch: 740, training loss: 1.2211917638778687, testing loss: 1.7972702980041504\n",
      "Accuracy: 0.3923\n",
      "epoch: 740, training loss: 1.2564585208892822, testing loss: 1.6705515384674072\n",
      "Accuracy: 0.3963\n",
      "epoch: 740, training loss: 1.2522103786468506, testing loss: 1.6890006065368652\n",
      "Accuracy: 0.4583\n",
      "epoch: 740, training loss: 1.2039812803268433, testing loss: 1.7614119052886963\n",
      "Accuracy: 0.4375\n",
      "epoch: 740, training loss: 1.231045126914978, testing loss: 1.5941824913024902\n",
      "Accuracy: 0.4309\n",
      "epoch: 740, training loss: 1.2482537031173706, testing loss: 1.6137925386428833\n",
      "Accuracy: 0.4359\n",
      "epoch: 740, training loss: 1.1698864698410034, testing loss: 1.7878121137619019\n",
      "Accuracy: 0.3959\n",
      "epoch: 740, training loss: 1.2395472526550293, testing loss: 1.5803388357162476\n",
      "Accuracy: 0.4948\n",
      "epoch: 740, training loss: 1.2806752920150757, testing loss: 1.745366096496582\n",
      "Accuracy: 0.4131\n",
      "epoch: 740, training loss: 1.2584275007247925, testing loss: 1.6687426567077637\n",
      "Accuracy: 0.4319\n",
      "epoch: 740, training loss: 1.2431639432907104, testing loss: 1.6624155044555664\n",
      "Accuracy: 0.4025\n",
      "epoch: 740, training loss: 1.2982454299926758, testing loss: 1.7206188440322876\n",
      "Accuracy: 0.4013\n",
      "epoch: 740, training loss: 1.2935669422149658, testing loss: 1.6465243101119995\n",
      "Accuracy: 0.4121\n",
      "epoch: 740, training loss: 1.2234771251678467, testing loss: 1.6899162530899048\n",
      "Accuracy: 0.4000\n",
      "epoch: 740, training loss: 1.2013994455337524, testing loss: 1.6638457775115967\n",
      "Accuracy: 0.3824\n",
      "epoch: 740, training loss: 1.2149690389633179, testing loss: 1.7013660669326782\n",
      "Accuracy: 0.4332\n",
      "epoch: 740, training loss: 1.2381972074508667, testing loss: 1.7676377296447754\n",
      "Accuracy: 0.4379\n",
      "epoch: 740, training loss: 1.2892022132873535, testing loss: 1.9137887954711914\n",
      "Accuracy: 0.4729\n",
      "epoch: 740, training loss: 1.2882905006408691, testing loss: 1.7954257726669312\n",
      "Accuracy: 0.4096\n",
      "epoch: 740, training loss: 1.259813666343689, testing loss: 1.6844792366027832\n",
      "Accuracy: 0.4026\n",
      "epoch: 740, training loss: 1.2244806289672852, testing loss: 1.733516812324524\n",
      "Accuracy: 0.3967\n",
      "epoch: 740, training loss: 1.2529338598251343, testing loss: 1.7301534414291382\n",
      "Accuracy: 0.4239\n",
      "epoch: 740, training loss: 1.1862884759902954, testing loss: 1.6378066539764404\n",
      "Accuracy: 0.4268\n",
      "epoch: 740, training loss: 1.11920964717865, testing loss: 1.7124477624893188\n",
      "Accuracy: 0.3800\n",
      "epoch: 740, training loss: 1.2036508321762085, testing loss: 1.6792606115341187\n",
      "Accuracy: 0.3989\n",
      "epoch: 740, training loss: 1.2709306478500366, testing loss: 1.6209468841552734\n",
      "Accuracy: 0.4243\n",
      "epoch: 740, training loss: 1.2389380931854248, testing loss: 1.7035812139511108\n",
      "Accuracy: 0.4375\n",
      "epoch: 740, training loss: 1.2437185049057007, testing loss: 1.748434066772461\n",
      "Accuracy: 0.3871\n",
      "epoch: 740, training loss: 1.3145318031311035, testing loss: 1.8353182077407837\n",
      "Accuracy: 0.3902\n",
      "epoch: 740, training loss: 1.3284209966659546, testing loss: 1.785006046295166\n",
      "Accuracy: 0.4130\n",
      "epoch: 740, training loss: 1.2369120121002197, testing loss: 1.7645052671432495\n",
      "Accuracy: 0.3788\n",
      "epoch: 740, training loss: 1.2744574546813965, testing loss: 1.7963345050811768\n",
      "Accuracy: 0.4109\n",
      "epoch: 740, training loss: 1.2729573249816895, testing loss: 1.7799339294433594\n",
      "Accuracy: 0.4373\n",
      "epoch: 740, training loss: 1.2421817779541016, testing loss: 1.769705891609192\n",
      "Accuracy: 0.3746\n",
      "epoch: 740, training loss: 1.2705706357955933, testing loss: 1.7319680452346802\n",
      "Accuracy: 0.4063\n",
      "epoch: 740, training loss: 1.2565698623657227, testing loss: 1.6757416725158691\n",
      "Accuracy: 0.4476\n",
      "epoch: 740, training loss: 1.2787998914718628, testing loss: 1.7482868432998657\n",
      "Accuracy: 0.4082\n",
      "epoch: 740, training loss: 1.221416711807251, testing loss: 1.7309589385986328\n",
      "Accuracy: 0.3704\n",
      "epoch: 740, training loss: 1.225364089012146, testing loss: 1.6834481954574585\n",
      "Accuracy: 0.4471\n",
      "epoch: 740, training loss: 1.2670965194702148, testing loss: 1.7530057430267334\n",
      "Accuracy: 0.4323\n",
      "epoch: 740, training loss: 1.1875898838043213, testing loss: 1.7765053510665894\n",
      "Accuracy: 0.4233\n",
      "epoch: 740, training loss: 1.2525155544281006, testing loss: 1.7501370906829834\n",
      "Accuracy: 0.4299\n",
      "epoch: 740, training loss: 1.1481831073760986, testing loss: 1.8267921209335327\n",
      "Accuracy: 0.3814\n",
      "epoch: 740, training loss: 1.2282403707504272, testing loss: 1.766445279121399\n",
      "Accuracy: 0.3981\n",
      "epoch: 740, training loss: 1.2535027265548706, testing loss: 1.8096609115600586\n",
      "Accuracy: 0.4164\n",
      "epoch: 740, training loss: 1.2627673149108887, testing loss: 1.6314424276351929\n",
      "Accuracy: 0.4404\n",
      "epoch: 740, training loss: 1.224224328994751, testing loss: 1.6044821739196777\n",
      "Accuracy: 0.4539\n",
      "epoch: 740, training loss: 1.23586905002594, testing loss: 1.711194396018982\n",
      "Accuracy: 0.4369\n",
      "epoch: 740, training loss: 1.25725519657135, testing loss: 1.716866374015808\n",
      "Accuracy: 0.4465\n",
      "epoch: 740, training loss: 1.2930046319961548, testing loss: 1.6151776313781738\n",
      "Accuracy: 0.4613\n",
      "epoch: 740, training loss: 1.1814665794372559, testing loss: 1.6359820365905762\n",
      "Accuracy: 0.4007\n",
      "epoch: 740, training loss: 1.2904596328735352, testing loss: 1.7533973455429077\n",
      "Accuracy: 0.4164\n",
      "epoch: 740, training loss: 1.214359998703003, testing loss: 1.6044155359268188\n",
      "Accuracy: 0.4604\n",
      "epoch: 740, training loss: 1.2191215753555298, testing loss: 1.77962064743042\n",
      "Accuracy: 0.4102\n",
      "epoch: 740, training loss: 1.209309697151184, testing loss: 1.6555168628692627\n",
      "Accuracy: 0.4693\n",
      "epoch: 740, training loss: 1.3538167476654053, testing loss: 1.7086005210876465\n",
      "Accuracy: 0.4268\n",
      "epoch: 740, training loss: 1.2406833171844482, testing loss: 1.6792173385620117\n",
      "Accuracy: 0.4155\n",
      "epoch: 740, training loss: 1.3073219060897827, testing loss: 1.6448276042938232\n",
      "Accuracy: 0.3958\n",
      "epoch: 740, training loss: 1.1940490007400513, testing loss: 1.6357526779174805\n",
      "Accuracy: 0.4367\n",
      "epoch: 750, training loss: 1.2654904127120972, testing loss: 1.6472771167755127\n",
      "Accuracy: 0.4565\n",
      "epoch: 750, training loss: 1.230799913406372, testing loss: 1.7514532804489136\n",
      "Accuracy: 0.4007\n",
      "epoch: 750, training loss: 1.388149619102478, testing loss: 1.8134467601776123\n",
      "Accuracy: 0.3906\n",
      "epoch: 750, training loss: 1.2209290266036987, testing loss: 1.644787311553955\n",
      "Accuracy: 0.4051\n",
      "epoch: 750, training loss: 1.2854642868041992, testing loss: 1.7972699403762817\n",
      "Accuracy: 0.3870\n",
      "epoch: 750, training loss: 1.160558819770813, testing loss: 1.6301484107971191\n",
      "Accuracy: 0.4728\n",
      "epoch: 750, training loss: 1.2661733627319336, testing loss: 1.752029299736023\n",
      "Accuracy: 0.4088\n",
      "epoch: 750, training loss: 1.246518611907959, testing loss: 1.7150386571884155\n",
      "Accuracy: 0.3934\n",
      "epoch: 750, training loss: 1.214665174484253, testing loss: 1.8302942514419556\n",
      "Accuracy: 0.4259\n",
      "epoch: 750, training loss: 1.2607208490371704, testing loss: 1.8076884746551514\n",
      "Accuracy: 0.4114\n",
      "epoch: 750, training loss: 1.2386808395385742, testing loss: 1.722788691520691\n",
      "Accuracy: 0.4183\n",
      "epoch: 750, training loss: 1.3416216373443604, testing loss: 1.648903250694275\n",
      "Accuracy: 0.4247\n",
      "epoch: 750, training loss: 1.236240267753601, testing loss: 1.8019500970840454\n",
      "Accuracy: 0.4147\n",
      "epoch: 750, training loss: 1.1847518682479858, testing loss: 1.664152979850769\n",
      "Accuracy: 0.4052\n",
      "epoch: 750, training loss: 1.1972891092300415, testing loss: 1.6719383001327515\n",
      "Accuracy: 0.4196\n",
      "epoch: 750, training loss: 1.1992664337158203, testing loss: 1.748026967048645\n",
      "Accuracy: 0.4121\n",
      "epoch: 750, training loss: 1.2222540378570557, testing loss: 1.643048882484436\n",
      "Accuracy: 0.4151\n",
      "epoch: 750, training loss: 1.2807799577713013, testing loss: 1.7627546787261963\n",
      "Accuracy: 0.4048\n",
      "epoch: 750, training loss: 1.3696612119674683, testing loss: 1.7239457368850708\n",
      "Accuracy: 0.3927\n",
      "epoch: 750, training loss: 1.2862136363983154, testing loss: 1.6726735830307007\n",
      "Accuracy: 0.4452\n",
      "epoch: 750, training loss: 1.2527470588684082, testing loss: 1.6928349733352661\n",
      "Accuracy: 0.4305\n",
      "epoch: 750, training loss: 1.3117378950119019, testing loss: 1.7039401531219482\n",
      "Accuracy: 0.3981\n",
      "epoch: 750, training loss: 1.215071439743042, testing loss: 1.6300417184829712\n",
      "Accuracy: 0.4385\n",
      "epoch: 750, training loss: 1.2585062980651855, testing loss: 1.684522032737732\n",
      "Accuracy: 0.3891\n",
      "epoch: 750, training loss: 1.2174676656723022, testing loss: 1.596039056777954\n",
      "Accuracy: 0.4185\n",
      "epoch: 750, training loss: 1.2859817743301392, testing loss: 1.6522331237792969\n",
      "Accuracy: 0.4516\n",
      "epoch: 750, training loss: 1.2349092960357666, testing loss: 1.7236766815185547\n",
      "Accuracy: 0.4441\n",
      "epoch: 750, training loss: 1.3393205404281616, testing loss: 1.771472454071045\n",
      "Accuracy: 0.4356\n",
      "epoch: 750, training loss: 1.1599451303482056, testing loss: 1.610449194908142\n",
      "Accuracy: 0.4408\n",
      "epoch: 750, training loss: 1.2195957899093628, testing loss: 1.7768017053604126\n",
      "Accuracy: 0.4119\n",
      "epoch: 750, training loss: 1.2299138307571411, testing loss: 1.7245361804962158\n",
      "Accuracy: 0.3695\n",
      "epoch: 750, training loss: 1.2767109870910645, testing loss: 1.8431017398834229\n",
      "Accuracy: 0.3878\n",
      "epoch: 750, training loss: 1.2360035181045532, testing loss: 1.584152102470398\n",
      "Accuracy: 0.4558\n",
      "epoch: 750, training loss: 1.2889350652694702, testing loss: 1.7290129661560059\n",
      "Accuracy: 0.3825\n",
      "epoch: 750, training loss: 1.2362767457962036, testing loss: 1.7663124799728394\n",
      "Accuracy: 0.4215\n",
      "epoch: 750, training loss: 1.2643210887908936, testing loss: 1.827155590057373\n",
      "Accuracy: 0.3779\n",
      "epoch: 750, training loss: 1.2773874998092651, testing loss: 1.674818515777588\n",
      "Accuracy: 0.4209\n",
      "epoch: 750, training loss: 1.2605230808258057, testing loss: 1.6538196802139282\n",
      "Accuracy: 0.4164\n",
      "epoch: 750, training loss: 1.2355163097381592, testing loss: 1.5768072605133057\n",
      "Accuracy: 0.4567\n",
      "epoch: 750, training loss: 1.2648805379867554, testing loss: 1.6296038627624512\n",
      "Accuracy: 0.3942\n",
      "epoch: 750, training loss: 1.2669048309326172, testing loss: 1.7034131288528442\n",
      "Accuracy: 0.4195\n",
      "epoch: 750, training loss: 1.2017101049423218, testing loss: 1.7087780237197876\n",
      "Accuracy: 0.4185\n",
      "epoch: 750, training loss: 1.2464796304702759, testing loss: 1.7723307609558105\n",
      "Accuracy: 0.3746\n",
      "epoch: 750, training loss: 1.2793357372283936, testing loss: 1.795378565788269\n",
      "Accuracy: 0.3846\n",
      "epoch: 750, training loss: 1.2546465396881104, testing loss: 1.7637698650360107\n",
      "Accuracy: 0.3987\n",
      "epoch: 750, training loss: 1.2081618309020996, testing loss: 1.731243371963501\n",
      "Accuracy: 0.4135\n",
      "epoch: 750, training loss: 1.2042713165283203, testing loss: 1.70747709274292\n",
      "Accuracy: 0.4073\n",
      "epoch: 750, training loss: 1.1949975490570068, testing loss: 1.880426049232483\n",
      "Accuracy: 0.4048\n",
      "epoch: 750, training loss: 1.2609450817108154, testing loss: 1.6617870330810547\n",
      "Accuracy: 0.4611\n",
      "epoch: 750, training loss: 1.1927303075790405, testing loss: 1.6770585775375366\n",
      "Accuracy: 0.3902\n",
      "epoch: 750, training loss: 1.1707621812820435, testing loss: 1.6371089220046997\n",
      "Accuracy: 0.4335\n",
      "epoch: 750, training loss: 1.221197247505188, testing loss: 1.71865713596344\n",
      "Accuracy: 0.4146\n",
      "epoch: 750, training loss: 1.3169528245925903, testing loss: 1.715849757194519\n",
      "Accuracy: 0.3938\n",
      "epoch: 750, training loss: 1.2526353597640991, testing loss: 1.8208017349243164\n",
      "Accuracy: 0.3968\n",
      "epoch: 750, training loss: 1.280994176864624, testing loss: 1.7759653329849243\n",
      "Accuracy: 0.4527\n",
      "epoch: 750, training loss: 1.266314148902893, testing loss: 1.844089150428772\n",
      "Accuracy: 0.4068\n",
      "epoch: 750, training loss: 1.2191721200942993, testing loss: 1.9561998844146729\n",
      "Accuracy: 0.3883\n",
      "epoch: 750, training loss: 1.267282485961914, testing loss: 1.6452105045318604\n",
      "Accuracy: 0.4128\n",
      "epoch: 750, training loss: 1.1914470195770264, testing loss: 1.6206203699111938\n",
      "Accuracy: 0.4383\n",
      "epoch: 750, training loss: 1.20579993724823, testing loss: 1.7565640211105347\n",
      "Accuracy: 0.4187\n",
      "epoch: 750, training loss: 1.206659197807312, testing loss: 1.7713052034378052\n",
      "Accuracy: 0.4043\n",
      "epoch: 750, training loss: 1.2169588804244995, testing loss: 1.6527289152145386\n",
      "Accuracy: 0.3855\n",
      "epoch: 750, training loss: 1.2789113521575928, testing loss: 1.7358092069625854\n",
      "Accuracy: 0.4050\n",
      "epoch: 750, training loss: 1.2457681894302368, testing loss: 1.6975094079971313\n",
      "Accuracy: 0.3885\n",
      "epoch: 750, training loss: 1.2919979095458984, testing loss: 1.6773144006729126\n",
      "Accuracy: 0.4296\n",
      "epoch: 750, training loss: 1.2880895137786865, testing loss: 1.6901146173477173\n",
      "Accuracy: 0.3975\n",
      "epoch: 750, training loss: 1.2400566339492798, testing loss: 1.7604678869247437\n",
      "Accuracy: 0.3826\n",
      "epoch: 750, training loss: 1.2615963220596313, testing loss: 1.6907895803451538\n",
      "Accuracy: 0.3931\n",
      "epoch: 750, training loss: 1.2479168176651, testing loss: 1.8098595142364502\n",
      "Accuracy: 0.3987\n",
      "epoch: 750, training loss: 1.2083953619003296, testing loss: 1.753919005393982\n",
      "Accuracy: 0.3975\n",
      "epoch: 750, training loss: 1.2533624172210693, testing loss: 1.7601394653320312\n",
      "Accuracy: 0.4411\n",
      "epoch: 750, training loss: 1.2969856262207031, testing loss: 1.7358120679855347\n",
      "Accuracy: 0.4102\n",
      "epoch: 750, training loss: 1.2191909551620483, testing loss: 1.7270097732543945\n",
      "Accuracy: 0.4107\n",
      "epoch: 750, training loss: 1.2672052383422852, testing loss: 1.9448866844177246\n",
      "Accuracy: 0.3627\n",
      "epoch: 750, training loss: 1.2107075452804565, testing loss: 1.7387640476226807\n",
      "Accuracy: 0.4355\n",
      "epoch: 750, training loss: 1.2849012613296509, testing loss: 1.6317814588546753\n",
      "Accuracy: 0.4006\n",
      "epoch: 750, training loss: 1.2799932956695557, testing loss: 1.7871562242507935\n",
      "Accuracy: 0.3738\n",
      "epoch: 750, training loss: 1.2176212072372437, testing loss: 1.6952269077301025\n",
      "Accuracy: 0.4040\n",
      "epoch: 750, training loss: 1.27962327003479, testing loss: 1.6572399139404297\n",
      "Accuracy: 0.4281\n",
      "epoch: 750, training loss: 1.2344855070114136, testing loss: 1.7082176208496094\n",
      "Accuracy: 0.4212\n",
      "epoch: 750, training loss: 1.207080602645874, testing loss: 1.6826249361038208\n",
      "Accuracy: 0.4572\n",
      "epoch: 750, training loss: 1.2172095775604248, testing loss: 1.8171777725219727\n",
      "Accuracy: 0.4021\n",
      "epoch: 750, training loss: 1.3057154417037964, testing loss: 1.6855401992797852\n",
      "Accuracy: 0.3824\n",
      "epoch: 750, training loss: 1.232720971107483, testing loss: 1.6129050254821777\n",
      "Accuracy: 0.4262\n",
      "epoch: 750, training loss: 1.2424964904785156, testing loss: 1.508894443511963\n",
      "Accuracy: 0.4682\n",
      "epoch: 750, training loss: 1.2304589748382568, testing loss: 1.669963002204895\n",
      "Accuracy: 0.4299\n",
      "epoch: 750, training loss: 1.260221242904663, testing loss: 1.728287935256958\n",
      "Accuracy: 0.3941\n",
      "epoch: 750, training loss: 1.2969000339508057, testing loss: 1.7309775352478027\n",
      "Accuracy: 0.4105\n",
      "epoch: 750, training loss: 1.296250581741333, testing loss: 1.7142829895019531\n",
      "Accuracy: 0.3950\n",
      "epoch: 750, training loss: 1.2523012161254883, testing loss: 1.6012119054794312\n",
      "Accuracy: 0.4572\n",
      "epoch: 750, training loss: 1.220002293586731, testing loss: 1.6650663614273071\n",
      "Accuracy: 0.3980\n",
      "epoch: 750, training loss: 1.2518537044525146, testing loss: 1.6683831214904785\n",
      "Accuracy: 0.4140\n",
      "epoch: 750, training loss: 1.2191493511199951, testing loss: 1.689755916595459\n",
      "Accuracy: 0.4233\n",
      "epoch: 750, training loss: 1.2307767868041992, testing loss: 1.8991013765335083\n",
      "Accuracy: 0.4149\n",
      "epoch: 750, training loss: 1.319702386856079, testing loss: 1.84274160861969\n",
      "Accuracy: 0.4078\n",
      "epoch: 750, training loss: 1.2241402864456177, testing loss: 1.5984022617340088\n",
      "Accuracy: 0.4410\n",
      "epoch: 750, training loss: 1.240204095840454, testing loss: 1.7036851644515991\n",
      "Accuracy: 0.4018\n",
      "epoch: 750, training loss: 1.1942906379699707, testing loss: 1.683127760887146\n",
      "Accuracy: 0.4137\n",
      "epoch: 750, training loss: 1.2218953371047974, testing loss: 1.6170157194137573\n",
      "Accuracy: 0.3944\n",
      "epoch: 750, training loss: 1.206248164176941, testing loss: 1.614006757736206\n",
      "Accuracy: 0.4144\n",
      "epoch: 760, training loss: 1.2346917390823364, testing loss: 1.6603137254714966\n",
      "Accuracy: 0.3948\n",
      "epoch: 760, training loss: 1.263993263244629, testing loss: 1.6912444829940796\n",
      "Accuracy: 0.4669\n",
      "epoch: 760, training loss: 1.18418288230896, testing loss: 1.7407610416412354\n",
      "Accuracy: 0.3713\n",
      "epoch: 760, training loss: 1.2433663606643677, testing loss: 1.7482006549835205\n",
      "Accuracy: 0.4026\n",
      "epoch: 760, training loss: 1.2019387483596802, testing loss: 1.7083327770233154\n",
      "Accuracy: 0.4414\n",
      "epoch: 760, training loss: 1.3300750255584717, testing loss: 1.842244267463684\n",
      "Accuracy: 0.3828\n",
      "epoch: 760, training loss: 1.22361159324646, testing loss: 1.6543329954147339\n",
      "Accuracy: 0.4104\n",
      "epoch: 760, training loss: 1.2481085062026978, testing loss: 1.5720700025558472\n",
      "Accuracy: 0.4313\n",
      "epoch: 760, training loss: 1.2625268697738647, testing loss: 1.7699447870254517\n",
      "Accuracy: 0.4114\n",
      "epoch: 760, training loss: 1.2494678497314453, testing loss: 1.764296293258667\n",
      "Accuracy: 0.3939\n",
      "epoch: 760, training loss: 1.2290964126586914, testing loss: 1.7310729026794434\n",
      "Accuracy: 0.4169\n",
      "epoch: 760, training loss: 1.2780879735946655, testing loss: 1.695245623588562\n",
      "Accuracy: 0.4112\n",
      "epoch: 760, training loss: 1.2190372943878174, testing loss: 1.7267712354660034\n",
      "Accuracy: 0.3576\n",
      "epoch: 760, training loss: 1.2460225820541382, testing loss: 1.679896354675293\n",
      "Accuracy: 0.4101\n",
      "epoch: 760, training loss: 1.1943916082382202, testing loss: 1.8047517538070679\n",
      "Accuracy: 0.4145\n",
      "epoch: 760, training loss: 1.2469699382781982, testing loss: 1.8205797672271729\n",
      "Accuracy: 0.3333\n",
      "epoch: 760, training loss: 1.2268857955932617, testing loss: 1.5851716995239258\n",
      "Accuracy: 0.4337\n",
      "epoch: 760, training loss: 1.2560268640518188, testing loss: 1.7675223350524902\n",
      "Accuracy: 0.3963\n",
      "epoch: 760, training loss: 1.2301177978515625, testing loss: 1.8002674579620361\n",
      "Accuracy: 0.4393\n",
      "epoch: 760, training loss: 1.1934611797332764, testing loss: 1.6587958335876465\n",
      "Accuracy: 0.4136\n",
      "epoch: 760, training loss: 1.255335807800293, testing loss: 1.6726343631744385\n",
      "Accuracy: 0.4342\n",
      "epoch: 760, training loss: 1.2326902151107788, testing loss: 1.7467519044876099\n",
      "Accuracy: 0.3816\n",
      "epoch: 760, training loss: 1.286109209060669, testing loss: 1.6195179224014282\n",
      "Accuracy: 0.4578\n",
      "epoch: 760, training loss: 1.22980797290802, testing loss: 1.7953145503997803\n",
      "Accuracy: 0.3703\n",
      "epoch: 760, training loss: 1.2705284357070923, testing loss: 1.728670358657837\n",
      "Accuracy: 0.4102\n",
      "epoch: 760, training loss: 1.2407854795455933, testing loss: 1.779540777206421\n",
      "Accuracy: 0.3851\n",
      "epoch: 760, training loss: 1.2951204776763916, testing loss: 1.5917023420333862\n",
      "Accuracy: 0.4062\n",
      "epoch: 760, training loss: 1.2146432399749756, testing loss: 1.8618054389953613\n",
      "Accuracy: 0.3170\n",
      "epoch: 760, training loss: 1.1753883361816406, testing loss: 1.764002799987793\n",
      "Accuracy: 0.3816\n",
      "epoch: 760, training loss: 1.2940001487731934, testing loss: 1.6281003952026367\n",
      "Accuracy: 0.4424\n",
      "epoch: 760, training loss: 1.1829662322998047, testing loss: 1.7463395595550537\n",
      "Accuracy: 0.4330\n",
      "epoch: 760, training loss: 1.1752893924713135, testing loss: 1.728847622871399\n",
      "Accuracy: 0.4390\n",
      "epoch: 760, training loss: 1.1935189962387085, testing loss: 1.7754237651824951\n",
      "Accuracy: 0.3841\n",
      "epoch: 760, training loss: 1.207016944885254, testing loss: 1.6928514242172241\n",
      "Accuracy: 0.4397\n",
      "epoch: 760, training loss: 1.264245867729187, testing loss: 1.8534157276153564\n",
      "Accuracy: 0.4237\n",
      "epoch: 760, training loss: 1.2695201635360718, testing loss: 1.6551127433776855\n",
      "Accuracy: 0.4347\n",
      "epoch: 760, training loss: 1.2805267572402954, testing loss: 1.8415062427520752\n",
      "Accuracy: 0.4034\n",
      "epoch: 760, training loss: 1.2860896587371826, testing loss: 1.6248878240585327\n",
      "Accuracy: 0.4642\n",
      "epoch: 760, training loss: 1.2570645809173584, testing loss: 1.8033738136291504\n",
      "Accuracy: 0.4107\n",
      "epoch: 760, training loss: 1.1575031280517578, testing loss: 1.6938055753707886\n",
      "Accuracy: 0.4295\n",
      "epoch: 760, training loss: 1.2488468885421753, testing loss: 1.6655750274658203\n",
      "Accuracy: 0.4311\n",
      "epoch: 760, training loss: 1.185538649559021, testing loss: 1.7853485345840454\n",
      "Accuracy: 0.3761\n",
      "epoch: 760, training loss: 1.2450031042099, testing loss: 1.8120695352554321\n",
      "Accuracy: 0.3819\n",
      "epoch: 760, training loss: 1.2338720560073853, testing loss: 1.6905194520950317\n",
      "Accuracy: 0.4172\n",
      "epoch: 760, training loss: 1.195347547531128, testing loss: 1.676969289779663\n",
      "Accuracy: 0.4575\n",
      "epoch: 760, training loss: 1.2963626384735107, testing loss: 1.862256646156311\n",
      "Accuracy: 0.3721\n",
      "epoch: 760, training loss: 1.2528764009475708, testing loss: 1.6724153757095337\n",
      "Accuracy: 0.4441\n",
      "epoch: 760, training loss: 1.207292079925537, testing loss: 1.7138930559158325\n",
      "Accuracy: 0.4490\n",
      "epoch: 760, training loss: 1.265658974647522, testing loss: 1.8865238428115845\n",
      "Accuracy: 0.4019\n",
      "epoch: 760, training loss: 1.165751338005066, testing loss: 1.6116093397140503\n",
      "Accuracy: 0.4335\n",
      "epoch: 760, training loss: 1.232300043106079, testing loss: 1.630790114402771\n",
      "Accuracy: 0.4543\n",
      "epoch: 760, training loss: 1.2572969198226929, testing loss: 1.673323631286621\n",
      "Accuracy: 0.4411\n",
      "epoch: 760, training loss: 1.270490288734436, testing loss: 1.6517966985702515\n",
      "Accuracy: 0.4486\n",
      "epoch: 760, training loss: 1.218366265296936, testing loss: 1.674303650856018\n",
      "Accuracy: 0.4599\n",
      "epoch: 760, training loss: 1.1784073114395142, testing loss: 1.7343344688415527\n",
      "Accuracy: 0.4371\n",
      "epoch: 760, training loss: 1.1862634420394897, testing loss: 1.7482712268829346\n",
      "Accuracy: 0.4437\n",
      "epoch: 760, training loss: 1.2475990056991577, testing loss: 1.7066130638122559\n",
      "Accuracy: 0.4180\n",
      "epoch: 760, training loss: 1.2961511611938477, testing loss: 1.7020342350006104\n",
      "Accuracy: 0.4370\n",
      "epoch: 760, training loss: 1.2367923259735107, testing loss: 1.6784025430679321\n",
      "Accuracy: 0.4062\n",
      "epoch: 760, training loss: 1.2109336853027344, testing loss: 1.7227307558059692\n",
      "Accuracy: 0.4039\n",
      "epoch: 760, training loss: 1.272865891456604, testing loss: 1.8285369873046875\n",
      "Accuracy: 0.4149\n",
      "epoch: 760, training loss: 1.2439695596694946, testing loss: 1.8989990949630737\n",
      "Accuracy: 0.3908\n",
      "epoch: 760, training loss: 1.2146445512771606, testing loss: 1.7103902101516724\n",
      "Accuracy: 0.4212\n",
      "epoch: 760, training loss: 1.2190355062484741, testing loss: 1.7439980506896973\n",
      "Accuracy: 0.4218\n",
      "epoch: 760, training loss: 1.3018487691879272, testing loss: 1.765620231628418\n",
      "Accuracy: 0.4320\n",
      "epoch: 760, training loss: 1.2191494703292847, testing loss: 1.8435359001159668\n",
      "Accuracy: 0.3824\n",
      "epoch: 760, training loss: 1.2773982286453247, testing loss: 1.6834948062896729\n",
      "Accuracy: 0.4268\n",
      "epoch: 760, training loss: 1.2340149879455566, testing loss: 1.7475181818008423\n",
      "Accuracy: 0.4209\n",
      "epoch: 760, training loss: 1.3068125247955322, testing loss: 1.822524905204773\n",
      "Accuracy: 0.3758\n",
      "epoch: 760, training loss: 1.2408335208892822, testing loss: 1.7199962139129639\n",
      "Accuracy: 0.4164\n",
      "epoch: 760, training loss: 1.2901850938796997, testing loss: 1.7760214805603027\n",
      "Accuracy: 0.4518\n",
      "epoch: 760, training loss: 1.2333282232284546, testing loss: 1.6164062023162842\n",
      "Accuracy: 0.4587\n",
      "epoch: 760, training loss: 1.2633001804351807, testing loss: 1.662652611732483\n",
      "Accuracy: 0.4566\n",
      "epoch: 760, training loss: 1.289287805557251, testing loss: 1.5224006175994873\n",
      "Accuracy: 0.4229\n",
      "epoch: 760, training loss: 1.2422634363174438, testing loss: 1.681334376335144\n",
      "Accuracy: 0.4624\n",
      "epoch: 760, training loss: 1.1953916549682617, testing loss: 1.6289777755737305\n",
      "Accuracy: 0.4096\n",
      "epoch: 760, training loss: 1.2853469848632812, testing loss: 1.5094681978225708\n",
      "Accuracy: 0.4409\n",
      "epoch: 760, training loss: 1.1780978441238403, testing loss: 1.7275128364562988\n",
      "Accuracy: 0.4335\n",
      "epoch: 760, training loss: 1.3167673349380493, testing loss: 1.6585605144500732\n",
      "Accuracy: 0.3980\n",
      "epoch: 760, training loss: 1.205497145652771, testing loss: 1.6873897314071655\n",
      "Accuracy: 0.4000\n",
      "epoch: 760, training loss: 1.2775300741195679, testing loss: 1.6815557479858398\n",
      "Accuracy: 0.4239\n",
      "epoch: 760, training loss: 1.262725591659546, testing loss: 1.6728333234786987\n",
      "Accuracy: 0.4281\n",
      "epoch: 760, training loss: 1.2391685247421265, testing loss: 1.675704002380371\n",
      "Accuracy: 0.3754\n",
      "epoch: 760, training loss: 1.2084286212921143, testing loss: 1.8001081943511963\n",
      "Accuracy: 0.3795\n",
      "epoch: 760, training loss: 1.1785850524902344, testing loss: 1.7302998304367065\n",
      "Accuracy: 0.3584\n",
      "epoch: 760, training loss: 1.1689698696136475, testing loss: 1.6793245077133179\n",
      "Accuracy: 0.4346\n",
      "epoch: 760, training loss: 1.1653608083724976, testing loss: 1.7407515048980713\n",
      "Accuracy: 0.4198\n",
      "epoch: 760, training loss: 1.2251120805740356, testing loss: 1.7053821086883545\n",
      "Accuracy: 0.4156\n",
      "epoch: 760, training loss: 1.2408273220062256, testing loss: 1.7188888788223267\n",
      "Accuracy: 0.4338\n",
      "epoch: 760, training loss: 1.247188687324524, testing loss: 1.7567837238311768\n",
      "Accuracy: 0.4370\n",
      "epoch: 760, training loss: 1.252641201019287, testing loss: 1.7246482372283936\n",
      "Accuracy: 0.4167\n",
      "epoch: 760, training loss: 1.2052947282791138, testing loss: 1.613587498664856\n",
      "Accuracy: 0.4476\n",
      "epoch: 760, training loss: 1.2579727172851562, testing loss: 1.781480073928833\n",
      "Accuracy: 0.4046\n",
      "epoch: 760, training loss: 1.3153107166290283, testing loss: 1.649702787399292\n",
      "Accuracy: 0.4573\n",
      "epoch: 760, training loss: 1.2381513118743896, testing loss: 1.7245358228683472\n",
      "Accuracy: 0.4286\n",
      "epoch: 760, training loss: 1.212457537651062, testing loss: 1.809815526008606\n",
      "Accuracy: 0.3810\n",
      "epoch: 760, training loss: 1.230137825012207, testing loss: 1.6733862161636353\n",
      "Accuracy: 0.4383\n",
      "epoch: 760, training loss: 1.2449918985366821, testing loss: 1.7074648141860962\n",
      "Accuracy: 0.4359\n",
      "epoch: 760, training loss: 1.1725932359695435, testing loss: 1.725682020187378\n",
      "Accuracy: 0.4167\n",
      "epoch: 760, training loss: 1.2380897998809814, testing loss: 1.9509638547897339\n",
      "Accuracy: 0.4048\n",
      "epoch: 770, training loss: 1.263949990272522, testing loss: 1.6331799030303955\n",
      "Accuracy: 0.4416\n",
      "epoch: 770, training loss: 1.2135742902755737, testing loss: 1.7561336755752563\n",
      "Accuracy: 0.3973\n",
      "epoch: 770, training loss: 1.2307344675064087, testing loss: 1.6023533344268799\n",
      "Accuracy: 0.4385\n",
      "epoch: 770, training loss: 1.2428311109542847, testing loss: 1.8864402770996094\n",
      "Accuracy: 0.3922\n",
      "epoch: 770, training loss: 1.279977798461914, testing loss: 1.8139853477478027\n",
      "Accuracy: 0.4303\n",
      "epoch: 770, training loss: 1.2327111959457397, testing loss: 1.8349359035491943\n",
      "Accuracy: 0.3406\n",
      "epoch: 770, training loss: 1.2186354398727417, testing loss: 1.7031961679458618\n",
      "Accuracy: 0.3824\n",
      "epoch: 770, training loss: 1.2103478908538818, testing loss: 1.7783445119857788\n",
      "Accuracy: 0.4455\n",
      "epoch: 770, training loss: 1.2288907766342163, testing loss: 1.7580676078796387\n",
      "Accuracy: 0.4377\n",
      "epoch: 770, training loss: 1.25520920753479, testing loss: 1.7735953330993652\n",
      "Accuracy: 0.4180\n",
      "epoch: 770, training loss: 1.2688839435577393, testing loss: 1.7044185400009155\n",
      "Accuracy: 0.3975\n",
      "epoch: 770, training loss: 1.3286137580871582, testing loss: 1.8168764114379883\n",
      "Accuracy: 0.3529\n",
      "epoch: 770, training loss: 1.261277198791504, testing loss: 1.735990047454834\n",
      "Accuracy: 0.4007\n",
      "epoch: 770, training loss: 1.2277193069458008, testing loss: 1.5878567695617676\n",
      "Accuracy: 0.4444\n",
      "epoch: 770, training loss: 1.2375680208206177, testing loss: 1.611525058746338\n",
      "Accuracy: 0.4675\n",
      "epoch: 770, training loss: 1.2178183794021606, testing loss: 1.7071269750595093\n",
      "Accuracy: 0.3918\n",
      "epoch: 770, training loss: 1.2940011024475098, testing loss: 1.7242494821548462\n",
      "Accuracy: 0.4388\n",
      "epoch: 770, training loss: 1.2669532299041748, testing loss: 1.8000614643096924\n",
      "Accuracy: 0.3963\n",
      "epoch: 770, training loss: 1.2033936977386475, testing loss: 1.595616102218628\n",
      "Accuracy: 0.5120\n",
      "epoch: 770, training loss: 1.2480374574661255, testing loss: 1.6454938650131226\n",
      "Accuracy: 0.4375\n",
      "epoch: 770, training loss: 1.2088181972503662, testing loss: 1.5872999429702759\n",
      "Accuracy: 0.4602\n",
      "epoch: 770, training loss: 1.2720927000045776, testing loss: 1.6630494594573975\n",
      "Accuracy: 0.4020\n",
      "epoch: 770, training loss: 1.2873719930648804, testing loss: 1.725767731666565\n",
      "Accuracy: 0.4335\n",
      "epoch: 770, training loss: 1.2085390090942383, testing loss: 1.8021756410598755\n",
      "Accuracy: 0.4130\n",
      "epoch: 770, training loss: 1.1651660203933716, testing loss: 1.6434417963027954\n",
      "Accuracy: 0.4411\n",
      "epoch: 770, training loss: 1.284717321395874, testing loss: 1.7295793294906616\n",
      "Accuracy: 0.4434\n",
      "epoch: 770, training loss: 1.2422664165496826, testing loss: 1.7307419776916504\n",
      "Accuracy: 0.4326\n",
      "epoch: 770, training loss: 1.2041245698928833, testing loss: 1.8758174180984497\n",
      "Accuracy: 0.4232\n",
      "epoch: 770, training loss: 1.2050018310546875, testing loss: 1.7110177278518677\n",
      "Accuracy: 0.4277\n",
      "epoch: 770, training loss: 1.2172447443008423, testing loss: 1.6420034170150757\n",
      "Accuracy: 0.4545\n",
      "epoch: 770, training loss: 1.2392991781234741, testing loss: 1.937142014503479\n",
      "Accuracy: 0.4081\n",
      "epoch: 770, training loss: 1.2153534889221191, testing loss: 1.6960068941116333\n",
      "Accuracy: 0.4032\n",
      "epoch: 770, training loss: 1.2634696960449219, testing loss: 1.6648958921432495\n",
      "Accuracy: 0.4198\n",
      "epoch: 770, training loss: 1.3502333164215088, testing loss: 1.799744725227356\n",
      "Accuracy: 0.3879\n",
      "epoch: 770, training loss: 1.2203160524368286, testing loss: 1.7467637062072754\n",
      "Accuracy: 0.3639\n",
      "epoch: 770, training loss: 1.2634063959121704, testing loss: 1.7404892444610596\n",
      "Accuracy: 0.4020\n",
      "epoch: 770, training loss: 1.2185593843460083, testing loss: 1.6672184467315674\n",
      "Accuracy: 0.4371\n",
      "epoch: 770, training loss: 1.2078711986541748, testing loss: 1.7014423608779907\n",
      "Accuracy: 0.4514\n",
      "epoch: 770, training loss: 1.2295019626617432, testing loss: 1.6903748512268066\n",
      "Accuracy: 0.4586\n",
      "epoch: 770, training loss: 1.2804559469223022, testing loss: 1.7846336364746094\n",
      "Accuracy: 0.4470\n",
      "epoch: 770, training loss: 1.246150255203247, testing loss: 1.664862036705017\n",
      "Accuracy: 0.4716\n",
      "epoch: 770, training loss: 1.2673354148864746, testing loss: 1.7093391418457031\n",
      "Accuracy: 0.4167\n",
      "epoch: 770, training loss: 1.221113681793213, testing loss: 1.7787861824035645\n",
      "Accuracy: 0.3994\n",
      "epoch: 770, training loss: 1.1847281455993652, testing loss: 1.6160447597503662\n",
      "Accuracy: 0.4620\n",
      "epoch: 770, training loss: 1.1831780672073364, testing loss: 1.7382620573043823\n",
      "Accuracy: 0.4223\n",
      "epoch: 770, training loss: 1.259400486946106, testing loss: 1.886542797088623\n",
      "Accuracy: 0.3495\n",
      "epoch: 770, training loss: 1.2300900220870972, testing loss: 1.621795654296875\n",
      "Accuracy: 0.4512\n",
      "epoch: 770, training loss: 1.2471914291381836, testing loss: 1.6414591073989868\n",
      "Accuracy: 0.4310\n",
      "epoch: 770, training loss: 1.2371207475662231, testing loss: 1.787161111831665\n",
      "Accuracy: 0.4719\n",
      "epoch: 770, training loss: 1.2805231809616089, testing loss: 1.6538002490997314\n",
      "Accuracy: 0.4112\n",
      "epoch: 770, training loss: 1.316948413848877, testing loss: 1.759857416152954\n",
      "Accuracy: 0.4161\n",
      "epoch: 770, training loss: 1.2346612215042114, testing loss: 1.6831388473510742\n",
      "Accuracy: 0.4049\n",
      "epoch: 770, training loss: 1.2210603952407837, testing loss: 1.6624773740768433\n",
      "Accuracy: 0.3709\n",
      "epoch: 770, training loss: 1.2168165445327759, testing loss: 1.8723585605621338\n",
      "Accuracy: 0.3611\n",
      "epoch: 770, training loss: 1.2518446445465088, testing loss: 1.70999014377594\n",
      "Accuracy: 0.4598\n",
      "epoch: 770, training loss: 1.190657377243042, testing loss: 1.804192066192627\n",
      "Accuracy: 0.4119\n",
      "epoch: 770, training loss: 1.2038027048110962, testing loss: 1.5812631845474243\n",
      "Accuracy: 0.4585\n",
      "epoch: 770, training loss: 1.2720420360565186, testing loss: 1.7924824953079224\n",
      "Accuracy: 0.3974\n",
      "epoch: 770, training loss: 1.2687535285949707, testing loss: 1.7013003826141357\n",
      "Accuracy: 0.3639\n",
      "epoch: 770, training loss: 1.2337725162506104, testing loss: 1.7517104148864746\n",
      "Accuracy: 0.4114\n",
      "epoch: 770, training loss: 1.2417714595794678, testing loss: 1.7946665287017822\n",
      "Accuracy: 0.3829\n",
      "epoch: 770, training loss: 1.2695292234420776, testing loss: 1.7721389532089233\n",
      "Accuracy: 0.3930\n",
      "epoch: 770, training loss: 1.2657549381256104, testing loss: 1.8175331354141235\n",
      "Accuracy: 0.3988\n",
      "epoch: 770, training loss: 1.2551286220550537, testing loss: 1.7514240741729736\n",
      "Accuracy: 0.3851\n",
      "epoch: 770, training loss: 1.2893685102462769, testing loss: 1.7252253293991089\n",
      "Accuracy: 0.4095\n",
      "epoch: 770, training loss: 1.267320990562439, testing loss: 1.7044920921325684\n",
      "Accuracy: 0.4451\n",
      "epoch: 770, training loss: 1.1647138595581055, testing loss: 1.5832743644714355\n",
      "Accuracy: 0.4034\n",
      "epoch: 770, training loss: 1.273826241493225, testing loss: 1.8373165130615234\n",
      "Accuracy: 0.3750\n",
      "epoch: 770, training loss: 1.2682814598083496, testing loss: 1.638185977935791\n",
      "Accuracy: 0.4422\n",
      "epoch: 770, training loss: 1.3345149755477905, testing loss: 1.6879124641418457\n",
      "Accuracy: 0.4539\n",
      "epoch: 770, training loss: 1.1997740268707275, testing loss: 1.5662987232208252\n",
      "Accuracy: 0.4126\n",
      "epoch: 770, training loss: 1.2589040994644165, testing loss: 1.5577889680862427\n",
      "Accuracy: 0.4337\n",
      "epoch: 770, training loss: 1.2321478128433228, testing loss: 1.7365643978118896\n",
      "Accuracy: 0.3815\n",
      "epoch: 770, training loss: 1.2736636400222778, testing loss: 1.793892741203308\n",
      "Accuracy: 0.3813\n",
      "epoch: 770, training loss: 1.22725510597229, testing loss: 1.7412288188934326\n",
      "Accuracy: 0.4112\n",
      "epoch: 770, training loss: 1.1458799839019775, testing loss: 1.6341222524642944\n",
      "Accuracy: 0.4108\n",
      "epoch: 770, training loss: 1.2217659950256348, testing loss: 1.6970784664154053\n",
      "Accuracy: 0.4243\n",
      "epoch: 770, training loss: 1.2425938844680786, testing loss: 1.8923213481903076\n",
      "Accuracy: 0.3945\n",
      "epoch: 770, training loss: 1.2814592123031616, testing loss: 1.68724524974823\n",
      "Accuracy: 0.4201\n",
      "epoch: 770, training loss: 1.2331253290176392, testing loss: 1.7812920808792114\n",
      "Accuracy: 0.4181\n",
      "epoch: 770, training loss: 1.2592891454696655, testing loss: 1.6162112951278687\n",
      "Accuracy: 0.4681\n",
      "epoch: 770, training loss: 1.192307710647583, testing loss: 1.7233054637908936\n",
      "Accuracy: 0.3758\n",
      "epoch: 770, training loss: 1.2130509614944458, testing loss: 1.7719147205352783\n",
      "Accuracy: 0.4337\n",
      "epoch: 770, training loss: 1.2913240194320679, testing loss: 1.6379213333129883\n",
      "Accuracy: 0.4808\n",
      "epoch: 770, training loss: 1.2802139520645142, testing loss: 1.7775593996047974\n",
      "Accuracy: 0.4281\n",
      "epoch: 770, training loss: 1.2288557291030884, testing loss: 1.6801992654800415\n",
      "Accuracy: 0.4174\n",
      "epoch: 770, training loss: 1.2163745164871216, testing loss: 1.7316251993179321\n",
      "Accuracy: 0.3932\n",
      "epoch: 770, training loss: 1.1876182556152344, testing loss: 1.6960479021072388\n",
      "Accuracy: 0.4207\n",
      "epoch: 770, training loss: 1.2552706003189087, testing loss: 1.6621476411819458\n",
      "Accuracy: 0.4526\n",
      "epoch: 770, training loss: 1.2518608570098877, testing loss: 1.7050793170928955\n",
      "Accuracy: 0.4304\n",
      "epoch: 770, training loss: 1.197945237159729, testing loss: 1.798775315284729\n",
      "Accuracy: 0.4094\n",
      "epoch: 770, training loss: 1.2179687023162842, testing loss: 1.7726150751113892\n",
      "Accuracy: 0.4098\n",
      "epoch: 770, training loss: 1.208240032196045, testing loss: 1.783449411392212\n",
      "Accuracy: 0.4349\n",
      "epoch: 770, training loss: 1.2827883958816528, testing loss: 1.6219539642333984\n",
      "Accuracy: 0.4502\n",
      "epoch: 770, training loss: 1.258013129234314, testing loss: 1.6398308277130127\n",
      "Accuracy: 0.4305\n",
      "epoch: 770, training loss: 1.2700111865997314, testing loss: 1.6764212846755981\n",
      "Accuracy: 0.4422\n",
      "epoch: 770, training loss: 1.228408932685852, testing loss: 1.6233755350112915\n",
      "Accuracy: 0.4362\n",
      "epoch: 770, training loss: 1.2224518060684204, testing loss: 1.594085454940796\n",
      "Accuracy: 0.4422\n",
      "epoch: 770, training loss: 1.253149390220642, testing loss: 1.60906183719635\n",
      "Accuracy: 0.4214\n",
      "epoch: 770, training loss: 1.2291799783706665, testing loss: 1.7683852910995483\n",
      "Accuracy: 0.3659\n",
      "epoch: 780, training loss: 1.1720216274261475, testing loss: 1.7750942707061768\n",
      "Accuracy: 0.4012\n",
      "epoch: 780, training loss: 1.2731438875198364, testing loss: 1.6644014120101929\n",
      "Accuracy: 0.4122\n",
      "epoch: 780, training loss: 1.2448949813842773, testing loss: 1.758315920829773\n",
      "Accuracy: 0.4355\n",
      "epoch: 780, training loss: 1.2089948654174805, testing loss: 1.6549750566482544\n",
      "Accuracy: 0.4488\n",
      "epoch: 780, training loss: 1.2338734865188599, testing loss: 1.6821098327636719\n",
      "Accuracy: 0.3716\n",
      "epoch: 780, training loss: 1.2440460920333862, testing loss: 1.6948505640029907\n",
      "Accuracy: 0.4291\n",
      "epoch: 780, training loss: 1.2410972118377686, testing loss: 1.577734112739563\n",
      "Accuracy: 0.4191\n",
      "epoch: 780, training loss: 1.146026849746704, testing loss: 1.7024972438812256\n",
      "Accuracy: 0.4000\n",
      "epoch: 780, training loss: 1.2164758443832397, testing loss: 1.6877806186676025\n",
      "Accuracy: 0.4105\n",
      "epoch: 780, training loss: 1.2459323406219482, testing loss: 1.6885986328125\n",
      "Accuracy: 0.4459\n",
      "epoch: 780, training loss: 1.2470943927764893, testing loss: 1.7880812883377075\n",
      "Accuracy: 0.4058\n",
      "epoch: 780, training loss: 1.267324686050415, testing loss: 1.7029969692230225\n",
      "Accuracy: 0.4236\n",
      "epoch: 780, training loss: 1.2658357620239258, testing loss: 1.7921514511108398\n",
      "Accuracy: 0.3783\n",
      "epoch: 780, training loss: 1.1935527324676514, testing loss: 1.5747536420822144\n",
      "Accuracy: 0.4258\n",
      "epoch: 780, training loss: 1.2259464263916016, testing loss: 1.7602636814117432\n",
      "Accuracy: 0.3887\n",
      "epoch: 780, training loss: 1.278611660003662, testing loss: 1.5385886430740356\n",
      "Accuracy: 0.4581\n",
      "epoch: 780, training loss: 1.3438386917114258, testing loss: 1.79916250705719\n",
      "Accuracy: 0.4167\n",
      "epoch: 780, training loss: 1.200920581817627, testing loss: 1.640123963356018\n",
      "Accuracy: 0.4338\n",
      "epoch: 780, training loss: 1.2060514688491821, testing loss: 1.7081371545791626\n",
      "Accuracy: 0.4321\n",
      "epoch: 780, training loss: 1.2310165166854858, testing loss: 1.588426947593689\n",
      "Accuracy: 0.4149\n",
      "epoch: 780, training loss: 1.2628968954086304, testing loss: 1.6114252805709839\n",
      "Accuracy: 0.3942\n",
      "epoch: 780, training loss: 1.191935420036316, testing loss: 1.6316980123519897\n",
      "Accuracy: 0.4077\n",
      "epoch: 780, training loss: 1.2298024892807007, testing loss: 1.7515616416931152\n",
      "Accuracy: 0.4437\n",
      "epoch: 780, training loss: 1.2843488454818726, testing loss: 1.6296565532684326\n",
      "Accuracy: 0.4259\n",
      "epoch: 780, training loss: 1.1603810787200928, testing loss: 1.7010934352874756\n",
      "Accuracy: 0.4693\n",
      "epoch: 780, training loss: 1.208274245262146, testing loss: 1.6256074905395508\n",
      "Accuracy: 0.4611\n",
      "epoch: 780, training loss: 1.1929049491882324, testing loss: 1.5611162185668945\n",
      "Accuracy: 0.4686\n",
      "epoch: 780, training loss: 1.2421680688858032, testing loss: 1.7344602346420288\n",
      "Accuracy: 0.4686\n",
      "epoch: 780, training loss: 1.2727057933807373, testing loss: 1.7619643211364746\n",
      "Accuracy: 0.3980\n",
      "epoch: 780, training loss: 1.1866337060928345, testing loss: 1.8150262832641602\n",
      "Accuracy: 0.3574\n",
      "epoch: 780, training loss: 1.22200608253479, testing loss: 1.8340286016464233\n",
      "Accuracy: 0.3968\n",
      "epoch: 780, training loss: 1.272779107093811, testing loss: 1.8233344554901123\n",
      "Accuracy: 0.4300\n",
      "epoch: 780, training loss: 1.22149658203125, testing loss: 1.8010839223861694\n",
      "Accuracy: 0.3918\n",
      "epoch: 780, training loss: 1.335480809211731, testing loss: 1.8209095001220703\n",
      "Accuracy: 0.4236\n",
      "epoch: 780, training loss: 1.2536338567733765, testing loss: 1.6210029125213623\n",
      "Accuracy: 0.4514\n",
      "epoch: 780, training loss: 1.3046996593475342, testing loss: 1.8013451099395752\n",
      "Accuracy: 0.4149\n",
      "epoch: 780, training loss: 1.2308520078659058, testing loss: 1.6459777355194092\n",
      "Accuracy: 0.4379\n",
      "epoch: 780, training loss: 1.259083867073059, testing loss: 1.7514153718948364\n",
      "Accuracy: 0.3829\n",
      "epoch: 780, training loss: 1.2812199592590332, testing loss: 1.8171608448028564\n",
      "Accuracy: 0.3754\n",
      "epoch: 780, training loss: 1.2521837949752808, testing loss: 1.6949059963226318\n",
      "Accuracy: 0.4103\n",
      "epoch: 780, training loss: 1.184423804283142, testing loss: 1.7290507555007935\n",
      "Accuracy: 0.3917\n",
      "epoch: 780, training loss: 1.2857885360717773, testing loss: 1.6647166013717651\n",
      "Accuracy: 0.4286\n",
      "epoch: 780, training loss: 1.2185848951339722, testing loss: 1.737790584564209\n",
      "Accuracy: 0.4142\n",
      "epoch: 780, training loss: 1.2383204698562622, testing loss: 1.8499634265899658\n",
      "Accuracy: 0.4007\n",
      "epoch: 780, training loss: 1.2631124258041382, testing loss: 1.8083261251449585\n",
      "Accuracy: 0.4216\n",
      "epoch: 780, training loss: 1.1689103841781616, testing loss: 1.712898850440979\n",
      "Accuracy: 0.4384\n",
      "epoch: 780, training loss: 1.1904003620147705, testing loss: 1.9342228174209595\n",
      "Accuracy: 0.3857\n",
      "epoch: 780, training loss: 1.2728110551834106, testing loss: 1.7736326456069946\n",
      "Accuracy: 0.3964\n",
      "epoch: 780, training loss: 1.2804291248321533, testing loss: 1.6288565397262573\n",
      "Accuracy: 0.4151\n",
      "epoch: 780, training loss: 1.3185322284698486, testing loss: 1.6299729347229004\n",
      "Accuracy: 0.4420\n",
      "epoch: 780, training loss: 1.2744585275650024, testing loss: 1.7235002517700195\n",
      "Accuracy: 0.4032\n",
      "epoch: 780, training loss: 1.2467288970947266, testing loss: 1.6082079410552979\n",
      "Accuracy: 0.4610\n",
      "epoch: 780, training loss: 1.2031649351119995, testing loss: 1.7130279541015625\n",
      "Accuracy: 0.4483\n",
      "epoch: 780, training loss: 1.2599198818206787, testing loss: 1.8084696531295776\n",
      "Accuracy: 0.4052\n",
      "epoch: 780, training loss: 1.3070499897003174, testing loss: 1.5312129259109497\n",
      "Accuracy: 0.4636\n",
      "epoch: 780, training loss: 1.2460851669311523, testing loss: 1.8950964212417603\n",
      "Accuracy: 0.3721\n",
      "epoch: 780, training loss: 1.2653310298919678, testing loss: 1.7119624614715576\n",
      "Accuracy: 0.4128\n",
      "epoch: 780, training loss: 1.2161064147949219, testing loss: 1.7038038969039917\n",
      "Accuracy: 0.4099\n",
      "epoch: 780, training loss: 1.176755428314209, testing loss: 1.6087217330932617\n",
      "Accuracy: 0.4276\n",
      "epoch: 780, training loss: 1.3458482027053833, testing loss: 1.6021291017532349\n",
      "Accuracy: 0.4268\n",
      "epoch: 780, training loss: 1.3002715110778809, testing loss: 1.8129427433013916\n",
      "Accuracy: 0.3873\n",
      "epoch: 780, training loss: 1.2122606039047241, testing loss: 1.6648670434951782\n",
      "Accuracy: 0.4437\n",
      "epoch: 780, training loss: 1.2504616975784302, testing loss: 1.741500973701477\n",
      "Accuracy: 0.4222\n",
      "epoch: 780, training loss: 1.2865372896194458, testing loss: 1.7767165899276733\n",
      "Accuracy: 0.3901\n",
      "epoch: 780, training loss: 1.291305422782898, testing loss: 1.7946871519088745\n",
      "Accuracy: 0.4033\n",
      "epoch: 780, training loss: 1.2871928215026855, testing loss: 1.6470310688018799\n",
      "Accuracy: 0.4385\n",
      "epoch: 780, training loss: 1.245133638381958, testing loss: 1.8296213150024414\n",
      "Accuracy: 0.3922\n",
      "epoch: 780, training loss: 1.3286019563674927, testing loss: 1.6025865077972412\n",
      "Accuracy: 0.4448\n",
      "epoch: 780, training loss: 1.2467918395996094, testing loss: 1.7532265186309814\n",
      "Accuracy: 0.4476\n",
      "epoch: 780, training loss: 1.2964736223220825, testing loss: 1.8274247646331787\n",
      "Accuracy: 0.3931\n",
      "epoch: 780, training loss: 1.3225533962249756, testing loss: 1.6451281309127808\n",
      "Accuracy: 0.4383\n",
      "epoch: 780, training loss: 1.3288640975952148, testing loss: 1.794993281364441\n",
      "Accuracy: 0.3951\n",
      "epoch: 780, training loss: 1.2750358581542969, testing loss: 1.693808913230896\n",
      "Accuracy: 0.4120\n",
      "epoch: 780, training loss: 1.2435979843139648, testing loss: 1.707236409187317\n",
      "Accuracy: 0.4311\n",
      "epoch: 780, training loss: 1.2277135848999023, testing loss: 1.7357170581817627\n",
      "Accuracy: 0.4310\n",
      "epoch: 780, training loss: 1.2450528144836426, testing loss: 1.757066011428833\n",
      "Accuracy: 0.4471\n",
      "epoch: 780, training loss: 1.2428795099258423, testing loss: 1.6576896905899048\n",
      "Accuracy: 0.4182\n",
      "epoch: 780, training loss: 1.2250399589538574, testing loss: 1.6506068706512451\n",
      "Accuracy: 0.4575\n",
      "epoch: 780, training loss: 1.2666693925857544, testing loss: 1.7498418092727661\n",
      "Accuracy: 0.3923\n",
      "epoch: 780, training loss: 1.2643225193023682, testing loss: 1.6493724584579468\n",
      "Accuracy: 0.3863\n",
      "epoch: 780, training loss: 1.2593114376068115, testing loss: 1.651760220527649\n",
      "Accuracy: 0.4304\n",
      "epoch: 780, training loss: 1.240958571434021, testing loss: 1.6796624660491943\n",
      "Accuracy: 0.4037\n",
      "epoch: 780, training loss: 1.2890803813934326, testing loss: 1.8590315580368042\n",
      "Accuracy: 0.3993\n",
      "epoch: 780, training loss: 1.2523581981658936, testing loss: 1.811740517616272\n",
      "Accuracy: 0.3478\n",
      "epoch: 780, training loss: 1.2630276679992676, testing loss: 1.720914363861084\n",
      "Accuracy: 0.4399\n",
      "epoch: 780, training loss: 1.2718076705932617, testing loss: 1.6668027639389038\n",
      "Accuracy: 0.4319\n",
      "epoch: 780, training loss: 1.2525193691253662, testing loss: 1.618201732635498\n",
      "Accuracy: 0.4104\n",
      "epoch: 780, training loss: 1.1978342533111572, testing loss: 1.5981885194778442\n",
      "Accuracy: 0.4277\n",
      "epoch: 780, training loss: 1.2008050680160522, testing loss: 1.7660688161849976\n",
      "Accuracy: 0.4006\n",
      "epoch: 780, training loss: 1.352881908416748, testing loss: 1.6577744483947754\n",
      "Accuracy: 0.4244\n",
      "epoch: 780, training loss: 1.259055495262146, testing loss: 1.7227779626846313\n",
      "Accuracy: 0.3967\n",
      "epoch: 780, training loss: 1.264155626296997, testing loss: 1.7948856353759766\n",
      "Accuracy: 0.4237\n",
      "epoch: 780, training loss: 1.222388505935669, testing loss: 1.773150086402893\n",
      "Accuracy: 0.4339\n",
      "epoch: 780, training loss: 1.3431947231292725, testing loss: 1.645527720451355\n",
      "Accuracy: 0.4625\n",
      "epoch: 780, training loss: 1.2836252450942993, testing loss: 1.5664207935333252\n",
      "Accuracy: 0.4635\n",
      "epoch: 780, training loss: 1.3192814588546753, testing loss: 1.6870313882827759\n",
      "Accuracy: 0.3943\n",
      "epoch: 780, training loss: 1.2376116514205933, testing loss: 1.6034464836120605\n",
      "Accuracy: 0.4277\n",
      "epoch: 780, training loss: 1.265289545059204, testing loss: 1.7159852981567383\n",
      "Accuracy: 0.3567\n",
      "epoch: 780, training loss: 1.2462270259857178, testing loss: 1.746545672416687\n",
      "Accuracy: 0.3854\n",
      "epoch: 780, training loss: 1.3017457723617554, testing loss: 1.6365554332733154\n",
      "Accuracy: 0.4248\n",
      "epoch: 790, training loss: 1.2410098314285278, testing loss: 1.7097374200820923\n",
      "Accuracy: 0.3887\n",
      "epoch: 790, training loss: 1.2453655004501343, testing loss: 1.7302827835083008\n",
      "Accuracy: 0.4393\n",
      "epoch: 790, training loss: 1.1955287456512451, testing loss: 1.6939796209335327\n",
      "Accuracy: 0.4404\n",
      "epoch: 790, training loss: 1.2367490530014038, testing loss: 1.885529637336731\n",
      "Accuracy: 0.4630\n",
      "epoch: 790, training loss: 1.3486660718917847, testing loss: 1.7499022483825684\n",
      "Accuracy: 0.4087\n",
      "epoch: 790, training loss: 1.2341996431350708, testing loss: 1.7428674697875977\n",
      "Accuracy: 0.4036\n",
      "epoch: 790, training loss: 1.2553222179412842, testing loss: 1.811210036277771\n",
      "Accuracy: 0.4196\n",
      "epoch: 790, training loss: 1.214398980140686, testing loss: 1.7417477369308472\n",
      "Accuracy: 0.4371\n",
      "epoch: 790, training loss: 1.3143396377563477, testing loss: 1.7874006032943726\n",
      "Accuracy: 0.3810\n",
      "epoch: 790, training loss: 1.1511023044586182, testing loss: 1.6589398384094238\n",
      "Accuracy: 0.4554\n",
      "epoch: 790, training loss: 1.2046257257461548, testing loss: 1.7458477020263672\n",
      "Accuracy: 0.4158\n",
      "epoch: 790, training loss: 1.2551697492599487, testing loss: 1.5686875581741333\n",
      "Accuracy: 0.4792\n",
      "epoch: 790, training loss: 1.249138593673706, testing loss: 1.6826655864715576\n",
      "Accuracy: 0.4132\n",
      "epoch: 790, training loss: 1.3310569524765015, testing loss: 1.8819869756698608\n",
      "Accuracy: 0.3766\n",
      "epoch: 790, training loss: 1.245724081993103, testing loss: 1.8312596082687378\n",
      "Accuracy: 0.3834\n",
      "epoch: 790, training loss: 1.2413686513900757, testing loss: 1.7020360231399536\n",
      "Accuracy: 0.4503\n",
      "epoch: 790, training loss: 1.302842378616333, testing loss: 1.715126395225525\n",
      "Accuracy: 0.4390\n",
      "epoch: 790, training loss: 1.2450203895568848, testing loss: 1.6104813814163208\n",
      "Accuracy: 0.4242\n",
      "epoch: 790, training loss: 1.2451668977737427, testing loss: 1.714579463005066\n",
      "Accuracy: 0.3789\n",
      "epoch: 790, training loss: 1.2151780128479004, testing loss: 1.6406084299087524\n",
      "Accuracy: 0.4646\n",
      "epoch: 790, training loss: 1.2196636199951172, testing loss: 1.7460428476333618\n",
      "Accuracy: 0.4167\n",
      "epoch: 790, training loss: 1.2612327337265015, testing loss: 1.667271614074707\n",
      "Accuracy: 0.4389\n",
      "epoch: 790, training loss: 1.3005486726760864, testing loss: 1.7107393741607666\n",
      "Accuracy: 0.4108\n",
      "epoch: 790, training loss: 1.2921885251998901, testing loss: 1.7406264543533325\n",
      "Accuracy: 0.4198\n",
      "epoch: 790, training loss: 1.210571527481079, testing loss: 1.765574336051941\n",
      "Accuracy: 0.3924\n",
      "epoch: 790, training loss: 1.2567628622055054, testing loss: 1.6587858200073242\n",
      "Accuracy: 0.3803\n",
      "epoch: 790, training loss: 1.3381553888320923, testing loss: 1.7024245262145996\n",
      "Accuracy: 0.4088\n",
      "epoch: 790, training loss: 1.2275030612945557, testing loss: 1.6046407222747803\n",
      "Accuracy: 0.4531\n",
      "epoch: 790, training loss: 1.222592830657959, testing loss: 1.7954440116882324\n",
      "Accuracy: 0.3696\n",
      "epoch: 790, training loss: 1.217819333076477, testing loss: 1.7081865072250366\n",
      "Accuracy: 0.4040\n",
      "epoch: 790, training loss: 1.1847813129425049, testing loss: 1.6288695335388184\n",
      "Accuracy: 0.4548\n",
      "epoch: 790, training loss: 1.2182395458221436, testing loss: 1.8758262395858765\n",
      "Accuracy: 0.4545\n",
      "epoch: 790, training loss: 1.2120482921600342, testing loss: 1.579361915588379\n",
      "Accuracy: 0.4281\n",
      "epoch: 790, training loss: 1.2462929487228394, testing loss: 1.6830365657806396\n",
      "Accuracy: 0.4506\n",
      "epoch: 790, training loss: 1.2570525407791138, testing loss: 1.7301697731018066\n",
      "Accuracy: 0.4363\n",
      "epoch: 790, training loss: 1.2105860710144043, testing loss: 1.8079863786697388\n",
      "Accuracy: 0.3904\n",
      "epoch: 790, training loss: 1.2686198949813843, testing loss: 1.8103243112564087\n",
      "Accuracy: 0.3973\n",
      "epoch: 790, training loss: 1.1826804876327515, testing loss: 1.793368935585022\n",
      "Accuracy: 0.3965\n",
      "epoch: 790, training loss: 1.2719305753707886, testing loss: 1.6692736148834229\n",
      "Accuracy: 0.4261\n",
      "epoch: 790, training loss: 1.2593624591827393, testing loss: 1.7191554307937622\n",
      "Accuracy: 0.3796\n",
      "epoch: 790, training loss: 1.2555826902389526, testing loss: 1.8881897926330566\n",
      "Accuracy: 0.3539\n",
      "epoch: 790, training loss: 1.2471110820770264, testing loss: 1.6780014038085938\n",
      "Accuracy: 0.4340\n",
      "epoch: 790, training loss: 1.2563055753707886, testing loss: 1.6991724967956543\n",
      "Accuracy: 0.4259\n",
      "epoch: 790, training loss: 1.2620784044265747, testing loss: 1.6785441637039185\n",
      "Accuracy: 0.4167\n",
      "epoch: 790, training loss: 1.2467188835144043, testing loss: 1.5564895868301392\n",
      "Accuracy: 0.4618\n",
      "epoch: 790, training loss: 1.2346320152282715, testing loss: 1.7689100503921509\n",
      "Accuracy: 0.3826\n",
      "epoch: 790, training loss: 1.2784627676010132, testing loss: 1.8214577436447144\n",
      "Accuracy: 0.3926\n",
      "epoch: 790, training loss: 1.259354829788208, testing loss: 1.7125275135040283\n",
      "Accuracy: 0.4430\n",
      "epoch: 790, training loss: 1.2786461114883423, testing loss: 1.630629301071167\n",
      "Accuracy: 0.4565\n",
      "epoch: 790, training loss: 1.272292971611023, testing loss: 1.722327470779419\n",
      "Accuracy: 0.3955\n",
      "epoch: 790, training loss: 1.1746653318405151, testing loss: 1.6561260223388672\n",
      "Accuracy: 0.3750\n",
      "epoch: 790, training loss: 1.1961642503738403, testing loss: 1.6803734302520752\n",
      "Accuracy: 0.4331\n",
      "epoch: 790, training loss: 1.2094749212265015, testing loss: 1.7953966856002808\n",
      "Accuracy: 0.4048\n",
      "epoch: 790, training loss: 1.2613959312438965, testing loss: 1.7222102880477905\n",
      "Accuracy: 0.3986\n",
      "epoch: 790, training loss: 1.2877100706100464, testing loss: 1.7112674713134766\n",
      "Accuracy: 0.3986\n",
      "epoch: 790, training loss: 1.19435715675354, testing loss: 1.8022949695587158\n",
      "Accuracy: 0.4202\n",
      "epoch: 790, training loss: 1.180613398551941, testing loss: 1.777962327003479\n",
      "Accuracy: 0.4207\n",
      "epoch: 790, training loss: 1.2127331495285034, testing loss: 1.7123165130615234\n",
      "Accuracy: 0.3976\n",
      "epoch: 790, training loss: 1.2499995231628418, testing loss: 1.8504550457000732\n",
      "Accuracy: 0.4029\n",
      "epoch: 790, training loss: 1.2198994159698486, testing loss: 1.7426091432571411\n",
      "Accuracy: 0.4061\n",
      "epoch: 790, training loss: 1.287239909172058, testing loss: 1.93086838722229\n",
      "Accuracy: 0.3628\n",
      "epoch: 790, training loss: 1.346779465675354, testing loss: 1.6684932708740234\n",
      "Accuracy: 0.4125\n",
      "epoch: 790, training loss: 1.224535346031189, testing loss: 1.672276258468628\n",
      "Accuracy: 0.4290\n",
      "epoch: 790, training loss: 1.2756822109222412, testing loss: 1.6703112125396729\n",
      "Accuracy: 0.4143\n",
      "epoch: 790, training loss: 1.2855643033981323, testing loss: 1.592280626296997\n",
      "Accuracy: 0.4539\n",
      "epoch: 790, training loss: 1.1830949783325195, testing loss: 1.6722139120101929\n",
      "Accuracy: 0.4209\n",
      "epoch: 790, training loss: 1.3071900606155396, testing loss: 1.7728471755981445\n",
      "Accuracy: 0.4135\n",
      "epoch: 790, training loss: 1.2557138204574585, testing loss: 1.642944097518921\n",
      "Accuracy: 0.4104\n",
      "epoch: 790, training loss: 1.255200982093811, testing loss: 1.6143348217010498\n",
      "Accuracy: 0.4238\n",
      "epoch: 790, training loss: 1.2625916004180908, testing loss: 1.692649006843567\n",
      "Accuracy: 0.4231\n",
      "epoch: 790, training loss: 1.231213927268982, testing loss: 1.7584024667739868\n",
      "Accuracy: 0.4272\n",
      "epoch: 790, training loss: 1.2641369104385376, testing loss: 1.8227132558822632\n",
      "Accuracy: 0.3790\n",
      "epoch: 790, training loss: 1.2449251413345337, testing loss: 1.7676703929901123\n",
      "Accuracy: 0.4259\n",
      "epoch: 790, training loss: 1.222081184387207, testing loss: 1.6743717193603516\n",
      "Accuracy: 0.4313\n",
      "epoch: 790, training loss: 1.2842344045639038, testing loss: 1.6338423490524292\n",
      "Accuracy: 0.4175\n",
      "epoch: 790, training loss: 1.2469240427017212, testing loss: 1.479214072227478\n",
      "Accuracy: 0.4245\n",
      "epoch: 790, training loss: 1.2369927167892456, testing loss: 1.5957319736480713\n",
      "Accuracy: 0.4040\n",
      "epoch: 790, training loss: 1.2065620422363281, testing loss: 1.6406493186950684\n",
      "Accuracy: 0.3920\n",
      "epoch: 790, training loss: 1.185583233833313, testing loss: 1.7146332263946533\n",
      "Accuracy: 0.4418\n",
      "epoch: 790, training loss: 1.2154059410095215, testing loss: 1.7770658731460571\n",
      "Accuracy: 0.4211\n",
      "epoch: 790, training loss: 1.2714626789093018, testing loss: 1.777829885482788\n",
      "Accuracy: 0.4327\n",
      "epoch: 790, training loss: 1.2963682413101196, testing loss: 1.6990885734558105\n",
      "Accuracy: 0.3974\n",
      "epoch: 790, training loss: 1.2300158739089966, testing loss: 1.6066616773605347\n",
      "Accuracy: 0.4410\n",
      "epoch: 790, training loss: 1.1933470964431763, testing loss: 1.7461481094360352\n",
      "Accuracy: 0.4407\n",
      "epoch: 790, training loss: 1.2712010145187378, testing loss: 1.7541276216506958\n",
      "Accuracy: 0.3932\n",
      "epoch: 790, training loss: 1.2009648084640503, testing loss: 1.6738346815109253\n",
      "Accuracy: 0.4660\n",
      "epoch: 790, training loss: 1.215762972831726, testing loss: 1.6142003536224365\n",
      "Accuracy: 0.4618\n",
      "epoch: 790, training loss: 1.2279396057128906, testing loss: 1.5546156167984009\n",
      "Accuracy: 0.4913\n",
      "epoch: 790, training loss: 1.3403713703155518, testing loss: 1.6518123149871826\n",
      "Accuracy: 0.4146\n",
      "epoch: 790, training loss: 1.2847741842269897, testing loss: 1.6761565208435059\n",
      "Accuracy: 0.4134\n",
      "epoch: 790, training loss: 1.2814586162567139, testing loss: 1.6819210052490234\n",
      "Accuracy: 0.4495\n",
      "epoch: 790, training loss: 1.2870204448699951, testing loss: 1.6246895790100098\n",
      "Accuracy: 0.3993\n",
      "epoch: 790, training loss: 1.189936637878418, testing loss: 1.7058956623077393\n",
      "Accuracy: 0.4175\n",
      "epoch: 790, training loss: 1.174848198890686, testing loss: 1.706085205078125\n",
      "Accuracy: 0.4207\n",
      "epoch: 790, training loss: 1.1872868537902832, testing loss: 1.692667841911316\n",
      "Accuracy: 0.4295\n",
      "epoch: 790, training loss: 1.2577999830245972, testing loss: 1.7454203367233276\n",
      "Accuracy: 0.4277\n",
      "epoch: 790, training loss: 1.189549446105957, testing loss: 1.7746593952178955\n",
      "Accuracy: 0.4039\n",
      "epoch: 790, training loss: 1.2218151092529297, testing loss: 1.7794157266616821\n",
      "Accuracy: 0.4080\n",
      "epoch: 790, training loss: 1.3147215843200684, testing loss: 1.632087230682373\n",
      "Accuracy: 0.4020\n",
      "epoch: 790, training loss: 1.1952754259109497, testing loss: 1.732608437538147\n",
      "Accuracy: 0.4169\n",
      "epoch: 800, training loss: 1.2260812520980835, testing loss: 1.519044280052185\n",
      "Accuracy: 0.4426\n",
      "epoch: 800, training loss: 1.2824549674987793, testing loss: 1.7250508069992065\n",
      "Accuracy: 0.4392\n",
      "epoch: 800, training loss: 1.2324713468551636, testing loss: 1.826473355293274\n",
      "Accuracy: 0.3906\n",
      "epoch: 800, training loss: 1.3278495073318481, testing loss: 1.6504251956939697\n",
      "Accuracy: 0.4305\n",
      "epoch: 800, training loss: 1.2534068822860718, testing loss: 1.8204898834228516\n",
      "Accuracy: 0.4135\n",
      "epoch: 800, training loss: 1.2088956832885742, testing loss: 1.8858180046081543\n",
      "Accuracy: 0.3881\n",
      "epoch: 800, training loss: 1.1246356964111328, testing loss: 1.7119213342666626\n",
      "Accuracy: 0.4340\n",
      "epoch: 800, training loss: 1.2532987594604492, testing loss: 1.7704129219055176\n",
      "Accuracy: 0.3910\n",
      "epoch: 800, training loss: 1.2243365049362183, testing loss: 1.782224416732788\n",
      "Accuracy: 0.3916\n",
      "epoch: 800, training loss: 1.231814980506897, testing loss: 1.7513033151626587\n",
      "Accuracy: 0.3907\n",
      "epoch: 800, training loss: 1.2372124195098877, testing loss: 1.7498078346252441\n",
      "Accuracy: 0.4024\n",
      "epoch: 800, training loss: 1.2539432048797607, testing loss: 1.5510411262512207\n",
      "Accuracy: 0.4719\n",
      "epoch: 800, training loss: 1.2035374641418457, testing loss: 1.6486567258834839\n",
      "Accuracy: 0.4877\n",
      "epoch: 800, training loss: 1.170101523399353, testing loss: 1.7791106700897217\n",
      "Accuracy: 0.3981\n",
      "epoch: 800, training loss: 1.2745496034622192, testing loss: 1.7973852157592773\n",
      "Accuracy: 0.4241\n",
      "epoch: 800, training loss: 1.273420810699463, testing loss: 1.7319625616073608\n",
      "Accuracy: 0.4375\n",
      "epoch: 800, training loss: 1.2149953842163086, testing loss: 1.8148658275604248\n",
      "Accuracy: 0.4136\n",
      "epoch: 800, training loss: 1.2544070482254028, testing loss: 1.604889988899231\n",
      "Accuracy: 0.4273\n",
      "epoch: 800, training loss: 1.2335882186889648, testing loss: 1.640621542930603\n",
      "Accuracy: 0.4227\n",
      "epoch: 800, training loss: 1.1996136903762817, testing loss: 1.57904851436615\n",
      "Accuracy: 0.4236\n",
      "epoch: 800, training loss: 1.2623803615570068, testing loss: 1.6931049823760986\n",
      "Accuracy: 0.4192\n",
      "epoch: 800, training loss: 1.2153934240341187, testing loss: 1.7602368593215942\n",
      "Accuracy: 0.4215\n",
      "epoch: 800, training loss: 1.2551391124725342, testing loss: 1.8221715688705444\n",
      "Accuracy: 0.4258\n",
      "epoch: 800, training loss: 1.327493667602539, testing loss: 1.715393304824829\n",
      "Accuracy: 0.4156\n",
      "epoch: 800, training loss: 1.2087957859039307, testing loss: 1.7976164817810059\n",
      "Accuracy: 0.3964\n",
      "epoch: 800, training loss: 1.2823668718338013, testing loss: 1.717637538909912\n",
      "Accuracy: 0.3849\n",
      "epoch: 800, training loss: 1.2964075803756714, testing loss: 1.8551031351089478\n",
      "Accuracy: 0.4130\n",
      "epoch: 800, training loss: 1.2188199758529663, testing loss: 1.7862160205841064\n",
      "Accuracy: 0.3868\n",
      "epoch: 800, training loss: 1.2825953960418701, testing loss: 1.6493653059005737\n",
      "Accuracy: 0.4543\n",
      "epoch: 800, training loss: 1.2681375741958618, testing loss: 1.6941099166870117\n",
      "Accuracy: 0.4393\n",
      "epoch: 800, training loss: 1.28352689743042, testing loss: 1.564731478691101\n",
      "Accuracy: 0.4334\n",
      "epoch: 800, training loss: 1.2899905443191528, testing loss: 1.6380201578140259\n",
      "Accuracy: 0.4437\n",
      "epoch: 800, training loss: 1.1986548900604248, testing loss: 1.7235594987869263\n",
      "Accuracy: 0.4332\n",
      "epoch: 800, training loss: 1.2533124685287476, testing loss: 1.7941193580627441\n",
      "Accuracy: 0.3826\n",
      "epoch: 800, training loss: 1.2523330450057983, testing loss: 1.7473567724227905\n",
      "Accuracy: 0.4140\n",
      "epoch: 800, training loss: 1.1910793781280518, testing loss: 1.9010522365570068\n",
      "Accuracy: 0.3595\n",
      "epoch: 800, training loss: 1.2021512985229492, testing loss: 1.655764102935791\n",
      "Accuracy: 0.4510\n",
      "epoch: 800, training loss: 1.275337815284729, testing loss: 1.7916721105575562\n",
      "Accuracy: 0.4389\n",
      "epoch: 800, training loss: 1.2495795488357544, testing loss: 1.7239065170288086\n",
      "Accuracy: 0.4379\n",
      "epoch: 800, training loss: 1.225559115409851, testing loss: 1.6834090948104858\n",
      "Accuracy: 0.3994\n",
      "epoch: 800, training loss: 1.2757892608642578, testing loss: 1.7216447591781616\n",
      "Accuracy: 0.4084\n",
      "epoch: 800, training loss: 1.2288857698440552, testing loss: 1.7442569732666016\n",
      "Accuracy: 0.4053\n",
      "epoch: 800, training loss: 1.160473108291626, testing loss: 1.6342543363571167\n",
      "Accuracy: 0.4844\n",
      "epoch: 800, training loss: 1.295656681060791, testing loss: 1.680726170539856\n",
      "Accuracy: 0.5000\n",
      "epoch: 800, training loss: 1.2321399450302124, testing loss: 1.6342902183532715\n",
      "Accuracy: 0.4467\n",
      "epoch: 800, training loss: 1.2867519855499268, testing loss: 1.6764888763427734\n",
      "Accuracy: 0.4189\n",
      "epoch: 800, training loss: 1.2523672580718994, testing loss: 1.6440619230270386\n",
      "Accuracy: 0.3901\n",
      "epoch: 800, training loss: 1.1636666059494019, testing loss: 1.6233222484588623\n",
      "Accuracy: 0.4310\n",
      "epoch: 800, training loss: 1.2176316976547241, testing loss: 1.702306866645813\n",
      "Accuracy: 0.4316\n",
      "epoch: 800, training loss: 1.2312533855438232, testing loss: 1.696263313293457\n",
      "Accuracy: 0.4356\n",
      "epoch: 800, training loss: 1.1720508337020874, testing loss: 1.7263578176498413\n",
      "Accuracy: 0.3869\n",
      "epoch: 800, training loss: 1.23804771900177, testing loss: 1.7531931400299072\n",
      "Accuracy: 0.3957\n",
      "epoch: 800, training loss: 1.2539491653442383, testing loss: 1.7930124998092651\n",
      "Accuracy: 0.3925\n",
      "epoch: 800, training loss: 1.2607769966125488, testing loss: 1.778910517692566\n",
      "Accuracy: 0.3894\n",
      "epoch: 800, training loss: 1.2090367078781128, testing loss: 1.716542363166809\n",
      "Accuracy: 0.4051\n",
      "epoch: 800, training loss: 1.2550643682479858, testing loss: 1.7126238346099854\n",
      "Accuracy: 0.4081\n",
      "epoch: 800, training loss: 1.2616902589797974, testing loss: 1.5827913284301758\n",
      "Accuracy: 0.4236\n",
      "epoch: 800, training loss: 1.249445915222168, testing loss: 1.7182319164276123\n",
      "Accuracy: 0.4394\n",
      "epoch: 800, training loss: 1.2478272914886475, testing loss: 1.6579736471176147\n",
      "Accuracy: 0.4228\n",
      "epoch: 800, training loss: 1.244768738746643, testing loss: 1.667523980140686\n",
      "Accuracy: 0.4594\n",
      "epoch: 800, training loss: 1.2346752882003784, testing loss: 1.6344462633132935\n",
      "Accuracy: 0.4791\n",
      "epoch: 800, training loss: 1.205689787864685, testing loss: 1.716550350189209\n",
      "Accuracy: 0.4085\n",
      "epoch: 800, training loss: 1.2528661489486694, testing loss: 1.707733392715454\n",
      "Accuracy: 0.4142\n",
      "epoch: 800, training loss: 1.2426496744155884, testing loss: 1.7575314044952393\n",
      "Accuracy: 0.4145\n",
      "epoch: 800, training loss: 1.2598859071731567, testing loss: 1.6969400644302368\n",
      "Accuracy: 0.4221\n",
      "epoch: 800, training loss: 1.28387451171875, testing loss: 1.8650269508361816\n",
      "Accuracy: 0.4309\n",
      "epoch: 800, training loss: 1.2475790977478027, testing loss: 1.762715458869934\n",
      "Accuracy: 0.4393\n",
      "epoch: 800, training loss: 1.2299823760986328, testing loss: 1.790939450263977\n",
      "Accuracy: 0.4172\n",
      "epoch: 800, training loss: 1.2720584869384766, testing loss: 1.7198430299758911\n",
      "Accuracy: 0.4013\n",
      "epoch: 800, training loss: 1.2643152475357056, testing loss: 1.6683590412139893\n",
      "Accuracy: 0.3782\n",
      "epoch: 800, training loss: 1.2333678007125854, testing loss: 1.6032750606536865\n",
      "Accuracy: 0.4399\n",
      "epoch: 800, training loss: 1.240330696105957, testing loss: 1.6455354690551758\n",
      "Accuracy: 0.4167\n",
      "epoch: 800, training loss: 1.221976399421692, testing loss: 1.6457244157791138\n",
      "Accuracy: 0.4252\n",
      "epoch: 800, training loss: 1.2418603897094727, testing loss: 1.783436894416809\n",
      "Accuracy: 0.3794\n",
      "epoch: 800, training loss: 1.2013037204742432, testing loss: 1.8212841749191284\n",
      "Accuracy: 0.4032\n",
      "epoch: 800, training loss: 1.2357761859893799, testing loss: 1.7389626502990723\n",
      "Accuracy: 0.4158\n",
      "epoch: 800, training loss: 1.2218900918960571, testing loss: 1.7094014883041382\n",
      "Accuracy: 0.3770\n",
      "epoch: 800, training loss: 1.2185378074645996, testing loss: 1.7647699117660522\n",
      "Accuracy: 0.3889\n",
      "epoch: 800, training loss: 1.2336488962173462, testing loss: 1.595907211303711\n",
      "Accuracy: 0.4500\n",
      "epoch: 800, training loss: 1.2327957153320312, testing loss: 1.7935229539871216\n",
      "Accuracy: 0.4167\n",
      "epoch: 800, training loss: 1.196509838104248, testing loss: 1.593914270401001\n",
      "Accuracy: 0.4774\n",
      "epoch: 800, training loss: 1.201778769493103, testing loss: 1.6331504583358765\n",
      "Accuracy: 0.4405\n",
      "epoch: 800, training loss: 1.1576149463653564, testing loss: 1.7645412683486938\n",
      "Accuracy: 0.4000\n",
      "epoch: 800, training loss: 1.213144063949585, testing loss: 1.9438730478286743\n",
      "Accuracy: 0.3546\n",
      "epoch: 800, training loss: 1.2883790731430054, testing loss: 1.638474464416504\n",
      "Accuracy: 0.3937\n",
      "epoch: 800, training loss: 1.188520908355713, testing loss: 1.701732873916626\n",
      "Accuracy: 0.4366\n",
      "epoch: 800, training loss: 1.2478008270263672, testing loss: 1.777189016342163\n",
      "Accuracy: 0.4020\n",
      "epoch: 800, training loss: 1.2259228229522705, testing loss: 1.7153444290161133\n",
      "Accuracy: 0.4098\n",
      "epoch: 800, training loss: 1.3204269409179688, testing loss: 1.8434338569641113\n",
      "Accuracy: 0.3879\n",
      "epoch: 800, training loss: 1.242942214012146, testing loss: 1.8071297407150269\n",
      "Accuracy: 0.3910\n",
      "epoch: 800, training loss: 1.2520989179611206, testing loss: 1.7347030639648438\n",
      "Accuracy: 0.4239\n",
      "epoch: 800, training loss: 1.2236720323562622, testing loss: 1.7423577308654785\n",
      "Accuracy: 0.4348\n",
      "epoch: 800, training loss: 1.2321116924285889, testing loss: 1.83489191532135\n",
      "Accuracy: 0.4236\n",
      "epoch: 800, training loss: 1.1615400314331055, testing loss: 1.7697392702102661\n",
      "Accuracy: 0.4477\n",
      "epoch: 800, training loss: 1.2695682048797607, testing loss: 1.5823478698730469\n",
      "Accuracy: 0.4172\n",
      "epoch: 800, training loss: 1.2747852802276611, testing loss: 1.6535344123840332\n",
      "Accuracy: 0.4455\n",
      "epoch: 800, training loss: 1.2487276792526245, testing loss: 1.7221089601516724\n",
      "Accuracy: 0.4232\n",
      "epoch: 800, training loss: 1.2050055265426636, testing loss: 1.6060888767242432\n",
      "Accuracy: 0.4340\n",
      "epoch: 800, training loss: 1.2231734991073608, testing loss: 1.6886156797409058\n",
      "Accuracy: 0.4060\n",
      "epoch: 800, training loss: 1.2505384683609009, testing loss: 1.6285778284072876\n",
      "Accuracy: 0.4058\n",
      "epoch: 810, training loss: 1.1891193389892578, testing loss: 1.7999714612960815\n",
      "Accuracy: 0.4413\n",
      "epoch: 810, training loss: 1.3027209043502808, testing loss: 1.5730531215667725\n",
      "Accuracy: 0.4813\n",
      "epoch: 810, training loss: 1.2663682699203491, testing loss: 1.660719633102417\n",
      "Accuracy: 0.4308\n",
      "epoch: 810, training loss: 1.2967925071716309, testing loss: 1.8302528858184814\n",
      "Accuracy: 0.4272\n",
      "epoch: 810, training loss: 1.257346749305725, testing loss: 1.6189172267913818\n",
      "Accuracy: 0.3946\n",
      "epoch: 810, training loss: 1.2421283721923828, testing loss: 1.7941004037857056\n",
      "Accuracy: 0.3733\n",
      "epoch: 810, training loss: 1.2631521224975586, testing loss: 1.8318889141082764\n",
      "Accuracy: 0.4122\n",
      "epoch: 810, training loss: 1.2160210609436035, testing loss: 1.6186524629592896\n",
      "Accuracy: 0.4211\n",
      "epoch: 810, training loss: 1.294395923614502, testing loss: 1.639272689819336\n",
      "Accuracy: 0.4369\n",
      "epoch: 810, training loss: 1.1797723770141602, testing loss: 1.5993677377700806\n",
      "Accuracy: 0.4263\n",
      "epoch: 810, training loss: 1.231766939163208, testing loss: 1.621861457824707\n",
      "Accuracy: 0.4570\n",
      "epoch: 810, training loss: 1.275225281715393, testing loss: 1.8114261627197266\n",
      "Accuracy: 0.3968\n",
      "epoch: 810, training loss: 1.3149164915084839, testing loss: 1.6369719505310059\n",
      "Accuracy: 0.4673\n",
      "epoch: 810, training loss: 1.2526452541351318, testing loss: 1.7118560075759888\n",
      "Accuracy: 0.4110\n",
      "epoch: 810, training loss: 1.251625895500183, testing loss: 1.6973085403442383\n",
      "Accuracy: 0.4337\n",
      "epoch: 810, training loss: 1.1957018375396729, testing loss: 1.7731820344924927\n",
      "Accuracy: 0.4054\n",
      "epoch: 810, training loss: 1.2825477123260498, testing loss: 1.6867519617080688\n",
      "Accuracy: 0.4481\n",
      "epoch: 810, training loss: 1.1675950288772583, testing loss: 1.6663392782211304\n",
      "Accuracy: 0.4341\n",
      "epoch: 810, training loss: 1.3145548105239868, testing loss: 1.7807323932647705\n",
      "Accuracy: 0.4466\n",
      "epoch: 810, training loss: 1.2081942558288574, testing loss: 1.6538238525390625\n",
      "Accuracy: 0.4216\n",
      "epoch: 810, training loss: 1.2565056085586548, testing loss: 1.7600762844085693\n",
      "Accuracy: 0.3741\n",
      "epoch: 810, training loss: 1.217181921005249, testing loss: 1.6914054155349731\n",
      "Accuracy: 0.4437\n",
      "epoch: 810, training loss: 1.2305101156234741, testing loss: 1.6863209009170532\n",
      "Accuracy: 0.3902\n",
      "epoch: 810, training loss: 1.2895348072052002, testing loss: 1.7962746620178223\n",
      "Accuracy: 0.3915\n",
      "epoch: 810, training loss: 1.2483004331588745, testing loss: 1.710614562034607\n",
      "Accuracy: 0.4012\n",
      "epoch: 810, training loss: 1.288930892944336, testing loss: 1.8257168531417847\n",
      "Accuracy: 0.4225\n",
      "epoch: 810, training loss: 1.2099677324295044, testing loss: 1.7393431663513184\n",
      "Accuracy: 0.4331\n",
      "epoch: 810, training loss: 1.2344017028808594, testing loss: 1.6808197498321533\n",
      "Accuracy: 0.4331\n",
      "epoch: 810, training loss: 1.2137316465377808, testing loss: 1.686050534248352\n",
      "Accuracy: 0.4491\n",
      "epoch: 810, training loss: 1.2601772546768188, testing loss: 1.652221441268921\n",
      "Accuracy: 0.3780\n",
      "epoch: 810, training loss: 1.1977165937423706, testing loss: 1.6380232572555542\n",
      "Accuracy: 0.4620\n",
      "epoch: 810, training loss: 1.310767412185669, testing loss: 1.6916694641113281\n",
      "Accuracy: 0.4396\n",
      "epoch: 810, training loss: 1.2674002647399902, testing loss: 1.768939733505249\n",
      "Accuracy: 0.4218\n",
      "epoch: 810, training loss: 1.2308287620544434, testing loss: 1.6430407762527466\n",
      "Accuracy: 0.4599\n",
      "epoch: 810, training loss: 1.2128832340240479, testing loss: 1.8341546058654785\n",
      "Accuracy: 0.4038\n",
      "epoch: 810, training loss: 1.256520390510559, testing loss: 1.7687394618988037\n",
      "Accuracy: 0.4428\n",
      "epoch: 810, training loss: 1.2410446405410767, testing loss: 1.7085450887680054\n",
      "Accuracy: 0.4164\n",
      "epoch: 810, training loss: 1.2561008930206299, testing loss: 1.7859200239181519\n",
      "Accuracy: 0.4081\n",
      "epoch: 810, training loss: 1.1890783309936523, testing loss: 1.726165533065796\n",
      "Accuracy: 0.4154\n",
      "epoch: 810, training loss: 1.2278658151626587, testing loss: 1.7850521802902222\n",
      "Accuracy: 0.4188\n",
      "epoch: 810, training loss: 1.3281922340393066, testing loss: 1.6552038192749023\n",
      "Accuracy: 0.3851\n",
      "epoch: 810, training loss: 1.2653416395187378, testing loss: 1.6370512247085571\n",
      "Accuracy: 0.4469\n",
      "epoch: 810, training loss: 1.2358156442642212, testing loss: 1.7456135749816895\n",
      "Accuracy: 0.4110\n",
      "epoch: 810, training loss: 1.2009491920471191, testing loss: 1.8235578536987305\n",
      "Accuracy: 0.3926\n",
      "epoch: 810, training loss: 1.219367265701294, testing loss: 1.7278261184692383\n",
      "Accuracy: 0.4216\n",
      "epoch: 810, training loss: 1.2318209409713745, testing loss: 1.6274174451828003\n",
      "Accuracy: 0.3982\n",
      "epoch: 810, training loss: 1.2175703048706055, testing loss: 1.6548930406570435\n",
      "Accuracy: 0.4196\n",
      "epoch: 810, training loss: 1.2107062339782715, testing loss: 1.8150017261505127\n",
      "Accuracy: 0.4340\n",
      "epoch: 810, training loss: 1.2980986833572388, testing loss: 1.6523019075393677\n",
      "Accuracy: 0.4125\n",
      "epoch: 810, training loss: 1.2346444129943848, testing loss: 1.6286927461624146\n",
      "Accuracy: 0.4765\n",
      "epoch: 810, training loss: 1.2020848989486694, testing loss: 1.712121844291687\n",
      "Accuracy: 0.4272\n",
      "epoch: 810, training loss: 1.2380237579345703, testing loss: 1.736954689025879\n",
      "Accuracy: 0.4464\n",
      "epoch: 810, training loss: 1.1793859004974365, testing loss: 1.6249269247055054\n",
      "Accuracy: 0.4194\n",
      "epoch: 810, training loss: 1.2597861289978027, testing loss: 1.7563786506652832\n",
      "Accuracy: 0.4203\n",
      "epoch: 810, training loss: 1.2314879894256592, testing loss: 1.772168755531311\n",
      "Accuracy: 0.4019\n",
      "epoch: 810, training loss: 1.2701869010925293, testing loss: 1.6361733675003052\n",
      "Accuracy: 0.4497\n",
      "epoch: 810, training loss: 1.2823598384857178, testing loss: 1.7765206098556519\n",
      "Accuracy: 0.4087\n",
      "epoch: 810, training loss: 1.3306466341018677, testing loss: 1.7917935848236084\n",
      "Accuracy: 0.4222\n",
      "epoch: 810, training loss: 1.2717502117156982, testing loss: 1.5870170593261719\n",
      "Accuracy: 0.4599\n",
      "epoch: 810, training loss: 1.164801001548767, testing loss: 1.6392382383346558\n",
      "Accuracy: 0.4355\n",
      "epoch: 810, training loss: 1.2808281183242798, testing loss: 1.861743450164795\n",
      "Accuracy: 0.3709\n",
      "epoch: 810, training loss: 1.253932237625122, testing loss: 1.6662307977676392\n",
      "Accuracy: 0.4038\n",
      "epoch: 810, training loss: 1.207425832748413, testing loss: 1.747093915939331\n",
      "Accuracy: 0.3651\n",
      "epoch: 810, training loss: 1.2040050029754639, testing loss: 1.6377886533737183\n",
      "Accuracy: 0.4871\n",
      "epoch: 810, training loss: 1.363226294517517, testing loss: 1.719757318496704\n",
      "Accuracy: 0.4138\n",
      "epoch: 810, training loss: 1.1666221618652344, testing loss: 1.6231629848480225\n",
      "Accuracy: 0.4226\n",
      "epoch: 810, training loss: 1.1926342248916626, testing loss: 1.6218693256378174\n",
      "Accuracy: 0.4153\n",
      "epoch: 810, training loss: 1.243217945098877, testing loss: 1.7007007598876953\n",
      "Accuracy: 0.4105\n",
      "epoch: 810, training loss: 1.1516212224960327, testing loss: 1.6965018510818481\n",
      "Accuracy: 0.4161\n",
      "epoch: 810, training loss: 1.1884467601776123, testing loss: 1.739341139793396\n",
      "Accuracy: 0.4259\n",
      "epoch: 810, training loss: 1.1723271608352661, testing loss: 1.695425033569336\n",
      "Accuracy: 0.4094\n",
      "epoch: 810, training loss: 1.2141469717025757, testing loss: 1.7715837955474854\n",
      "Accuracy: 0.3933\n",
      "epoch: 810, training loss: 1.270907998085022, testing loss: 1.7147412300109863\n",
      "Accuracy: 0.4199\n",
      "epoch: 810, training loss: 1.2256684303283691, testing loss: 1.6937276124954224\n",
      "Accuracy: 0.4241\n",
      "epoch: 810, training loss: 1.2069956064224243, testing loss: 1.6743381023406982\n",
      "Accuracy: 0.4352\n",
      "epoch: 810, training loss: 1.2440996170043945, testing loss: 1.7237939834594727\n",
      "Accuracy: 0.4760\n",
      "epoch: 810, training loss: 1.2394969463348389, testing loss: 1.7170554399490356\n",
      "Accuracy: 0.4618\n",
      "epoch: 810, training loss: 1.257469654083252, testing loss: 1.7269272804260254\n",
      "Accuracy: 0.4411\n",
      "epoch: 810, training loss: 1.257556438446045, testing loss: 1.7194334268569946\n",
      "Accuracy: 0.4473\n",
      "epoch: 810, training loss: 1.2673301696777344, testing loss: 1.7979135513305664\n",
      "Accuracy: 0.4224\n",
      "epoch: 810, training loss: 1.2341625690460205, testing loss: 1.6912916898727417\n",
      "Accuracy: 0.4790\n",
      "epoch: 810, training loss: 1.1901661157608032, testing loss: 1.7719959020614624\n",
      "Accuracy: 0.4300\n",
      "epoch: 810, training loss: 1.225774884223938, testing loss: 1.8714919090270996\n",
      "Accuracy: 0.3716\n",
      "epoch: 810, training loss: 1.1482881307601929, testing loss: 1.746880292892456\n",
      "Accuracy: 0.4058\n",
      "epoch: 810, training loss: 1.245207667350769, testing loss: 1.7335785627365112\n",
      "Accuracy: 0.4227\n",
      "epoch: 810, training loss: 1.1932114362716675, testing loss: 1.7350926399230957\n",
      "Accuracy: 0.4266\n",
      "epoch: 810, training loss: 1.2225412130355835, testing loss: 1.6035033464431763\n",
      "Accuracy: 0.4362\n",
      "epoch: 810, training loss: 1.2379034757614136, testing loss: 1.8443074226379395\n",
      "Accuracy: 0.3844\n",
      "epoch: 810, training loss: 1.209384560585022, testing loss: 1.691603422164917\n",
      "Accuracy: 0.3805\n",
      "epoch: 810, training loss: 1.2838869094848633, testing loss: 1.6880838871002197\n",
      "Accuracy: 0.4551\n",
      "epoch: 810, training loss: 1.2918421030044556, testing loss: 1.8433537483215332\n",
      "Accuracy: 0.4320\n",
      "epoch: 810, training loss: 1.2033872604370117, testing loss: 1.7010693550109863\n",
      "Accuracy: 0.4037\n",
      "epoch: 810, training loss: 1.174119234085083, testing loss: 1.6584070920944214\n",
      "Accuracy: 0.4371\n",
      "epoch: 810, training loss: 1.2430907487869263, testing loss: 1.6987227201461792\n",
      "Accuracy: 0.4267\n",
      "epoch: 810, training loss: 1.2059741020202637, testing loss: 1.7143150568008423\n",
      "Accuracy: 0.4092\n",
      "epoch: 810, training loss: 1.2423557043075562, testing loss: 1.7422791719436646\n",
      "Accuracy: 0.3920\n",
      "epoch: 810, training loss: 1.211104154586792, testing loss: 1.8493027687072754\n",
      "Accuracy: 0.4048\n",
      "epoch: 810, training loss: 1.195205807685852, testing loss: 1.80984628200531\n",
      "Accuracy: 0.4159\n",
      "epoch: 810, training loss: 1.2021524906158447, testing loss: 1.5611265897750854\n",
      "Accuracy: 0.4553\n",
      "epoch: 810, training loss: 1.2686254978179932, testing loss: 1.6076945066452026\n",
      "Accuracy: 0.3986\n",
      "epoch: 820, training loss: 1.2214571237564087, testing loss: 1.745836615562439\n",
      "Accuracy: 0.4371\n",
      "epoch: 820, training loss: 1.2686203718185425, testing loss: 1.6304607391357422\n",
      "Accuracy: 0.4430\n",
      "epoch: 820, training loss: 1.1981301307678223, testing loss: 1.7625502347946167\n",
      "Accuracy: 0.4286\n",
      "epoch: 820, training loss: 1.1963098049163818, testing loss: 1.708117127418518\n",
      "Accuracy: 0.4277\n",
      "epoch: 820, training loss: 1.2712957859039307, testing loss: 1.7284804582595825\n",
      "Accuracy: 0.4243\n",
      "epoch: 820, training loss: 1.2165441513061523, testing loss: 1.8783396482467651\n",
      "Accuracy: 0.3782\n",
      "epoch: 820, training loss: 1.2042497396469116, testing loss: 1.7837811708450317\n",
      "Accuracy: 0.3513\n",
      "epoch: 820, training loss: 1.2061073780059814, testing loss: 1.7511204481124878\n",
      "Accuracy: 0.3973\n",
      "epoch: 820, training loss: 1.233718752861023, testing loss: 1.6256744861602783\n",
      "Accuracy: 0.4206\n",
      "epoch: 820, training loss: 1.2791224718093872, testing loss: 1.8279948234558105\n",
      "Accuracy: 0.4007\n",
      "epoch: 820, training loss: 1.3186156749725342, testing loss: 1.7285075187683105\n",
      "Accuracy: 0.4448\n",
      "epoch: 820, training loss: 1.3011783361434937, testing loss: 1.6766895055770874\n",
      "Accuracy: 0.4102\n",
      "epoch: 820, training loss: 1.2532625198364258, testing loss: 1.5813755989074707\n",
      "Accuracy: 0.4278\n",
      "epoch: 820, training loss: 1.2127788066864014, testing loss: 1.616856336593628\n",
      "Accuracy: 0.4209\n",
      "epoch: 820, training loss: 1.2528493404388428, testing loss: 1.720798373222351\n",
      "Accuracy: 0.4552\n",
      "epoch: 820, training loss: 1.2973517179489136, testing loss: 1.6938620805740356\n",
      "Accuracy: 0.4276\n",
      "epoch: 820, training loss: 1.3263431787490845, testing loss: 1.6118205785751343\n",
      "Accuracy: 0.4300\n",
      "epoch: 820, training loss: 1.2837162017822266, testing loss: 1.94180166721344\n",
      "Accuracy: 0.3596\n",
      "epoch: 820, training loss: 1.229254961013794, testing loss: 1.6236366033554077\n",
      "Accuracy: 0.4319\n",
      "epoch: 820, training loss: 1.200626254081726, testing loss: 1.7249784469604492\n",
      "Accuracy: 0.4334\n",
      "epoch: 820, training loss: 1.2554271221160889, testing loss: 1.761519193649292\n",
      "Accuracy: 0.4192\n",
      "epoch: 820, training loss: 1.1599628925323486, testing loss: 1.7522846460342407\n",
      "Accuracy: 0.4255\n",
      "epoch: 820, training loss: 1.2524477243423462, testing loss: 1.725606083869934\n",
      "Accuracy: 0.3661\n",
      "epoch: 820, training loss: 1.262743353843689, testing loss: 1.77329683303833\n",
      "Accuracy: 0.4369\n",
      "epoch: 820, training loss: 1.248415470123291, testing loss: 1.8487416505813599\n",
      "Accuracy: 0.3769\n",
      "epoch: 820, training loss: 1.2205095291137695, testing loss: 1.7386860847473145\n",
      "Accuracy: 0.4507\n",
      "epoch: 820, training loss: 1.2002933025360107, testing loss: 1.7809133529663086\n",
      "Accuracy: 0.4135\n",
      "epoch: 820, training loss: 1.1912237405776978, testing loss: 1.8765816688537598\n",
      "Accuracy: 0.4225\n",
      "epoch: 820, training loss: 1.1896371841430664, testing loss: 1.7196040153503418\n",
      "Accuracy: 0.4252\n",
      "epoch: 820, training loss: 1.1928155422210693, testing loss: 1.6945327520370483\n",
      "Accuracy: 0.3636\n",
      "epoch: 820, training loss: 1.2397446632385254, testing loss: 1.710744857788086\n",
      "Accuracy: 0.4214\n",
      "epoch: 820, training loss: 1.2597479820251465, testing loss: 1.7264230251312256\n",
      "Accuracy: 0.3818\n",
      "epoch: 820, training loss: 1.2213470935821533, testing loss: 1.7505152225494385\n",
      "Accuracy: 0.4028\n",
      "epoch: 820, training loss: 1.2679542303085327, testing loss: 1.6973414421081543\n",
      "Accuracy: 0.4387\n",
      "epoch: 820, training loss: 1.297613263130188, testing loss: 1.764606237411499\n",
      "Accuracy: 0.4486\n",
      "epoch: 820, training loss: 1.2728757858276367, testing loss: 1.8136897087097168\n",
      "Accuracy: 0.3825\n",
      "epoch: 820, training loss: 1.2720069885253906, testing loss: 1.8366639614105225\n",
      "Accuracy: 0.4286\n",
      "epoch: 820, training loss: 1.2472169399261475, testing loss: 1.784928798675537\n",
      "Accuracy: 0.3914\n",
      "epoch: 820, training loss: 1.2278313636779785, testing loss: 1.7003846168518066\n",
      "Accuracy: 0.4164\n",
      "epoch: 820, training loss: 1.1640448570251465, testing loss: 1.7370737791061401\n",
      "Accuracy: 0.4883\n",
      "epoch: 820, training loss: 1.176774501800537, testing loss: 1.747794270515442\n",
      "Accuracy: 0.3762\n",
      "epoch: 820, training loss: 1.3176860809326172, testing loss: 1.7389196157455444\n",
      "Accuracy: 0.3944\n",
      "epoch: 820, training loss: 1.27039635181427, testing loss: 1.715999722480774\n",
      "Accuracy: 0.3980\n",
      "epoch: 820, training loss: 1.2458686828613281, testing loss: 1.7451404333114624\n",
      "Accuracy: 0.4470\n",
      "epoch: 820, training loss: 1.2238185405731201, testing loss: 1.7345155477523804\n",
      "Accuracy: 0.4125\n",
      "epoch: 820, training loss: 1.2453855276107788, testing loss: 1.8263342380523682\n",
      "Accuracy: 0.3778\n",
      "epoch: 820, training loss: 1.2947617769241333, testing loss: 1.6387841701507568\n",
      "Accuracy: 0.4228\n",
      "epoch: 820, training loss: 1.2143282890319824, testing loss: 1.5907328128814697\n",
      "Accuracy: 0.4281\n",
      "epoch: 820, training loss: 1.2820805311203003, testing loss: 1.6755456924438477\n",
      "Accuracy: 0.4290\n",
      "epoch: 820, training loss: 1.185240626335144, testing loss: 1.627045750617981\n",
      "Accuracy: 0.4780\n",
      "epoch: 820, training loss: 1.2639082670211792, testing loss: 1.771531343460083\n",
      "Accuracy: 0.3525\n",
      "epoch: 820, training loss: 1.2044672966003418, testing loss: 1.6410698890686035\n",
      "Accuracy: 0.4533\n",
      "epoch: 820, training loss: 1.3063045740127563, testing loss: 1.6482192277908325\n",
      "Accuracy: 0.4110\n",
      "epoch: 820, training loss: 1.2390429973602295, testing loss: 1.6881234645843506\n",
      "Accuracy: 0.3849\n",
      "epoch: 820, training loss: 1.2966824769973755, testing loss: 1.752819299697876\n",
      "Accuracy: 0.4132\n",
      "epoch: 820, training loss: 1.275041103363037, testing loss: 1.646273136138916\n",
      "Accuracy: 0.4187\n",
      "epoch: 820, training loss: 1.3273409605026245, testing loss: 1.7908896207809448\n",
      "Accuracy: 0.3789\n",
      "epoch: 820, training loss: 1.2310117483139038, testing loss: 1.7800241708755493\n",
      "Accuracy: 0.4025\n",
      "epoch: 820, training loss: 1.2318044900894165, testing loss: 1.6718107461929321\n",
      "Accuracy: 0.4545\n",
      "epoch: 820, training loss: 1.215630292892456, testing loss: 1.7832167148590088\n",
      "Accuracy: 0.3662\n",
      "epoch: 820, training loss: 1.2906779050827026, testing loss: 1.7986372709274292\n",
      "Accuracy: 0.4020\n",
      "epoch: 820, training loss: 1.2352551221847534, testing loss: 1.70712149143219\n",
      "Accuracy: 0.4277\n",
      "epoch: 820, training loss: 1.2383760213851929, testing loss: 1.6387300491333008\n",
      "Accuracy: 0.4174\n",
      "epoch: 820, training loss: 1.243198275566101, testing loss: 1.684718370437622\n",
      "Accuracy: 0.4082\n",
      "epoch: 820, training loss: 1.1855958700180054, testing loss: 1.7026731967926025\n",
      "Accuracy: 0.4264\n",
      "epoch: 820, training loss: 1.1566767692565918, testing loss: 1.769533395767212\n",
      "Accuracy: 0.4332\n",
      "epoch: 820, training loss: 1.2614871263504028, testing loss: 1.7775447368621826\n",
      "Accuracy: 0.4158\n",
      "epoch: 820, training loss: 1.3083690404891968, testing loss: 1.7802118062973022\n",
      "Accuracy: 0.3906\n",
      "epoch: 820, training loss: 1.2633910179138184, testing loss: 1.6265004873275757\n",
      "Accuracy: 0.4627\n",
      "epoch: 820, training loss: 1.2409417629241943, testing loss: 1.6906367540359497\n",
      "Accuracy: 0.3891\n",
      "epoch: 820, training loss: 1.2327189445495605, testing loss: 1.6828227043151855\n",
      "Accuracy: 0.3918\n",
      "epoch: 820, training loss: 1.2928714752197266, testing loss: 1.642079472541809\n",
      "Accuracy: 0.4136\n",
      "epoch: 820, training loss: 1.2037463188171387, testing loss: 1.6394001245498657\n",
      "Accuracy: 0.4531\n",
      "epoch: 820, training loss: 1.217143177986145, testing loss: 1.8208317756652832\n",
      "Accuracy: 0.3867\n",
      "epoch: 820, training loss: 1.1956921815872192, testing loss: 1.7278779745101929\n",
      "Accuracy: 0.4341\n",
      "epoch: 820, training loss: 1.2094756364822388, testing loss: 1.6127210855484009\n",
      "Accuracy: 0.4320\n",
      "epoch: 820, training loss: 1.2459428310394287, testing loss: 1.6600078344345093\n",
      "Accuracy: 0.4380\n",
      "epoch: 820, training loss: 1.2420169115066528, testing loss: 1.7689132690429688\n",
      "Accuracy: 0.4079\n",
      "epoch: 820, training loss: 1.1664535999298096, testing loss: 1.6572977304458618\n",
      "Accuracy: 0.4537\n",
      "epoch: 820, training loss: 1.3128209114074707, testing loss: 1.769810676574707\n",
      "Accuracy: 0.3806\n",
      "epoch: 820, training loss: 1.1632657051086426, testing loss: 1.80409574508667\n",
      "Accuracy: 0.3583\n",
      "epoch: 820, training loss: 1.2255282402038574, testing loss: 1.8342671394348145\n",
      "Accuracy: 0.3976\n",
      "epoch: 820, training loss: 1.2949473857879639, testing loss: 1.7510528564453125\n",
      "Accuracy: 0.4459\n",
      "epoch: 820, training loss: 1.209449291229248, testing loss: 1.6264188289642334\n",
      "Accuracy: 0.4509\n",
      "epoch: 820, training loss: 1.2268308401107788, testing loss: 1.7609717845916748\n",
      "Accuracy: 0.4281\n",
      "epoch: 820, training loss: 1.2317625284194946, testing loss: 1.6833919286727905\n",
      "Accuracy: 0.4385\n",
      "epoch: 820, training loss: 1.2084475755691528, testing loss: 1.6654082536697388\n",
      "Accuracy: 0.4108\n",
      "epoch: 820, training loss: 1.1888580322265625, testing loss: 1.7418640851974487\n",
      "Accuracy: 0.4618\n",
      "epoch: 820, training loss: 1.2550444602966309, testing loss: 1.7321159839630127\n",
      "Accuracy: 0.4037\n",
      "epoch: 820, training loss: 1.2594634294509888, testing loss: 1.6937105655670166\n",
      "Accuracy: 0.3727\n",
      "epoch: 820, training loss: 1.230656385421753, testing loss: 1.6747403144836426\n",
      "Accuracy: 0.4072\n",
      "epoch: 820, training loss: 1.2648921012878418, testing loss: 1.7021640539169312\n",
      "Accuracy: 0.4024\n",
      "epoch: 820, training loss: 1.2251216173171997, testing loss: 1.6702418327331543\n",
      "Accuracy: 0.4620\n",
      "epoch: 820, training loss: 1.2933204174041748, testing loss: 1.7071810960769653\n",
      "Accuracy: 0.4150\n",
      "epoch: 820, training loss: 1.2913466691970825, testing loss: 1.7271679639816284\n",
      "Accuracy: 0.4000\n",
      "epoch: 820, training loss: 1.2627655267715454, testing loss: 1.787696361541748\n",
      "Accuracy: 0.3960\n",
      "epoch: 820, training loss: 1.2665051221847534, testing loss: 1.6117130517959595\n",
      "Accuracy: 0.4426\n",
      "epoch: 820, training loss: 1.208165168762207, testing loss: 1.6637104749679565\n",
      "Accuracy: 0.3836\n",
      "epoch: 820, training loss: 1.2313752174377441, testing loss: 1.6628025770187378\n",
      "Accuracy: 0.4104\n",
      "epoch: 820, training loss: 1.1934934854507446, testing loss: 1.7256964445114136\n",
      "Accuracy: 0.4404\n",
      "epoch: 830, training loss: 1.2709882259368896, testing loss: 1.6938111782073975\n",
      "Accuracy: 0.4314\n",
      "epoch: 830, training loss: 1.2207074165344238, testing loss: 1.792463779449463\n",
      "Accuracy: 0.4242\n",
      "epoch: 830, training loss: 1.170210599899292, testing loss: 1.7163201570510864\n",
      "Accuracy: 0.4040\n",
      "epoch: 830, training loss: 1.2209467887878418, testing loss: 1.7969926595687866\n",
      "Accuracy: 0.3630\n",
      "epoch: 830, training loss: 1.247696042060852, testing loss: 1.6782379150390625\n",
      "Accuracy: 0.4551\n",
      "epoch: 830, training loss: 1.2282651662826538, testing loss: 1.6068617105484009\n",
      "Accuracy: 0.4308\n",
      "epoch: 830, training loss: 1.1983532905578613, testing loss: 1.74053955078125\n",
      "Accuracy: 0.3742\n",
      "epoch: 830, training loss: 1.3173145055770874, testing loss: 1.6545852422714233\n",
      "Accuracy: 0.4169\n",
      "epoch: 830, training loss: 1.2495001554489136, testing loss: 1.6970869302749634\n",
      "Accuracy: 0.4114\n",
      "epoch: 830, training loss: 1.1827584505081177, testing loss: 1.717637538909912\n",
      "Accuracy: 0.4013\n",
      "epoch: 830, training loss: 1.2166229486465454, testing loss: 1.7980940341949463\n",
      "Accuracy: 0.3887\n",
      "epoch: 830, training loss: 1.211543321609497, testing loss: 1.6173628568649292\n",
      "Accuracy: 0.4479\n",
      "epoch: 830, training loss: 1.2254023551940918, testing loss: 1.63473641872406\n",
      "Accuracy: 0.4150\n",
      "epoch: 830, training loss: 1.1957170963287354, testing loss: 1.6420485973358154\n",
      "Accuracy: 0.4798\n",
      "epoch: 830, training loss: 1.2419633865356445, testing loss: 1.6684600114822388\n",
      "Accuracy: 0.3818\n",
      "epoch: 830, training loss: 1.237594723701477, testing loss: 1.799608826637268\n",
      "Accuracy: 0.4219\n",
      "epoch: 830, training loss: 1.2814834117889404, testing loss: 1.5032047033309937\n",
      "Accuracy: 0.4623\n",
      "epoch: 830, training loss: 1.2323635816574097, testing loss: 1.6626311540603638\n",
      "Accuracy: 0.4516\n",
      "epoch: 830, training loss: 1.21769118309021, testing loss: 1.7329198122024536\n",
      "Accuracy: 0.3892\n",
      "epoch: 830, training loss: 1.2121481895446777, testing loss: 1.722561001777649\n",
      "Accuracy: 0.4299\n",
      "epoch: 830, training loss: 1.2098026275634766, testing loss: 1.6580806970596313\n",
      "Accuracy: 0.3993\n",
      "epoch: 830, training loss: 1.2072263956069946, testing loss: 1.7551374435424805\n",
      "Accuracy: 0.3906\n",
      "epoch: 830, training loss: 1.187462568283081, testing loss: 1.8327418565750122\n",
      "Accuracy: 0.3512\n",
      "epoch: 830, training loss: 1.2271145582199097, testing loss: 1.760982871055603\n",
      "Accuracy: 0.4497\n",
      "epoch: 830, training loss: 1.3082095384597778, testing loss: 1.766079068183899\n",
      "Accuracy: 0.4169\n",
      "epoch: 830, training loss: 1.2525005340576172, testing loss: 1.7054517269134521\n",
      "Accuracy: 0.3882\n",
      "epoch: 830, training loss: 1.2296688556671143, testing loss: 1.6951606273651123\n",
      "Accuracy: 0.4474\n",
      "epoch: 830, training loss: 1.2443426847457886, testing loss: 1.6765508651733398\n",
      "Accuracy: 0.4246\n",
      "epoch: 830, training loss: 1.24619722366333, testing loss: 1.728487253189087\n",
      "Accuracy: 0.4251\n",
      "epoch: 830, training loss: 1.3095283508300781, testing loss: 1.7167482376098633\n",
      "Accuracy: 0.4000\n",
      "epoch: 830, training loss: 1.2647767066955566, testing loss: 1.701650619506836\n",
      "Accuracy: 0.4597\n",
      "epoch: 830, training loss: 1.2594796419143677, testing loss: 1.7149157524108887\n",
      "Accuracy: 0.4566\n",
      "epoch: 830, training loss: 1.2897447347640991, testing loss: 1.8490443229675293\n",
      "Accuracy: 0.3844\n",
      "epoch: 830, training loss: 1.257359504699707, testing loss: 1.7874869108200073\n",
      "Accuracy: 0.3824\n",
      "epoch: 830, training loss: 1.3000394105911255, testing loss: 1.783835530281067\n",
      "Accuracy: 0.4062\n",
      "epoch: 830, training loss: 1.243236780166626, testing loss: 1.7546724081039429\n",
      "Accuracy: 0.4238\n",
      "epoch: 830, training loss: 1.232824683189392, testing loss: 1.8755525350570679\n",
      "Accuracy: 0.3702\n",
      "epoch: 830, training loss: 1.2315354347229004, testing loss: 1.7392369508743286\n",
      "Accuracy: 0.4214\n",
      "epoch: 830, training loss: 1.2528845071792603, testing loss: 1.5935313701629639\n",
      "Accuracy: 0.4791\n",
      "epoch: 830, training loss: 1.194622278213501, testing loss: 1.6969283819198608\n",
      "Accuracy: 0.4390\n",
      "epoch: 830, training loss: 1.2496269941329956, testing loss: 1.7932549715042114\n",
      "Accuracy: 0.3700\n",
      "epoch: 830, training loss: 1.2306171655654907, testing loss: 1.7519227266311646\n",
      "Accuracy: 0.4053\n",
      "epoch: 830, training loss: 1.2885544300079346, testing loss: 1.661380648612976\n",
      "Accuracy: 0.4592\n",
      "epoch: 830, training loss: 1.233514428138733, testing loss: 1.8032135963439941\n",
      "Accuracy: 0.4066\n",
      "epoch: 830, training loss: 1.1947153806686401, testing loss: 1.7178435325622559\n",
      "Accuracy: 0.3755\n",
      "epoch: 830, training loss: 1.2346657514572144, testing loss: 1.6840808391571045\n",
      "Accuracy: 0.3973\n",
      "epoch: 830, training loss: 1.2058911323547363, testing loss: 1.8025928735733032\n",
      "Accuracy: 0.3710\n",
      "epoch: 830, training loss: 1.1834713220596313, testing loss: 1.8621035814285278\n",
      "Accuracy: 0.4044\n",
      "epoch: 830, training loss: 1.2071360349655151, testing loss: 1.7149419784545898\n",
      "Accuracy: 0.4369\n",
      "epoch: 830, training loss: 1.2541003227233887, testing loss: 1.673344612121582\n",
      "Accuracy: 0.4397\n",
      "epoch: 830, training loss: 1.2549582719802856, testing loss: 1.6622745990753174\n",
      "Accuracy: 0.4387\n",
      "epoch: 830, training loss: 1.2403513193130493, testing loss: 1.6851627826690674\n",
      "Accuracy: 0.4328\n",
      "epoch: 830, training loss: 1.1797348260879517, testing loss: 1.7336279153823853\n",
      "Accuracy: 0.4217\n",
      "epoch: 830, training loss: 1.2911564111709595, testing loss: 1.8436142206192017\n",
      "Accuracy: 0.4024\n",
      "epoch: 830, training loss: 1.2462432384490967, testing loss: 1.7414813041687012\n",
      "Accuracy: 0.4232\n",
      "epoch: 830, training loss: 1.274869680404663, testing loss: 1.6965007781982422\n",
      "Accuracy: 0.4518\n",
      "epoch: 830, training loss: 1.194563865661621, testing loss: 1.716792106628418\n",
      "Accuracy: 0.3924\n",
      "epoch: 830, training loss: 1.22653067111969, testing loss: 1.8062299489974976\n",
      "Accuracy: 0.3954\n",
      "epoch: 830, training loss: 1.2405637502670288, testing loss: 1.6509895324707031\n",
      "Accuracy: 0.4343\n",
      "epoch: 830, training loss: 1.2350366115570068, testing loss: 1.7614214420318604\n",
      "Accuracy: 0.3940\n",
      "epoch: 830, training loss: 1.2947016954421997, testing loss: 1.7895735502243042\n",
      "Accuracy: 0.4500\n",
      "epoch: 830, training loss: 1.2784569263458252, testing loss: 1.6916580200195312\n",
      "Accuracy: 0.4194\n",
      "epoch: 830, training loss: 1.185314655303955, testing loss: 1.5168838500976562\n",
      "Accuracy: 0.4862\n",
      "epoch: 830, training loss: 1.226174235343933, testing loss: 1.7221252918243408\n",
      "Accuracy: 0.4444\n",
      "epoch: 830, training loss: 1.1659486293792725, testing loss: 1.7777961492538452\n",
      "Accuracy: 0.4314\n",
      "epoch: 830, training loss: 1.308813452720642, testing loss: 1.8024234771728516\n",
      "Accuracy: 0.4299\n",
      "epoch: 830, training loss: 1.2723129987716675, testing loss: 1.7838010787963867\n",
      "Accuracy: 0.4098\n",
      "epoch: 830, training loss: 1.2364188432693481, testing loss: 1.7572609186172485\n",
      "Accuracy: 0.3946\n",
      "epoch: 830, training loss: 1.167848825454712, testing loss: 1.7760095596313477\n",
      "Accuracy: 0.3944\n",
      "epoch: 830, training loss: 1.2352862358093262, testing loss: 1.7772051095962524\n",
      "Accuracy: 0.3917\n",
      "epoch: 830, training loss: 1.2760488986968994, testing loss: 1.6521450281143188\n",
      "Accuracy: 0.4549\n",
      "epoch: 830, training loss: 1.2072839736938477, testing loss: 1.8372395038604736\n",
      "Accuracy: 0.3510\n",
      "epoch: 830, training loss: 1.1761552095413208, testing loss: 1.7112360000610352\n",
      "Accuracy: 0.3960\n",
      "epoch: 830, training loss: 1.2244678735733032, testing loss: 1.665221929550171\n",
      "Accuracy: 0.4295\n",
      "epoch: 830, training loss: 1.215654730796814, testing loss: 1.8051484823226929\n",
      "Accuracy: 0.3447\n",
      "epoch: 830, training loss: 1.1910804510116577, testing loss: 1.7087411880493164\n",
      "Accuracy: 0.4542\n",
      "epoch: 830, training loss: 1.2990353107452393, testing loss: 1.830086350440979\n",
      "Accuracy: 0.4038\n",
      "epoch: 830, training loss: 1.2259457111358643, testing loss: 1.7608263492584229\n",
      "Accuracy: 0.4019\n",
      "epoch: 830, training loss: 1.2331280708312988, testing loss: 1.8240678310394287\n",
      "Accuracy: 0.4570\n",
      "epoch: 830, training loss: 1.2317078113555908, testing loss: 1.634952425956726\n",
      "Accuracy: 0.4291\n",
      "epoch: 830, training loss: 1.1769425868988037, testing loss: 1.8013334274291992\n",
      "Accuracy: 0.3880\n",
      "epoch: 830, training loss: 1.2634305953979492, testing loss: 1.588769793510437\n",
      "Accuracy: 0.4385\n",
      "epoch: 830, training loss: 1.2219226360321045, testing loss: 1.6638946533203125\n",
      "Accuracy: 0.4438\n",
      "epoch: 830, training loss: 1.252091884613037, testing loss: 1.7467337846755981\n",
      "Accuracy: 0.4227\n",
      "epoch: 830, training loss: 1.2812561988830566, testing loss: 1.6908214092254639\n",
      "Accuracy: 0.4608\n",
      "epoch: 830, training loss: 1.2902064323425293, testing loss: 1.7956128120422363\n",
      "Accuracy: 0.4007\n",
      "epoch: 830, training loss: 1.178784966468811, testing loss: 1.8516125679016113\n",
      "Accuracy: 0.3909\n",
      "epoch: 830, training loss: 1.351360559463501, testing loss: 1.8620715141296387\n",
      "Accuracy: 0.3716\n",
      "epoch: 830, training loss: 1.2075554132461548, testing loss: 1.6996538639068604\n",
      "Accuracy: 0.4312\n",
      "epoch: 830, training loss: 1.266855001449585, testing loss: 1.82814359664917\n",
      "Accuracy: 0.3905\n",
      "epoch: 830, training loss: 1.31222403049469, testing loss: 1.756458044052124\n",
      "Accuracy: 0.4391\n",
      "epoch: 830, training loss: 1.2104547023773193, testing loss: 1.635970115661621\n",
      "Accuracy: 0.3588\n",
      "epoch: 830, training loss: 1.1942329406738281, testing loss: 1.662400245666504\n",
      "Accuracy: 0.4026\n",
      "epoch: 830, training loss: 1.1744095087051392, testing loss: 1.7151089906692505\n",
      "Accuracy: 0.3860\n",
      "epoch: 830, training loss: 1.2487642765045166, testing loss: 1.6854147911071777\n",
      "Accuracy: 0.3994\n",
      "epoch: 830, training loss: 1.2347923517227173, testing loss: 1.6721407175064087\n",
      "Accuracy: 0.4250\n",
      "epoch: 830, training loss: 1.1729408502578735, testing loss: 1.7806915044784546\n",
      "Accuracy: 0.3896\n",
      "epoch: 830, training loss: 1.2242767810821533, testing loss: 1.7013686895370483\n",
      "Accuracy: 0.3994\n",
      "epoch: 830, training loss: 1.2843749523162842, testing loss: 1.7851588726043701\n",
      "Accuracy: 0.4250\n",
      "epoch: 830, training loss: 1.2617086172103882, testing loss: 1.6333343982696533\n",
      "Accuracy: 0.4094\n",
      "epoch: 840, training loss: 1.286860466003418, testing loss: 1.8541820049285889\n",
      "Accuracy: 0.3868\n",
      "epoch: 840, training loss: 1.267349123954773, testing loss: 1.6279680728912354\n",
      "Accuracy: 0.4075\n",
      "epoch: 840, training loss: 1.2348051071166992, testing loss: 1.5761873722076416\n",
      "Accuracy: 0.4723\n",
      "epoch: 840, training loss: 1.2761974334716797, testing loss: 1.6931655406951904\n",
      "Accuracy: 0.4276\n",
      "epoch: 840, training loss: 1.2550805807113647, testing loss: 1.8411712646484375\n",
      "Accuracy: 0.3873\n",
      "epoch: 840, training loss: 1.2556748390197754, testing loss: 1.6033706665039062\n",
      "Accuracy: 0.4545\n",
      "epoch: 840, training loss: 1.2154276371002197, testing loss: 1.833670735359192\n",
      "Accuracy: 0.3982\n",
      "epoch: 840, training loss: 1.2378323078155518, testing loss: 1.8678070306777954\n",
      "Accuracy: 0.3719\n",
      "epoch: 840, training loss: 1.235916256904602, testing loss: 1.7264649868011475\n",
      "Accuracy: 0.4167\n",
      "epoch: 840, training loss: 1.2208586931228638, testing loss: 1.8458166122436523\n",
      "Accuracy: 0.4211\n",
      "epoch: 840, training loss: 1.2166402339935303, testing loss: 1.5641021728515625\n",
      "Accuracy: 0.4359\n",
      "epoch: 840, training loss: 1.2832075357437134, testing loss: 1.8472154140472412\n",
      "Accuracy: 0.4020\n",
      "epoch: 840, training loss: 1.2503377199172974, testing loss: 1.7587746381759644\n",
      "Accuracy: 0.4286\n",
      "epoch: 840, training loss: 1.1831917762756348, testing loss: 1.7957563400268555\n",
      "Accuracy: 0.4132\n",
      "epoch: 840, training loss: 1.1866605281829834, testing loss: 1.5771560668945312\n",
      "Accuracy: 0.4301\n",
      "epoch: 840, training loss: 1.1964744329452515, testing loss: 1.7820427417755127\n",
      "Accuracy: 0.4107\n",
      "epoch: 840, training loss: 1.3026714324951172, testing loss: 1.7333742380142212\n",
      "Accuracy: 0.4257\n",
      "epoch: 840, training loss: 1.3242350816726685, testing loss: 1.7437187433242798\n",
      "Accuracy: 0.4037\n",
      "epoch: 840, training loss: 1.208912968635559, testing loss: 1.6464835405349731\n",
      "Accuracy: 0.4272\n",
      "epoch: 840, training loss: 1.2685192823410034, testing loss: 1.8356748819351196\n",
      "Accuracy: 0.3470\n",
      "epoch: 840, training loss: 1.2254472970962524, testing loss: 1.6689119338989258\n",
      "Accuracy: 0.4427\n",
      "epoch: 840, training loss: 1.310653567314148, testing loss: 1.616660237312317\n",
      "Accuracy: 0.4411\n",
      "epoch: 840, training loss: 1.2343921661376953, testing loss: 1.7593423128128052\n",
      "Accuracy: 0.3882\n",
      "epoch: 840, training loss: 1.285828948020935, testing loss: 1.6071369647979736\n",
      "Accuracy: 0.4307\n",
      "epoch: 840, training loss: 1.2648860216140747, testing loss: 1.639399766921997\n",
      "Accuracy: 0.3832\n",
      "epoch: 840, training loss: 1.1994390487670898, testing loss: 1.645939826965332\n",
      "Accuracy: 0.4613\n",
      "epoch: 840, training loss: 1.21178138256073, testing loss: 1.598998785018921\n",
      "Accuracy: 0.4286\n",
      "epoch: 840, training loss: 1.2584058046340942, testing loss: 1.6043318510055542\n",
      "Accuracy: 0.4530\n",
      "epoch: 840, training loss: 1.1900027990341187, testing loss: 1.5817354917526245\n",
      "Accuracy: 0.4114\n",
      "epoch: 840, training loss: 1.2530336380004883, testing loss: 1.6350703239440918\n",
      "Accuracy: 0.4262\n",
      "epoch: 840, training loss: 1.2264505624771118, testing loss: 1.7607113122940063\n",
      "Accuracy: 0.3949\n",
      "epoch: 840, training loss: 1.233297348022461, testing loss: 1.7582483291625977\n",
      "Accuracy: 0.4192\n",
      "epoch: 840, training loss: 1.2549560070037842, testing loss: 1.710524559020996\n",
      "Accuracy: 0.4476\n",
      "epoch: 840, training loss: 1.2469805479049683, testing loss: 1.785205602645874\n",
      "Accuracy: 0.4228\n",
      "epoch: 840, training loss: 1.261610984802246, testing loss: 1.672664999961853\n",
      "Accuracy: 0.4371\n",
      "epoch: 840, training loss: 1.2987542152404785, testing loss: 1.8150951862335205\n",
      "Accuracy: 0.4204\n",
      "epoch: 840, training loss: 1.2400234937667847, testing loss: 1.6034644842147827\n",
      "Accuracy: 0.4462\n",
      "epoch: 840, training loss: 1.204236388206482, testing loss: 1.7043673992156982\n",
      "Accuracy: 0.4426\n",
      "epoch: 840, training loss: 1.1887085437774658, testing loss: 1.7126634120941162\n",
      "Accuracy: 0.4270\n",
      "epoch: 840, training loss: 1.2189520597457886, testing loss: 1.6700339317321777\n",
      "Accuracy: 0.4488\n",
      "epoch: 840, training loss: 1.2376015186309814, testing loss: 1.866151213645935\n",
      "Accuracy: 0.3686\n",
      "epoch: 840, training loss: 1.2297261953353882, testing loss: 1.6863447427749634\n",
      "Accuracy: 0.4255\n",
      "epoch: 840, training loss: 1.3104265928268433, testing loss: 1.6959511041641235\n",
      "Accuracy: 0.4161\n",
      "epoch: 840, training loss: 1.212141752243042, testing loss: 1.7106480598449707\n",
      "Accuracy: 0.3794\n",
      "epoch: 840, training loss: 1.2634179592132568, testing loss: 1.7894846200942993\n",
      "Accuracy: 0.4286\n",
      "epoch: 840, training loss: 1.2949587106704712, testing loss: 1.64936101436615\n",
      "Accuracy: 0.4424\n",
      "epoch: 840, training loss: 1.312716007232666, testing loss: 1.6272233724594116\n",
      "Accuracy: 0.4430\n",
      "epoch: 840, training loss: 1.2986575365066528, testing loss: 1.76888108253479\n",
      "Accuracy: 0.3974\n",
      "epoch: 840, training loss: 1.2285996675491333, testing loss: 1.6783078908920288\n",
      "Accuracy: 0.3828\n",
      "epoch: 840, training loss: 1.192417025566101, testing loss: 1.6246247291564941\n",
      "Accuracy: 0.4353\n",
      "epoch: 840, training loss: 1.2289042472839355, testing loss: 1.6376103162765503\n",
      "Accuracy: 0.4934\n",
      "epoch: 840, training loss: 1.2130869626998901, testing loss: 1.7803711891174316\n",
      "Accuracy: 0.4153\n",
      "epoch: 840, training loss: 1.1420193910598755, testing loss: 1.7957683801651\n",
      "Accuracy: 0.3958\n",
      "epoch: 840, training loss: 1.2377973794937134, testing loss: 1.6800204515457153\n",
      "Accuracy: 0.3869\n",
      "epoch: 840, training loss: 1.2360806465148926, testing loss: 1.7227519750595093\n",
      "Accuracy: 0.4255\n",
      "epoch: 840, training loss: 1.219663381576538, testing loss: 1.6617248058319092\n",
      "Accuracy: 0.4181\n",
      "epoch: 840, training loss: 1.2753831148147583, testing loss: 1.7987236976623535\n",
      "Accuracy: 0.3920\n",
      "epoch: 840, training loss: 1.2770390510559082, testing loss: 1.6906630992889404\n",
      "Accuracy: 0.4343\n",
      "epoch: 840, training loss: 1.3318181037902832, testing loss: 1.755183458328247\n",
      "Accuracy: 0.4495\n",
      "epoch: 840, training loss: 1.218727946281433, testing loss: 1.5794219970703125\n",
      "Accuracy: 0.4153\n",
      "epoch: 840, training loss: 1.265150547027588, testing loss: 1.6545075178146362\n",
      "Accuracy: 0.4072\n",
      "epoch: 840, training loss: 1.2464202642440796, testing loss: 1.752319574356079\n",
      "Accuracy: 0.4182\n",
      "epoch: 840, training loss: 1.2946337461471558, testing loss: 1.7360848188400269\n",
      "Accuracy: 0.3839\n",
      "epoch: 840, training loss: 1.2720909118652344, testing loss: 1.816208839416504\n",
      "Accuracy: 0.4026\n",
      "epoch: 840, training loss: 1.2241241931915283, testing loss: 1.753948450088501\n",
      "Accuracy: 0.4127\n",
      "epoch: 840, training loss: 1.2292295694351196, testing loss: 1.9089231491088867\n",
      "Accuracy: 0.3529\n",
      "epoch: 840, training loss: 1.225016713142395, testing loss: 1.6984246969223022\n",
      "Accuracy: 0.4479\n",
      "epoch: 840, training loss: 1.2610585689544678, testing loss: 1.6458922624588013\n",
      "Accuracy: 0.4951\n",
      "epoch: 840, training loss: 1.2306129932403564, testing loss: 1.620909333229065\n",
      "Accuracy: 0.4384\n",
      "epoch: 840, training loss: 1.2034986019134521, testing loss: 1.5860648155212402\n",
      "Accuracy: 0.4481\n",
      "epoch: 840, training loss: 1.135767936706543, testing loss: 1.6585906744003296\n",
      "Accuracy: 0.4364\n",
      "epoch: 840, training loss: 1.2392990589141846, testing loss: 1.7724380493164062\n",
      "Accuracy: 0.3980\n",
      "epoch: 840, training loss: 1.2164697647094727, testing loss: 1.7658305168151855\n",
      "Accuracy: 0.3951\n",
      "epoch: 840, training loss: 1.1950503587722778, testing loss: 1.6855536699295044\n",
      "Accuracy: 0.4661\n",
      "epoch: 840, training loss: 1.2408076524734497, testing loss: 1.8338308334350586\n",
      "Accuracy: 0.3805\n",
      "epoch: 840, training loss: 1.1510282754898071, testing loss: 1.6639158725738525\n",
      "Accuracy: 0.3889\n",
      "epoch: 840, training loss: 1.115271806716919, testing loss: 1.711010456085205\n",
      "Accuracy: 0.4036\n",
      "epoch: 840, training loss: 1.3010222911834717, testing loss: 1.8662641048431396\n",
      "Accuracy: 0.4277\n",
      "epoch: 840, training loss: 1.1492514610290527, testing loss: 1.798487901687622\n",
      "Accuracy: 0.4202\n",
      "epoch: 840, training loss: 1.247026801109314, testing loss: 1.8161332607269287\n",
      "Accuracy: 0.3810\n",
      "epoch: 840, training loss: 1.2708945274353027, testing loss: 1.6559549570083618\n",
      "Accuracy: 0.4597\n",
      "epoch: 840, training loss: 1.265357494354248, testing loss: 1.6787526607513428\n",
      "Accuracy: 0.4224\n",
      "epoch: 840, training loss: 1.2558774948120117, testing loss: 1.8139344453811646\n",
      "Accuracy: 0.4244\n",
      "epoch: 840, training loss: 1.1852203607559204, testing loss: 1.7075762748718262\n",
      "Accuracy: 0.4103\n",
      "epoch: 840, training loss: 1.2594916820526123, testing loss: 1.8213144540786743\n",
      "Accuracy: 0.4277\n",
      "epoch: 840, training loss: 1.2223786115646362, testing loss: 1.7853562831878662\n",
      "Accuracy: 0.3962\n",
      "epoch: 840, training loss: 1.2285112142562866, testing loss: 1.6695384979248047\n",
      "Accuracy: 0.3945\n",
      "epoch: 840, training loss: 1.2542641162872314, testing loss: 1.7381941080093384\n",
      "Accuracy: 0.4276\n",
      "epoch: 840, training loss: 1.163007378578186, testing loss: 1.792158603668213\n",
      "Accuracy: 0.4033\n",
      "epoch: 840, training loss: 1.2733864784240723, testing loss: 1.8321820497512817\n",
      "Accuracy: 0.3864\n",
      "epoch: 840, training loss: 1.2833609580993652, testing loss: 1.6886855363845825\n",
      "Accuracy: 0.4113\n",
      "epoch: 840, training loss: 1.3044005632400513, testing loss: 1.7391786575317383\n",
      "Accuracy: 0.3937\n",
      "epoch: 840, training loss: 1.2676459550857544, testing loss: 1.7058160305023193\n",
      "Accuracy: 0.4452\n",
      "epoch: 840, training loss: 1.3141013383865356, testing loss: 1.731461763381958\n",
      "Accuracy: 0.3875\n",
      "epoch: 840, training loss: 1.271968126296997, testing loss: 1.83085298538208\n",
      "Accuracy: 0.3818\n",
      "epoch: 840, training loss: 1.1955931186676025, testing loss: 1.6533936262130737\n",
      "Accuracy: 0.4296\n",
      "epoch: 840, training loss: 1.2701035737991333, testing loss: 1.74995756149292\n",
      "Accuracy: 0.3946\n",
      "epoch: 840, training loss: 1.275113582611084, testing loss: 1.6559288501739502\n",
      "Accuracy: 0.3974\n",
      "epoch: 840, training loss: 1.2007378339767456, testing loss: 1.5916537046432495\n",
      "Accuracy: 0.4331\n",
      "epoch: 840, training loss: 1.282233476638794, testing loss: 1.7407935857772827\n",
      "Accuracy: 0.3924\n",
      "epoch: 850, training loss: 1.2235515117645264, testing loss: 1.7507797479629517\n",
      "Accuracy: 0.4136\n",
      "epoch: 850, training loss: 1.3377832174301147, testing loss: 1.73777437210083\n",
      "Accuracy: 0.3883\n",
      "epoch: 850, training loss: 1.1568925380706787, testing loss: 1.7646870613098145\n",
      "Accuracy: 0.3877\n",
      "epoch: 850, training loss: 1.1990723609924316, testing loss: 1.7927356958389282\n",
      "Accuracy: 0.4007\n",
      "epoch: 850, training loss: 1.2704795598983765, testing loss: 1.7567209005355835\n",
      "Accuracy: 0.4131\n",
      "epoch: 850, training loss: 1.20363450050354, testing loss: 1.80170738697052\n",
      "Accuracy: 0.3892\n",
      "epoch: 850, training loss: 1.3041621446609497, testing loss: 1.764971137046814\n",
      "Accuracy: 0.4305\n",
      "epoch: 850, training loss: 1.3294435739517212, testing loss: 1.6954659223556519\n",
      "Accuracy: 0.4349\n",
      "epoch: 850, training loss: 1.2893822193145752, testing loss: 1.778603434562683\n",
      "Accuracy: 0.3768\n",
      "epoch: 850, training loss: 1.2924728393554688, testing loss: 1.6677534580230713\n",
      "Accuracy: 0.4307\n",
      "epoch: 850, training loss: 1.2755324840545654, testing loss: 1.7038670778274536\n",
      "Accuracy: 0.3993\n",
      "epoch: 850, training loss: 1.2007474899291992, testing loss: 1.5540255308151245\n",
      "Accuracy: 0.4164\n",
      "epoch: 850, training loss: 1.2321817874908447, testing loss: 1.6753283739089966\n",
      "Accuracy: 0.4448\n",
      "epoch: 850, training loss: 1.239337682723999, testing loss: 1.7028611898422241\n",
      "Accuracy: 0.4156\n",
      "epoch: 850, training loss: 1.2679976224899292, testing loss: 1.6903411149978638\n",
      "Accuracy: 0.4090\n",
      "epoch: 850, training loss: 1.2715657949447632, testing loss: 1.810705304145813\n",
      "Accuracy: 0.4067\n",
      "epoch: 850, training loss: 1.2628847360610962, testing loss: 1.770676612854004\n",
      "Accuracy: 0.3861\n",
      "epoch: 850, training loss: 1.2766526937484741, testing loss: 1.6179606914520264\n",
      "Accuracy: 0.4389\n",
      "epoch: 850, training loss: 1.26263427734375, testing loss: 1.7423237562179565\n",
      "Accuracy: 0.4164\n",
      "epoch: 850, training loss: 1.2780427932739258, testing loss: 1.7154837846755981\n",
      "Accuracy: 0.4309\n",
      "epoch: 850, training loss: 1.2283128499984741, testing loss: 1.7064659595489502\n",
      "Accuracy: 0.3711\n",
      "epoch: 850, training loss: 1.1906614303588867, testing loss: 1.7643182277679443\n",
      "Accuracy: 0.3981\n",
      "epoch: 850, training loss: 1.273557186126709, testing loss: 1.9953798055648804\n",
      "Accuracy: 0.3631\n",
      "epoch: 850, training loss: 1.3110032081604004, testing loss: 1.7883901596069336\n",
      "Accuracy: 0.4125\n",
      "epoch: 850, training loss: 1.2449994087219238, testing loss: 1.7135462760925293\n",
      "Accuracy: 0.3769\n",
      "epoch: 850, training loss: 1.1636675596237183, testing loss: 1.727461814880371\n",
      "Accuracy: 0.4441\n",
      "epoch: 850, training loss: 1.2308584451675415, testing loss: 1.5919909477233887\n",
      "Accuracy: 0.4281\n",
      "epoch: 850, training loss: 1.2238514423370361, testing loss: 1.7177542448043823\n",
      "Accuracy: 0.4092\n",
      "epoch: 850, training loss: 1.1811718940734863, testing loss: 1.6073596477508545\n",
      "Accuracy: 0.3962\n",
      "epoch: 850, training loss: 1.2467812299728394, testing loss: 1.6750621795654297\n",
      "Accuracy: 0.4565\n",
      "epoch: 850, training loss: 1.2745141983032227, testing loss: 1.7135452032089233\n",
      "Accuracy: 0.4227\n",
      "epoch: 850, training loss: 1.28251314163208, testing loss: 1.789458155632019\n",
      "Accuracy: 0.4361\n",
      "epoch: 850, training loss: 1.2122305631637573, testing loss: 1.779870867729187\n",
      "Accuracy: 0.4371\n",
      "epoch: 850, training loss: 1.2480703592300415, testing loss: 1.7393909692764282\n",
      "Accuracy: 0.3807\n",
      "epoch: 850, training loss: 1.2067737579345703, testing loss: 1.7659701108932495\n",
      "Accuracy: 0.3622\n",
      "epoch: 850, training loss: 1.2797417640686035, testing loss: 1.5816764831542969\n",
      "Accuracy: 0.4984\n",
      "epoch: 850, training loss: 1.240289568901062, testing loss: 1.7059026956558228\n",
      "Accuracy: 0.4351\n",
      "epoch: 850, training loss: 1.2250473499298096, testing loss: 1.6963344812393188\n",
      "Accuracy: 0.4375\n",
      "epoch: 850, training loss: 1.2021455764770508, testing loss: 1.618309497833252\n",
      "Accuracy: 0.4169\n",
      "epoch: 850, training loss: 1.2405707836151123, testing loss: 1.672485113143921\n",
      "Accuracy: 0.3892\n",
      "epoch: 850, training loss: 1.2047150135040283, testing loss: 1.6194196939468384\n",
      "Accuracy: 0.4412\n",
      "epoch: 850, training loss: 1.2288146018981934, testing loss: 1.9891561269760132\n",
      "Accuracy: 0.3230\n",
      "epoch: 850, training loss: 1.2824132442474365, testing loss: 1.7617743015289307\n",
      "Accuracy: 0.3966\n",
      "epoch: 850, training loss: 1.2091666460037231, testing loss: 1.7051787376403809\n",
      "Accuracy: 0.4080\n",
      "epoch: 850, training loss: 1.2603627443313599, testing loss: 1.7507905960083008\n",
      "Accuracy: 0.4367\n",
      "epoch: 850, training loss: 1.2241233587265015, testing loss: 1.6444724798202515\n",
      "Accuracy: 0.4224\n",
      "epoch: 850, training loss: 1.2688982486724854, testing loss: 1.6699090003967285\n",
      "Accuracy: 0.4328\n",
      "epoch: 850, training loss: 1.29298996925354, testing loss: 1.689505934715271\n",
      "Accuracy: 0.4551\n",
      "epoch: 850, training loss: 1.1773121356964111, testing loss: 1.7615422010421753\n",
      "Accuracy: 0.3943\n",
      "epoch: 850, training loss: 1.2424627542495728, testing loss: 1.701879858970642\n",
      "Accuracy: 0.4303\n",
      "epoch: 850, training loss: 1.2991071939468384, testing loss: 1.6517393589019775\n",
      "Accuracy: 0.4539\n",
      "epoch: 850, training loss: 1.2233226299285889, testing loss: 1.6806615591049194\n",
      "Accuracy: 0.4074\n",
      "epoch: 850, training loss: 1.2134039402008057, testing loss: 1.651179552078247\n",
      "Accuracy: 0.4167\n",
      "epoch: 850, training loss: 1.251294493675232, testing loss: 1.735813021659851\n",
      "Accuracy: 0.4013\n",
      "epoch: 850, training loss: 1.228952169418335, testing loss: 1.6365303993225098\n",
      "Accuracy: 0.4121\n",
      "epoch: 850, training loss: 1.171236515045166, testing loss: 1.7814533710479736\n",
      "Accuracy: 0.3961\n",
      "epoch: 850, training loss: 1.2027839422225952, testing loss: 1.6136999130249023\n",
      "Accuracy: 0.4601\n",
      "epoch: 850, training loss: 1.2666136026382446, testing loss: 1.7949678897857666\n",
      "Accuracy: 0.3951\n",
      "epoch: 850, training loss: 1.2638964653015137, testing loss: 1.833720326423645\n",
      "Accuracy: 0.4045\n",
      "epoch: 850, training loss: 1.1664992570877075, testing loss: 1.7064703702926636\n",
      "Accuracy: 0.4272\n",
      "epoch: 850, training loss: 1.3001601696014404, testing loss: 1.8350496292114258\n",
      "Accuracy: 0.3722\n",
      "epoch: 850, training loss: 1.1975760459899902, testing loss: 1.818695068359375\n",
      "Accuracy: 0.3642\n",
      "epoch: 850, training loss: 1.1631494760513306, testing loss: 1.6855090856552124\n",
      "Accuracy: 0.4271\n",
      "epoch: 850, training loss: 1.1796495914459229, testing loss: 1.7240530252456665\n",
      "Accuracy: 0.4037\n",
      "epoch: 850, training loss: 1.2150660753250122, testing loss: 1.6912026405334473\n",
      "Accuracy: 0.4057\n",
      "epoch: 850, training loss: 1.2294986248016357, testing loss: 1.5682004690170288\n",
      "Accuracy: 0.4130\n",
      "epoch: 850, training loss: 1.2060915231704712, testing loss: 1.8445404767990112\n",
      "Accuracy: 0.4437\n",
      "epoch: 850, training loss: 1.2660917043685913, testing loss: 1.6759907007217407\n",
      "Accuracy: 0.4342\n",
      "epoch: 850, training loss: 1.207275629043579, testing loss: 1.601536750793457\n",
      "Accuracy: 0.4120\n",
      "epoch: 850, training loss: 1.2521865367889404, testing loss: 1.709962248802185\n",
      "Accuracy: 0.3746\n",
      "epoch: 850, training loss: 1.2148646116256714, testing loss: 1.7770804166793823\n",
      "Accuracy: 0.4225\n",
      "epoch: 850, training loss: 1.2152179479599, testing loss: 1.6410526037216187\n",
      "Accuracy: 0.4444\n",
      "epoch: 850, training loss: 1.1867324113845825, testing loss: 1.8597196340560913\n",
      "Accuracy: 0.3654\n",
      "epoch: 850, training loss: 1.237552523612976, testing loss: 1.7302213907241821\n",
      "Accuracy: 0.4164\n",
      "epoch: 850, training loss: 1.2438689470291138, testing loss: 1.7855640649795532\n",
      "Accuracy: 0.4332\n",
      "epoch: 850, training loss: 1.2778725624084473, testing loss: 1.5868983268737793\n",
      "Accuracy: 0.4218\n",
      "epoch: 850, training loss: 1.2277783155441284, testing loss: 1.6368962526321411\n",
      "Accuracy: 0.4636\n",
      "epoch: 850, training loss: 1.280896782875061, testing loss: 1.793988823890686\n",
      "Accuracy: 0.4045\n",
      "epoch: 850, training loss: 1.2445253133773804, testing loss: 1.7357280254364014\n",
      "Accuracy: 0.4686\n",
      "epoch: 850, training loss: 1.2447315454483032, testing loss: 1.673277735710144\n",
      "Accuracy: 0.4200\n",
      "epoch: 850, training loss: 1.2661151885986328, testing loss: 1.7682489156723022\n",
      "Accuracy: 0.4112\n",
      "epoch: 850, training loss: 1.3017443418502808, testing loss: 1.6842726469039917\n",
      "Accuracy: 0.4263\n",
      "epoch: 850, training loss: 1.2647165060043335, testing loss: 1.642073154449463\n",
      "Accuracy: 0.4919\n",
      "epoch: 850, training loss: 1.2086917161941528, testing loss: 1.5973950624465942\n",
      "Accuracy: 0.4740\n",
      "epoch: 850, training loss: 1.1818660497665405, testing loss: 1.670806884765625\n",
      "Accuracy: 0.4248\n",
      "epoch: 850, training loss: 1.222357988357544, testing loss: 1.6466635465621948\n",
      "Accuracy: 0.4507\n",
      "epoch: 850, training loss: 1.2992267608642578, testing loss: 1.8333919048309326\n",
      "Accuracy: 0.4000\n",
      "epoch: 850, training loss: 1.2424614429473877, testing loss: 1.570493221282959\n",
      "Accuracy: 0.4821\n",
      "epoch: 850, training loss: 1.2493958473205566, testing loss: 1.9459480047225952\n",
      "Accuracy: 0.3724\n",
      "epoch: 850, training loss: 1.1421817541122437, testing loss: 1.7016206979751587\n",
      "Accuracy: 0.4218\n",
      "epoch: 850, training loss: 1.2528419494628906, testing loss: 1.6843550205230713\n",
      "Accuracy: 0.4367\n",
      "epoch: 850, training loss: 1.2160851955413818, testing loss: 1.6923445463180542\n",
      "Accuracy: 0.4609\n",
      "epoch: 850, training loss: 1.179729700088501, testing loss: 1.7157176733016968\n",
      "Accuracy: 0.4377\n",
      "epoch: 850, training loss: 1.2137373685836792, testing loss: 1.6423895359039307\n",
      "Accuracy: 0.4467\n",
      "epoch: 850, training loss: 1.2295407056808472, testing loss: 1.6422594785690308\n",
      "Accuracy: 0.4247\n",
      "epoch: 850, training loss: 1.1884461641311646, testing loss: 1.8398951292037964\n",
      "Accuracy: 0.4000\n",
      "epoch: 850, training loss: 1.221602201461792, testing loss: 1.6058893203735352\n",
      "Accuracy: 0.4570\n",
      "epoch: 850, training loss: 1.1476359367370605, testing loss: 1.7773966789245605\n",
      "Accuracy: 0.4369\n",
      "epoch: 850, training loss: 1.222571849822998, testing loss: 1.7356499433517456\n",
      "Accuracy: 0.4342\n",
      "epoch: 850, training loss: 1.2850265502929688, testing loss: 1.8120940923690796\n",
      "Accuracy: 0.3900\n",
      "epoch: 860, training loss: 1.1986879110336304, testing loss: 1.6644550561904907\n",
      "Accuracy: 0.4723\n",
      "epoch: 860, training loss: 1.193407654762268, testing loss: 1.707214117050171\n",
      "Accuracy: 0.4389\n",
      "epoch: 860, training loss: 1.2507232427597046, testing loss: 1.7332346439361572\n",
      "Accuracy: 0.4164\n",
      "epoch: 860, training loss: 1.243930459022522, testing loss: 1.7080304622650146\n",
      "Accuracy: 0.4430\n",
      "epoch: 860, training loss: 1.1927049160003662, testing loss: 1.6650550365447998\n",
      "Accuracy: 0.4360\n",
      "epoch: 860, training loss: 1.2275038957595825, testing loss: 1.6300374269485474\n",
      "Accuracy: 0.4032\n",
      "epoch: 860, training loss: 1.3057548999786377, testing loss: 1.767735242843628\n",
      "Accuracy: 0.4096\n",
      "epoch: 860, training loss: 1.2173597812652588, testing loss: 1.7250868082046509\n",
      "Accuracy: 0.4309\n",
      "epoch: 860, training loss: 1.211213231086731, testing loss: 1.799649715423584\n",
      "Accuracy: 0.4019\n",
      "epoch: 860, training loss: 1.2565525770187378, testing loss: 1.7592852115631104\n",
      "Accuracy: 0.4119\n",
      "epoch: 860, training loss: 1.2063970565795898, testing loss: 1.8068153858184814\n",
      "Accuracy: 0.4094\n",
      "epoch: 860, training loss: 1.2011908292770386, testing loss: 1.588388204574585\n",
      "Accuracy: 0.4494\n",
      "epoch: 860, training loss: 1.2132151126861572, testing loss: 1.6656014919281006\n",
      "Accuracy: 0.4290\n",
      "epoch: 860, training loss: 1.128236174583435, testing loss: 1.6430765390396118\n",
      "Accuracy: 0.3713\n",
      "epoch: 860, training loss: 1.2004413604736328, testing loss: 1.6965000629425049\n",
      "Accuracy: 0.4106\n",
      "epoch: 860, training loss: 1.3425709009170532, testing loss: 1.7130582332611084\n",
      "Accuracy: 0.3960\n",
      "epoch: 860, training loss: 1.2127277851104736, testing loss: 1.7114505767822266\n",
      "Accuracy: 0.4000\n",
      "epoch: 860, training loss: 1.2206382751464844, testing loss: 1.6868973970413208\n",
      "Accuracy: 0.4128\n",
      "epoch: 860, training loss: 1.2113970518112183, testing loss: 1.7989587783813477\n",
      "Accuracy: 0.4185\n",
      "epoch: 860, training loss: 1.3121927976608276, testing loss: 1.6942496299743652\n",
      "Accuracy: 0.4038\n",
      "epoch: 860, training loss: 1.2341760396957397, testing loss: 1.8546980619430542\n",
      "Accuracy: 0.4060\n",
      "epoch: 860, training loss: 1.246001958847046, testing loss: 1.6636930704116821\n",
      "Accuracy: 0.4534\n",
      "epoch: 860, training loss: 1.276070237159729, testing loss: 1.6549021005630493\n",
      "Accuracy: 0.4373\n",
      "epoch: 860, training loss: 1.1305400133132935, testing loss: 1.7931735515594482\n",
      "Accuracy: 0.4103\n",
      "epoch: 860, training loss: 1.1724133491516113, testing loss: 1.7273221015930176\n",
      "Accuracy: 0.3861\n",
      "epoch: 860, training loss: 1.3065162897109985, testing loss: 1.7739697694778442\n",
      "Accuracy: 0.4300\n",
      "epoch: 860, training loss: 1.3275196552276611, testing loss: 1.7526476383209229\n",
      "Accuracy: 0.3910\n",
      "epoch: 860, training loss: 1.273878574371338, testing loss: 1.7573858499526978\n",
      "Accuracy: 0.4341\n",
      "epoch: 860, training loss: 1.299737572669983, testing loss: 1.8144631385803223\n",
      "Accuracy: 0.4220\n",
      "epoch: 860, training loss: 1.279890537261963, testing loss: 1.7201265096664429\n",
      "Accuracy: 0.4654\n",
      "epoch: 860, training loss: 1.2323037385940552, testing loss: 1.6768865585327148\n",
      "Accuracy: 0.4452\n",
      "epoch: 860, training loss: 1.2621886730194092, testing loss: 1.7058196067810059\n",
      "Accuracy: 0.3805\n",
      "epoch: 860, training loss: 1.2688748836517334, testing loss: 1.757063388824463\n",
      "Accuracy: 0.3931\n",
      "epoch: 860, training loss: 1.1970833539962769, testing loss: 1.8034594058990479\n",
      "Accuracy: 0.3851\n",
      "epoch: 860, training loss: 1.2965894937515259, testing loss: 1.6774572134017944\n",
      "Accuracy: 0.4027\n",
      "epoch: 860, training loss: 1.217482089996338, testing loss: 1.6339608430862427\n",
      "Accuracy: 0.3937\n",
      "epoch: 860, training loss: 1.2424757480621338, testing loss: 1.5765361785888672\n",
      "Accuracy: 0.4038\n",
      "epoch: 860, training loss: 1.2249866724014282, testing loss: 1.7067537307739258\n",
      "Accuracy: 0.4310\n",
      "epoch: 860, training loss: 1.267988920211792, testing loss: 1.5368620157241821\n",
      "Accuracy: 0.4579\n",
      "epoch: 860, training loss: 1.2996654510498047, testing loss: 1.6546647548675537\n",
      "Accuracy: 0.4205\n",
      "epoch: 860, training loss: 1.2674944400787354, testing loss: 1.6585336923599243\n",
      "Accuracy: 0.4322\n",
      "epoch: 860, training loss: 1.2608948945999146, testing loss: 1.7182501554489136\n",
      "Accuracy: 0.4237\n",
      "epoch: 860, training loss: 1.2948013544082642, testing loss: 1.6855404376983643\n",
      "Accuracy: 0.4407\n",
      "epoch: 860, training loss: 1.2171175479888916, testing loss: 1.6622990369796753\n",
      "Accuracy: 0.3839\n",
      "epoch: 860, training loss: 1.2428449392318726, testing loss: 1.6477988958358765\n",
      "Accuracy: 0.4074\n",
      "epoch: 860, training loss: 1.2474976778030396, testing loss: 1.8062773942947388\n",
      "Accuracy: 0.3746\n",
      "epoch: 860, training loss: 1.2102404832839966, testing loss: 1.672770380973816\n",
      "Accuracy: 0.4012\n",
      "epoch: 860, training loss: 1.3088726997375488, testing loss: 1.7045888900756836\n",
      "Accuracy: 0.3929\n",
      "epoch: 860, training loss: 1.251355528831482, testing loss: 1.6736639738082886\n",
      "Accuracy: 0.3864\n",
      "epoch: 860, training loss: 1.214779257774353, testing loss: 1.5922021865844727\n",
      "Accuracy: 0.4554\n",
      "epoch: 860, training loss: 1.1802705526351929, testing loss: 1.7008519172668457\n",
      "Accuracy: 0.4471\n",
      "epoch: 860, training loss: 1.2082350254058838, testing loss: 1.8820854425430298\n",
      "Accuracy: 0.3455\n",
      "epoch: 860, training loss: 1.2293561697006226, testing loss: 1.8174859285354614\n",
      "Accuracy: 0.4205\n",
      "epoch: 860, training loss: 1.2057336568832397, testing loss: 1.7175571918487549\n",
      "Accuracy: 0.4073\n",
      "epoch: 860, training loss: 1.2974388599395752, testing loss: 1.6853123903274536\n",
      "Accuracy: 0.4262\n",
      "epoch: 860, training loss: 1.207140326499939, testing loss: 1.7951111793518066\n",
      "Accuracy: 0.4214\n",
      "epoch: 860, training loss: 1.1796339750289917, testing loss: 1.784563660621643\n",
      "Accuracy: 0.4419\n",
      "epoch: 860, training loss: 1.2821263074874878, testing loss: 1.750462532043457\n",
      "Accuracy: 0.4441\n",
      "epoch: 860, training loss: 1.2509740591049194, testing loss: 1.674704909324646\n",
      "Accuracy: 0.4421\n",
      "epoch: 860, training loss: 1.2636961936950684, testing loss: 1.7198866605758667\n",
      "Accuracy: 0.3937\n",
      "epoch: 860, training loss: 1.3334909677505493, testing loss: 1.770975947380066\n",
      "Accuracy: 0.3709\n",
      "epoch: 860, training loss: 1.2166554927825928, testing loss: 1.7160614728927612\n",
      "Accuracy: 0.4426\n",
      "epoch: 860, training loss: 1.261866807937622, testing loss: 1.7918893098831177\n",
      "Accuracy: 0.3782\n",
      "epoch: 860, training loss: 1.1605753898620605, testing loss: 1.6541085243225098\n",
      "Accuracy: 0.4249\n",
      "epoch: 860, training loss: 1.2480604648590088, testing loss: 1.655236005783081\n",
      "Accuracy: 0.3898\n",
      "epoch: 860, training loss: 1.1965022087097168, testing loss: 1.8085482120513916\n",
      "Accuracy: 0.4273\n",
      "epoch: 860, training loss: 1.2813010215759277, testing loss: 1.7822428941726685\n",
      "Accuracy: 0.4257\n",
      "epoch: 860, training loss: 1.2480132579803467, testing loss: 1.7414222955703735\n",
      "Accuracy: 0.4396\n",
      "epoch: 860, training loss: 1.1956692934036255, testing loss: 1.7013163566589355\n",
      "Accuracy: 0.4545\n",
      "epoch: 860, training loss: 1.1947076320648193, testing loss: 1.8388644456863403\n",
      "Accuracy: 0.3949\n",
      "epoch: 860, training loss: 1.278237223625183, testing loss: 1.6928008794784546\n",
      "Accuracy: 0.3707\n",
      "epoch: 860, training loss: 1.216507077217102, testing loss: 1.7084510326385498\n",
      "Accuracy: 0.3608\n",
      "epoch: 860, training loss: 1.2806380987167358, testing loss: 1.7887505292892456\n",
      "Accuracy: 0.4007\n",
      "epoch: 860, training loss: 1.2153102159500122, testing loss: 1.8002023696899414\n",
      "Accuracy: 0.4129\n",
      "epoch: 860, training loss: 1.251847267150879, testing loss: 1.750093698501587\n",
      "Accuracy: 0.4075\n",
      "epoch: 860, training loss: 1.2767994403839111, testing loss: 1.7613091468811035\n",
      "Accuracy: 0.3957\n",
      "epoch: 860, training loss: 1.2093390226364136, testing loss: 1.827423334121704\n",
      "Accuracy: 0.3600\n",
      "epoch: 860, training loss: 1.2571007013320923, testing loss: 1.8422263860702515\n",
      "Accuracy: 0.4214\n",
      "epoch: 860, training loss: 1.250732421875, testing loss: 1.751851201057434\n",
      "Accuracy: 0.4184\n",
      "epoch: 860, training loss: 1.2877440452575684, testing loss: 1.6819273233413696\n",
      "Accuracy: 0.4351\n",
      "epoch: 860, training loss: 1.2696809768676758, testing loss: 1.6706677675247192\n",
      "Accuracy: 0.4155\n",
      "epoch: 860, training loss: 1.211289882659912, testing loss: 1.8370667695999146\n",
      "Accuracy: 0.4136\n",
      "epoch: 860, training loss: 1.3061563968658447, testing loss: 1.7687180042266846\n",
      "Accuracy: 0.3590\n",
      "epoch: 860, training loss: 1.2699183225631714, testing loss: 1.7625095844268799\n",
      "Accuracy: 0.3981\n",
      "epoch: 860, training loss: 1.2746256589889526, testing loss: 1.6769870519638062\n",
      "Accuracy: 0.4513\n",
      "epoch: 860, training loss: 1.2536464929580688, testing loss: 1.6856024265289307\n",
      "Accuracy: 0.4395\n",
      "epoch: 860, training loss: 1.2620455026626587, testing loss: 1.8309558629989624\n",
      "Accuracy: 0.3750\n",
      "epoch: 860, training loss: 1.1934764385223389, testing loss: 1.7321850061416626\n",
      "Accuracy: 0.4313\n",
      "epoch: 860, training loss: 1.2088370323181152, testing loss: 1.5982120037078857\n",
      "Accuracy: 0.4394\n",
      "epoch: 860, training loss: 1.1881529092788696, testing loss: 1.8476738929748535\n",
      "Accuracy: 0.4141\n",
      "epoch: 860, training loss: 1.23760986328125, testing loss: 1.6195393800735474\n",
      "Accuracy: 0.4344\n",
      "epoch: 860, training loss: 1.249307632446289, testing loss: 1.6254568099975586\n",
      "Accuracy: 0.4399\n",
      "epoch: 860, training loss: 1.230789065361023, testing loss: 1.817421317100525\n",
      "Accuracy: 0.3754\n",
      "epoch: 860, training loss: 1.1553740501403809, testing loss: 1.758880376815796\n",
      "Accuracy: 0.4228\n",
      "epoch: 860, training loss: 1.3199487924575806, testing loss: 1.780465006828308\n",
      "Accuracy: 0.3994\n",
      "epoch: 860, training loss: 1.206838846206665, testing loss: 1.6567007303237915\n",
      "Accuracy: 0.4300\n",
      "epoch: 860, training loss: 1.2337634563446045, testing loss: 1.6913998126983643\n",
      "Accuracy: 0.4307\n",
      "epoch: 860, training loss: 1.1980156898498535, testing loss: 1.6891993284225464\n",
      "Accuracy: 0.4013\n",
      "epoch: 860, training loss: 1.258997917175293, testing loss: 1.8472164869308472\n",
      "Accuracy: 0.3558\n",
      "epoch: 860, training loss: 1.226159930229187, testing loss: 1.6939668655395508\n",
      "Accuracy: 0.3962\n",
      "epoch: 870, training loss: 1.2632819414138794, testing loss: 1.7329286336898804\n",
      "Accuracy: 0.4345\n",
      "epoch: 870, training loss: 1.3091797828674316, testing loss: 1.6828058958053589\n",
      "Accuracy: 0.3689\n",
      "epoch: 870, training loss: 1.224118947982788, testing loss: 1.736374855041504\n",
      "Accuracy: 0.3886\n",
      "epoch: 870, training loss: 1.2312067747116089, testing loss: 1.787828803062439\n",
      "Accuracy: 0.4172\n",
      "epoch: 870, training loss: 1.2851015329360962, testing loss: 1.6460844278335571\n",
      "Accuracy: 0.4295\n",
      "epoch: 870, training loss: 1.2106245756149292, testing loss: 1.7146345376968384\n",
      "Accuracy: 0.4216\n",
      "epoch: 870, training loss: 1.217231273651123, testing loss: 1.8584988117218018\n",
      "Accuracy: 0.3851\n",
      "epoch: 870, training loss: 1.2026504278182983, testing loss: 1.7891863584518433\n",
      "Accuracy: 0.3946\n",
      "epoch: 870, training loss: 1.2353878021240234, testing loss: 1.669177532196045\n",
      "Accuracy: 0.4326\n",
      "epoch: 870, training loss: 1.3009774684906006, testing loss: 1.694237470626831\n",
      "Accuracy: 0.4406\n",
      "epoch: 870, training loss: 1.252041220664978, testing loss: 1.7892591953277588\n",
      "Accuracy: 0.3426\n",
      "epoch: 870, training loss: 1.3453961610794067, testing loss: 1.7334163188934326\n",
      "Accuracy: 0.4043\n",
      "epoch: 870, training loss: 1.2940670251846313, testing loss: 1.772574543952942\n",
      "Accuracy: 0.3934\n",
      "epoch: 870, training loss: 1.1962707042694092, testing loss: 1.6228841543197632\n",
      "Accuracy: 0.4186\n",
      "epoch: 870, training loss: 1.254474401473999, testing loss: 1.8505845069885254\n",
      "Accuracy: 0.4013\n",
      "epoch: 870, training loss: 1.197555661201477, testing loss: 1.64119553565979\n",
      "Accuracy: 0.3885\n",
      "epoch: 870, training loss: 1.2268669605255127, testing loss: 1.719375729560852\n",
      "Accuracy: 0.4286\n",
      "epoch: 870, training loss: 1.2789523601531982, testing loss: 1.683651328086853\n",
      "Accuracy: 0.4645\n",
      "epoch: 870, training loss: 1.1187427043914795, testing loss: 1.7761257886886597\n",
      "Accuracy: 0.4299\n",
      "epoch: 870, training loss: 1.2916734218597412, testing loss: 1.6931341886520386\n",
      "Accuracy: 0.3806\n",
      "epoch: 870, training loss: 1.2970261573791504, testing loss: 1.8553273677825928\n",
      "Accuracy: 0.4086\n",
      "epoch: 870, training loss: 1.1913230419158936, testing loss: 1.686564564704895\n",
      "Accuracy: 0.4308\n",
      "epoch: 870, training loss: 1.2829946279525757, testing loss: 1.6840862035751343\n",
      "Accuracy: 0.4050\n",
      "epoch: 870, training loss: 1.2593415975570679, testing loss: 1.8094595670700073\n",
      "Accuracy: 0.4103\n",
      "epoch: 870, training loss: 1.2385497093200684, testing loss: 1.6011033058166504\n",
      "Accuracy: 0.4267\n",
      "epoch: 870, training loss: 1.2925082445144653, testing loss: 1.588640570640564\n",
      "Accuracy: 0.4693\n",
      "epoch: 870, training loss: 1.2530999183654785, testing loss: 1.6154696941375732\n",
      "Accuracy: 0.4362\n",
      "epoch: 870, training loss: 1.2366366386413574, testing loss: 1.706100344657898\n",
      "Accuracy: 0.4669\n",
      "epoch: 870, training loss: 1.2289711236953735, testing loss: 1.66171395778656\n",
      "Accuracy: 0.4116\n",
      "epoch: 870, training loss: 1.22416090965271, testing loss: 1.6466889381408691\n",
      "Accuracy: 0.4058\n",
      "epoch: 870, training loss: 1.1644319295883179, testing loss: 1.7723087072372437\n",
      "Accuracy: 0.4007\n",
      "epoch: 870, training loss: 1.1720730066299438, testing loss: 1.7447220087051392\n",
      "Accuracy: 0.3621\n",
      "epoch: 870, training loss: 1.2029732465744019, testing loss: 1.7144931554794312\n",
      "Accuracy: 0.4362\n",
      "epoch: 870, training loss: 1.2880518436431885, testing loss: 1.739920735359192\n",
      "Accuracy: 0.3620\n",
      "epoch: 870, training loss: 1.2117177248001099, testing loss: 1.7059086561203003\n",
      "Accuracy: 0.4509\n",
      "epoch: 870, training loss: 1.2124143838882446, testing loss: 1.7617249488830566\n",
      "Accuracy: 0.4770\n",
      "epoch: 870, training loss: 1.1687222719192505, testing loss: 1.744890809059143\n",
      "Accuracy: 0.4158\n",
      "epoch: 870, training loss: 1.2627931833267212, testing loss: 1.7671048641204834\n",
      "Accuracy: 0.4074\n",
      "epoch: 870, training loss: 1.1455672979354858, testing loss: 1.6898930072784424\n",
      "Accuracy: 0.4337\n",
      "epoch: 870, training loss: 1.296909213066101, testing loss: 1.6829767227172852\n",
      "Accuracy: 0.4047\n",
      "epoch: 870, training loss: 1.2198786735534668, testing loss: 1.6404623985290527\n",
      "Accuracy: 0.4411\n",
      "epoch: 870, training loss: 1.2333176136016846, testing loss: 1.661124348640442\n",
      "Accuracy: 0.4148\n",
      "epoch: 870, training loss: 1.2402950525283813, testing loss: 1.6460729837417603\n",
      "Accuracy: 0.4164\n",
      "epoch: 870, training loss: 1.2286086082458496, testing loss: 1.7476800680160522\n",
      "Accuracy: 0.4082\n",
      "epoch: 870, training loss: 1.2463423013687134, testing loss: 1.7032325267791748\n",
      "Accuracy: 0.4063\n",
      "epoch: 870, training loss: 1.1884558200836182, testing loss: 1.7599201202392578\n",
      "Accuracy: 0.4507\n",
      "epoch: 870, training loss: 1.2519313097000122, testing loss: 1.7563222646713257\n",
      "Accuracy: 0.4525\n",
      "epoch: 870, training loss: 1.2683169841766357, testing loss: 1.6820861101150513\n",
      "Accuracy: 0.4490\n",
      "epoch: 870, training loss: 1.2767528295516968, testing loss: 1.6618874073028564\n",
      "Accuracy: 0.4523\n",
      "epoch: 870, training loss: 1.1641571521759033, testing loss: 1.729565143585205\n",
      "Accuracy: 0.4241\n",
      "epoch: 870, training loss: 1.2436985969543457, testing loss: 1.8069158792495728\n",
      "Accuracy: 0.4366\n",
      "epoch: 870, training loss: 1.153176188468933, testing loss: 1.7152704000473022\n",
      "Accuracy: 0.4100\n",
      "epoch: 870, training loss: 1.241564154624939, testing loss: 1.7045414447784424\n",
      "Accuracy: 0.4253\n",
      "epoch: 870, training loss: 1.2452716827392578, testing loss: 1.8162651062011719\n",
      "Accuracy: 0.3900\n",
      "epoch: 870, training loss: 1.2659465074539185, testing loss: 1.7394237518310547\n",
      "Accuracy: 0.4268\n",
      "epoch: 870, training loss: 1.2221688032150269, testing loss: 1.556635856628418\n",
      "Accuracy: 0.4651\n",
      "epoch: 870, training loss: 1.1761265993118286, testing loss: 1.7488915920257568\n",
      "Accuracy: 0.3864\n",
      "epoch: 870, training loss: 1.2500213384628296, testing loss: 1.9135929346084595\n",
      "Accuracy: 0.3739\n",
      "epoch: 870, training loss: 1.244643211364746, testing loss: 1.7077275514602661\n",
      "Accuracy: 0.4169\n",
      "epoch: 870, training loss: 1.1992098093032837, testing loss: 1.7529017925262451\n",
      "Accuracy: 0.4020\n",
      "epoch: 870, training loss: 1.1553704738616943, testing loss: 1.76828932762146\n",
      "Accuracy: 0.4029\n",
      "epoch: 870, training loss: 1.2563841342926025, testing loss: 1.7154712677001953\n",
      "Accuracy: 0.4484\n",
      "epoch: 870, training loss: 1.195920705795288, testing loss: 1.6262253522872925\n",
      "Accuracy: 0.3856\n",
      "epoch: 870, training loss: 1.2062833309173584, testing loss: 1.667104959487915\n",
      "Accuracy: 0.4130\n",
      "epoch: 870, training loss: 1.233909010887146, testing loss: 1.768054485321045\n",
      "Accuracy: 0.4054\n",
      "epoch: 870, training loss: 1.2551053762435913, testing loss: 1.6755526065826416\n",
      "Accuracy: 0.4299\n",
      "epoch: 870, training loss: 1.2369085550308228, testing loss: 1.8180865049362183\n",
      "Accuracy: 0.4076\n",
      "epoch: 870, training loss: 1.2931448221206665, testing loss: 1.7327677011489868\n",
      "Accuracy: 0.4281\n",
      "epoch: 870, training loss: 1.2300890684127808, testing loss: 1.6514335870742798\n",
      "Accuracy: 0.4277\n",
      "epoch: 870, training loss: 1.293052077293396, testing loss: 1.8377834558486938\n",
      "Accuracy: 0.4566\n",
      "epoch: 870, training loss: 1.270142674446106, testing loss: 1.76760733127594\n",
      "Accuracy: 0.4299\n",
      "epoch: 870, training loss: 1.2186464071273804, testing loss: 1.7042322158813477\n",
      "Accuracy: 0.3838\n",
      "epoch: 870, training loss: 1.2261356115341187, testing loss: 1.766198754310608\n",
      "Accuracy: 0.4242\n",
      "epoch: 870, training loss: 1.2571771144866943, testing loss: 1.6572641134262085\n",
      "Accuracy: 0.4455\n",
      "epoch: 870, training loss: 1.2244428396224976, testing loss: 1.724631667137146\n",
      "Accuracy: 0.4000\n",
      "epoch: 870, training loss: 1.2414488792419434, testing loss: 1.7350825071334839\n",
      "Accuracy: 0.3885\n",
      "epoch: 870, training loss: 1.2394226789474487, testing loss: 1.848159909248352\n",
      "Accuracy: 0.3787\n",
      "epoch: 870, training loss: 1.2395519018173218, testing loss: 1.7785263061523438\n",
      "Accuracy: 0.4069\n",
      "epoch: 870, training loss: 1.3041530847549438, testing loss: 1.727699637413025\n",
      "Accuracy: 0.4024\n",
      "epoch: 870, training loss: 1.2051172256469727, testing loss: 1.6448616981506348\n",
      "Accuracy: 0.4214\n",
      "epoch: 870, training loss: 1.2438894510269165, testing loss: 1.6996139287948608\n",
      "Accuracy: 0.4509\n",
      "epoch: 870, training loss: 1.2830694913864136, testing loss: 1.780526041984558\n",
      "Accuracy: 0.3650\n",
      "epoch: 870, training loss: 1.217341661453247, testing loss: 1.7794890403747559\n",
      "Accuracy: 0.3587\n",
      "epoch: 870, training loss: 1.2115256786346436, testing loss: 1.6444426774978638\n",
      "Accuracy: 0.4328\n",
      "epoch: 870, training loss: 1.2938987016677856, testing loss: 1.670784592628479\n",
      "Accuracy: 0.4330\n",
      "epoch: 870, training loss: 1.2485065460205078, testing loss: 1.744694709777832\n",
      "Accuracy: 0.4198\n",
      "epoch: 870, training loss: 1.2598460912704468, testing loss: 1.7000645399093628\n",
      "Accuracy: 0.4410\n",
      "epoch: 870, training loss: 1.2607645988464355, testing loss: 1.8898742198944092\n",
      "Accuracy: 0.4091\n",
      "epoch: 870, training loss: 1.3058046102523804, testing loss: 1.6026374101638794\n",
      "Accuracy: 0.4263\n",
      "epoch: 870, training loss: 1.306761622428894, testing loss: 1.8421531915664673\n",
      "Accuracy: 0.4141\n",
      "epoch: 870, training loss: 1.1994438171386719, testing loss: 1.7147871255874634\n",
      "Accuracy: 0.4576\n",
      "epoch: 870, training loss: 1.269014596939087, testing loss: 1.7085367441177368\n",
      "Accuracy: 0.4521\n",
      "epoch: 870, training loss: 1.2110755443572998, testing loss: 1.815781593322754\n",
      "Accuracy: 0.3612\n",
      "epoch: 870, training loss: 1.169975757598877, testing loss: 1.7574743032455444\n",
      "Accuracy: 0.4286\n",
      "epoch: 870, training loss: 1.1719499826431274, testing loss: 1.7313740253448486\n",
      "Accuracy: 0.4221\n",
      "epoch: 870, training loss: 1.2426515817642212, testing loss: 1.7429414987564087\n",
      "Accuracy: 0.4277\n",
      "epoch: 870, training loss: 1.2650431394577026, testing loss: 1.659101128578186\n",
      "Accuracy: 0.4179\n",
      "epoch: 870, training loss: 1.3296829462051392, testing loss: 1.7428958415985107\n",
      "Accuracy: 0.4127\n",
      "epoch: 870, training loss: 1.2916679382324219, testing loss: 1.7294939756393433\n",
      "Accuracy: 0.4419\n",
      "epoch: 870, training loss: 1.268027901649475, testing loss: 1.7413426637649536\n",
      "Accuracy: 0.4281\n",
      "epoch: 880, training loss: 1.2332392930984497, testing loss: 1.9160590171813965\n",
      "Accuracy: 0.3946\n",
      "epoch: 880, training loss: 1.209093689918518, testing loss: 1.6555219888687134\n",
      "Accuracy: 0.4138\n",
      "epoch: 880, training loss: 1.210937738418579, testing loss: 1.744866967201233\n",
      "Accuracy: 0.3994\n",
      "epoch: 880, training loss: 1.2314622402191162, testing loss: 1.6042016744613647\n",
      "Accuracy: 0.3943\n",
      "epoch: 880, training loss: 1.2499727010726929, testing loss: 1.67246413230896\n",
      "Accuracy: 0.4427\n",
      "epoch: 880, training loss: 1.261610984802246, testing loss: 1.7624926567077637\n",
      "Accuracy: 0.4147\n",
      "epoch: 880, training loss: 1.2207907438278198, testing loss: 1.6554183959960938\n",
      "Accuracy: 0.4228\n",
      "epoch: 880, training loss: 1.281027913093567, testing loss: 1.7878881692886353\n",
      "Accuracy: 0.4272\n",
      "epoch: 880, training loss: 1.2260888814926147, testing loss: 1.654055118560791\n",
      "Accuracy: 0.4328\n",
      "epoch: 880, training loss: 1.233747124671936, testing loss: 1.7693469524383545\n",
      "Accuracy: 0.4300\n",
      "epoch: 880, training loss: 1.2433494329452515, testing loss: 1.7499912977218628\n",
      "Accuracy: 0.4351\n",
      "epoch: 880, training loss: 1.2554949522018433, testing loss: 1.8016648292541504\n",
      "Accuracy: 0.4045\n",
      "epoch: 880, training loss: 1.1702154874801636, testing loss: 1.7664716243743896\n",
      "Accuracy: 0.3908\n",
      "epoch: 880, training loss: 1.2621521949768066, testing loss: 1.7686001062393188\n",
      "Accuracy: 0.4205\n",
      "epoch: 880, training loss: 1.2379679679870605, testing loss: 1.6686069965362549\n",
      "Accuracy: 0.4514\n",
      "epoch: 880, training loss: 1.1958916187286377, testing loss: 1.6543073654174805\n",
      "Accuracy: 0.4381\n",
      "epoch: 880, training loss: 1.207858681678772, testing loss: 1.6375737190246582\n",
      "Accuracy: 0.3919\n",
      "epoch: 880, training loss: 1.1738111972808838, testing loss: 1.7434548139572144\n",
      "Accuracy: 0.4091\n",
      "epoch: 880, training loss: 1.2375952005386353, testing loss: 1.7969459295272827\n",
      "Accuracy: 0.4338\n",
      "epoch: 880, training loss: 1.2409502267837524, testing loss: 1.626592993736267\n",
      "Accuracy: 0.4006\n",
      "epoch: 880, training loss: 1.2785065174102783, testing loss: 1.794317364692688\n",
      "Accuracy: 0.4177\n",
      "epoch: 880, training loss: 1.242623209953308, testing loss: 1.7339540719985962\n",
      "Accuracy: 0.4364\n",
      "epoch: 880, training loss: 1.2353755235671997, testing loss: 1.7718578577041626\n",
      "Accuracy: 0.4152\n",
      "epoch: 880, training loss: 1.180484414100647, testing loss: 1.6707197427749634\n",
      "Accuracy: 0.4169\n",
      "epoch: 880, training loss: 1.210402011871338, testing loss: 1.7152916193008423\n",
      "Accuracy: 0.4407\n",
      "epoch: 880, training loss: 1.280383825302124, testing loss: 1.7882882356643677\n",
      "Accuracy: 0.3891\n",
      "epoch: 880, training loss: 1.2414828538894653, testing loss: 1.6181560754776\n",
      "Accuracy: 0.4489\n",
      "epoch: 880, training loss: 1.169463872909546, testing loss: 1.749250054359436\n",
      "Accuracy: 0.4281\n",
      "epoch: 880, training loss: 1.1952539682388306, testing loss: 1.7160507440567017\n",
      "Accuracy: 0.3563\n",
      "epoch: 880, training loss: 1.2115631103515625, testing loss: 1.7887418270111084\n",
      "Accuracy: 0.4100\n",
      "epoch: 880, training loss: 1.1811680793762207, testing loss: 1.8426182270050049\n",
      "Accuracy: 0.3907\n",
      "epoch: 880, training loss: 1.2507367134094238, testing loss: 1.7118659019470215\n",
      "Accuracy: 0.3960\n",
      "epoch: 880, training loss: 1.2299586534500122, testing loss: 1.6455259323120117\n",
      "Accuracy: 0.4401\n",
      "epoch: 880, training loss: 1.223598599433899, testing loss: 1.6217920780181885\n",
      "Accuracy: 0.4254\n",
      "epoch: 880, training loss: 1.2926673889160156, testing loss: 1.8797317743301392\n",
      "Accuracy: 0.4248\n",
      "epoch: 880, training loss: 1.2491180896759033, testing loss: 1.6242409944534302\n",
      "Accuracy: 0.4377\n",
      "epoch: 880, training loss: 1.237016201019287, testing loss: 1.7318260669708252\n",
      "Accuracy: 0.4195\n",
      "epoch: 880, training loss: 1.1851035356521606, testing loss: 1.7113627195358276\n",
      "Accuracy: 0.4118\n",
      "epoch: 880, training loss: 1.2143772840499878, testing loss: 1.7375730276107788\n",
      "Accuracy: 0.3574\n",
      "epoch: 880, training loss: 1.2067025899887085, testing loss: 1.849305272102356\n",
      "Accuracy: 0.4000\n",
      "epoch: 880, training loss: 1.1530166864395142, testing loss: 1.8319579362869263\n",
      "Accuracy: 0.3817\n",
      "epoch: 880, training loss: 1.2386634349822998, testing loss: 1.791077733039856\n",
      "Accuracy: 0.4035\n",
      "epoch: 880, training loss: 1.1993836164474487, testing loss: 1.7405405044555664\n",
      "Accuracy: 0.3994\n",
      "epoch: 880, training loss: 1.1849116086959839, testing loss: 1.7265981435775757\n",
      "Accuracy: 0.4286\n",
      "epoch: 880, training loss: 1.2739988565444946, testing loss: 1.620654582977295\n",
      "Accuracy: 0.4276\n",
      "epoch: 880, training loss: 1.250985860824585, testing loss: 1.6440964937210083\n",
      "Accuracy: 0.4388\n",
      "epoch: 880, training loss: 1.31944739818573, testing loss: 1.7471516132354736\n",
      "Accuracy: 0.4123\n",
      "epoch: 880, training loss: 1.2600574493408203, testing loss: 1.7118247747421265\n",
      "Accuracy: 0.4317\n",
      "epoch: 880, training loss: 1.2173117399215698, testing loss: 1.6956501007080078\n",
      "Accuracy: 0.4019\n",
      "epoch: 880, training loss: 1.2892177104949951, testing loss: 1.6057534217834473\n",
      "Accuracy: 0.3956\n",
      "epoch: 880, training loss: 1.2430726289749146, testing loss: 1.7733521461486816\n",
      "Accuracy: 0.3962\n",
      "epoch: 880, training loss: 1.2603938579559326, testing loss: 1.7316066026687622\n",
      "Accuracy: 0.4019\n",
      "epoch: 880, training loss: 1.2989428043365479, testing loss: 1.7303264141082764\n",
      "Accuracy: 0.3963\n",
      "epoch: 880, training loss: 1.219900369644165, testing loss: 1.789079189300537\n",
      "Accuracy: 0.3636\n",
      "epoch: 880, training loss: 1.1608295440673828, testing loss: 1.6724635362625122\n",
      "Accuracy: 0.4143\n",
      "epoch: 880, training loss: 1.2507786750793457, testing loss: 1.706531047821045\n",
      "Accuracy: 0.4169\n",
      "epoch: 880, training loss: 1.2508022785186768, testing loss: 1.7057137489318848\n",
      "Accuracy: 0.4232\n",
      "epoch: 880, training loss: 1.2310506105422974, testing loss: 1.7901242971420288\n",
      "Accuracy: 0.3874\n",
      "epoch: 880, training loss: 1.2977665662765503, testing loss: 1.7550373077392578\n",
      "Accuracy: 0.4272\n",
      "epoch: 880, training loss: 1.3151640892028809, testing loss: 1.7597287893295288\n",
      "Accuracy: 0.4422\n",
      "epoch: 880, training loss: 1.2231513261795044, testing loss: 1.8300081491470337\n",
      "Accuracy: 0.3715\n",
      "epoch: 880, training loss: 1.1928232908248901, testing loss: 1.7556960582733154\n",
      "Accuracy: 0.3773\n",
      "epoch: 880, training loss: 1.2376753091812134, testing loss: 1.6890796422958374\n",
      "Accuracy: 0.4201\n",
      "epoch: 880, training loss: 1.208389401435852, testing loss: 1.7944682836532593\n",
      "Accuracy: 0.4057\n",
      "epoch: 880, training loss: 1.1962120532989502, testing loss: 1.5749497413635254\n",
      "Accuracy: 0.4359\n",
      "epoch: 880, training loss: 1.2361279726028442, testing loss: 1.712424635887146\n",
      "Accuracy: 0.3876\n",
      "epoch: 880, training loss: 1.2967184782028198, testing loss: 1.77906334400177\n",
      "Accuracy: 0.3553\n",
      "epoch: 880, training loss: 1.192101240158081, testing loss: 1.8122062683105469\n",
      "Accuracy: 0.4094\n",
      "epoch: 880, training loss: 1.2164183855056763, testing loss: 1.659600853919983\n",
      "Accuracy: 0.4193\n",
      "epoch: 880, training loss: 1.2054582834243774, testing loss: 1.6804279088974\n",
      "Accuracy: 0.4655\n",
      "epoch: 880, training loss: 1.2282686233520508, testing loss: 1.7636518478393555\n",
      "Accuracy: 0.4187\n",
      "epoch: 880, training loss: 1.1875553131103516, testing loss: 1.6736645698547363\n",
      "Accuracy: 0.4150\n",
      "epoch: 880, training loss: 1.2631570100784302, testing loss: 1.6298527717590332\n",
      "Accuracy: 0.4706\n",
      "epoch: 880, training loss: 1.2431037425994873, testing loss: 1.6445775032043457\n",
      "Accuracy: 0.4771\n",
      "epoch: 880, training loss: 1.2131133079528809, testing loss: 1.791648507118225\n",
      "Accuracy: 0.3859\n",
      "epoch: 880, training loss: 1.2393747568130493, testing loss: 1.8477286100387573\n",
      "Accuracy: 0.4503\n",
      "epoch: 880, training loss: 1.2481521368026733, testing loss: 1.7649500370025635\n",
      "Accuracy: 0.4125\n",
      "epoch: 880, training loss: 1.2220072746276855, testing loss: 1.7608741521835327\n",
      "Accuracy: 0.4232\n",
      "epoch: 880, training loss: 1.304650068283081, testing loss: 1.7720985412597656\n",
      "Accuracy: 0.3924\n",
      "epoch: 880, training loss: 1.2546476125717163, testing loss: 1.6869754791259766\n",
      "Accuracy: 0.4191\n",
      "epoch: 880, training loss: 1.2274541854858398, testing loss: 1.7165210247039795\n",
      "Accuracy: 0.3836\n",
      "epoch: 880, training loss: 1.2475907802581787, testing loss: 1.678114414215088\n",
      "Accuracy: 0.4037\n",
      "epoch: 880, training loss: 1.1905186176300049, testing loss: 1.69549560546875\n",
      "Accuracy: 0.4254\n",
      "epoch: 880, training loss: 1.247694730758667, testing loss: 1.6775983572006226\n",
      "Accuracy: 0.4737\n",
      "epoch: 880, training loss: 1.1918151378631592, testing loss: 1.6832762956619263\n",
      "Accuracy: 0.4022\n",
      "epoch: 880, training loss: 1.2447566986083984, testing loss: 1.8093823194503784\n",
      "Accuracy: 0.3558\n",
      "epoch: 880, training loss: 1.2599680423736572, testing loss: 1.7124019861221313\n",
      "Accuracy: 0.3954\n",
      "epoch: 880, training loss: 1.2378137111663818, testing loss: 1.6613709926605225\n",
      "Accuracy: 0.4290\n",
      "epoch: 880, training loss: 1.241679072380066, testing loss: 1.6890417337417603\n",
      "Accuracy: 0.4309\n",
      "epoch: 880, training loss: 1.2379745244979858, testing loss: 1.663171410560608\n",
      "Accuracy: 0.4444\n",
      "epoch: 880, training loss: 1.2495558261871338, testing loss: 1.844184160232544\n",
      "Accuracy: 0.3624\n",
      "epoch: 880, training loss: 1.25411856174469, testing loss: 1.9415500164031982\n",
      "Accuracy: 0.3905\n",
      "epoch: 880, training loss: 1.2412298917770386, testing loss: 1.7355483770370483\n",
      "Accuracy: 0.4013\n",
      "epoch: 880, training loss: 1.1819276809692383, testing loss: 1.7384042739868164\n",
      "Accuracy: 0.4107\n",
      "epoch: 880, training loss: 1.2108675241470337, testing loss: 1.6774277687072754\n",
      "Accuracy: 0.4460\n",
      "epoch: 880, training loss: 1.3812345266342163, testing loss: 1.6634691953659058\n",
      "Accuracy: 0.4349\n",
      "epoch: 880, training loss: 1.2265616655349731, testing loss: 1.693766713142395\n",
      "Accuracy: 0.3891\n",
      "epoch: 880, training loss: 1.3394430875778198, testing loss: 1.7868781089782715\n",
      "Accuracy: 0.4047\n",
      "epoch: 880, training loss: 1.2891517877578735, testing loss: 1.6431976556777954\n",
      "Accuracy: 0.4252\n",
      "epoch: 880, training loss: 1.1436409950256348, testing loss: 1.7972596883773804\n",
      "Accuracy: 0.3918\n",
      "epoch: 890, training loss: 1.2185304164886475, testing loss: 1.6979104280471802\n",
      "Accuracy: 0.4209\n",
      "epoch: 890, training loss: 1.214227557182312, testing loss: 1.7400482892990112\n",
      "Accuracy: 0.4084\n",
      "epoch: 890, training loss: 1.2597973346710205, testing loss: 1.8966526985168457\n",
      "Accuracy: 0.4067\n",
      "epoch: 890, training loss: 1.2264775037765503, testing loss: 1.6621301174163818\n",
      "Accuracy: 0.3982\n",
      "epoch: 890, training loss: 1.2253063917160034, testing loss: 1.6270227432250977\n",
      "Accuracy: 0.4401\n",
      "epoch: 890, training loss: 1.2295973300933838, testing loss: 1.7581989765167236\n",
      "Accuracy: 0.4172\n",
      "epoch: 890, training loss: 1.2443739175796509, testing loss: 1.9389687776565552\n",
      "Accuracy: 0.3599\n",
      "epoch: 890, training loss: 1.173768401145935, testing loss: 1.6726828813552856\n",
      "Accuracy: 0.3818\n",
      "epoch: 890, training loss: 1.2084558010101318, testing loss: 1.55238938331604\n",
      "Accuracy: 0.4482\n",
      "epoch: 890, training loss: 1.2580574750900269, testing loss: 1.6962944269180298\n",
      "Accuracy: 0.4360\n",
      "epoch: 890, training loss: 1.2366317510604858, testing loss: 1.796297311782837\n",
      "Accuracy: 0.4261\n",
      "epoch: 890, training loss: 1.1834120750427246, testing loss: 1.6648831367492676\n",
      "Accuracy: 0.4404\n",
      "epoch: 890, training loss: 1.1529510021209717, testing loss: 1.6401302814483643\n",
      "Accuracy: 0.4172\n",
      "epoch: 890, training loss: 1.2804973125457764, testing loss: 1.7522088289260864\n",
      "Accuracy: 0.4528\n",
      "epoch: 890, training loss: 1.2735756635665894, testing loss: 1.7450697422027588\n",
      "Accuracy: 0.3711\n",
      "epoch: 890, training loss: 1.2207136154174805, testing loss: 1.7637932300567627\n",
      "Accuracy: 0.4272\n",
      "epoch: 890, training loss: 1.2206155061721802, testing loss: 1.9558511972427368\n",
      "Accuracy: 0.3934\n",
      "epoch: 890, training loss: 1.2368783950805664, testing loss: 1.6920686960220337\n",
      "Accuracy: 0.4461\n",
      "epoch: 890, training loss: 1.1737804412841797, testing loss: 1.6851931810379028\n",
      "Accuracy: 0.4291\n",
      "epoch: 890, training loss: 1.255961537361145, testing loss: 1.7788887023925781\n",
      "Accuracy: 0.4196\n",
      "epoch: 890, training loss: 1.3001766204833984, testing loss: 1.724956750869751\n",
      "Accuracy: 0.4073\n",
      "epoch: 890, training loss: 1.2968103885650635, testing loss: 1.8668252229690552\n",
      "Accuracy: 0.3682\n",
      "epoch: 890, training loss: 1.2738851308822632, testing loss: 1.6172573566436768\n",
      "Accuracy: 0.4164\n",
      "epoch: 890, training loss: 1.275330901145935, testing loss: 1.7863218784332275\n",
      "Accuracy: 0.3880\n",
      "epoch: 890, training loss: 1.2426013946533203, testing loss: 1.6555979251861572\n",
      "Accuracy: 0.4281\n",
      "epoch: 890, training loss: 1.2601721286773682, testing loss: 1.798211693763733\n",
      "Accuracy: 0.3841\n",
      "epoch: 890, training loss: 1.224615454673767, testing loss: 1.632606863975525\n",
      "Accuracy: 0.4046\n",
      "epoch: 890, training loss: 1.2664581537246704, testing loss: 1.6587539911270142\n",
      "Accuracy: 0.4007\n",
      "epoch: 890, training loss: 1.2723037004470825, testing loss: 1.7550582885742188\n",
      "Accuracy: 0.3673\n",
      "epoch: 890, training loss: 1.2719184160232544, testing loss: 1.792055606842041\n",
      "Accuracy: 0.3633\n",
      "epoch: 890, training loss: 1.3313558101654053, testing loss: 1.6579185724258423\n",
      "Accuracy: 0.4571\n",
      "epoch: 890, training loss: 1.2481887340545654, testing loss: 1.645690679550171\n",
      "Accuracy: 0.4500\n",
      "epoch: 890, training loss: 1.1991902589797974, testing loss: 1.6220428943634033\n",
      "Accuracy: 0.4727\n",
      "epoch: 890, training loss: 1.2438608407974243, testing loss: 1.9004420042037964\n",
      "Accuracy: 0.3722\n",
      "epoch: 890, training loss: 1.3378642797470093, testing loss: 1.8329347372055054\n",
      "Accuracy: 0.4448\n",
      "epoch: 890, training loss: 1.2876498699188232, testing loss: 1.7348904609680176\n",
      "Accuracy: 0.3993\n",
      "epoch: 890, training loss: 1.254050374031067, testing loss: 1.7492564916610718\n",
      "Accuracy: 0.4139\n",
      "epoch: 890, training loss: 1.3206737041473389, testing loss: 1.8106417655944824\n",
      "Accuracy: 0.4098\n",
      "epoch: 890, training loss: 1.2325936555862427, testing loss: 1.7369943857192993\n",
      "Accuracy: 0.4309\n",
      "epoch: 890, training loss: 1.1804317235946655, testing loss: 1.748046636581421\n",
      "Accuracy: 0.4227\n",
      "epoch: 890, training loss: 1.2154382467269897, testing loss: 1.734630823135376\n",
      "Accuracy: 0.4114\n",
      "epoch: 890, training loss: 1.2396894693374634, testing loss: 1.7095187902450562\n",
      "Accuracy: 0.4256\n",
      "epoch: 890, training loss: 1.2768819332122803, testing loss: 1.6864955425262451\n",
      "Accuracy: 0.4408\n",
      "epoch: 890, training loss: 1.2361658811569214, testing loss: 1.7539069652557373\n",
      "Accuracy: 0.4338\n",
      "epoch: 890, training loss: 1.1514791250228882, testing loss: 1.6193411350250244\n",
      "Accuracy: 0.4044\n",
      "epoch: 890, training loss: 1.2250844240188599, testing loss: 1.7556706666946411\n",
      "Accuracy: 0.3929\n",
      "epoch: 890, training loss: 1.2933391332626343, testing loss: 1.8663321733474731\n",
      "Accuracy: 0.3846\n",
      "epoch: 890, training loss: 1.2599247694015503, testing loss: 1.7737730741500854\n",
      "Accuracy: 0.3979\n",
      "epoch: 890, training loss: 1.208147644996643, testing loss: 1.7325414419174194\n",
      "Accuracy: 0.4071\n",
      "epoch: 890, training loss: 1.2654601335525513, testing loss: 1.7294262647628784\n",
      "Accuracy: 0.3874\n",
      "epoch: 890, training loss: 1.1950536966323853, testing loss: 1.7978911399841309\n",
      "Accuracy: 0.4044\n",
      "epoch: 890, training loss: 1.2347642183303833, testing loss: 1.7877380847930908\n",
      "Accuracy: 0.4360\n",
      "epoch: 890, training loss: 1.2165141105651855, testing loss: 1.8443485498428345\n",
      "Accuracy: 0.3656\n",
      "epoch: 890, training loss: 1.1901023387908936, testing loss: 1.880067229270935\n",
      "Accuracy: 0.3639\n",
      "epoch: 890, training loss: 1.1601566076278687, testing loss: 1.7677512168884277\n",
      "Accuracy: 0.4262\n",
      "epoch: 890, training loss: 1.2242053747177124, testing loss: 1.7821462154388428\n",
      "Accuracy: 0.4364\n",
      "epoch: 890, training loss: 1.2635427713394165, testing loss: 1.750662088394165\n",
      "Accuracy: 0.4319\n",
      "epoch: 890, training loss: 1.2693638801574707, testing loss: 1.572399616241455\n",
      "Accuracy: 0.4665\n",
      "epoch: 890, training loss: 1.208980917930603, testing loss: 1.6953431367874146\n",
      "Accuracy: 0.4260\n",
      "epoch: 890, training loss: 1.1966707706451416, testing loss: 1.6883623600006104\n",
      "Accuracy: 0.4245\n",
      "epoch: 890, training loss: 1.277700424194336, testing loss: 1.6889344453811646\n",
      "Accuracy: 0.4469\n",
      "epoch: 890, training loss: 1.2781107425689697, testing loss: 1.5899147987365723\n",
      "Accuracy: 0.4178\n",
      "epoch: 890, training loss: 1.226834774017334, testing loss: 1.7015728950500488\n",
      "Accuracy: 0.3987\n",
      "epoch: 890, training loss: 1.2448397874832153, testing loss: 1.6760945320129395\n",
      "Accuracy: 0.4650\n",
      "epoch: 890, training loss: 1.2061108350753784, testing loss: 1.7399418354034424\n",
      "Accuracy: 0.4262\n",
      "epoch: 890, training loss: 1.2078442573547363, testing loss: 1.706407070159912\n",
      "Accuracy: 0.4403\n",
      "epoch: 890, training loss: 1.1965075731277466, testing loss: 1.7664867639541626\n",
      "Accuracy: 0.3909\n",
      "epoch: 890, training loss: 1.2795510292053223, testing loss: 1.6991782188415527\n",
      "Accuracy: 0.4152\n",
      "epoch: 890, training loss: 1.2932697534561157, testing loss: 1.6513216495513916\n",
      "Accuracy: 0.4412\n",
      "epoch: 890, training loss: 1.176315426826477, testing loss: 1.6605900526046753\n",
      "Accuracy: 0.4494\n",
      "epoch: 890, training loss: 1.296296238899231, testing loss: 1.7726027965545654\n",
      "Accuracy: 0.4180\n",
      "epoch: 890, training loss: 1.2801047563552856, testing loss: 1.7607375383377075\n",
      "Accuracy: 0.3980\n",
      "epoch: 890, training loss: 1.1839587688446045, testing loss: 1.7442353963851929\n",
      "Accuracy: 0.4590\n",
      "epoch: 890, training loss: 1.2249631881713867, testing loss: 1.8941162824630737\n",
      "Accuracy: 0.3962\n",
      "epoch: 890, training loss: 1.266244649887085, testing loss: 1.6986645460128784\n",
      "Accuracy: 0.3873\n",
      "epoch: 890, training loss: 1.2674715518951416, testing loss: 1.8139501810073853\n",
      "Accuracy: 0.3844\n",
      "epoch: 890, training loss: 1.2935612201690674, testing loss: 1.7468101978302002\n",
      "Accuracy: 0.4412\n",
      "epoch: 890, training loss: 1.2965139150619507, testing loss: 1.6636518239974976\n",
      "Accuracy: 0.4030\n",
      "epoch: 890, training loss: 1.218993067741394, testing loss: 1.7484980821609497\n",
      "Accuracy: 0.4019\n",
      "epoch: 890, training loss: 1.2483115196228027, testing loss: 1.5897244215011597\n",
      "Accuracy: 0.4647\n",
      "epoch: 890, training loss: 1.2425880432128906, testing loss: 1.7088911533355713\n",
      "Accuracy: 0.4206\n",
      "epoch: 890, training loss: 1.187661051750183, testing loss: 1.7395720481872559\n",
      "Accuracy: 0.3916\n",
      "epoch: 890, training loss: 1.1975969076156616, testing loss: 1.8119207620620728\n",
      "Accuracy: 0.4026\n",
      "epoch: 890, training loss: 1.2276769876480103, testing loss: 1.8507285118103027\n",
      "Accuracy: 0.3914\n",
      "epoch: 890, training loss: 1.1790683269500732, testing loss: 1.6935218572616577\n",
      "Accuracy: 0.3994\n",
      "epoch: 890, training loss: 1.2089918851852417, testing loss: 1.8274753093719482\n",
      "Accuracy: 0.4169\n",
      "epoch: 890, training loss: 1.2274097204208374, testing loss: 1.8332945108413696\n",
      "Accuracy: 0.3849\n",
      "epoch: 890, training loss: 1.244162678718567, testing loss: 1.8173633813858032\n",
      "Accuracy: 0.3746\n",
      "epoch: 890, training loss: 1.2062338590621948, testing loss: 1.6472351551055908\n",
      "Accuracy: 0.3903\n",
      "epoch: 890, training loss: 1.1918249130249023, testing loss: 1.7771704196929932\n",
      "Accuracy: 0.3686\n",
      "epoch: 890, training loss: 1.2046512365341187, testing loss: 1.6544233560562134\n",
      "Accuracy: 0.4518\n",
      "epoch: 890, training loss: 1.2230905294418335, testing loss: 1.685151219367981\n",
      "Accuracy: 0.4595\n",
      "epoch: 890, training loss: 1.209341287612915, testing loss: 1.8895782232284546\n",
      "Accuracy: 0.3658\n",
      "epoch: 890, training loss: 1.2303155660629272, testing loss: 1.6066690683364868\n",
      "Accuracy: 0.4486\n",
      "epoch: 890, training loss: 1.2180203199386597, testing loss: 1.7745152711868286\n",
      "Accuracy: 0.4257\n",
      "epoch: 890, training loss: 1.233047604560852, testing loss: 1.6116434335708618\n",
      "Accuracy: 0.4144\n",
      "epoch: 890, training loss: 1.2100104093551636, testing loss: 1.7806788682937622\n",
      "Accuracy: 0.3679\n",
      "epoch: 890, training loss: 1.2969331741333008, testing loss: 1.7805448770523071\n",
      "Accuracy: 0.4074\n",
      "epoch: 890, training loss: 1.2716479301452637, testing loss: 1.620323896408081\n",
      "Accuracy: 0.4227\n",
      "epoch: 890, training loss: 1.1781895160675049, testing loss: 1.7992968559265137\n",
      "Accuracy: 0.4135\n",
      "epoch: 900, training loss: 1.1899268627166748, testing loss: 1.7859402894973755\n",
      "Accuracy: 0.3785\n",
      "epoch: 900, training loss: 1.2198799848556519, testing loss: 1.836734414100647\n",
      "Accuracy: 0.4013\n",
      "epoch: 900, training loss: 1.2437056303024292, testing loss: 1.6808418035507202\n",
      "Accuracy: 0.4623\n",
      "epoch: 900, training loss: 1.2390483617782593, testing loss: 1.7680119276046753\n",
      "Accuracy: 0.4379\n",
      "epoch: 900, training loss: 1.1978713274002075, testing loss: 1.7103898525238037\n",
      "Accuracy: 0.4367\n",
      "epoch: 900, training loss: 1.2322837114334106, testing loss: 1.7584936618804932\n",
      "Accuracy: 0.4850\n",
      "epoch: 900, training loss: 1.2352263927459717, testing loss: 1.6789603233337402\n",
      "Accuracy: 0.4438\n",
      "epoch: 900, training loss: 1.2305667400360107, testing loss: 1.6394104957580566\n",
      "Accuracy: 0.4172\n",
      "epoch: 900, training loss: 1.2451146841049194, testing loss: 1.7368299961090088\n",
      "Accuracy: 0.4373\n",
      "epoch: 900, training loss: 1.2801873683929443, testing loss: 1.6474108695983887\n",
      "Accuracy: 0.4197\n",
      "epoch: 900, training loss: 1.2145084142684937, testing loss: 1.6889744997024536\n",
      "Accuracy: 0.4211\n",
      "epoch: 900, training loss: 1.2199294567108154, testing loss: 1.6168268918991089\n",
      "Accuracy: 0.4651\n",
      "epoch: 900, training loss: 1.301886796951294, testing loss: 1.6510504484176636\n",
      "Accuracy: 0.4329\n",
      "epoch: 900, training loss: 1.2175202369689941, testing loss: 1.595019817352295\n",
      "Accuracy: 0.3932\n",
      "epoch: 900, training loss: 1.2225046157836914, testing loss: 1.8058946132659912\n",
      "Accuracy: 0.3902\n",
      "epoch: 900, training loss: 1.2639919519424438, testing loss: 1.6969711780548096\n",
      "Accuracy: 0.4214\n",
      "epoch: 900, training loss: 1.2272650003433228, testing loss: 1.7451318502426147\n",
      "Accuracy: 0.3969\n",
      "epoch: 900, training loss: 1.183968186378479, testing loss: 1.6558303833007812\n",
      "Accuracy: 0.4373\n",
      "epoch: 900, training loss: 1.3274736404418945, testing loss: 1.793576955795288\n",
      "Accuracy: 0.4176\n",
      "epoch: 900, training loss: 1.2268092632293701, testing loss: 1.7525832653045654\n",
      "Accuracy: 0.4232\n",
      "epoch: 900, training loss: 1.2308927774429321, testing loss: 1.6956723928451538\n",
      "Accuracy: 0.3775\n",
      "epoch: 900, training loss: 1.2161836624145508, testing loss: 1.6651474237442017\n",
      "Accuracy: 0.4500\n",
      "epoch: 900, training loss: 1.2392089366912842, testing loss: 1.6777435541152954\n",
      "Accuracy: 0.3922\n",
      "epoch: 900, training loss: 1.2963521480560303, testing loss: 1.7698767185211182\n",
      "Accuracy: 0.4141\n",
      "epoch: 900, training loss: 1.2611806392669678, testing loss: 1.6817009449005127\n",
      "Accuracy: 0.4562\n",
      "epoch: 900, training loss: 1.1884653568267822, testing loss: 1.6138830184936523\n",
      "Accuracy: 0.4201\n",
      "epoch: 900, training loss: 1.2504781484603882, testing loss: 1.8513177633285522\n",
      "Accuracy: 0.3899\n",
      "epoch: 900, training loss: 1.1678861379623413, testing loss: 1.8007639646530151\n",
      "Accuracy: 0.4049\n",
      "epoch: 900, training loss: 1.1598979234695435, testing loss: 1.748999834060669\n",
      "Accuracy: 0.3877\n",
      "epoch: 900, training loss: 1.1898778676986694, testing loss: 1.8284655809402466\n",
      "Accuracy: 0.3916\n",
      "epoch: 900, training loss: 1.233542561531067, testing loss: 1.712133765220642\n",
      "Accuracy: 0.4030\n",
      "epoch: 900, training loss: 1.2934503555297852, testing loss: 1.6719136238098145\n",
      "Accuracy: 0.3896\n",
      "epoch: 900, training loss: 1.2595051527023315, testing loss: 1.673553228378296\n",
      "Accuracy: 0.4727\n",
      "epoch: 900, training loss: 1.299438238143921, testing loss: 1.5086643695831299\n",
      "Accuracy: 0.4548\n",
      "epoch: 900, training loss: 1.2412055730819702, testing loss: 1.8481208086013794\n",
      "Accuracy: 0.4257\n",
      "epoch: 900, training loss: 1.227529525756836, testing loss: 1.7375519275665283\n",
      "Accuracy: 0.4540\n",
      "epoch: 900, training loss: 1.2834596633911133, testing loss: 1.8703376054763794\n",
      "Accuracy: 0.4080\n",
      "epoch: 900, training loss: 1.2401816844940186, testing loss: 1.5534101724624634\n",
      "Accuracy: 0.4498\n",
      "epoch: 900, training loss: 1.2151764631271362, testing loss: 1.8535422086715698\n",
      "Accuracy: 0.3591\n",
      "epoch: 900, training loss: 1.2510108947753906, testing loss: 1.5784733295440674\n",
      "Accuracy: 0.4709\n",
      "epoch: 900, training loss: 1.2708793878555298, testing loss: 1.6512155532836914\n",
      "Accuracy: 0.4039\n",
      "epoch: 900, training loss: 1.2651323080062866, testing loss: 1.7998981475830078\n",
      "Accuracy: 0.4006\n",
      "epoch: 900, training loss: 1.2267154455184937, testing loss: 1.8391510248184204\n",
      "Accuracy: 0.4048\n",
      "epoch: 900, training loss: 1.2131298780441284, testing loss: 1.8428784608840942\n",
      "Accuracy: 0.3692\n",
      "epoch: 900, training loss: 1.2924968004226685, testing loss: 1.7386090755462646\n",
      "Accuracy: 0.4377\n",
      "epoch: 900, training loss: 1.2467278242111206, testing loss: 1.6856974363327026\n",
      "Accuracy: 0.4185\n",
      "epoch: 900, training loss: 1.2294100522994995, testing loss: 1.5836011171340942\n",
      "Accuracy: 0.4300\n",
      "epoch: 900, training loss: 1.279531717300415, testing loss: 1.7440118789672852\n",
      "Accuracy: 0.4277\n",
      "epoch: 900, training loss: 1.2478173971176147, testing loss: 1.6275995969772339\n",
      "Accuracy: 0.4393\n",
      "epoch: 900, training loss: 1.2387639284133911, testing loss: 1.7010881900787354\n",
      "Accuracy: 0.3542\n",
      "epoch: 900, training loss: 1.234972357749939, testing loss: 1.7938458919525146\n",
      "Accuracy: 0.3642\n",
      "epoch: 900, training loss: 1.2656701803207397, testing loss: 1.7002772092819214\n",
      "Accuracy: 0.4098\n",
      "epoch: 900, training loss: 1.2500965595245361, testing loss: 1.7871087789535522\n",
      "Accuracy: 0.4338\n",
      "epoch: 900, training loss: 1.1463873386383057, testing loss: 1.6976515054702759\n",
      "Accuracy: 0.4217\n",
      "epoch: 900, training loss: 1.2638682126998901, testing loss: 1.7261919975280762\n",
      "Accuracy: 0.4337\n",
      "epoch: 900, training loss: 1.2634061574935913, testing loss: 1.7609379291534424\n",
      "Accuracy: 0.4164\n",
      "epoch: 900, training loss: 1.14729905128479, testing loss: 1.639172077178955\n",
      "Accuracy: 0.4108\n",
      "epoch: 900, training loss: 1.2400768995285034, testing loss: 1.6641813516616821\n",
      "Accuracy: 0.4573\n",
      "epoch: 900, training loss: 1.2337589263916016, testing loss: 1.7376760244369507\n",
      "Accuracy: 0.4231\n",
      "epoch: 900, training loss: 1.2215532064437866, testing loss: 1.7134065628051758\n",
      "Accuracy: 0.3931\n",
      "epoch: 900, training loss: 1.2170389890670776, testing loss: 1.820334792137146\n",
      "Accuracy: 0.3777\n",
      "epoch: 900, training loss: 1.2299338579177856, testing loss: 1.7355672121047974\n",
      "Accuracy: 0.4061\n",
      "epoch: 900, training loss: 1.2154370546340942, testing loss: 1.7215222120285034\n",
      "Accuracy: 0.3975\n",
      "epoch: 900, training loss: 1.1807247400283813, testing loss: 1.661381721496582\n",
      "Accuracy: 0.3967\n",
      "epoch: 900, training loss: 1.248388409614563, testing loss: 1.6814039945602417\n",
      "Accuracy: 0.3994\n",
      "epoch: 900, training loss: 1.2117023468017578, testing loss: 1.634158968925476\n",
      "Accuracy: 0.4889\n",
      "epoch: 900, training loss: 1.3306385278701782, testing loss: 1.7193686962127686\n",
      "Accuracy: 0.4060\n",
      "epoch: 900, training loss: 1.18216073513031, testing loss: 1.768236517906189\n",
      "Accuracy: 0.4159\n",
      "epoch: 900, training loss: 1.1641885042190552, testing loss: 1.819890022277832\n",
      "Accuracy: 0.4667\n",
      "epoch: 900, training loss: 1.2850182056427002, testing loss: 1.6970776319503784\n",
      "Accuracy: 0.4276\n",
      "epoch: 900, training loss: 1.2567402124404907, testing loss: 1.742866039276123\n",
      "Accuracy: 0.3993\n",
      "epoch: 900, training loss: 1.256189227104187, testing loss: 1.611517071723938\n",
      "Accuracy: 0.4427\n",
      "epoch: 900, training loss: 1.2428548336029053, testing loss: 1.738713026046753\n",
      "Accuracy: 0.4049\n",
      "epoch: 900, training loss: 1.2247127294540405, testing loss: 1.9287638664245605\n",
      "Accuracy: 0.3820\n",
      "epoch: 900, training loss: 1.2043733596801758, testing loss: 1.6562767028808594\n",
      "Accuracy: 0.4134\n",
      "epoch: 900, training loss: 1.1751407384872437, testing loss: 1.6759591102600098\n",
      "Accuracy: 0.4294\n",
      "epoch: 900, training loss: 1.222055196762085, testing loss: 1.7575218677520752\n",
      "Accuracy: 0.3732\n",
      "epoch: 900, training loss: 1.1969895362854004, testing loss: 1.7662155628204346\n",
      "Accuracy: 0.4085\n",
      "epoch: 900, training loss: 1.2487152814865112, testing loss: 1.7471073865890503\n",
      "Accuracy: 0.4020\n",
      "epoch: 900, training loss: 1.2418348789215088, testing loss: 1.73160982131958\n",
      "Accuracy: 0.3966\n",
      "epoch: 900, training loss: 1.201263189315796, testing loss: 1.8024909496307373\n",
      "Accuracy: 0.4228\n",
      "epoch: 900, training loss: 1.2817761898040771, testing loss: 1.792718529701233\n",
      "Accuracy: 0.4074\n",
      "epoch: 900, training loss: 1.2440108060836792, testing loss: 1.6247073411941528\n",
      "Accuracy: 0.4728\n",
      "epoch: 900, training loss: 1.2310223579406738, testing loss: 1.8510866165161133\n",
      "Accuracy: 0.3821\n",
      "epoch: 900, training loss: 1.2505830526351929, testing loss: 1.6499277353286743\n",
      "Accuracy: 0.4167\n",
      "epoch: 900, training loss: 1.235945701599121, testing loss: 1.6843446493148804\n",
      "Accuracy: 0.4322\n",
      "epoch: 900, training loss: 1.2520782947540283, testing loss: 1.7777347564697266\n",
      "Accuracy: 0.4518\n",
      "epoch: 900, training loss: 1.2962981462478638, testing loss: 1.6782759428024292\n",
      "Accuracy: 0.4178\n",
      "epoch: 900, training loss: 1.1841315031051636, testing loss: 1.6723226308822632\n",
      "Accuracy: 0.4014\n",
      "epoch: 900, training loss: 1.1916810274124146, testing loss: 1.8087126016616821\n",
      "Accuracy: 0.3831\n",
      "epoch: 900, training loss: 1.1964411735534668, testing loss: 1.5951635837554932\n",
      "Accuracy: 0.4528\n",
      "epoch: 900, training loss: 1.2366504669189453, testing loss: 1.6398701667785645\n",
      "Accuracy: 0.4528\n",
      "epoch: 900, training loss: 1.2485324144363403, testing loss: 1.6139241456985474\n",
      "Accuracy: 0.4452\n",
      "epoch: 900, training loss: 1.2606552839279175, testing loss: 1.7286005020141602\n",
      "Accuracy: 0.4199\n",
      "epoch: 900, training loss: 1.1910479068756104, testing loss: 1.7324680089950562\n",
      "Accuracy: 0.3831\n",
      "epoch: 900, training loss: 1.2999372482299805, testing loss: 1.7716617584228516\n",
      "Accuracy: 0.3933\n",
      "epoch: 900, training loss: 1.177852749824524, testing loss: 1.7366124391555786\n",
      "Accuracy: 0.4286\n",
      "epoch: 900, training loss: 1.2835218906402588, testing loss: 1.6968178749084473\n",
      "Accuracy: 0.4242\n",
      "epoch: 900, training loss: 1.2310230731964111, testing loss: 1.6721299886703491\n",
      "Accuracy: 0.4116\n",
      "epoch: 900, training loss: 1.216333031654358, testing loss: 1.6503878831863403\n",
      "Accuracy: 0.4272\n",
      "epoch: 910, training loss: 1.2250562906265259, testing loss: 1.8109034299850464\n",
      "Accuracy: 0.4322\n",
      "epoch: 910, training loss: 1.223799228668213, testing loss: 1.6628409624099731\n",
      "Accuracy: 0.4281\n",
      "epoch: 910, training loss: 1.2404730319976807, testing loss: 1.6549806594848633\n",
      "Accuracy: 0.4684\n",
      "epoch: 910, training loss: 1.224249243736267, testing loss: 1.781354546546936\n",
      "Accuracy: 0.4437\n",
      "epoch: 910, training loss: 1.249566674232483, testing loss: 1.7826781272888184\n",
      "Accuracy: 0.4207\n",
      "epoch: 910, training loss: 1.1876267194747925, testing loss: 1.6558698415756226\n",
      "Accuracy: 0.3951\n",
      "epoch: 910, training loss: 1.2405611276626587, testing loss: 1.7800251245498657\n",
      "Accuracy: 0.4104\n",
      "epoch: 910, training loss: 1.2283391952514648, testing loss: 1.6974362134933472\n",
      "Accuracy: 0.3973\n",
      "epoch: 910, training loss: 1.261252999305725, testing loss: 1.6502054929733276\n",
      "Accuracy: 0.3776\n",
      "epoch: 910, training loss: 1.1839061975479126, testing loss: 1.8189269304275513\n",
      "Accuracy: 0.4318\n",
      "epoch: 910, training loss: 1.326969861984253, testing loss: 1.692481279373169\n",
      "Accuracy: 0.4635\n",
      "epoch: 910, training loss: 1.2069813013076782, testing loss: 1.7664871215820312\n",
      "Accuracy: 0.4159\n",
      "epoch: 910, training loss: 1.2662277221679688, testing loss: 1.7397862672805786\n",
      "Accuracy: 0.3775\n",
      "epoch: 910, training loss: 1.2777866125106812, testing loss: 1.76450514793396\n",
      "Accuracy: 0.3836\n",
      "epoch: 910, training loss: 1.2469278573989868, testing loss: 1.683932900428772\n",
      "Accuracy: 0.3993\n",
      "epoch: 910, training loss: 1.200990915298462, testing loss: 1.8889964818954468\n",
      "Accuracy: 0.3981\n",
      "epoch: 910, training loss: 1.2223243713378906, testing loss: 1.736924648284912\n",
      "Accuracy: 0.4181\n",
      "epoch: 910, training loss: 1.225260615348816, testing loss: 1.8466918468475342\n",
      "Accuracy: 0.4037\n",
      "epoch: 910, training loss: 1.2156519889831543, testing loss: 1.687412142753601\n",
      "Accuracy: 0.3994\n",
      "epoch: 910, training loss: 1.184202790260315, testing loss: 1.7188048362731934\n",
      "Accuracy: 0.3912\n",
      "epoch: 910, training loss: 1.2603281736373901, testing loss: 1.708886981010437\n",
      "Accuracy: 0.4204\n",
      "epoch: 910, training loss: 1.215253233909607, testing loss: 1.7688724994659424\n",
      "Accuracy: 0.4067\n",
      "epoch: 910, training loss: 1.2247313261032104, testing loss: 1.7707884311676025\n",
      "Accuracy: 0.3750\n",
      "epoch: 910, training loss: 1.217780351638794, testing loss: 1.6841671466827393\n",
      "Accuracy: 0.4286\n",
      "epoch: 910, training loss: 1.1944242715835571, testing loss: 1.7535828351974487\n",
      "Accuracy: 0.3569\n",
      "epoch: 910, training loss: 1.2174943685531616, testing loss: 1.6400071382522583\n",
      "Accuracy: 0.4607\n",
      "epoch: 910, training loss: 1.2806124687194824, testing loss: 1.6502286195755005\n",
      "Accuracy: 0.4441\n",
      "epoch: 910, training loss: 1.3292039632797241, testing loss: 1.599402666091919\n",
      "Accuracy: 0.4704\n",
      "epoch: 910, training loss: 1.2363845109939575, testing loss: 1.630899429321289\n",
      "Accuracy: 0.4487\n",
      "epoch: 910, training loss: 1.248427152633667, testing loss: 1.6709414720535278\n",
      "Accuracy: 0.4352\n",
      "epoch: 910, training loss: 1.2811940908432007, testing loss: 1.7885669469833374\n",
      "Accuracy: 0.4065\n",
      "epoch: 910, training loss: 1.2321230173110962, testing loss: 1.7127071619033813\n",
      "Accuracy: 0.4092\n",
      "epoch: 910, training loss: 1.2028733491897583, testing loss: 1.7759631872177124\n",
      "Accuracy: 0.4399\n",
      "epoch: 910, training loss: 1.2652157545089722, testing loss: 1.7117449045181274\n",
      "Accuracy: 0.3761\n",
      "epoch: 910, training loss: 1.2011057138442993, testing loss: 1.645412564277649\n",
      "Accuracy: 0.3726\n",
      "epoch: 910, training loss: 1.2216267585754395, testing loss: 1.7674450874328613\n",
      "Accuracy: 0.4189\n",
      "epoch: 910, training loss: 1.2429600954055786, testing loss: 1.5298347473144531\n",
      "Accuracy: 0.4405\n",
      "epoch: 910, training loss: 1.2071936130523682, testing loss: 1.6397682428359985\n",
      "Accuracy: 0.4231\n",
      "epoch: 910, training loss: 1.1993604898452759, testing loss: 1.6937850713729858\n",
      "Accuracy: 0.4642\n",
      "epoch: 910, training loss: 1.2401002645492554, testing loss: 1.744431734085083\n",
      "Accuracy: 0.4505\n",
      "epoch: 910, training loss: 1.2374863624572754, testing loss: 1.7898436784744263\n",
      "Accuracy: 0.4211\n",
      "epoch: 910, training loss: 1.177922248840332, testing loss: 1.6661697626113892\n",
      "Accuracy: 0.4290\n",
      "epoch: 910, training loss: 1.2054853439331055, testing loss: 1.7574869394302368\n",
      "Accuracy: 0.4272\n",
      "epoch: 910, training loss: 1.3161183595657349, testing loss: 1.7518458366394043\n",
      "Accuracy: 0.3975\n",
      "epoch: 910, training loss: 1.201615571975708, testing loss: 1.784717321395874\n",
      "Accuracy: 0.4441\n",
      "epoch: 910, training loss: 1.2826259136199951, testing loss: 1.690850019454956\n",
      "Accuracy: 0.4485\n",
      "epoch: 910, training loss: 1.235370397567749, testing loss: 1.814698576927185\n",
      "Accuracy: 0.3965\n",
      "epoch: 910, training loss: 1.1142498254776, testing loss: 1.5571832656860352\n",
      "Accuracy: 0.4037\n",
      "epoch: 910, training loss: 1.2153137922286987, testing loss: 1.6420363187789917\n",
      "Accuracy: 0.4677\n",
      "epoch: 910, training loss: 1.2200556993484497, testing loss: 1.739414930343628\n",
      "Accuracy: 0.3920\n",
      "epoch: 910, training loss: 1.2605820894241333, testing loss: 1.7172791957855225\n",
      "Accuracy: 0.4291\n",
      "epoch: 910, training loss: 1.2565313577651978, testing loss: 1.729069709777832\n",
      "Accuracy: 0.4178\n",
      "epoch: 910, training loss: 1.3073952198028564, testing loss: 1.7098212242126465\n",
      "Accuracy: 0.4238\n",
      "epoch: 910, training loss: 1.2572532892227173, testing loss: 1.737754225730896\n",
      "Accuracy: 0.4268\n",
      "epoch: 910, training loss: 1.2180660963058472, testing loss: 1.6906306743621826\n",
      "Accuracy: 0.4000\n",
      "epoch: 910, training loss: 1.2345706224441528, testing loss: 1.8123154640197754\n",
      "Accuracy: 0.3903\n",
      "epoch: 910, training loss: 1.1311993598937988, testing loss: 1.647872805595398\n",
      "Accuracy: 0.4375\n",
      "epoch: 910, training loss: 1.2546749114990234, testing loss: 1.8401669263839722\n",
      "Accuracy: 0.3896\n",
      "epoch: 910, training loss: 1.189112901687622, testing loss: 1.8273736238479614\n",
      "Accuracy: 0.3804\n",
      "epoch: 910, training loss: 1.277680516242981, testing loss: 1.7582207918167114\n",
      "Accuracy: 0.3981\n",
      "epoch: 910, training loss: 1.2232749462127686, testing loss: 1.8037325143814087\n",
      "Accuracy: 0.4271\n",
      "epoch: 910, training loss: 1.1993539333343506, testing loss: 1.6646888256072998\n",
      "Accuracy: 0.4371\n",
      "epoch: 910, training loss: 1.2331435680389404, testing loss: 1.7064374685287476\n",
      "Accuracy: 0.4554\n",
      "epoch: 910, training loss: 1.1574416160583496, testing loss: 1.8069785833358765\n",
      "Accuracy: 0.3754\n",
      "epoch: 910, training loss: 1.2517073154449463, testing loss: 1.7078502178192139\n",
      "Accuracy: 0.4268\n",
      "epoch: 910, training loss: 1.2220979928970337, testing loss: 1.7101261615753174\n",
      "Accuracy: 0.3994\n",
      "epoch: 910, training loss: 1.3457891941070557, testing loss: 1.782453179359436\n",
      "Accuracy: 0.3887\n",
      "epoch: 910, training loss: 1.2813775539398193, testing loss: 1.7996116876602173\n",
      "Accuracy: 0.4214\n",
      "epoch: 910, training loss: 1.292219877243042, testing loss: 1.7731480598449707\n",
      "Accuracy: 0.3814\n",
      "epoch: 910, training loss: 1.2865893840789795, testing loss: 1.8948675394058228\n",
      "Accuracy: 0.4126\n",
      "epoch: 910, training loss: 1.2490808963775635, testing loss: 1.7613000869750977\n",
      "Accuracy: 0.4359\n",
      "epoch: 910, training loss: 1.202373743057251, testing loss: 1.6626478433609009\n",
      "Accuracy: 0.4172\n",
      "epoch: 910, training loss: 1.211020827293396, testing loss: 1.7486735582351685\n",
      "Accuracy: 0.4392\n",
      "epoch: 910, training loss: 1.1852160692214966, testing loss: 1.6566824913024902\n",
      "Accuracy: 0.3900\n",
      "epoch: 910, training loss: 1.2067793607711792, testing loss: 1.6857805252075195\n",
      "Accuracy: 0.4110\n",
      "epoch: 910, training loss: 1.2346446514129639, testing loss: 1.7491031885147095\n",
      "Accuracy: 0.3919\n",
      "epoch: 910, training loss: 1.2522937059402466, testing loss: 1.7252843379974365\n",
      "Accuracy: 0.4156\n",
      "epoch: 910, training loss: 1.275735855102539, testing loss: 1.7499173879623413\n",
      "Accuracy: 0.4233\n",
      "epoch: 910, training loss: 1.2804211378097534, testing loss: 1.7106385231018066\n",
      "Accuracy: 0.4062\n",
      "epoch: 910, training loss: 1.1556508541107178, testing loss: 1.8016223907470703\n",
      "Accuracy: 0.4258\n",
      "epoch: 910, training loss: 1.2697066068649292, testing loss: 1.7656358480453491\n",
      "Accuracy: 0.4578\n",
      "epoch: 910, training loss: 1.1980023384094238, testing loss: 1.7562799453735352\n",
      "Accuracy: 0.4387\n",
      "epoch: 910, training loss: 1.2067302465438843, testing loss: 1.740415334701538\n",
      "Accuracy: 0.4051\n",
      "epoch: 910, training loss: 1.211092472076416, testing loss: 1.7320619821548462\n",
      "Accuracy: 0.3943\n",
      "epoch: 910, training loss: 1.2673449516296387, testing loss: 1.6246938705444336\n",
      "Accuracy: 0.4832\n",
      "epoch: 910, training loss: 1.2336046695709229, testing loss: 1.7706769704818726\n",
      "Accuracy: 0.4308\n",
      "epoch: 910, training loss: 1.2191109657287598, testing loss: 1.8441474437713623\n",
      "Accuracy: 0.3898\n",
      "epoch: 910, training loss: 1.2560595273971558, testing loss: 1.6084067821502686\n",
      "Accuracy: 0.4508\n",
      "epoch: 910, training loss: 1.2316805124282837, testing loss: 1.7146800756454468\n",
      "Accuracy: 0.4448\n",
      "epoch: 910, training loss: 1.3268791437149048, testing loss: 1.7201989889144897\n",
      "Accuracy: 0.4157\n",
      "epoch: 910, training loss: 1.2849745750427246, testing loss: 1.6092106103897095\n",
      "Accuracy: 0.4562\n",
      "epoch: 910, training loss: 1.3035632371902466, testing loss: 1.6599678993225098\n",
      "Accuracy: 0.4585\n",
      "epoch: 910, training loss: 1.2076728343963623, testing loss: 1.753674864768982\n",
      "Accuracy: 0.4159\n",
      "epoch: 910, training loss: 1.1929465532302856, testing loss: 1.7371227741241455\n",
      "Accuracy: 0.4123\n",
      "epoch: 910, training loss: 1.2130217552185059, testing loss: 1.749778151512146\n",
      "Accuracy: 0.3956\n",
      "epoch: 910, training loss: 1.2614861726760864, testing loss: 1.691589593887329\n",
      "Accuracy: 0.3923\n",
      "epoch: 910, training loss: 1.2427442073822021, testing loss: 1.7559560537338257\n",
      "Accuracy: 0.4334\n",
      "epoch: 910, training loss: 1.2725701332092285, testing loss: 1.719260573387146\n",
      "Accuracy: 0.4101\n",
      "epoch: 910, training loss: 1.2311720848083496, testing loss: 1.7019188404083252\n",
      "Accuracy: 0.4190\n",
      "epoch: 910, training loss: 1.2401925325393677, testing loss: 1.7128173112869263\n",
      "Accuracy: 0.4379\n",
      "epoch: 920, training loss: 1.2512990236282349, testing loss: 1.7965139150619507\n",
      "Accuracy: 0.4033\n",
      "epoch: 920, training loss: 1.238654375076294, testing loss: 1.646877408027649\n",
      "Accuracy: 0.4399\n",
      "epoch: 920, training loss: 1.239628791809082, testing loss: 1.6797542572021484\n",
      "Accuracy: 0.4129\n",
      "epoch: 920, training loss: 1.2537078857421875, testing loss: 1.631594181060791\n",
      "Accuracy: 0.4395\n",
      "epoch: 920, training loss: 1.2506284713745117, testing loss: 1.6740508079528809\n",
      "Accuracy: 0.4389\n",
      "epoch: 920, training loss: 1.2697930335998535, testing loss: 1.7064613103866577\n",
      "Accuracy: 0.3987\n",
      "epoch: 920, training loss: 1.2204668521881104, testing loss: 1.7137880325317383\n",
      "Accuracy: 0.3981\n",
      "epoch: 920, training loss: 1.258565902709961, testing loss: 1.8112492561340332\n",
      "Accuracy: 0.4007\n",
      "epoch: 920, training loss: 1.2671456336975098, testing loss: 1.7136313915252686\n",
      "Accuracy: 0.4296\n",
      "epoch: 920, training loss: 1.2391804456710815, testing loss: 1.5875465869903564\n",
      "Accuracy: 0.4810\n",
      "epoch: 920, training loss: 1.1596989631652832, testing loss: 1.6707510948181152\n",
      "Accuracy: 0.4138\n",
      "epoch: 920, training loss: 1.2477532625198364, testing loss: 1.6496912240982056\n",
      "Accuracy: 0.4404\n",
      "epoch: 920, training loss: 1.2396605014801025, testing loss: 1.7928656339645386\n",
      "Accuracy: 0.4260\n",
      "epoch: 920, training loss: 1.2512917518615723, testing loss: 1.693838119506836\n",
      "Accuracy: 0.4353\n",
      "epoch: 920, training loss: 1.2318724393844604, testing loss: 1.657926321029663\n",
      "Accuracy: 0.4262\n",
      "epoch: 920, training loss: 1.2837507724761963, testing loss: 1.6740427017211914\n",
      "Accuracy: 0.4528\n",
      "epoch: 920, training loss: 1.2521271705627441, testing loss: 1.5813204050064087\n",
      "Accuracy: 0.4354\n",
      "epoch: 920, training loss: 1.2583001852035522, testing loss: 1.5959545373916626\n",
      "Accuracy: 0.4448\n",
      "epoch: 920, training loss: 1.2362565994262695, testing loss: 1.627755045890808\n",
      "Accuracy: 0.4286\n",
      "epoch: 920, training loss: 1.3307349681854248, testing loss: 1.831238865852356\n",
      "Accuracy: 0.3987\n",
      "epoch: 920, training loss: 1.1793216466903687, testing loss: 1.801011323928833\n",
      "Accuracy: 0.3900\n",
      "epoch: 920, training loss: 1.1895737648010254, testing loss: 1.7913758754730225\n",
      "Accuracy: 0.3931\n",
      "epoch: 920, training loss: 1.1877518892288208, testing loss: 1.696556568145752\n",
      "Accuracy: 0.4184\n",
      "epoch: 920, training loss: 1.2356153726577759, testing loss: 1.805136799812317\n",
      "Accuracy: 0.3935\n",
      "epoch: 920, training loss: 1.2921535968780518, testing loss: 1.6248561143875122\n",
      "Accuracy: 0.4444\n",
      "epoch: 920, training loss: 1.216346263885498, testing loss: 1.6122560501098633\n",
      "Accuracy: 0.4330\n",
      "epoch: 920, training loss: 1.2111577987670898, testing loss: 1.6785954236984253\n",
      "Accuracy: 0.4062\n",
      "epoch: 920, training loss: 1.2982232570648193, testing loss: 1.618155837059021\n",
      "Accuracy: 0.4809\n",
      "epoch: 920, training loss: 1.3470607995986938, testing loss: 1.7480647563934326\n",
      "Accuracy: 0.4076\n",
      "epoch: 920, training loss: 1.3338420391082764, testing loss: 1.8494569063186646\n",
      "Accuracy: 0.4267\n",
      "epoch: 920, training loss: 1.2020145654678345, testing loss: 1.8237419128417969\n",
      "Accuracy: 0.3513\n",
      "epoch: 920, training loss: 1.246509313583374, testing loss: 1.6696034669876099\n",
      "Accuracy: 0.4281\n",
      "epoch: 920, training loss: 1.2205568552017212, testing loss: 1.6847755908966064\n",
      "Accuracy: 0.4000\n",
      "epoch: 920, training loss: 1.1563358306884766, testing loss: 1.681715965270996\n",
      "Accuracy: 0.4345\n",
      "epoch: 920, training loss: 1.2420810461044312, testing loss: 1.6812148094177246\n",
      "Accuracy: 0.4045\n",
      "epoch: 920, training loss: 1.2513529062271118, testing loss: 1.613352656364441\n",
      "Accuracy: 0.4434\n",
      "epoch: 920, training loss: 1.1861099004745483, testing loss: 1.6810710430145264\n",
      "Accuracy: 0.4196\n",
      "epoch: 920, training loss: 1.260340690612793, testing loss: 1.7114102840423584\n",
      "Accuracy: 0.4343\n",
      "epoch: 920, training loss: 1.1855738162994385, testing loss: 1.8236069679260254\n",
      "Accuracy: 0.4353\n",
      "epoch: 920, training loss: 1.2361328601837158, testing loss: 1.6826237440109253\n",
      "Accuracy: 0.3949\n",
      "epoch: 920, training loss: 1.2463299036026, testing loss: 1.734088659286499\n",
      "Accuracy: 0.3734\n",
      "epoch: 920, training loss: 1.2112581729888916, testing loss: 1.6419140100479126\n",
      "Accuracy: 0.4272\n",
      "epoch: 920, training loss: 1.2005752325057983, testing loss: 1.6829571723937988\n",
      "Accuracy: 0.4263\n",
      "epoch: 920, training loss: 1.165019154548645, testing loss: 1.6784871816635132\n",
      "Accuracy: 0.4144\n",
      "epoch: 920, training loss: 1.209054946899414, testing loss: 1.8567429780960083\n",
      "Accuracy: 0.3849\n",
      "epoch: 920, training loss: 1.1808408498764038, testing loss: 1.7341755628585815\n",
      "Accuracy: 0.4215\n",
      "epoch: 920, training loss: 1.2060832977294922, testing loss: 1.8387525081634521\n",
      "Accuracy: 0.4317\n",
      "epoch: 920, training loss: 1.216127634048462, testing loss: 1.6356945037841797\n",
      "Accuracy: 0.4384\n",
      "epoch: 920, training loss: 1.248978853225708, testing loss: 1.8116426467895508\n",
      "Accuracy: 0.4159\n",
      "epoch: 920, training loss: 1.2038757801055908, testing loss: 1.5604497194290161\n",
      "Accuracy: 0.4272\n",
      "epoch: 920, training loss: 1.2518901824951172, testing loss: 1.6292146444320679\n",
      "Accuracy: 0.4579\n",
      "epoch: 920, training loss: 1.188950777053833, testing loss: 1.7106597423553467\n",
      "Accuracy: 0.4051\n",
      "epoch: 920, training loss: 1.3185516595840454, testing loss: 1.8093615770339966\n",
      "Accuracy: 0.3914\n",
      "epoch: 920, training loss: 1.295764446258545, testing loss: 1.6734732389450073\n",
      "Accuracy: 0.4057\n",
      "epoch: 920, training loss: 1.2694711685180664, testing loss: 1.69625723361969\n",
      "Accuracy: 0.4252\n",
      "epoch: 920, training loss: 1.1900049448013306, testing loss: 1.5811811685562134\n",
      "Accuracy: 0.4253\n",
      "epoch: 920, training loss: 1.1704140901565552, testing loss: 1.7630479335784912\n",
      "Accuracy: 0.4312\n",
      "epoch: 920, training loss: 1.314958930015564, testing loss: 1.7142585515975952\n",
      "Accuracy: 0.3898\n",
      "epoch: 920, training loss: 1.216001272201538, testing loss: 1.6967966556549072\n",
      "Accuracy: 0.4259\n",
      "epoch: 920, training loss: 1.199474811553955, testing loss: 1.7102632522583008\n",
      "Accuracy: 0.4482\n",
      "epoch: 920, training loss: 1.2010945081710815, testing loss: 1.6024515628814697\n",
      "Accuracy: 0.4164\n",
      "epoch: 920, training loss: 1.1979061365127563, testing loss: 1.7580375671386719\n",
      "Accuracy: 0.4073\n",
      "epoch: 920, training loss: 1.1789135932922363, testing loss: 1.698467493057251\n",
      "Accuracy: 0.4603\n",
      "epoch: 920, training loss: 1.2549805641174316, testing loss: 1.6479228734970093\n",
      "Accuracy: 0.4474\n",
      "epoch: 920, training loss: 1.1906551122665405, testing loss: 1.6326735019683838\n",
      "Accuracy: 0.4585\n",
      "epoch: 920, training loss: 1.1885581016540527, testing loss: 1.7230185270309448\n",
      "Accuracy: 0.3980\n",
      "epoch: 920, training loss: 1.1871296167373657, testing loss: 1.641869068145752\n",
      "Accuracy: 0.4887\n",
      "epoch: 920, training loss: 1.2001888751983643, testing loss: 1.670451283454895\n",
      "Accuracy: 0.4147\n",
      "epoch: 920, training loss: 1.2659505605697632, testing loss: 1.8358489274978638\n",
      "Accuracy: 0.3779\n",
      "epoch: 920, training loss: 1.237470030784607, testing loss: 1.75387442111969\n",
      "Accuracy: 0.3689\n",
      "epoch: 920, training loss: 1.2755500078201294, testing loss: 1.669783115386963\n",
      "Accuracy: 0.4141\n",
      "epoch: 920, training loss: 1.2312053442001343, testing loss: 1.7855924367904663\n",
      "Accuracy: 0.3784\n",
      "epoch: 920, training loss: 1.2851619720458984, testing loss: 1.744135856628418\n",
      "Accuracy: 0.4308\n",
      "epoch: 920, training loss: 1.236344814300537, testing loss: 1.663189172744751\n",
      "Accuracy: 0.4348\n",
      "epoch: 920, training loss: 1.2198948860168457, testing loss: 1.7964245080947876\n",
      "Accuracy: 0.4252\n",
      "epoch: 920, training loss: 1.2195501327514648, testing loss: 1.9067591428756714\n",
      "Accuracy: 0.3868\n",
      "epoch: 920, training loss: 1.239996075630188, testing loss: 1.6194030046463013\n",
      "Accuracy: 0.4159\n",
      "epoch: 920, training loss: 1.222412109375, testing loss: 1.6926732063293457\n",
      "Accuracy: 0.4237\n",
      "epoch: 920, training loss: 1.2593543529510498, testing loss: 1.7650412321090698\n",
      "Accuracy: 0.3613\n",
      "epoch: 920, training loss: 1.3401498794555664, testing loss: 1.6493117809295654\n",
      "Accuracy: 0.4259\n",
      "epoch: 920, training loss: 1.2308019399642944, testing loss: 1.7029764652252197\n",
      "Accuracy: 0.4470\n",
      "epoch: 920, training loss: 1.3020949363708496, testing loss: 1.6960351467132568\n",
      "Accuracy: 0.4295\n",
      "epoch: 920, training loss: 1.2562958002090454, testing loss: 1.6273632049560547\n",
      "Accuracy: 0.4391\n",
      "epoch: 920, training loss: 1.2395151853561401, testing loss: 1.8223861455917358\n",
      "Accuracy: 0.4132\n",
      "epoch: 920, training loss: 1.2033098936080933, testing loss: 1.717651128768921\n",
      "Accuracy: 0.4007\n",
      "epoch: 920, training loss: 1.2402149438858032, testing loss: 1.6549458503723145\n",
      "Accuracy: 0.4419\n",
      "epoch: 920, training loss: 1.2062366008758545, testing loss: 1.6794873476028442\n",
      "Accuracy: 0.4320\n",
      "epoch: 920, training loss: 1.1623156070709229, testing loss: 1.7279695272445679\n",
      "Accuracy: 0.3972\n",
      "epoch: 920, training loss: 1.1955724954605103, testing loss: 1.6995447874069214\n",
      "Accuracy: 0.4228\n",
      "epoch: 920, training loss: 1.1843498945236206, testing loss: 1.7643500566482544\n",
      "Accuracy: 0.3974\n",
      "epoch: 920, training loss: 1.2078198194503784, testing loss: 2.0147082805633545\n",
      "Accuracy: 0.3550\n",
      "epoch: 920, training loss: 1.2719852924346924, testing loss: 1.785325527191162\n",
      "Accuracy: 0.4355\n",
      "epoch: 920, training loss: 1.3155150413513184, testing loss: 1.7130955457687378\n",
      "Accuracy: 0.4013\n",
      "epoch: 920, training loss: 1.2438066005706787, testing loss: 1.7525784969329834\n",
      "Accuracy: 0.4385\n",
      "epoch: 920, training loss: 1.24979829788208, testing loss: 1.747207522392273\n",
      "Accuracy: 0.4277\n",
      "epoch: 920, training loss: 1.3199764490127563, testing loss: 1.7231311798095703\n",
      "Accuracy: 0.4393\n",
      "epoch: 920, training loss: 1.2135554552078247, testing loss: 1.8010334968566895\n",
      "Accuracy: 0.4101\n",
      "epoch: 920, training loss: 1.219309687614441, testing loss: 1.6231985092163086\n",
      "Accuracy: 0.4139\n",
      "epoch: 920, training loss: 1.1666535139083862, testing loss: 1.7547945976257324\n",
      "Accuracy: 0.4086\n",
      "epoch: 920, training loss: 1.2602941989898682, testing loss: 1.7171992063522339\n",
      "Accuracy: 0.4649\n",
      "epoch: 930, training loss: 1.2075754404067993, testing loss: 1.6908831596374512\n",
      "Accuracy: 0.4371\n",
      "epoch: 930, training loss: 1.2461286783218384, testing loss: 1.801446795463562\n",
      "Accuracy: 0.4281\n",
      "epoch: 930, training loss: 1.312921404838562, testing loss: 1.8656529188156128\n",
      "Accuracy: 0.3556\n",
      "epoch: 930, training loss: 1.18426513671875, testing loss: 1.9190233945846558\n",
      "Accuracy: 0.3679\n",
      "epoch: 930, training loss: 1.2754353284835815, testing loss: 1.6349045038223267\n",
      "Accuracy: 0.4203\n",
      "epoch: 930, training loss: 1.226783275604248, testing loss: 1.7321381568908691\n",
      "Accuracy: 0.4317\n",
      "epoch: 930, training loss: 1.2358578443527222, testing loss: 1.6766082048416138\n",
      "Accuracy: 0.4129\n",
      "epoch: 930, training loss: 1.2480640411376953, testing loss: 1.6784940958023071\n",
      "Accuracy: 0.4020\n",
      "epoch: 930, training loss: 1.2544893026351929, testing loss: 1.7160637378692627\n",
      "Accuracy: 0.4215\n",
      "epoch: 930, training loss: 1.2581912279129028, testing loss: 1.6792807579040527\n",
      "Accuracy: 0.4094\n",
      "epoch: 930, training loss: 1.2599269151687622, testing loss: 1.7723047733306885\n",
      "Accuracy: 0.4317\n",
      "epoch: 930, training loss: 1.2641890048980713, testing loss: 1.6888000965118408\n",
      "Accuracy: 0.4194\n",
      "epoch: 930, training loss: 1.2122467756271362, testing loss: 1.7259624004364014\n",
      "Accuracy: 0.3810\n",
      "epoch: 930, training loss: 1.2254527807235718, testing loss: 1.7798353433609009\n",
      "Accuracy: 0.3916\n",
      "epoch: 930, training loss: 1.2456995248794556, testing loss: 1.722543478012085\n",
      "Accuracy: 0.4360\n",
      "epoch: 930, training loss: 1.2268797159194946, testing loss: 1.7837238311767578\n",
      "Accuracy: 0.4219\n",
      "epoch: 930, training loss: 1.187341570854187, testing loss: 1.6778392791748047\n",
      "Accuracy: 0.4227\n",
      "epoch: 930, training loss: 1.2166463136672974, testing loss: 1.9070258140563965\n",
      "Accuracy: 0.3553\n",
      "epoch: 930, training loss: 1.1853080987930298, testing loss: 1.835577368736267\n",
      "Accuracy: 0.3851\n",
      "epoch: 930, training loss: 1.2862776517868042, testing loss: 1.7189691066741943\n",
      "Accuracy: 0.4304\n",
      "epoch: 930, training loss: 1.2237775325775146, testing loss: 1.6773037910461426\n",
      "Accuracy: 0.4559\n",
      "epoch: 930, training loss: 1.2489547729492188, testing loss: 1.740688681602478\n",
      "Accuracy: 0.4040\n",
      "epoch: 930, training loss: 1.1821727752685547, testing loss: 1.7755069732666016\n",
      "Accuracy: 0.3887\n",
      "epoch: 930, training loss: 1.2611029148101807, testing loss: 1.6958431005477905\n",
      "Accuracy: 0.3962\n",
      "epoch: 930, training loss: 1.2997617721557617, testing loss: 1.6660832166671753\n",
      "Accuracy: 0.4557\n",
      "epoch: 930, training loss: 1.2816698551177979, testing loss: 1.6810190677642822\n",
      "Accuracy: 0.4132\n",
      "epoch: 930, training loss: 1.1778924465179443, testing loss: 1.7002387046813965\n",
      "Accuracy: 0.3960\n",
      "epoch: 930, training loss: 1.199410080909729, testing loss: 1.7275612354278564\n",
      "Accuracy: 0.4495\n",
      "epoch: 930, training loss: 1.2189956903457642, testing loss: 1.6479272842407227\n",
      "Accuracy: 0.4713\n",
      "epoch: 930, training loss: 1.2558139562606812, testing loss: 1.760673999786377\n",
      "Accuracy: 0.3643\n",
      "epoch: 930, training loss: 1.2983052730560303, testing loss: 1.6854404211044312\n",
      "Accuracy: 0.4747\n",
      "epoch: 930, training loss: 1.1928592920303345, testing loss: 1.704684853553772\n",
      "Accuracy: 0.4309\n",
      "epoch: 930, training loss: 1.2502251863479614, testing loss: 1.8502404689788818\n",
      "Accuracy: 0.3750\n",
      "epoch: 930, training loss: 1.2485644817352295, testing loss: 1.964764952659607\n",
      "Accuracy: 0.3516\n",
      "epoch: 930, training loss: 1.1765990257263184, testing loss: 1.8763799667358398\n",
      "Accuracy: 0.3812\n",
      "epoch: 930, training loss: 1.2834393978118896, testing loss: 1.6614876985549927\n",
      "Accuracy: 0.4488\n",
      "epoch: 930, training loss: 1.204605221748352, testing loss: 1.6258502006530762\n",
      "Accuracy: 0.4327\n",
      "epoch: 930, training loss: 1.2824965715408325, testing loss: 1.7905327081680298\n",
      "Accuracy: 0.4013\n",
      "epoch: 930, training loss: 1.226468801498413, testing loss: 1.6878043413162231\n",
      "Accuracy: 0.4219\n",
      "epoch: 930, training loss: 1.17531418800354, testing loss: 1.732069492340088\n",
      "Accuracy: 0.4299\n",
      "epoch: 930, training loss: 1.2637288570404053, testing loss: 1.7840936183929443\n",
      "Accuracy: 0.4061\n",
      "epoch: 930, training loss: 1.2178674936294556, testing loss: 1.5732277631759644\n",
      "Accuracy: 0.4061\n",
      "epoch: 930, training loss: 1.2561535835266113, testing loss: 1.6823288202285767\n",
      "Accuracy: 0.3948\n",
      "epoch: 930, training loss: 1.1751145124435425, testing loss: 1.7174066305160522\n",
      "Accuracy: 0.3718\n",
      "epoch: 930, training loss: 1.2619813680648804, testing loss: 1.762434482574463\n",
      "Accuracy: 0.4493\n",
      "epoch: 930, training loss: 1.2587172985076904, testing loss: 1.784460425376892\n",
      "Accuracy: 0.3600\n",
      "epoch: 930, training loss: 1.2294764518737793, testing loss: 1.774255394935608\n",
      "Accuracy: 0.3933\n",
      "epoch: 930, training loss: 1.1522793769836426, testing loss: 1.7777128219604492\n",
      "Accuracy: 0.4027\n",
      "epoch: 930, training loss: 1.2188225984573364, testing loss: 1.617006540298462\n",
      "Accuracy: 0.4602\n",
      "epoch: 930, training loss: 1.295619010925293, testing loss: 1.674425721168518\n",
      "Accuracy: 0.4119\n",
      "epoch: 930, training loss: 1.2601619958877563, testing loss: 2.0070011615753174\n",
      "Accuracy: 0.3777\n",
      "epoch: 930, training loss: 1.2024025917053223, testing loss: 1.7137876749038696\n",
      "Accuracy: 0.4479\n",
      "epoch: 930, training loss: 1.2020114660263062, testing loss: 1.840042233467102\n",
      "Accuracy: 0.4130\n",
      "epoch: 930, training loss: 1.2464807033538818, testing loss: 1.7071830034255981\n",
      "Accuracy: 0.4045\n",
      "epoch: 930, training loss: 1.2007396221160889, testing loss: 1.7442950010299683\n",
      "Accuracy: 0.3810\n",
      "epoch: 930, training loss: 1.2408902645111084, testing loss: 1.7313041687011719\n",
      "Accuracy: 0.3970\n",
      "epoch: 930, training loss: 1.175236701965332, testing loss: 1.72199547290802\n",
      "Accuracy: 0.3981\n",
      "epoch: 930, training loss: 1.195566177368164, testing loss: 1.6503913402557373\n",
      "Accuracy: 0.4437\n",
      "epoch: 930, training loss: 1.2394976615905762, testing loss: 1.6345173120498657\n",
      "Accuracy: 0.4389\n",
      "epoch: 930, training loss: 1.2455191612243652, testing loss: 1.6634856462478638\n",
      "Accuracy: 0.4218\n",
      "epoch: 930, training loss: 1.2415728569030762, testing loss: 1.660241723060608\n",
      "Accuracy: 0.4281\n",
      "epoch: 930, training loss: 1.2521427869796753, testing loss: 1.814401626586914\n",
      "Accuracy: 0.4103\n",
      "epoch: 930, training loss: 1.2354856729507446, testing loss: 1.710343599319458\n",
      "Accuracy: 0.4377\n",
      "epoch: 930, training loss: 1.2410179376602173, testing loss: 1.6701523065567017\n",
      "Accuracy: 0.4702\n",
      "epoch: 930, training loss: 1.2602299451828003, testing loss: 1.6704548597335815\n",
      "Accuracy: 0.4650\n",
      "epoch: 930, training loss: 1.3647072315216064, testing loss: 1.7645976543426514\n",
      "Accuracy: 0.3719\n",
      "epoch: 930, training loss: 1.2478629350662231, testing loss: 1.8575900793075562\n",
      "Accuracy: 0.3140\n",
      "epoch: 930, training loss: 1.211846947669983, testing loss: 1.8392771482467651\n",
      "Accuracy: 0.4119\n",
      "epoch: 930, training loss: 1.193297028541565, testing loss: 1.7448796033859253\n",
      "Accuracy: 0.4419\n",
      "epoch: 930, training loss: 1.2994352579116821, testing loss: 1.8986259698867798\n",
      "Accuracy: 0.3558\n",
      "epoch: 930, training loss: 1.2698214054107666, testing loss: 1.7681000232696533\n",
      "Accuracy: 0.3970\n",
      "epoch: 930, training loss: 1.2776073217391968, testing loss: 1.7986730337142944\n",
      "Accuracy: 0.3883\n",
      "epoch: 930, training loss: 1.2451006174087524, testing loss: 1.784033179283142\n",
      "Accuracy: 0.4161\n",
      "epoch: 930, training loss: 1.2104976177215576, testing loss: 1.8051395416259766\n",
      "Accuracy: 0.4732\n",
      "epoch: 930, training loss: 1.31128990650177, testing loss: 1.780630350112915\n",
      "Accuracy: 0.4422\n",
      "epoch: 930, training loss: 1.253991961479187, testing loss: 1.7636440992355347\n",
      "Accuracy: 0.3779\n",
      "epoch: 930, training loss: 1.2744534015655518, testing loss: 1.7393038272857666\n",
      "Accuracy: 0.3846\n",
      "epoch: 930, training loss: 1.2720407247543335, testing loss: 1.7805557250976562\n",
      "Accuracy: 0.3758\n",
      "epoch: 930, training loss: 1.179568886756897, testing loss: 1.6938353776931763\n",
      "Accuracy: 0.4448\n",
      "epoch: 930, training loss: 1.1924359798431396, testing loss: 1.5851984024047852\n",
      "Accuracy: 0.4591\n",
      "epoch: 930, training loss: 1.2801272869110107, testing loss: 1.6843563318252563\n",
      "Accuracy: 0.4007\n",
      "epoch: 930, training loss: 1.259939432144165, testing loss: 1.6863151788711548\n",
      "Accuracy: 0.4065\n",
      "epoch: 930, training loss: 1.2881996631622314, testing loss: 1.6922284364700317\n",
      "Accuracy: 0.4333\n",
      "epoch: 930, training loss: 1.237874150276184, testing loss: 1.7072621583938599\n",
      "Accuracy: 0.4036\n",
      "epoch: 930, training loss: 1.3334721326828003, testing loss: 1.7279233932495117\n",
      "Accuracy: 0.4007\n",
      "epoch: 930, training loss: 1.2418886423110962, testing loss: 1.756127119064331\n",
      "Accuracy: 0.4083\n",
      "epoch: 930, training loss: 1.1463652849197388, testing loss: 1.7825233936309814\n",
      "Accuracy: 0.4388\n",
      "epoch: 930, training loss: 1.193827509880066, testing loss: 1.644742727279663\n",
      "Accuracy: 0.4528\n",
      "epoch: 930, training loss: 1.2645949125289917, testing loss: 1.6285728216171265\n",
      "Accuracy: 0.4531\n",
      "epoch: 930, training loss: 1.2061002254486084, testing loss: 1.7829135656356812\n",
      "Accuracy: 0.4020\n",
      "epoch: 930, training loss: 1.176070213317871, testing loss: 1.6760002374649048\n",
      "Accuracy: 0.4040\n",
      "epoch: 930, training loss: 1.2007170915603638, testing loss: 1.7813595533370972\n",
      "Accuracy: 0.4236\n",
      "epoch: 930, training loss: 1.2049973011016846, testing loss: 1.764725923538208\n",
      "Accuracy: 0.3964\n",
      "epoch: 930, training loss: 1.1954761743545532, testing loss: 1.6680262088775635\n",
      "Accuracy: 0.4475\n",
      "epoch: 930, training loss: 1.3450047969818115, testing loss: 1.7779079675674438\n",
      "Accuracy: 0.4019\n",
      "epoch: 930, training loss: 1.1687959432601929, testing loss: 1.7047055959701538\n",
      "Accuracy: 0.3625\n",
      "epoch: 930, training loss: 1.2195175886154175, testing loss: 1.6961290836334229\n",
      "Accuracy: 0.4712\n",
      "epoch: 930, training loss: 1.2712764739990234, testing loss: 1.7525595426559448\n",
      "Accuracy: 0.3974\n",
      "epoch: 930, training loss: 1.3032221794128418, testing loss: 1.8058745861053467\n",
      "Accuracy: 0.4133\n",
      "epoch: 930, training loss: 1.260540246963501, testing loss: 1.7301182746887207\n",
      "Accuracy: 0.4007\n",
      "epoch: 940, training loss: 1.205543041229248, testing loss: 1.6688657999038696\n",
      "Accuracy: 0.4732\n",
      "epoch: 940, training loss: 1.1753621101379395, testing loss: 1.7768688201904297\n",
      "Accuracy: 0.4038\n",
      "epoch: 940, training loss: 1.112337589263916, testing loss: 1.745152235031128\n",
      "Accuracy: 0.4217\n",
      "epoch: 940, training loss: 1.2676923274993896, testing loss: 1.6691616773605347\n",
      "Accuracy: 0.4756\n",
      "epoch: 940, training loss: 1.2114897966384888, testing loss: 1.7292033433914185\n",
      "Accuracy: 0.4566\n",
      "epoch: 940, training loss: 1.3110798597335815, testing loss: 1.6101229190826416\n",
      "Accuracy: 0.4255\n",
      "epoch: 940, training loss: 1.1867659091949463, testing loss: 1.6091662645339966\n",
      "Accuracy: 0.4658\n",
      "epoch: 940, training loss: 1.1819427013397217, testing loss: 1.876706600189209\n",
      "Accuracy: 0.4012\n",
      "epoch: 940, training loss: 1.273559331893921, testing loss: 1.6957522630691528\n",
      "Accuracy: 0.3770\n",
      "epoch: 940, training loss: 1.1871508359909058, testing loss: 1.7619603872299194\n",
      "Accuracy: 0.3781\n",
      "epoch: 940, training loss: 1.212697148323059, testing loss: 1.6897262334823608\n",
      "Accuracy: 0.3927\n",
      "epoch: 940, training loss: 1.2398808002471924, testing loss: 1.5506234169006348\n",
      "Accuracy: 0.4620\n",
      "epoch: 940, training loss: 1.2464358806610107, testing loss: 1.736504316329956\n",
      "Accuracy: 0.4175\n",
      "epoch: 940, training loss: 1.2052339315414429, testing loss: 1.764341950416565\n",
      "Accuracy: 0.3946\n",
      "epoch: 940, training loss: 1.24690842628479, testing loss: 1.6883043050765991\n",
      "Accuracy: 0.4020\n",
      "epoch: 940, training loss: 1.3050739765167236, testing loss: 1.7128175497055054\n",
      "Accuracy: 0.4273\n",
      "epoch: 940, training loss: 1.194254994392395, testing loss: 1.7354248762130737\n",
      "Accuracy: 0.4560\n",
      "epoch: 940, training loss: 1.3150659799575806, testing loss: 1.7442013025283813\n",
      "Accuracy: 0.4575\n",
      "epoch: 940, training loss: 1.2011834383010864, testing loss: 1.6468417644500732\n",
      "Accuracy: 0.4530\n",
      "epoch: 940, training loss: 1.257551670074463, testing loss: 1.7598764896392822\n",
      "Accuracy: 0.4317\n",
      "epoch: 940, training loss: 1.2371110916137695, testing loss: 1.6747716665267944\n",
      "Accuracy: 0.4583\n",
      "epoch: 940, training loss: 1.2249919176101685, testing loss: 1.655950665473938\n",
      "Accuracy: 0.4183\n",
      "epoch: 940, training loss: 1.2329370975494385, testing loss: 1.6524583101272583\n",
      "Accuracy: 0.4548\n",
      "epoch: 940, training loss: 1.2467433214187622, testing loss: 1.76231050491333\n",
      "Accuracy: 0.3497\n",
      "epoch: 940, training loss: 1.265462875366211, testing loss: 1.8521077632904053\n",
      "Accuracy: 0.4251\n",
      "epoch: 940, training loss: 1.2444261312484741, testing loss: 1.6556193828582764\n",
      "Accuracy: 0.4525\n",
      "epoch: 940, training loss: 1.238713264465332, testing loss: 1.8997093439102173\n",
      "Accuracy: 0.3819\n",
      "epoch: 940, training loss: 1.3046979904174805, testing loss: 1.7589010000228882\n",
      "Accuracy: 0.4191\n",
      "epoch: 940, training loss: 1.1957238912582397, testing loss: 1.8701136112213135\n",
      "Accuracy: 0.3878\n",
      "epoch: 940, training loss: 1.270206332206726, testing loss: 1.682607650756836\n",
      "Accuracy: 0.4286\n",
      "epoch: 940, training loss: 1.203155279159546, testing loss: 1.7811918258666992\n",
      "Accuracy: 0.3912\n",
      "epoch: 940, training loss: 1.2625041007995605, testing loss: 1.7835365533828735\n",
      "Accuracy: 0.3651\n",
      "epoch: 940, training loss: 1.260197639465332, testing loss: 1.6774917840957642\n",
      "Accuracy: 0.4259\n",
      "epoch: 940, training loss: 1.220259666442871, testing loss: 1.6992589235305786\n",
      "Accuracy: 0.4353\n",
      "epoch: 940, training loss: 1.2963751554489136, testing loss: 1.7926868200302124\n",
      "Accuracy: 0.4007\n",
      "epoch: 940, training loss: 1.2623153924942017, testing loss: 1.6010202169418335\n",
      "Accuracy: 0.3882\n",
      "epoch: 940, training loss: 1.1992287635803223, testing loss: 1.6794648170471191\n",
      "Accuracy: 0.4246\n",
      "epoch: 940, training loss: 1.2680100202560425, testing loss: 1.7456207275390625\n",
      "Accuracy: 0.4507\n",
      "epoch: 940, training loss: 1.2212638854980469, testing loss: 1.7074956893920898\n",
      "Accuracy: 0.4241\n",
      "epoch: 940, training loss: 1.1790127754211426, testing loss: 1.8010802268981934\n",
      "Accuracy: 0.4020\n",
      "epoch: 940, training loss: 1.1933611631393433, testing loss: 1.795664668083191\n",
      "Accuracy: 0.3917\n",
      "epoch: 940, training loss: 1.2753549814224243, testing loss: 1.753677487373352\n",
      "Accuracy: 0.4424\n",
      "epoch: 940, training loss: 1.2357699871063232, testing loss: 1.7285691499710083\n",
      "Accuracy: 0.4032\n",
      "epoch: 940, training loss: 1.1971871852874756, testing loss: 1.797220230102539\n",
      "Accuracy: 0.4013\n",
      "epoch: 940, training loss: 1.2476750612258911, testing loss: 1.7183623313903809\n",
      "Accuracy: 0.4367\n",
      "epoch: 940, training loss: 1.2652592658996582, testing loss: 1.6838866472244263\n",
      "Accuracy: 0.4119\n",
      "epoch: 940, training loss: 1.2521560192108154, testing loss: 1.845934510231018\n",
      "Accuracy: 0.4205\n",
      "epoch: 940, training loss: 1.269642949104309, testing loss: 1.7436124086380005\n",
      "Accuracy: 0.4135\n",
      "epoch: 940, training loss: 1.1698952913284302, testing loss: 1.7912133932113647\n",
      "Accuracy: 0.4036\n",
      "epoch: 940, training loss: 1.2440723180770874, testing loss: 1.8565932512283325\n",
      "Accuracy: 0.3590\n",
      "epoch: 940, training loss: 1.233082890510559, testing loss: 1.7552496194839478\n",
      "Accuracy: 0.4191\n",
      "epoch: 940, training loss: 1.2202061414718628, testing loss: 1.609933853149414\n",
      "Accuracy: 0.4490\n",
      "epoch: 940, training loss: 1.2849485874176025, testing loss: 1.7696309089660645\n",
      "Accuracy: 0.3955\n",
      "epoch: 940, training loss: 1.2874770164489746, testing loss: 1.7601763010025024\n",
      "Accuracy: 0.3937\n",
      "epoch: 940, training loss: 1.2239125967025757, testing loss: 1.784711480140686\n",
      "Accuracy: 0.3839\n",
      "epoch: 940, training loss: 1.2777858972549438, testing loss: 1.751331090927124\n",
      "Accuracy: 0.3866\n",
      "epoch: 940, training loss: 1.202411413192749, testing loss: 1.7797538042068481\n",
      "Accuracy: 0.3649\n",
      "epoch: 940, training loss: 1.2133663892745972, testing loss: 1.8188319206237793\n",
      "Accuracy: 0.3571\n",
      "epoch: 940, training loss: 1.2197332382202148, testing loss: 1.7780470848083496\n",
      "Accuracy: 0.3914\n",
      "epoch: 940, training loss: 1.1812747716903687, testing loss: 1.7711973190307617\n",
      "Accuracy: 0.3975\n",
      "epoch: 940, training loss: 1.3106555938720703, testing loss: 1.7935259342193604\n",
      "Accuracy: 0.3960\n",
      "epoch: 940, training loss: 1.2375073432922363, testing loss: 1.7697674036026\n",
      "Accuracy: 0.4333\n",
      "epoch: 940, training loss: 1.2371715307235718, testing loss: 1.6434587240219116\n",
      "Accuracy: 0.4589\n",
      "epoch: 940, training loss: 1.259323000907898, testing loss: 1.70039963722229\n",
      "Accuracy: 0.4024\n",
      "epoch: 940, training loss: 1.2310924530029297, testing loss: 1.679795742034912\n",
      "Accuracy: 0.4527\n",
      "epoch: 940, training loss: 1.2525544166564941, testing loss: 1.7512972354888916\n",
      "Accuracy: 0.4226\n",
      "epoch: 940, training loss: 1.2154078483581543, testing loss: 1.6837782859802246\n",
      "Accuracy: 0.4341\n",
      "epoch: 940, training loss: 1.2674386501312256, testing loss: 1.757026195526123\n",
      "Accuracy: 0.3428\n",
      "epoch: 940, training loss: 1.21522855758667, testing loss: 1.7101892232894897\n",
      "Accuracy: 0.4561\n",
      "epoch: 940, training loss: 1.2787706851959229, testing loss: 1.7670284509658813\n",
      "Accuracy: 0.4125\n",
      "epoch: 940, training loss: 1.2007570266723633, testing loss: 1.5748316049575806\n",
      "Accuracy: 0.4630\n",
      "epoch: 940, training loss: 1.1722218990325928, testing loss: 1.6528033018112183\n",
      "Accuracy: 0.4064\n",
      "epoch: 940, training loss: 1.256202220916748, testing loss: 1.6730811595916748\n",
      "Accuracy: 0.3956\n",
      "epoch: 940, training loss: 1.300809383392334, testing loss: 1.7391008138656616\n",
      "Accuracy: 0.4552\n",
      "epoch: 940, training loss: 1.2420001029968262, testing loss: 1.9188891649246216\n",
      "Accuracy: 0.3724\n",
      "epoch: 940, training loss: 1.2478821277618408, testing loss: 1.8202400207519531\n",
      "Accuracy: 0.4091\n",
      "epoch: 940, training loss: 1.157033920288086, testing loss: 1.7869430780410767\n",
      "Accuracy: 0.4127\n",
      "epoch: 940, training loss: 1.1579562425613403, testing loss: 1.7524328231811523\n",
      "Accuracy: 0.4299\n",
      "epoch: 940, training loss: 1.3247298002243042, testing loss: 1.7359882593154907\n",
      "Accuracy: 0.3899\n",
      "epoch: 940, training loss: 1.237229585647583, testing loss: 1.7145178318023682\n",
      "Accuracy: 0.4518\n",
      "epoch: 940, training loss: 1.1423118114471436, testing loss: 1.8356339931488037\n",
      "Accuracy: 0.3500\n",
      "epoch: 940, training loss: 1.2079910039901733, testing loss: 1.855672836303711\n",
      "Accuracy: 0.3927\n",
      "epoch: 940, training loss: 1.2427504062652588, testing loss: 1.7269597053527832\n",
      "Accuracy: 0.4399\n",
      "epoch: 940, training loss: 1.2316590547561646, testing loss: 1.7325658798217773\n",
      "Accuracy: 0.4135\n",
      "epoch: 940, training loss: 1.1925888061523438, testing loss: 1.6387790441513062\n",
      "Accuracy: 0.3841\n",
      "epoch: 940, training loss: 1.2647664546966553, testing loss: 1.7753914594650269\n",
      "Accuracy: 0.4291\n",
      "epoch: 940, training loss: 1.0971317291259766, testing loss: 1.78323233127594\n",
      "Accuracy: 0.3909\n",
      "epoch: 940, training loss: 1.1978157758712769, testing loss: 1.7633228302001953\n",
      "Accuracy: 0.4114\n",
      "epoch: 940, training loss: 1.2371476888656616, testing loss: 1.8676844835281372\n",
      "Accuracy: 0.3689\n",
      "epoch: 940, training loss: 1.2809122800827026, testing loss: 1.7737337350845337\n",
      "Accuracy: 0.3865\n",
      "epoch: 940, training loss: 1.2249741554260254, testing loss: 1.6309269666671753\n",
      "Accuracy: 0.4158\n",
      "epoch: 940, training loss: 1.217007040977478, testing loss: 1.691678762435913\n",
      "Accuracy: 0.4167\n",
      "epoch: 940, training loss: 1.1610549688339233, testing loss: 1.59037446975708\n",
      "Accuracy: 0.4262\n",
      "epoch: 940, training loss: 1.2017555236816406, testing loss: 1.634590983390808\n",
      "Accuracy: 0.4574\n",
      "epoch: 940, training loss: 1.1960195302963257, testing loss: 1.835410237312317\n",
      "Accuracy: 0.3163\n",
      "epoch: 940, training loss: 1.2724878787994385, testing loss: 1.7605048418045044\n",
      "Accuracy: 0.4188\n",
      "epoch: 940, training loss: 1.2557601928710938, testing loss: 1.7093920707702637\n",
      "Accuracy: 0.4486\n",
      "epoch: 940, training loss: 1.2576735019683838, testing loss: 1.6257405281066895\n",
      "Accuracy: 0.4403\n",
      "epoch: 940, training loss: 1.2173571586608887, testing loss: 1.6730481386184692\n",
      "Accuracy: 0.4441\n",
      "epoch: 940, training loss: 1.2992216348648071, testing loss: 1.6398115158081055\n",
      "Accuracy: 0.4419\n",
      "epoch: 950, training loss: 1.3057523965835571, testing loss: 1.6778028011322021\n",
      "Accuracy: 0.4257\n",
      "epoch: 950, training loss: 1.185998558998108, testing loss: 1.8225414752960205\n",
      "Accuracy: 0.4107\n",
      "epoch: 950, training loss: 1.1970672607421875, testing loss: 1.6833590269088745\n",
      "Accuracy: 0.4128\n",
      "epoch: 950, training loss: 1.2834739685058594, testing loss: 1.7929021120071411\n",
      "Accuracy: 0.4034\n",
      "epoch: 950, training loss: 1.205501675605774, testing loss: 1.8153483867645264\n",
      "Accuracy: 0.3579\n",
      "epoch: 950, training loss: 1.267804503440857, testing loss: 1.7884972095489502\n",
      "Accuracy: 0.4343\n",
      "epoch: 950, training loss: 1.1877679824829102, testing loss: 1.6747609376907349\n",
      "Accuracy: 0.3951\n",
      "epoch: 950, training loss: 1.2273197174072266, testing loss: 1.7823169231414795\n",
      "Accuracy: 0.4043\n",
      "epoch: 950, training loss: 1.1998292207717896, testing loss: 1.7766311168670654\n",
      "Accuracy: 0.4225\n",
      "epoch: 950, training loss: 1.2266173362731934, testing loss: 1.8459945917129517\n",
      "Accuracy: 0.3816\n",
      "epoch: 950, training loss: 1.2730296850204468, testing loss: 1.5987145900726318\n",
      "Accuracy: 0.4152\n",
      "epoch: 950, training loss: 1.2894549369812012, testing loss: 1.8764499425888062\n",
      "Accuracy: 0.3851\n",
      "epoch: 950, training loss: 1.2499133348464966, testing loss: 1.8167264461517334\n",
      "Accuracy: 0.3462\n",
      "epoch: 950, training loss: 1.273514747619629, testing loss: 1.7437193393707275\n",
      "Accuracy: 0.4138\n",
      "epoch: 950, training loss: 1.2528055906295776, testing loss: 1.5431478023529053\n",
      "Accuracy: 0.4184\n",
      "epoch: 950, training loss: 1.247971773147583, testing loss: 1.7355091571807861\n",
      "Accuracy: 0.4478\n",
      "epoch: 950, training loss: 1.2269996404647827, testing loss: 1.6683992147445679\n",
      "Accuracy: 0.4239\n",
      "epoch: 950, training loss: 1.255476713180542, testing loss: 1.5032548904418945\n",
      "Accuracy: 0.4608\n",
      "epoch: 950, training loss: 1.2755547761917114, testing loss: 1.7495007514953613\n",
      "Accuracy: 0.3628\n",
      "epoch: 950, training loss: 1.2003748416900635, testing loss: 1.7989057302474976\n",
      "Accuracy: 0.3980\n",
      "epoch: 950, training loss: 1.2403931617736816, testing loss: 1.61073637008667\n",
      "Accuracy: 0.5185\n",
      "epoch: 950, training loss: 1.1527470350265503, testing loss: 1.7681382894515991\n",
      "Accuracy: 0.4018\n",
      "epoch: 950, training loss: 1.1683284044265747, testing loss: 1.8397672176361084\n",
      "Accuracy: 0.4277\n",
      "epoch: 950, training loss: 1.2044121026992798, testing loss: 1.748955249786377\n",
      "Accuracy: 0.4090\n",
      "epoch: 950, training loss: 1.2319003343582153, testing loss: 1.6948840618133545\n",
      "Accuracy: 0.3994\n",
      "epoch: 950, training loss: 1.1754798889160156, testing loss: 1.7469651699066162\n",
      "Accuracy: 0.3689\n",
      "epoch: 950, training loss: 1.2437025308609009, testing loss: 1.7557289600372314\n",
      "Accuracy: 0.4314\n",
      "epoch: 950, training loss: 1.2671149969100952, testing loss: 1.7975635528564453\n",
      "Accuracy: 0.4000\n",
      "epoch: 950, training loss: 1.1717437505722046, testing loss: 1.647489070892334\n",
      "Accuracy: 0.4417\n",
      "epoch: 950, training loss: 1.283463954925537, testing loss: 1.8504297733306885\n",
      "Accuracy: 0.3957\n",
      "epoch: 950, training loss: 1.2284787893295288, testing loss: 1.6617753505706787\n",
      "Accuracy: 0.4479\n",
      "epoch: 950, training loss: 1.223602056503296, testing loss: 1.8081892728805542\n",
      "Accuracy: 0.4038\n",
      "epoch: 950, training loss: 1.2635607719421387, testing loss: 1.6751233339309692\n",
      "Accuracy: 0.4136\n",
      "epoch: 950, training loss: 1.2025935649871826, testing loss: 1.8027716875076294\n",
      "Accuracy: 0.4236\n",
      "epoch: 950, training loss: 1.245702862739563, testing loss: 1.604341745376587\n",
      "Accuracy: 0.4821\n",
      "epoch: 950, training loss: 1.2861485481262207, testing loss: 1.735337257385254\n",
      "Accuracy: 0.3955\n",
      "epoch: 950, training loss: 1.239958643913269, testing loss: 1.8348345756530762\n",
      "Accuracy: 0.3528\n",
      "epoch: 950, training loss: 1.1774822473526, testing loss: 1.9174315929412842\n",
      "Accuracy: 0.3941\n",
      "epoch: 950, training loss: 1.202191710472107, testing loss: 1.6897302865982056\n",
      "Accuracy: 0.3892\n",
      "epoch: 950, training loss: 1.294631004333496, testing loss: 1.8116916418075562\n",
      "Accuracy: 0.4180\n",
      "epoch: 950, training loss: 1.2759584188461304, testing loss: 1.5049729347229004\n",
      "Accuracy: 0.5251\n",
      "epoch: 950, training loss: 1.2460706233978271, testing loss: 1.6970247030258179\n",
      "Accuracy: 0.4420\n",
      "epoch: 950, training loss: 1.2361174821853638, testing loss: 1.8663502931594849\n",
      "Accuracy: 0.4111\n",
      "epoch: 950, training loss: 1.2332007884979248, testing loss: 1.7472829818725586\n",
      "Accuracy: 0.4125\n",
      "epoch: 950, training loss: 1.179220199584961, testing loss: 1.7762019634246826\n",
      "Accuracy: 0.4465\n",
      "epoch: 950, training loss: 1.2172091007232666, testing loss: 1.6478285789489746\n",
      "Accuracy: 0.4545\n",
      "epoch: 950, training loss: 1.2597262859344482, testing loss: 1.794725775718689\n",
      "Accuracy: 0.4063\n",
      "epoch: 950, training loss: 1.2136685848236084, testing loss: 1.8552132844924927\n",
      "Accuracy: 0.3981\n",
      "epoch: 950, training loss: 1.232922911643982, testing loss: 1.9026339054107666\n",
      "Accuracy: 0.3947\n",
      "epoch: 950, training loss: 1.2640467882156372, testing loss: 1.9105005264282227\n",
      "Accuracy: 0.4248\n",
      "epoch: 950, training loss: 1.1804757118225098, testing loss: 1.6913857460021973\n",
      "Accuracy: 0.4281\n",
      "epoch: 950, training loss: 1.2699365615844727, testing loss: 1.6291147470474243\n",
      "Accuracy: 0.4215\n",
      "epoch: 950, training loss: 1.285563349723816, testing loss: 1.6918981075286865\n",
      "Accuracy: 0.3981\n",
      "epoch: 950, training loss: 1.1489232778549194, testing loss: 1.7627496719360352\n",
      "Accuracy: 0.3801\n",
      "epoch: 950, training loss: 1.278228998184204, testing loss: 1.7980514764785767\n",
      "Accuracy: 0.4523\n",
      "epoch: 950, training loss: 1.1641901731491089, testing loss: 1.638155460357666\n",
      "Accuracy: 0.4232\n",
      "epoch: 950, training loss: 1.2429068088531494, testing loss: 1.8822875022888184\n",
      "Accuracy: 0.3754\n",
      "epoch: 950, training loss: 1.2528760433197021, testing loss: 1.7064720392227173\n",
      "Accuracy: 0.4120\n",
      "epoch: 950, training loss: 1.1942429542541504, testing loss: 1.6606348752975464\n",
      "Accuracy: 0.4291\n",
      "epoch: 950, training loss: 1.2274432182312012, testing loss: 1.8805336952209473\n",
      "Accuracy: 0.4107\n",
      "epoch: 950, training loss: 1.2406256198883057, testing loss: 1.753818154335022\n",
      "Accuracy: 0.4094\n",
      "epoch: 950, training loss: 1.271117091178894, testing loss: 1.7641018629074097\n",
      "Accuracy: 0.3834\n",
      "epoch: 950, training loss: 1.2515789270401, testing loss: 1.6597076654434204\n",
      "Accuracy: 0.3969\n",
      "epoch: 950, training loss: 1.2337985038757324, testing loss: 1.7434710264205933\n",
      "Accuracy: 0.4503\n",
      "epoch: 950, training loss: 1.2662094831466675, testing loss: 1.6781635284423828\n",
      "Accuracy: 0.4030\n",
      "epoch: 950, training loss: 1.2020787000656128, testing loss: 1.7506498098373413\n",
      "Accuracy: 0.4207\n",
      "epoch: 950, training loss: 1.2244067192077637, testing loss: 1.6861605644226074\n",
      "Accuracy: 0.4020\n",
      "epoch: 950, training loss: 1.2143666744232178, testing loss: 1.8358086347579956\n",
      "Accuracy: 0.4147\n",
      "epoch: 950, training loss: 1.1677908897399902, testing loss: 1.6942384243011475\n",
      "Accuracy: 0.4351\n",
      "epoch: 950, training loss: 1.2787500619888306, testing loss: 1.712638020515442\n",
      "Accuracy: 0.4441\n",
      "epoch: 950, training loss: 1.2249869108200073, testing loss: 1.5599182844161987\n",
      "Accuracy: 0.4551\n",
      "epoch: 950, training loss: 1.3302178382873535, testing loss: 1.660481333732605\n",
      "Accuracy: 0.4097\n",
      "epoch: 950, training loss: 1.2416762113571167, testing loss: 1.7811965942382812\n",
      "Accuracy: 0.4330\n",
      "epoch: 950, training loss: 1.2473881244659424, testing loss: 1.672558307647705\n",
      "Accuracy: 0.4545\n",
      "epoch: 950, training loss: 1.2378613948822021, testing loss: 1.6674171686172485\n",
      "Accuracy: 0.4438\n",
      "epoch: 950, training loss: 1.2263518571853638, testing loss: 1.8140943050384521\n",
      "Accuracy: 0.4328\n",
      "epoch: 950, training loss: 1.2464001178741455, testing loss: 1.6888012886047363\n",
      "Accuracy: 0.3833\n",
      "epoch: 950, training loss: 1.2325587272644043, testing loss: 1.684171438217163\n",
      "Accuracy: 0.4314\n",
      "epoch: 950, training loss: 1.2793989181518555, testing loss: 1.727003812789917\n",
      "Accuracy: 0.4103\n",
      "epoch: 950, training loss: 1.2266974449157715, testing loss: 1.7795048952102661\n",
      "Accuracy: 0.4195\n",
      "epoch: 950, training loss: 1.2589672803878784, testing loss: 1.8790709972381592\n",
      "Accuracy: 0.3652\n",
      "epoch: 950, training loss: 1.2425646781921387, testing loss: 1.836923360824585\n",
      "Accuracy: 0.3951\n",
      "epoch: 950, training loss: 1.1618165969848633, testing loss: 1.6205753087997437\n",
      "Accuracy: 0.4286\n",
      "epoch: 950, training loss: 1.216325283050537, testing loss: 1.8989603519439697\n",
      "Accuracy: 0.3615\n",
      "epoch: 950, training loss: 1.2500971555709839, testing loss: 1.6175214052200317\n",
      "Accuracy: 0.4078\n",
      "epoch: 950, training loss: 1.2396351099014282, testing loss: 1.8042271137237549\n",
      "Accuracy: 0.4396\n",
      "epoch: 950, training loss: 1.3361713886260986, testing loss: 1.6857260465621948\n",
      "Accuracy: 0.4463\n",
      "epoch: 950, training loss: 1.1713298559188843, testing loss: 1.5708907842636108\n",
      "Accuracy: 0.4486\n",
      "epoch: 950, training loss: 1.2739061117172241, testing loss: 1.7305184602737427\n",
      "Accuracy: 0.3889\n",
      "epoch: 950, training loss: 1.2323884963989258, testing loss: 1.75905179977417\n",
      "Accuracy: 0.4221\n",
      "epoch: 950, training loss: 1.1827049255371094, testing loss: 1.6731839179992676\n",
      "Accuracy: 0.4263\n",
      "epoch: 950, training loss: 1.178113579750061, testing loss: 1.7412887811660767\n",
      "Accuracy: 0.3655\n",
      "epoch: 950, training loss: 1.2668230533599854, testing loss: 1.7256669998168945\n",
      "Accuracy: 0.3987\n",
      "epoch: 950, training loss: 1.2270008325576782, testing loss: 1.7292559146881104\n",
      "Accuracy: 0.3812\n",
      "epoch: 950, training loss: 1.28691828250885, testing loss: 1.7763901948928833\n",
      "Accuracy: 0.4014\n",
      "epoch: 950, training loss: 1.2635939121246338, testing loss: 1.7333719730377197\n",
      "Accuracy: 0.3936\n",
      "epoch: 950, training loss: 1.29891037940979, testing loss: 1.6154212951660156\n",
      "Accuracy: 0.4696\n",
      "epoch: 950, training loss: 1.3253791332244873, testing loss: 1.6830118894577026\n",
      "Accuracy: 0.4281\n",
      "epoch: 950, training loss: 1.2265692949295044, testing loss: 1.6880953311920166\n",
      "Accuracy: 0.4589\n",
      "epoch: 950, training loss: 1.250022292137146, testing loss: 1.6592713594436646\n",
      "Accuracy: 0.4548\n",
      "epoch: 960, training loss: 1.2232418060302734, testing loss: 1.8125625848770142\n",
      "Accuracy: 0.3481\n",
      "epoch: 960, training loss: 1.108325481414795, testing loss: 1.7302682399749756\n",
      "Accuracy: 0.3891\n",
      "epoch: 960, training loss: 1.2091917991638184, testing loss: 1.7984837293624878\n",
      "Accuracy: 0.3615\n",
      "epoch: 960, training loss: 1.2079486846923828, testing loss: 1.797992467880249\n",
      "Accuracy: 0.4182\n",
      "epoch: 960, training loss: 1.2089219093322754, testing loss: 1.8020637035369873\n",
      "Accuracy: 0.4114\n",
      "epoch: 960, training loss: 1.2203481197357178, testing loss: 1.6145436763763428\n",
      "Accuracy: 0.4267\n",
      "epoch: 960, training loss: 1.2367713451385498, testing loss: 1.6552821397781372\n",
      "Accuracy: 0.4299\n",
      "epoch: 960, training loss: 1.2556992769241333, testing loss: 1.7675491571426392\n",
      "Accuracy: 0.3960\n",
      "epoch: 960, training loss: 1.245029330253601, testing loss: 1.7008048295974731\n",
      "Accuracy: 0.3537\n",
      "epoch: 960, training loss: 1.2367130517959595, testing loss: 1.7504278421401978\n",
      "Accuracy: 0.4042\n",
      "epoch: 960, training loss: 1.207621455192566, testing loss: 1.7129771709442139\n",
      "Accuracy: 0.3831\n",
      "epoch: 960, training loss: 1.2491005659103394, testing loss: 1.7390103340148926\n",
      "Accuracy: 0.4311\n",
      "epoch: 960, training loss: 1.20340895652771, testing loss: 1.7303200960159302\n",
      "Accuracy: 0.4535\n",
      "epoch: 960, training loss: 1.3721410036087036, testing loss: 1.8094218969345093\n",
      "Accuracy: 0.3700\n",
      "epoch: 960, training loss: 1.2192647457122803, testing loss: 1.7663267850875854\n",
      "Accuracy: 0.4078\n",
      "epoch: 960, training loss: 1.2142084836959839, testing loss: 1.7957723140716553\n",
      "Accuracy: 0.3801\n",
      "epoch: 960, training loss: 1.247326135635376, testing loss: 1.618889331817627\n",
      "Accuracy: 0.4394\n",
      "epoch: 960, training loss: 1.2620244026184082, testing loss: 1.744310975074768\n",
      "Accuracy: 0.4371\n",
      "epoch: 960, training loss: 1.267171025276184, testing loss: 1.805816411972046\n",
      "Accuracy: 0.4227\n",
      "epoch: 960, training loss: 1.226624608039856, testing loss: 1.686828851699829\n",
      "Accuracy: 0.4147\n",
      "epoch: 960, training loss: 1.266639232635498, testing loss: 1.6749767065048218\n",
      "Accuracy: 0.4190\n",
      "epoch: 960, training loss: 1.238982081413269, testing loss: 1.6640740633010864\n",
      "Accuracy: 0.4094\n",
      "epoch: 960, training loss: 1.2543025016784668, testing loss: 1.802491307258606\n",
      "Accuracy: 0.4138\n",
      "epoch: 960, training loss: 1.190380573272705, testing loss: 1.6655027866363525\n",
      "Accuracy: 0.4656\n",
      "epoch: 960, training loss: 1.2604786157608032, testing loss: 1.834054946899414\n",
      "Accuracy: 0.3855\n",
      "epoch: 960, training loss: 1.195190191268921, testing loss: 1.7627183198928833\n",
      "Accuracy: 0.4430\n",
      "epoch: 960, training loss: 1.2375718355178833, testing loss: 1.7182813882827759\n",
      "Accuracy: 0.4308\n",
      "epoch: 960, training loss: 1.1356862783432007, testing loss: 1.7851173877716064\n",
      "Accuracy: 0.3966\n",
      "epoch: 960, training loss: 1.273415207862854, testing loss: 1.7525206804275513\n",
      "Accuracy: 0.4034\n",
      "epoch: 960, training loss: 1.277802586555481, testing loss: 1.8351765871047974\n",
      "Accuracy: 0.3970\n",
      "epoch: 960, training loss: 1.231121301651001, testing loss: 1.5400878190994263\n",
      "Accuracy: 0.4129\n",
      "epoch: 960, training loss: 1.2660030126571655, testing loss: 1.7059966325759888\n",
      "Accuracy: 0.3657\n",
      "epoch: 960, training loss: 1.2566092014312744, testing loss: 1.6586854457855225\n",
      "Accuracy: 0.4415\n",
      "epoch: 960, training loss: 1.232185959815979, testing loss: 1.6674550771713257\n",
      "Accuracy: 0.4026\n",
      "epoch: 960, training loss: 1.2157658338546753, testing loss: 1.7587435245513916\n",
      "Accuracy: 0.4097\n",
      "epoch: 960, training loss: 1.2375742197036743, testing loss: 1.8138937950134277\n",
      "Accuracy: 0.4266\n",
      "epoch: 960, training loss: 1.205100417137146, testing loss: 1.7274081707000732\n",
      "Accuracy: 0.4089\n",
      "epoch: 960, training loss: 1.248573660850525, testing loss: 1.8030158281326294\n",
      "Accuracy: 0.3980\n",
      "epoch: 960, training loss: 1.2394931316375732, testing loss: 1.7480965852737427\n",
      "Accuracy: 0.3981\n",
      "epoch: 960, training loss: 1.2791154384613037, testing loss: 1.8568189144134521\n",
      "Accuracy: 0.4108\n",
      "epoch: 960, training loss: 1.2980643510818481, testing loss: 1.7999684810638428\n",
      "Accuracy: 0.3969\n",
      "epoch: 960, training loss: 1.2380099296569824, testing loss: 1.6973321437835693\n",
      "Accuracy: 0.4581\n",
      "epoch: 960, training loss: 1.2096226215362549, testing loss: 1.7138850688934326\n",
      "Accuracy: 0.3966\n",
      "epoch: 960, training loss: 1.2587032318115234, testing loss: 1.7453479766845703\n",
      "Accuracy: 0.4346\n",
      "epoch: 960, training loss: 1.1532628536224365, testing loss: 1.7816938161849976\n",
      "Accuracy: 0.4247\n",
      "epoch: 960, training loss: 1.290219783782959, testing loss: 1.6769633293151855\n",
      "Accuracy: 0.4280\n",
      "epoch: 960, training loss: 1.2699332237243652, testing loss: 1.8008110523223877\n",
      "Accuracy: 0.3973\n",
      "epoch: 960, training loss: 1.2606310844421387, testing loss: 1.7091773748397827\n",
      "Accuracy: 0.3981\n",
      "epoch: 960, training loss: 1.1677143573760986, testing loss: 1.7935007810592651\n",
      "Accuracy: 0.3771\n",
      "epoch: 960, training loss: 1.1866776943206787, testing loss: 1.7280279397964478\n",
      "Accuracy: 0.3734\n",
      "epoch: 960, training loss: 1.2260938882827759, testing loss: 1.6668133735656738\n",
      "Accuracy: 0.3849\n",
      "epoch: 960, training loss: 1.265097737312317, testing loss: 1.6994901895523071\n",
      "Accuracy: 0.4038\n",
      "epoch: 960, training loss: 1.2508618831634521, testing loss: 1.7857377529144287\n",
      "Accuracy: 0.4696\n",
      "epoch: 960, training loss: 1.2734235525131226, testing loss: 1.764975905418396\n",
      "Accuracy: 0.4281\n",
      "epoch: 960, training loss: 1.2753874063491821, testing loss: 1.6755324602127075\n",
      "Accuracy: 0.3885\n",
      "epoch: 960, training loss: 1.2176823616027832, testing loss: 1.7240816354751587\n",
      "Accuracy: 0.4142\n",
      "epoch: 960, training loss: 1.2831515073776245, testing loss: 1.8109992742538452\n",
      "Accuracy: 0.4176\n",
      "epoch: 960, training loss: 1.2600070238113403, testing loss: 1.569857120513916\n",
      "Accuracy: 0.4325\n",
      "epoch: 960, training loss: 1.2723761796951294, testing loss: 1.821306824684143\n",
      "Accuracy: 0.3769\n",
      "epoch: 960, training loss: 1.2151837348937988, testing loss: 1.8502154350280762\n",
      "Accuracy: 0.3814\n",
      "epoch: 960, training loss: 1.2124160528182983, testing loss: 1.784856915473938\n",
      "Accuracy: 0.4192\n",
      "epoch: 960, training loss: 1.2641137838363647, testing loss: 1.798835039138794\n",
      "Accuracy: 0.3853\n",
      "epoch: 960, training loss: 1.2024624347686768, testing loss: 1.6600466966629028\n",
      "Accuracy: 0.4502\n",
      "epoch: 960, training loss: 1.3013489246368408, testing loss: 1.675601601600647\n",
      "Accuracy: 0.4315\n",
      "epoch: 960, training loss: 1.207540512084961, testing loss: 1.8204119205474854\n",
      "Accuracy: 0.4007\n",
      "epoch: 960, training loss: 1.295993685722351, testing loss: 1.5972703695297241\n",
      "Accuracy: 0.4688\n",
      "epoch: 960, training loss: 1.249759316444397, testing loss: 1.785800576210022\n",
      "Accuracy: 0.4516\n",
      "epoch: 960, training loss: 1.2492133378982544, testing loss: 1.7976558208465576\n",
      "Accuracy: 0.4092\n",
      "epoch: 960, training loss: 1.191994071006775, testing loss: 1.859237551689148\n",
      "Accuracy: 0.4193\n",
      "epoch: 960, training loss: 1.2019826173782349, testing loss: 1.8297902345657349\n",
      "Accuracy: 0.3729\n",
      "epoch: 960, training loss: 1.1926084756851196, testing loss: 1.7235411405563354\n",
      "Accuracy: 0.4291\n",
      "epoch: 960, training loss: 1.2621474266052246, testing loss: 1.8496214151382446\n",
      "Accuracy: 0.4028\n",
      "epoch: 960, training loss: 1.284036636352539, testing loss: 1.7847440242767334\n",
      "Accuracy: 0.4058\n",
      "epoch: 960, training loss: 1.1979745626449585, testing loss: 1.6956137418746948\n",
      "Accuracy: 0.4231\n",
      "epoch: 960, training loss: 1.1752893924713135, testing loss: 1.844591498374939\n",
      "Accuracy: 0.3742\n",
      "epoch: 960, training loss: 1.2492886781692505, testing loss: 1.7774444818496704\n",
      "Accuracy: 0.4056\n",
      "epoch: 960, training loss: 1.2919262647628784, testing loss: 1.8447476625442505\n",
      "Accuracy: 0.3937\n",
      "epoch: 960, training loss: 1.2237147092819214, testing loss: 1.7686723470687866\n",
      "Accuracy: 0.4239\n",
      "epoch: 960, training loss: 1.242374300956726, testing loss: 1.6937463283538818\n",
      "Accuracy: 0.4237\n",
      "epoch: 960, training loss: 1.2128397226333618, testing loss: 1.6683883666992188\n",
      "Accuracy: 0.4314\n",
      "epoch: 960, training loss: 1.2588222026824951, testing loss: 1.7020338773727417\n",
      "Accuracy: 0.4462\n",
      "epoch: 960, training loss: 1.2006174325942993, testing loss: 1.7859835624694824\n",
      "Accuracy: 0.4180\n",
      "epoch: 960, training loss: 1.2131195068359375, testing loss: 1.6355111598968506\n",
      "Accuracy: 0.4116\n",
      "epoch: 960, training loss: 1.271127462387085, testing loss: 1.8045181035995483\n",
      "Accuracy: 0.3556\n",
      "epoch: 960, training loss: 1.308132529258728, testing loss: 1.7842276096343994\n",
      "Accuracy: 0.4273\n",
      "epoch: 960, training loss: 1.215266466140747, testing loss: 1.6131449937820435\n",
      "Accuracy: 0.4339\n",
      "epoch: 960, training loss: 1.2523268461227417, testing loss: 1.7637721300125122\n",
      "Accuracy: 0.4148\n",
      "epoch: 960, training loss: 1.1902844905853271, testing loss: 1.725659966468811\n",
      "Accuracy: 0.4568\n",
      "epoch: 960, training loss: 1.1930429935455322, testing loss: 1.6618024110794067\n",
      "Accuracy: 0.4373\n",
      "epoch: 960, training loss: 1.2217016220092773, testing loss: 1.5420037508010864\n",
      "Accuracy: 0.4403\n",
      "epoch: 960, training loss: 1.2251901626586914, testing loss: 1.5899488925933838\n",
      "Accuracy: 0.4195\n",
      "epoch: 960, training loss: 1.2654321193695068, testing loss: 1.6930997371673584\n",
      "Accuracy: 0.3957\n",
      "epoch: 960, training loss: 1.264551043510437, testing loss: 1.7596639394760132\n",
      "Accuracy: 0.4642\n",
      "epoch: 960, training loss: 1.2411364316940308, testing loss: 1.7523574829101562\n",
      "Accuracy: 0.4069\n",
      "epoch: 960, training loss: 1.2454915046691895, testing loss: 1.742802381515503\n",
      "Accuracy: 0.4136\n",
      "epoch: 960, training loss: 1.1724200248718262, testing loss: 1.8429473638534546\n",
      "Accuracy: 0.3793\n",
      "epoch: 960, training loss: 1.2192925214767456, testing loss: 1.6551748514175415\n",
      "Accuracy: 0.4092\n",
      "epoch: 960, training loss: 1.247630000114441, testing loss: 1.8578574657440186\n",
      "Accuracy: 0.3770\n",
      "epoch: 960, training loss: 1.2251100540161133, testing loss: 1.7792253494262695\n",
      "Accuracy: 0.4228\n",
      "epoch: 960, training loss: 1.2581768035888672, testing loss: 1.5983153581619263\n",
      "Accuracy: 0.4354\n",
      "epoch: 970, training loss: 1.2577357292175293, testing loss: 1.6395702362060547\n",
      "Accuracy: 0.4262\n",
      "epoch: 970, training loss: 1.1773602962493896, testing loss: 1.906686782836914\n",
      "Accuracy: 0.4014\n",
      "epoch: 970, training loss: 1.1836951971054077, testing loss: 1.7439550161361694\n",
      "Accuracy: 0.4130\n",
      "epoch: 970, training loss: 1.2672724723815918, testing loss: 1.65079665184021\n",
      "Accuracy: 0.4573\n",
      "epoch: 970, training loss: 1.1853653192520142, testing loss: 1.725037932395935\n",
      "Accuracy: 0.4038\n",
      "epoch: 970, training loss: 1.1882250308990479, testing loss: 1.7572585344314575\n",
      "Accuracy: 0.3874\n",
      "epoch: 970, training loss: 1.224096417427063, testing loss: 1.6818749904632568\n",
      "Accuracy: 0.4554\n",
      "epoch: 970, training loss: 1.2129263877868652, testing loss: 1.7740880250930786\n",
      "Accuracy: 0.4143\n",
      "epoch: 970, training loss: 1.2376819849014282, testing loss: 1.6634536981582642\n",
      "Accuracy: 0.4136\n",
      "epoch: 970, training loss: 1.261285662651062, testing loss: 1.7974348068237305\n",
      "Accuracy: 0.3897\n",
      "epoch: 970, training loss: 1.2130138874053955, testing loss: 1.7552584409713745\n",
      "Accuracy: 0.4226\n",
      "epoch: 970, training loss: 1.268494963645935, testing loss: 1.6994532346725464\n",
      "Accuracy: 0.4545\n",
      "epoch: 970, training loss: 1.3019094467163086, testing loss: 1.7151837348937988\n",
      "Accuracy: 0.4018\n",
      "epoch: 970, training loss: 1.2950754165649414, testing loss: 1.6595113277435303\n",
      "Accuracy: 0.4702\n",
      "epoch: 970, training loss: 1.2594616413116455, testing loss: 1.7898950576782227\n",
      "Accuracy: 0.3994\n",
      "epoch: 970, training loss: 1.33066725730896, testing loss: 1.7175202369689941\n",
      "Accuracy: 0.3894\n",
      "epoch: 970, training loss: 1.209747552871704, testing loss: 1.5916029214859009\n",
      "Accuracy: 0.4570\n",
      "epoch: 970, training loss: 1.235663652420044, testing loss: 1.6760907173156738\n",
      "Accuracy: 0.4327\n",
      "epoch: 970, training loss: 1.2086185216903687, testing loss: 1.7431367635726929\n",
      "Accuracy: 0.3722\n",
      "epoch: 970, training loss: 1.2886278629302979, testing loss: 1.7830829620361328\n",
      "Accuracy: 0.4215\n",
      "epoch: 970, training loss: 1.196724534034729, testing loss: 1.6754385232925415\n",
      "Accuracy: 0.4235\n",
      "epoch: 970, training loss: 1.2356140613555908, testing loss: 1.47007155418396\n",
      "Accuracy: 0.4966\n",
      "epoch: 970, training loss: 1.2273088693618774, testing loss: 1.8412387371063232\n",
      "Accuracy: 0.4038\n",
      "epoch: 970, training loss: 1.258052945137024, testing loss: 1.6921319961547852\n",
      "Accuracy: 0.3948\n",
      "epoch: 970, training loss: 1.2111198902130127, testing loss: 1.7932157516479492\n",
      "Accuracy: 0.4069\n",
      "epoch: 970, training loss: 1.173246145248413, testing loss: 1.7706174850463867\n",
      "Accuracy: 0.3882\n",
      "epoch: 970, training loss: 1.23305082321167, testing loss: 1.6481869220733643\n",
      "Accuracy: 0.4371\n",
      "epoch: 970, training loss: 1.2525404691696167, testing loss: 1.905299186706543\n",
      "Accuracy: 0.4152\n",
      "epoch: 970, training loss: 1.227718710899353, testing loss: 1.7529932260513306\n",
      "Accuracy: 0.4026\n",
      "epoch: 970, training loss: 1.2449793815612793, testing loss: 1.7226842641830444\n",
      "Accuracy: 0.3784\n",
      "epoch: 970, training loss: 1.2369264364242554, testing loss: 1.7688558101654053\n",
      "Accuracy: 0.4071\n",
      "epoch: 970, training loss: 1.2461498975753784, testing loss: 1.7370280027389526\n",
      "Accuracy: 0.4123\n",
      "epoch: 970, training loss: 1.2210332155227661, testing loss: 1.623549222946167\n",
      "Accuracy: 0.4479\n",
      "epoch: 970, training loss: 1.21669340133667, testing loss: 1.7157179117202759\n",
      "Accuracy: 0.4261\n",
      "epoch: 970, training loss: 1.2467988729476929, testing loss: 1.6924762725830078\n",
      "Accuracy: 0.4341\n",
      "epoch: 970, training loss: 1.2749046087265015, testing loss: 1.7055014371871948\n",
      "Accuracy: 0.4560\n",
      "epoch: 970, training loss: 1.2064651250839233, testing loss: 1.7035890817642212\n",
      "Accuracy: 0.4381\n",
      "epoch: 970, training loss: 1.2391304969787598, testing loss: 1.7949944734573364\n",
      "Accuracy: 0.4080\n",
      "epoch: 970, training loss: 1.1735467910766602, testing loss: 1.5772161483764648\n",
      "Accuracy: 0.4414\n",
      "epoch: 970, training loss: 1.2239638566970825, testing loss: 1.7415157556533813\n",
      "Accuracy: 0.4219\n",
      "epoch: 970, training loss: 1.2367266416549683, testing loss: 1.7229950428009033\n",
      "Accuracy: 0.4177\n",
      "epoch: 970, training loss: 1.237291932106018, testing loss: 1.830318570137024\n",
      "Accuracy: 0.4027\n",
      "epoch: 970, training loss: 1.213366150856018, testing loss: 1.7705237865447998\n",
      "Accuracy: 0.3924\n",
      "epoch: 970, training loss: 1.2785919904708862, testing loss: 1.6902916431427002\n",
      "Accuracy: 0.4136\n",
      "epoch: 970, training loss: 1.2721502780914307, testing loss: 1.7562661170959473\n",
      "Accuracy: 0.4381\n",
      "epoch: 970, training loss: 1.193860411643982, testing loss: 1.8013042211532593\n",
      "Accuracy: 0.4069\n",
      "epoch: 970, training loss: 1.2371206283569336, testing loss: 1.6708500385284424\n",
      "Accuracy: 0.4490\n",
      "epoch: 970, training loss: 1.298609972000122, testing loss: 1.6505167484283447\n",
      "Accuracy: 0.4396\n",
      "epoch: 970, training loss: 1.225044846534729, testing loss: 1.7980725765228271\n",
      "Accuracy: 0.4419\n",
      "epoch: 970, training loss: 1.2870876789093018, testing loss: 1.886330008506775\n",
      "Accuracy: 0.3950\n",
      "epoch: 970, training loss: 1.2542387247085571, testing loss: 1.8693432807922363\n",
      "Accuracy: 0.4006\n",
      "epoch: 970, training loss: 1.3250200748443604, testing loss: 1.7252981662750244\n",
      "Accuracy: 0.4156\n",
      "epoch: 970, training loss: 1.2521380186080933, testing loss: 1.637637734413147\n",
      "Accuracy: 0.4412\n",
      "epoch: 970, training loss: 1.2776089906692505, testing loss: 1.8409926891326904\n",
      "Accuracy: 0.3796\n",
      "epoch: 970, training loss: 1.2059028148651123, testing loss: 1.6393460035324097\n",
      "Accuracy: 0.4164\n",
      "epoch: 970, training loss: 1.2932138442993164, testing loss: 1.6068134307861328\n",
      "Accuracy: 0.4325\n",
      "epoch: 970, training loss: 1.2991679906845093, testing loss: 1.7825859785079956\n",
      "Accuracy: 0.3691\n",
      "epoch: 970, training loss: 1.2210376262664795, testing loss: 1.6300336122512817\n",
      "Accuracy: 0.4330\n",
      "epoch: 970, training loss: 1.2906521558761597, testing loss: 1.8062067031860352\n",
      "Accuracy: 0.4126\n",
      "epoch: 970, training loss: 1.2025936841964722, testing loss: 1.7757693529129028\n",
      "Accuracy: 0.3963\n",
      "epoch: 970, training loss: 1.2343922853469849, testing loss: 1.682610034942627\n",
      "Accuracy: 0.4130\n",
      "epoch: 970, training loss: 1.2036752700805664, testing loss: 1.647355079650879\n",
      "Accuracy: 0.4258\n",
      "epoch: 970, training loss: 1.240487813949585, testing loss: 1.713236689567566\n",
      "Accuracy: 0.4510\n",
      "epoch: 970, training loss: 1.221160888671875, testing loss: 1.692823052406311\n",
      "Accuracy: 0.4146\n",
      "epoch: 970, training loss: 1.1409772634506226, testing loss: 1.6676733493804932\n",
      "Accuracy: 0.3987\n",
      "epoch: 970, training loss: 1.151889443397522, testing loss: 1.6671761274337769\n",
      "Accuracy: 0.4532\n",
      "epoch: 970, training loss: 1.2248177528381348, testing loss: 1.60197913646698\n",
      "Accuracy: 0.4237\n",
      "epoch: 970, training loss: 1.2509502172470093, testing loss: 1.6816433668136597\n",
      "Accuracy: 0.4305\n",
      "epoch: 970, training loss: 1.189139723777771, testing loss: 1.6379907131195068\n",
      "Accuracy: 0.4094\n",
      "epoch: 970, training loss: 1.2752184867858887, testing loss: 1.9449950456619263\n",
      "Accuracy: 0.4272\n",
      "epoch: 970, training loss: 1.248336911201477, testing loss: 1.6333801746368408\n",
      "Accuracy: 0.3878\n",
      "epoch: 970, training loss: 1.2981557846069336, testing loss: 1.8852392435073853\n",
      "Accuracy: 0.3980\n",
      "epoch: 970, training loss: 1.2099707126617432, testing loss: 1.787715196609497\n",
      "Accuracy: 0.4207\n",
      "epoch: 970, training loss: 1.295080542564392, testing loss: 1.7322361469268799\n",
      "Accuracy: 0.4064\n",
      "epoch: 970, training loss: 1.2243518829345703, testing loss: 1.7731963396072388\n",
      "Accuracy: 0.4167\n",
      "epoch: 970, training loss: 1.2259835004806519, testing loss: 1.5134254693984985\n",
      "Accuracy: 0.5114\n",
      "epoch: 970, training loss: 1.2386233806610107, testing loss: 1.9212188720703125\n",
      "Accuracy: 0.4026\n",
      "epoch: 970, training loss: 1.2149367332458496, testing loss: 1.6534241437911987\n",
      "Accuracy: 0.4307\n",
      "epoch: 970, training loss: 1.1907304525375366, testing loss: 1.589660406112671\n",
      "Accuracy: 0.4399\n",
      "epoch: 970, training loss: 1.2083139419555664, testing loss: 1.8044356107711792\n",
      "Accuracy: 0.3870\n",
      "epoch: 970, training loss: 1.2203607559204102, testing loss: 1.7901967763900757\n",
      "Accuracy: 0.3875\n",
      "epoch: 970, training loss: 1.2683496475219727, testing loss: 1.6910052299499512\n",
      "Accuracy: 0.4146\n",
      "epoch: 970, training loss: 1.3136825561523438, testing loss: 1.7547131776809692\n",
      "Accuracy: 0.3766\n",
      "epoch: 970, training loss: 1.1513488292694092, testing loss: 1.6276063919067383\n",
      "Accuracy: 0.3994\n",
      "epoch: 970, training loss: 1.2086071968078613, testing loss: 1.6309291124343872\n",
      "Accuracy: 0.4448\n",
      "epoch: 970, training loss: 1.2054367065429688, testing loss: 1.8872443437576294\n",
      "Accuracy: 0.3799\n",
      "epoch: 970, training loss: 1.1914396286010742, testing loss: 1.7429722547531128\n",
      "Accuracy: 0.4300\n",
      "epoch: 970, training loss: 1.2519248723983765, testing loss: 1.6986048221588135\n",
      "Accuracy: 0.4416\n",
      "epoch: 970, training loss: 1.2944391965866089, testing loss: 1.7241928577423096\n",
      "Accuracy: 0.4433\n",
      "epoch: 970, training loss: 1.215780258178711, testing loss: 1.6940593719482422\n",
      "Accuracy: 0.4148\n",
      "epoch: 970, training loss: 1.241625189781189, testing loss: 1.7434548139572144\n",
      "Accuracy: 0.3732\n",
      "epoch: 970, training loss: 1.2588568925857544, testing loss: 1.752335786819458\n",
      "Accuracy: 0.3988\n",
      "epoch: 970, training loss: 1.1769945621490479, testing loss: 1.6814121007919312\n",
      "Accuracy: 0.4167\n",
      "epoch: 970, training loss: 1.2737659215927124, testing loss: 1.7664287090301514\n",
      "Accuracy: 0.4479\n",
      "epoch: 970, training loss: 1.311558723449707, testing loss: 1.8415429592132568\n",
      "Accuracy: 0.3590\n",
      "epoch: 970, training loss: 1.3098742961883545, testing loss: 1.5835983753204346\n",
      "Accuracy: 0.4759\n",
      "epoch: 970, training loss: 1.23490309715271, testing loss: 1.627488136291504\n",
      "Accuracy: 0.4375\n",
      "epoch: 970, training loss: 1.251924753189087, testing loss: 1.6389962434768677\n",
      "Accuracy: 0.4222\n",
      "epoch: 970, training loss: 1.2161264419555664, testing loss: 1.6300002336502075\n",
      "Accuracy: 0.4739\n",
      "epoch: 970, training loss: 1.2221287488937378, testing loss: 1.6449754238128662\n",
      "Accuracy: 0.4393\n",
      "epoch: 980, training loss: 1.2924729585647583, testing loss: 1.6690926551818848\n",
      "Accuracy: 0.4373\n",
      "epoch: 980, training loss: 1.211559772491455, testing loss: 1.8684489727020264\n",
      "Accuracy: 0.3902\n",
      "epoch: 980, training loss: 1.2243229150772095, testing loss: 1.8335784673690796\n",
      "Accuracy: 0.3783\n",
      "epoch: 980, training loss: 1.1971325874328613, testing loss: 1.7196825742721558\n",
      "Accuracy: 0.4202\n",
      "epoch: 980, training loss: 1.2912687063217163, testing loss: 1.8037006855010986\n",
      "Accuracy: 0.3766\n",
      "epoch: 980, training loss: 1.2074145078659058, testing loss: 1.6871700286865234\n",
      "Accuracy: 0.4613\n",
      "epoch: 980, training loss: 1.2409296035766602, testing loss: 1.9129794836044312\n",
      "Accuracy: 0.3705\n",
      "epoch: 980, training loss: 1.2042359113693237, testing loss: 1.7266206741333008\n",
      "Accuracy: 0.4062\n",
      "epoch: 980, training loss: 1.1762487888336182, testing loss: 1.7065024375915527\n",
      "Accuracy: 0.4136\n",
      "epoch: 980, training loss: 1.2763276100158691, testing loss: 1.7795920372009277\n",
      "Accuracy: 0.3849\n",
      "epoch: 980, training loss: 1.2104741334915161, testing loss: 1.6595338582992554\n",
      "Accuracy: 0.4320\n",
      "epoch: 980, training loss: 1.277099847793579, testing loss: 1.7142677307128906\n",
      "Accuracy: 0.4074\n",
      "epoch: 980, training loss: 1.1831382513046265, testing loss: 1.6617231369018555\n",
      "Accuracy: 0.4399\n",
      "epoch: 980, training loss: 1.2269841432571411, testing loss: 1.7199259996414185\n",
      "Accuracy: 0.4055\n",
      "epoch: 980, training loss: 1.1633552312850952, testing loss: 1.5991121530532837\n",
      "Accuracy: 0.4414\n",
      "epoch: 980, training loss: 1.2011157274246216, testing loss: 1.7143909931182861\n",
      "Accuracy: 0.4158\n",
      "epoch: 980, training loss: 1.2154659032821655, testing loss: 1.7189786434173584\n",
      "Accuracy: 0.4387\n",
      "epoch: 980, training loss: 1.2285243272781372, testing loss: 1.616616129875183\n",
      "Accuracy: 0.4233\n",
      "epoch: 980, training loss: 1.2964850664138794, testing loss: 1.7961019277572632\n",
      "Accuracy: 0.4264\n",
      "epoch: 980, training loss: 1.2894752025604248, testing loss: 1.6596100330352783\n",
      "Accuracy: 0.4394\n",
      "epoch: 980, training loss: 1.200259804725647, testing loss: 1.7308239936828613\n",
      "Accuracy: 0.4106\n",
      "epoch: 980, training loss: 1.2348054647445679, testing loss: 1.784653663635254\n",
      "Accuracy: 0.3918\n",
      "epoch: 980, training loss: 1.178205132484436, testing loss: 1.8135621547698975\n",
      "Accuracy: 0.4000\n",
      "epoch: 980, training loss: 1.205188512802124, testing loss: 1.6803760528564453\n",
      "Accuracy: 0.4212\n",
      "epoch: 980, training loss: 1.2532035112380981, testing loss: 1.6882787942886353\n",
      "Accuracy: 0.4034\n",
      "epoch: 980, training loss: 1.238216519355774, testing loss: 1.6521937847137451\n",
      "Accuracy: 0.3986\n",
      "epoch: 980, training loss: 1.1614630222320557, testing loss: 1.7640413045883179\n",
      "Accuracy: 0.4385\n",
      "epoch: 980, training loss: 1.2860102653503418, testing loss: 1.7566848993301392\n",
      "Accuracy: 0.4188\n",
      "epoch: 980, training loss: 1.3358889818191528, testing loss: 1.6542176008224487\n",
      "Accuracy: 0.4276\n",
      "epoch: 980, training loss: 1.205295443534851, testing loss: 1.693372368812561\n",
      "Accuracy: 0.4239\n",
      "epoch: 980, training loss: 1.233155369758606, testing loss: 1.5630242824554443\n",
      "Accuracy: 0.4811\n",
      "epoch: 980, training loss: 1.2074414491653442, testing loss: 1.75473952293396\n",
      "Accuracy: 0.3738\n",
      "epoch: 980, training loss: 1.1791715621948242, testing loss: 1.8609890937805176\n",
      "Accuracy: 0.3805\n",
      "epoch: 980, training loss: 1.2178802490234375, testing loss: 1.748199701309204\n",
      "Accuracy: 0.4345\n",
      "epoch: 980, training loss: 1.2420387268066406, testing loss: 1.804707646369934\n",
      "Accuracy: 0.3925\n",
      "epoch: 980, training loss: 1.2513344287872314, testing loss: 1.684920310974121\n",
      "Accuracy: 0.4500\n",
      "epoch: 980, training loss: 1.2865684032440186, testing loss: 1.8472875356674194\n",
      "Accuracy: 0.4180\n",
      "epoch: 980, training loss: 1.2082431316375732, testing loss: 1.7216545343399048\n",
      "Accuracy: 0.4214\n",
      "epoch: 980, training loss: 1.2306126356124878, testing loss: 1.706984043121338\n",
      "Accuracy: 0.4260\n",
      "epoch: 980, training loss: 1.3031296730041504, testing loss: 1.7123266458511353\n",
      "Accuracy: 0.4101\n",
      "epoch: 980, training loss: 1.2710071802139282, testing loss: 1.605353593826294\n",
      "Accuracy: 0.4404\n",
      "epoch: 980, training loss: 1.1809656620025635, testing loss: 1.6490081548690796\n",
      "Accuracy: 0.4272\n",
      "epoch: 980, training loss: 1.2559853792190552, testing loss: 1.7454373836517334\n",
      "Accuracy: 0.4030\n",
      "epoch: 980, training loss: 1.2355165481567383, testing loss: 1.6342172622680664\n",
      "Accuracy: 0.4874\n",
      "epoch: 980, training loss: 1.235230803489685, testing loss: 1.6713167428970337\n",
      "Accuracy: 0.3876\n",
      "epoch: 980, training loss: 1.218461275100708, testing loss: 1.79030442237854\n",
      "Accuracy: 0.4225\n",
      "epoch: 980, training loss: 1.2860660552978516, testing loss: 1.7127118110656738\n",
      "Accuracy: 0.4199\n",
      "epoch: 980, training loss: 1.2714685201644897, testing loss: 1.7336055040359497\n",
      "Accuracy: 0.4062\n",
      "epoch: 980, training loss: 1.2063945531845093, testing loss: 1.7772324085235596\n",
      "Accuracy: 0.4435\n",
      "epoch: 980, training loss: 1.2199910879135132, testing loss: 1.8115578889846802\n",
      "Accuracy: 0.3550\n",
      "epoch: 980, training loss: 1.2590800523757935, testing loss: 1.7822891473770142\n",
      "Accuracy: 0.3938\n",
      "epoch: 980, training loss: 1.3148272037506104, testing loss: 1.7900323867797852\n",
      "Accuracy: 0.3754\n",
      "epoch: 980, training loss: 1.179147720336914, testing loss: 1.7718127965927124\n",
      "Accuracy: 0.3811\n",
      "epoch: 980, training loss: 1.2728855609893799, testing loss: 1.8817071914672852\n",
      "Accuracy: 0.4225\n",
      "epoch: 980, training loss: 1.192893385887146, testing loss: 1.64761221408844\n",
      "Accuracy: 0.4413\n",
      "epoch: 980, training loss: 1.2675011157989502, testing loss: 1.7850106954574585\n",
      "Accuracy: 0.4201\n",
      "epoch: 980, training loss: 1.2589266300201416, testing loss: 1.826927661895752\n",
      "Accuracy: 0.3831\n",
      "epoch: 980, training loss: 1.250466227531433, testing loss: 1.7664543390274048\n",
      "Accuracy: 0.4119\n",
      "epoch: 980, training loss: 1.2402453422546387, testing loss: 1.7759168148040771\n",
      "Accuracy: 0.4116\n",
      "epoch: 980, training loss: 1.293050765991211, testing loss: 1.7841846942901611\n",
      "Accuracy: 0.3939\n",
      "epoch: 980, training loss: 1.1833137273788452, testing loss: 1.7114686965942383\n",
      "Accuracy: 0.4007\n",
      "epoch: 980, training loss: 1.2624139785766602, testing loss: 1.663825273513794\n",
      "Accuracy: 0.4100\n",
      "epoch: 980, training loss: 1.304904818534851, testing loss: 1.7596993446350098\n",
      "Accuracy: 0.3675\n",
      "epoch: 980, training loss: 1.294416904449463, testing loss: 1.7321302890777588\n",
      "Accuracy: 0.4019\n",
      "epoch: 980, training loss: 1.240843653678894, testing loss: 1.7152214050292969\n",
      "Accuracy: 0.4181\n",
      "epoch: 980, training loss: 1.247349500656128, testing loss: 1.7244162559509277\n",
      "Accuracy: 0.4088\n",
      "epoch: 980, training loss: 1.2176933288574219, testing loss: 1.6967521905899048\n",
      "Accuracy: 0.4431\n",
      "epoch: 980, training loss: 1.2810996770858765, testing loss: 1.8016953468322754\n",
      "Accuracy: 0.3795\n",
      "epoch: 980, training loss: 1.1808476448059082, testing loss: 1.6482540369033813\n",
      "Accuracy: 0.3975\n",
      "epoch: 980, training loss: 1.20865797996521, testing loss: 1.5536913871765137\n",
      "Accuracy: 0.4665\n",
      "epoch: 980, training loss: 1.1885005235671997, testing loss: 1.613996148109436\n",
      "Accuracy: 0.4416\n",
      "epoch: 980, training loss: 1.2490633726119995, testing loss: 1.747276782989502\n",
      "Accuracy: 0.4057\n",
      "epoch: 980, training loss: 1.2164520025253296, testing loss: 1.6561925411224365\n",
      "Accuracy: 0.3883\n",
      "epoch: 980, training loss: 1.200191855430603, testing loss: 1.5939549207687378\n",
      "Accuracy: 0.4502\n",
      "epoch: 980, training loss: 1.2049076557159424, testing loss: 1.752617597579956\n",
      "Accuracy: 0.3953\n",
      "epoch: 980, training loss: 1.1743180751800537, testing loss: 1.82001531124115\n",
      "Accuracy: 0.3859\n",
      "epoch: 980, training loss: 1.2915167808532715, testing loss: 1.7523176670074463\n",
      "Accuracy: 0.4232\n",
      "epoch: 980, training loss: 1.2361161708831787, testing loss: 1.6531364917755127\n",
      "Accuracy: 0.3944\n",
      "epoch: 980, training loss: 1.183358907699585, testing loss: 1.699916958808899\n",
      "Accuracy: 0.4162\n",
      "epoch: 980, training loss: 1.2219445705413818, testing loss: 1.751258373260498\n",
      "Accuracy: 0.4169\n",
      "epoch: 980, training loss: 1.2807966470718384, testing loss: 1.5440152883529663\n",
      "Accuracy: 0.4281\n",
      "epoch: 980, training loss: 1.2253718376159668, testing loss: 1.6458275318145752\n",
      "Accuracy: 0.4212\n",
      "epoch: 980, training loss: 1.2371267080307007, testing loss: 1.6623777151107788\n",
      "Accuracy: 0.4058\n",
      "epoch: 980, training loss: 1.2552416324615479, testing loss: 1.658820629119873\n",
      "Accuracy: 0.4348\n",
      "epoch: 980, training loss: 1.189454197883606, testing loss: 1.7784022092819214\n",
      "Accuracy: 0.4048\n",
      "epoch: 980, training loss: 1.1937967538833618, testing loss: 1.8706505298614502\n",
      "Accuracy: 0.3576\n",
      "epoch: 980, training loss: 1.2885178327560425, testing loss: 1.6913337707519531\n",
      "Accuracy: 0.4169\n",
      "epoch: 980, training loss: 1.2367243766784668, testing loss: 1.7432526350021362\n",
      "Accuracy: 0.3813\n",
      "epoch: 980, training loss: 1.137017011642456, testing loss: 1.6721322536468506\n",
      "Accuracy: 0.4180\n",
      "epoch: 980, training loss: 1.2706528902053833, testing loss: 1.692555546760559\n",
      "Accuracy: 0.4214\n",
      "epoch: 980, training loss: 1.2311159372329712, testing loss: 1.7690578699111938\n",
      "Accuracy: 0.4123\n",
      "epoch: 980, training loss: 1.2502707242965698, testing loss: 1.8220480680465698\n",
      "Accuracy: 0.4144\n",
      "epoch: 980, training loss: 1.2996752262115479, testing loss: 1.6146793365478516\n",
      "Accuracy: 0.4237\n",
      "epoch: 980, training loss: 1.262525200843811, testing loss: 1.713465929031372\n",
      "Accuracy: 0.4220\n",
      "epoch: 980, training loss: 1.1820909976959229, testing loss: 1.7012544870376587\n",
      "Accuracy: 0.3865\n",
      "epoch: 980, training loss: 1.2357728481292725, testing loss: 1.8577803373336792\n",
      "Accuracy: 0.4183\n",
      "epoch: 980, training loss: 1.1870886087417603, testing loss: 1.6633464097976685\n",
      "Accuracy: 0.4209\n",
      "epoch: 980, training loss: 1.181803822517395, testing loss: 1.805547833442688\n",
      "Accuracy: 0.4320\n",
      "epoch: 980, training loss: 1.2383931875228882, testing loss: 1.8508914709091187\n",
      "Accuracy: 0.4018\n",
      "epoch: 980, training loss: 1.1880770921707153, testing loss: 1.8005964756011963\n",
      "Accuracy: 0.3784\n",
      "epoch: 990, training loss: 1.2605738639831543, testing loss: 1.740325689315796\n",
      "Accuracy: 0.4110\n",
      "epoch: 990, training loss: 1.181362271308899, testing loss: 1.8256504535675049\n",
      "Accuracy: 0.4058\n",
      "epoch: 990, training loss: 1.2194178104400635, testing loss: 1.7429254055023193\n",
      "Accuracy: 0.3963\n",
      "epoch: 990, training loss: 1.2818604707717896, testing loss: 1.63428795337677\n",
      "Accuracy: 0.4264\n",
      "epoch: 990, training loss: 1.2401201725006104, testing loss: 1.669081449508667\n",
      "Accuracy: 0.4375\n",
      "epoch: 990, training loss: 1.2360972166061401, testing loss: 1.6847290992736816\n",
      "Accuracy: 0.3993\n",
      "epoch: 990, training loss: 1.1747902631759644, testing loss: 1.6665774583816528\n",
      "Accuracy: 0.4315\n",
      "epoch: 990, training loss: 1.1931239366531372, testing loss: 1.8832263946533203\n",
      "Accuracy: 0.4086\n",
      "epoch: 990, training loss: 1.2342123985290527, testing loss: 1.7047241926193237\n",
      "Accuracy: 0.3750\n",
      "epoch: 990, training loss: 1.2658225297927856, testing loss: 1.6262397766113281\n",
      "Accuracy: 0.4610\n",
      "epoch: 990, training loss: 1.286940097808838, testing loss: 1.7091552019119263\n",
      "Accuracy: 0.4351\n",
      "epoch: 990, training loss: 1.226021647453308, testing loss: 1.7541683912277222\n",
      "Accuracy: 0.4338\n",
      "epoch: 990, training loss: 1.2764650583267212, testing loss: 1.7882440090179443\n",
      "Accuracy: 0.3789\n",
      "epoch: 990, training loss: 1.2585816383361816, testing loss: 1.867637038230896\n",
      "Accuracy: 0.3937\n",
      "epoch: 990, training loss: 1.239996075630188, testing loss: 1.8888745307922363\n",
      "Accuracy: 0.3655\n",
      "epoch: 990, training loss: 1.2261977195739746, testing loss: 1.7146047353744507\n",
      "Accuracy: 0.4138\n",
      "epoch: 990, training loss: 1.2699174880981445, testing loss: 1.803866982460022\n",
      "Accuracy: 0.4251\n",
      "epoch: 990, training loss: 1.2086021900177002, testing loss: 1.6368683576583862\n",
      "Accuracy: 0.4521\n",
      "epoch: 990, training loss: 1.335528016090393, testing loss: 1.6363427639007568\n",
      "Accuracy: 0.4307\n",
      "epoch: 990, training loss: 1.292975664138794, testing loss: 1.6628128290176392\n",
      "Accuracy: 0.3651\n",
      "epoch: 990, training loss: 1.2017966508865356, testing loss: 1.708820104598999\n",
      "Accuracy: 0.4327\n",
      "epoch: 990, training loss: 1.1891106367111206, testing loss: 1.6001510620117188\n",
      "Accuracy: 0.4304\n",
      "epoch: 990, training loss: 1.2898744344711304, testing loss: 1.7269608974456787\n",
      "Accuracy: 0.4299\n",
      "epoch: 990, training loss: 1.2391870021820068, testing loss: 1.7798118591308594\n",
      "Accuracy: 0.4212\n",
      "epoch: 990, training loss: 1.265390396118164, testing loss: 1.733705997467041\n",
      "Accuracy: 0.4387\n",
      "epoch: 990, training loss: 1.2590391635894775, testing loss: 1.7664536237716675\n",
      "Accuracy: 0.4098\n",
      "epoch: 990, training loss: 1.2350033521652222, testing loss: 1.598577618598938\n",
      "Accuracy: 0.4316\n",
      "epoch: 990, training loss: 1.213813304901123, testing loss: 1.6491326093673706\n",
      "Accuracy: 0.4012\n",
      "epoch: 990, training loss: 1.2679462432861328, testing loss: 1.7735768556594849\n",
      "Accuracy: 0.3939\n",
      "epoch: 990, training loss: 1.2054661512374878, testing loss: 1.65622079372406\n",
      "Accuracy: 0.4241\n",
      "epoch: 990, training loss: 1.2232160568237305, testing loss: 1.642893671989441\n",
      "Accuracy: 0.4315\n",
      "epoch: 990, training loss: 1.2116366624832153, testing loss: 1.7016055583953857\n",
      "Accuracy: 0.4309\n",
      "epoch: 990, training loss: 1.1807143688201904, testing loss: 1.605177879333496\n",
      "Accuracy: 0.4477\n",
      "epoch: 990, training loss: 1.274030327796936, testing loss: 1.8292627334594727\n",
      "Accuracy: 0.3798\n",
      "epoch: 990, training loss: 1.2400047779083252, testing loss: 1.5867631435394287\n",
      "Accuracy: 0.5183\n",
      "epoch: 990, training loss: 1.261949062347412, testing loss: 1.939202904701233\n",
      "Accuracy: 0.3397\n",
      "epoch: 990, training loss: 1.2082756757736206, testing loss: 1.782426357269287\n",
      "Accuracy: 0.4582\n",
      "epoch: 990, training loss: 1.2198351621627808, testing loss: 1.8495131731033325\n",
      "Accuracy: 0.4169\n",
      "epoch: 990, training loss: 1.2469873428344727, testing loss: 1.5347193479537964\n",
      "Accuracy: 0.4808\n",
      "epoch: 990, training loss: 1.1939717531204224, testing loss: 1.7718825340270996\n",
      "Accuracy: 0.4190\n",
      "epoch: 990, training loss: 1.2715600728988647, testing loss: 1.6652542352676392\n",
      "Accuracy: 0.4243\n",
      "epoch: 990, training loss: 1.2640101909637451, testing loss: 1.7624365091323853\n",
      "Accuracy: 0.4207\n",
      "epoch: 990, training loss: 1.244848608970642, testing loss: 1.803665280342102\n",
      "Accuracy: 0.4325\n",
      "epoch: 990, training loss: 1.2055410146713257, testing loss: 1.735648274421692\n",
      "Accuracy: 0.4107\n",
      "epoch: 990, training loss: 1.292170524597168, testing loss: 1.765270471572876\n",
      "Accuracy: 0.4047\n",
      "epoch: 990, training loss: 1.2135777473449707, testing loss: 1.800709843635559\n",
      "Accuracy: 0.4198\n",
      "epoch: 990, training loss: 1.181128978729248, testing loss: 1.839931607246399\n",
      "Accuracy: 0.3967\n",
      "epoch: 990, training loss: 1.2456488609313965, testing loss: 1.6244009733200073\n",
      "Accuracy: 0.4877\n",
      "epoch: 990, training loss: 1.2600924968719482, testing loss: 1.7974302768707275\n",
      "Accuracy: 0.4184\n",
      "epoch: 990, training loss: 1.2747362852096558, testing loss: 1.779571294784546\n",
      "Accuracy: 0.3918\n",
      "epoch: 990, training loss: 1.2555257081985474, testing loss: 1.7290821075439453\n",
      "Accuracy: 0.3615\n",
      "epoch: 990, training loss: 1.2830824851989746, testing loss: 1.842753529548645\n",
      "Accuracy: 0.3720\n",
      "epoch: 990, training loss: 1.3184517621994019, testing loss: 1.7610199451446533\n",
      "Accuracy: 0.3867\n",
      "epoch: 990, training loss: 1.2532438039779663, testing loss: 1.9069936275482178\n",
      "Accuracy: 0.3817\n",
      "epoch: 990, training loss: 1.2571077346801758, testing loss: 1.673586130142212\n",
      "Accuracy: 0.4498\n",
      "epoch: 990, training loss: 1.284879207611084, testing loss: 1.6769462823867798\n",
      "Accuracy: 0.4451\n",
      "epoch: 990, training loss: 1.1769084930419922, testing loss: 1.6440162658691406\n",
      "Accuracy: 0.4365\n",
      "epoch: 990, training loss: 1.268585205078125, testing loss: 1.708458423614502\n",
      "Accuracy: 0.4012\n",
      "epoch: 990, training loss: 1.1855382919311523, testing loss: 1.669916033744812\n",
      "Accuracy: 0.4238\n",
      "epoch: 990, training loss: 1.2615423202514648, testing loss: 1.7757598161697388\n",
      "Accuracy: 0.3896\n",
      "epoch: 990, training loss: 1.2167932987213135, testing loss: 1.8390995264053345\n",
      "Accuracy: 0.4029\n",
      "epoch: 990, training loss: 1.2394013404846191, testing loss: 1.7922050952911377\n",
      "Accuracy: 0.3859\n",
      "epoch: 990, training loss: 1.2906875610351562, testing loss: 1.6616326570510864\n",
      "Accuracy: 0.4007\n",
      "epoch: 990, training loss: 1.2474086284637451, testing loss: 1.7139461040496826\n",
      "Accuracy: 0.3908\n",
      "epoch: 990, training loss: 1.237445592880249, testing loss: 1.5528672933578491\n",
      "Accuracy: 0.4211\n",
      "epoch: 990, training loss: 1.212769627571106, testing loss: 1.6585625410079956\n",
      "Accuracy: 0.3988\n",
      "epoch: 990, training loss: 1.2659592628479004, testing loss: 1.6525639295578003\n",
      "Accuracy: 0.4175\n",
      "epoch: 990, training loss: 1.207163691520691, testing loss: 1.740708589553833\n",
      "Accuracy: 0.4290\n",
      "epoch: 990, training loss: 1.2207081317901611, testing loss: 1.7110071182250977\n",
      "Accuracy: 0.3907\n",
      "epoch: 990, training loss: 1.280863881111145, testing loss: 1.660341501235962\n",
      "Accuracy: 0.4329\n",
      "epoch: 990, training loss: 1.2784682512283325, testing loss: 1.689946174621582\n",
      "Accuracy: 0.4593\n",
      "epoch: 990, training loss: 1.2157795429229736, testing loss: 1.7755075693130493\n",
      "Accuracy: 0.4367\n",
      "epoch: 990, training loss: 1.1943387985229492, testing loss: 1.7433204650878906\n",
      "Accuracy: 0.4020\n",
      "epoch: 990, training loss: 1.2140774726867676, testing loss: 1.6682308912277222\n",
      "Accuracy: 0.4295\n",
      "epoch: 990, training loss: 1.2493597269058228, testing loss: 1.6919273138046265\n",
      "Accuracy: 0.4148\n",
      "epoch: 990, training loss: 1.3168556690216064, testing loss: 1.704314112663269\n",
      "Accuracy: 0.4358\n",
      "epoch: 990, training loss: 1.2747746706008911, testing loss: 1.6831130981445312\n",
      "Accuracy: 0.4152\n",
      "epoch: 990, training loss: 1.182267427444458, testing loss: 1.6094326972961426\n",
      "Accuracy: 0.4569\n",
      "epoch: 990, training loss: 1.2483859062194824, testing loss: 1.8014438152313232\n",
      "Accuracy: 0.3738\n",
      "epoch: 990, training loss: 1.207977294921875, testing loss: 1.8708964586257935\n",
      "Accuracy: 0.3953\n",
      "epoch: 990, training loss: 1.242799997329712, testing loss: 1.6390650272369385\n",
      "Accuracy: 0.4505\n",
      "epoch: 990, training loss: 1.2984789609909058, testing loss: 1.8110617399215698\n",
      "Accuracy: 0.4020\n",
      "epoch: 990, training loss: 1.280518651008606, testing loss: 1.7961559295654297\n",
      "Accuracy: 0.4094\n",
      "epoch: 990, training loss: 1.2187596559524536, testing loss: 1.7287631034851074\n",
      "Accuracy: 0.3695\n",
      "epoch: 990, training loss: 1.1832530498504639, testing loss: 1.7731391191482544\n",
      "Accuracy: 0.3726\n",
      "epoch: 990, training loss: 1.2156840562820435, testing loss: 1.6970014572143555\n",
      "Accuracy: 0.4500\n",
      "epoch: 990, training loss: 1.304090976715088, testing loss: 1.7445253133773804\n",
      "Accuracy: 0.4249\n",
      "epoch: 990, training loss: 1.2264028787612915, testing loss: 1.6889824867248535\n",
      "Accuracy: 0.3961\n",
      "epoch: 990, training loss: 1.2300093173980713, testing loss: 1.6403793096542358\n",
      "Accuracy: 0.4266\n",
      "epoch: 990, training loss: 1.2499644756317139, testing loss: 1.6802221536636353\n",
      "Accuracy: 0.3737\n",
      "epoch: 990, training loss: 1.269771933555603, testing loss: 1.7513459920883179\n",
      "Accuracy: 0.4202\n",
      "epoch: 990, training loss: 1.2316021919250488, testing loss: 1.7275445461273193\n",
      "Accuracy: 0.3873\n",
      "epoch: 990, training loss: 1.2145055532455444, testing loss: 1.6014392375946045\n",
      "Accuracy: 0.4542\n",
      "epoch: 990, training loss: 1.1829833984375, testing loss: 1.730460286140442\n",
      "Accuracy: 0.4520\n",
      "epoch: 990, training loss: 1.226631760597229, testing loss: 1.7517845630645752\n",
      "Accuracy: 0.4036\n",
      "epoch: 990, training loss: 1.2687960863113403, testing loss: 1.7709869146347046\n",
      "Accuracy: 0.4455\n",
      "epoch: 990, training loss: 1.2806236743927002, testing loss: 1.8194377422332764\n",
      "Accuracy: 0.3921\n",
      "epoch: 990, training loss: 1.1741503477096558, testing loss: 1.747118592262268\n",
      "Accuracy: 0.4448\n",
      "epoch: 990, training loss: 1.2675533294677734, testing loss: 1.6344590187072754\n",
      "Accuracy: 0.4243\n",
      "epoch: 990, training loss: 1.2049007415771484, testing loss: 1.5989620685577393\n",
      "Accuracy: 0.4343\n",
      "epoch: 1000, training loss: 1.1767756938934326, testing loss: 1.7204829454421997\n",
      "Accuracy: 0.4281\n",
      "epoch: 1000, training loss: 1.2910250425338745, testing loss: 1.96908438205719\n",
      "Accuracy: 0.3252\n",
      "epoch: 1000, training loss: 1.1889890432357788, testing loss: 1.6721594333648682\n",
      "Accuracy: 0.4286\n",
      "epoch: 1000, training loss: 1.2252960205078125, testing loss: 1.7061662673950195\n",
      "Accuracy: 0.4202\n",
      "epoch: 1000, training loss: 1.290574073791504, testing loss: 1.6298191547393799\n",
      "Accuracy: 0.4183\n",
      "epoch: 1000, training loss: 1.2732046842575073, testing loss: 1.7628509998321533\n",
      "Accuracy: 0.4333\n",
      "epoch: 1000, training loss: 1.229332685470581, testing loss: 1.683996558189392\n",
      "Accuracy: 0.4404\n",
      "epoch: 1000, training loss: 1.266438603401184, testing loss: 1.606480360031128\n",
      "Accuracy: 0.4125\n",
      "epoch: 1000, training loss: 1.1812162399291992, testing loss: 1.8240467309951782\n",
      "Accuracy: 0.3897\n",
      "epoch: 1000, training loss: 1.2858436107635498, testing loss: 1.7115592956542969\n",
      "Accuracy: 0.4379\n",
      "epoch: 1000, training loss: 1.2729743719100952, testing loss: 1.8208000659942627\n",
      "Accuracy: 0.4011\n",
      "epoch: 1000, training loss: 1.2992181777954102, testing loss: 1.7719789743423462\n",
      "Accuracy: 0.4218\n",
      "epoch: 1000, training loss: 1.166008710861206, testing loss: 1.7783349752426147\n",
      "Accuracy: 0.4122\n",
      "epoch: 1000, training loss: 1.2043159008026123, testing loss: 1.6976186037063599\n",
      "Accuracy: 0.4330\n",
      "epoch: 1000, training loss: 1.30708909034729, testing loss: 1.7126245498657227\n",
      "Accuracy: 0.4212\n",
      "epoch: 1000, training loss: 1.2129486799240112, testing loss: 1.7562761306762695\n",
      "Accuracy: 0.4190\n",
      "epoch: 1000, training loss: 1.2249304056167603, testing loss: 1.6256835460662842\n",
      "Accuracy: 0.4375\n",
      "epoch: 1000, training loss: 1.1651690006256104, testing loss: 1.7430038452148438\n",
      "Accuracy: 0.4342\n",
      "epoch: 1000, training loss: 1.1816061735153198, testing loss: 1.6736516952514648\n",
      "Accuracy: 0.4205\n",
      "epoch: 1000, training loss: 1.2250925302505493, testing loss: 1.71590256690979\n",
      "Accuracy: 0.4164\n",
      "epoch: 1000, training loss: 1.2577600479125977, testing loss: 1.7508772611618042\n",
      "Accuracy: 0.4268\n",
      "epoch: 1000, training loss: 1.2115435600280762, testing loss: 1.6248424053192139\n",
      "Accuracy: 0.4209\n",
      "epoch: 1000, training loss: 1.2584614753723145, testing loss: 1.612004041671753\n",
      "Accuracy: 0.4406\n",
      "epoch: 1000, training loss: 1.228917121887207, testing loss: 1.6869217157363892\n",
      "Accuracy: 0.4013\n",
      "epoch: 1000, training loss: 1.3370139598846436, testing loss: 1.6336281299591064\n",
      "Accuracy: 0.4137\n",
      "epoch: 1000, training loss: 1.2490003108978271, testing loss: 1.681352972984314\n",
      "Accuracy: 0.3891\n",
      "epoch: 1000, training loss: 1.2702748775482178, testing loss: 1.867916464805603\n",
      "Accuracy: 0.3887\n",
      "epoch: 1000, training loss: 1.261911153793335, testing loss: 1.751032829284668\n",
      "Accuracy: 0.4250\n",
      "epoch: 1000, training loss: 1.2509610652923584, testing loss: 1.654849648475647\n",
      "Accuracy: 0.4180\n",
      "epoch: 1000, training loss: 1.2291679382324219, testing loss: 1.769120216369629\n",
      "Accuracy: 0.3981\n",
      "epoch: 1000, training loss: 1.1876245737075806, testing loss: 1.7745568752288818\n",
      "Accuracy: 0.3667\n",
      "epoch: 1000, training loss: 1.2341701984405518, testing loss: 1.7068915367126465\n",
      "Accuracy: 0.3994\n",
      "epoch: 1000, training loss: 1.252687692642212, testing loss: 1.627989411354065\n",
      "Accuracy: 0.4649\n",
      "epoch: 1000, training loss: 1.2785475254058838, testing loss: 1.9057432413101196\n",
      "Accuracy: 0.3742\n",
      "epoch: 1000, training loss: 1.2199116945266724, testing loss: 1.749258041381836\n",
      "Accuracy: 0.3675\n",
      "epoch: 1000, training loss: 1.2513686418533325, testing loss: 1.8020108938217163\n",
      "Accuracy: 0.3497\n",
      "epoch: 1000, training loss: 1.2963141202926636, testing loss: 1.8320571184158325\n",
      "Accuracy: 0.4217\n",
      "epoch: 1000, training loss: 1.2685425281524658, testing loss: 1.8059688806533813\n",
      "Accuracy: 0.3923\n",
      "epoch: 1000, training loss: 1.2321730852127075, testing loss: 1.6477268934249878\n",
      "Accuracy: 0.4595\n",
      "epoch: 1000, training loss: 1.2179772853851318, testing loss: 1.5933057069778442\n",
      "Accuracy: 0.4332\n",
      "epoch: 1000, training loss: 1.2287102937698364, testing loss: 1.8627995252609253\n",
      "Accuracy: 0.3592\n",
      "epoch: 1000, training loss: 1.2222051620483398, testing loss: 1.7684468030929565\n",
      "Accuracy: 0.4032\n",
      "epoch: 1000, training loss: 1.2558246850967407, testing loss: 1.6060429811477661\n",
      "Accuracy: 0.4214\n",
      "epoch: 1000, training loss: 1.265589952468872, testing loss: 1.785234808921814\n",
      "Accuracy: 0.4092\n",
      "epoch: 1000, training loss: 1.2515124082565308, testing loss: 1.7592073678970337\n",
      "Accuracy: 0.4299\n",
      "epoch: 1000, training loss: 1.2253602743148804, testing loss: 1.6441996097564697\n",
      "Accuracy: 0.4096\n",
      "epoch: 1000, training loss: 1.2304840087890625, testing loss: 1.7013945579528809\n",
      "Accuracy: 0.4358\n",
      "epoch: 1000, training loss: 1.2895432710647583, testing loss: 1.7150181531906128\n",
      "Accuracy: 0.4092\n",
      "epoch: 1000, training loss: 1.2424442768096924, testing loss: 1.7359455823898315\n",
      "Accuracy: 0.3982\n",
      "epoch: 1000, training loss: 1.3020814657211304, testing loss: 1.6700873374938965\n",
      "Accuracy: 0.4320\n",
      "epoch: 1000, training loss: 1.2135794162750244, testing loss: 1.7242863178253174\n",
      "Accuracy: 0.3883\n",
      "epoch: 1000, training loss: 1.194874882698059, testing loss: 1.77218496799469\n",
      "Accuracy: 0.4048\n",
      "epoch: 1000, training loss: 1.2621053457260132, testing loss: 1.8079942464828491\n",
      "Accuracy: 0.4315\n",
      "epoch: 1000, training loss: 1.2334970235824585, testing loss: 1.796944499015808\n",
      "Accuracy: 0.4290\n",
      "epoch: 1000, training loss: 1.2695519924163818, testing loss: 1.7637897729873657\n",
      "Accuracy: 0.4203\n",
      "epoch: 1000, training loss: 1.1751490831375122, testing loss: 1.7399568557739258\n",
      "Accuracy: 0.4073\n",
      "epoch: 1000, training loss: 1.1692352294921875, testing loss: 1.8083875179290771\n",
      "Accuracy: 0.4452\n",
      "epoch: 1000, training loss: 1.1800706386566162, testing loss: 1.7575639486312866\n",
      "Accuracy: 0.4394\n",
      "epoch: 1000, training loss: 1.2055009603500366, testing loss: 1.7642302513122559\n",
      "Accuracy: 0.3901\n",
      "epoch: 1000, training loss: 1.2180956602096558, testing loss: 1.5754021406173706\n",
      "Accuracy: 0.4348\n",
      "epoch: 1000, training loss: 1.171572208404541, testing loss: 1.7844916582107544\n",
      "Accuracy: 0.4042\n",
      "epoch: 1000, training loss: 1.2383928298950195, testing loss: 1.679358959197998\n",
      "Accuracy: 0.4207\n",
      "epoch: 1000, training loss: 1.223027229309082, testing loss: 1.6656768321990967\n",
      "Accuracy: 0.4299\n",
      "epoch: 1000, training loss: 1.2344120740890503, testing loss: 1.7126390933990479\n",
      "Accuracy: 0.4320\n",
      "epoch: 1000, training loss: 1.193299412727356, testing loss: 1.7986247539520264\n",
      "Accuracy: 0.4180\n",
      "epoch: 1000, training loss: 1.2309280633926392, testing loss: 1.7343467473983765\n",
      "Accuracy: 0.4241\n",
      "epoch: 1000, training loss: 1.2912243604660034, testing loss: 1.855666160583496\n",
      "Accuracy: 0.4183\n",
      "epoch: 1000, training loss: 1.2294433116912842, testing loss: 1.7813283205032349\n",
      "Accuracy: 0.4610\n",
      "epoch: 1000, training loss: 1.2581584453582764, testing loss: 1.7427008152008057\n",
      "Accuracy: 0.4419\n",
      "epoch: 1000, training loss: 1.285548210144043, testing loss: 1.5544044971466064\n",
      "Accuracy: 0.4954\n",
      "epoch: 1000, training loss: 1.1832793951034546, testing loss: 1.8607673645019531\n",
      "Accuracy: 0.4164\n",
      "epoch: 1000, training loss: 1.2063572406768799, testing loss: 1.8573963642120361\n",
      "Accuracy: 0.3673\n",
      "epoch: 1000, training loss: 1.2304006814956665, testing loss: 1.7744944095611572\n",
      "Accuracy: 0.4107\n",
      "epoch: 1000, training loss: 1.1774568557739258, testing loss: 1.6860802173614502\n",
      "Accuracy: 0.4147\n",
      "epoch: 1000, training loss: 1.2458940744400024, testing loss: 1.7389084100723267\n",
      "Accuracy: 0.4593\n",
      "epoch: 1000, training loss: 1.2654430866241455, testing loss: 1.7418930530548096\n",
      "Accuracy: 0.3790\n",
      "epoch: 1000, training loss: 1.2629973888397217, testing loss: 1.7657889127731323\n",
      "Accuracy: 0.4091\n",
      "epoch: 1000, training loss: 1.252505898475647, testing loss: 1.7315572500228882\n",
      "Accuracy: 0.4571\n",
      "epoch: 1000, training loss: 1.2503752708435059, testing loss: 1.7111960649490356\n",
      "Accuracy: 0.4334\n",
      "epoch: 1000, training loss: 1.2182872295379639, testing loss: 1.8691670894622803\n",
      "Accuracy: 0.3941\n",
      "epoch: 1000, training loss: 1.2695715427398682, testing loss: 1.6785215139389038\n",
      "Accuracy: 0.3957\n",
      "epoch: 1000, training loss: 1.1902029514312744, testing loss: 1.6930713653564453\n",
      "Accuracy: 0.4264\n",
      "epoch: 1000, training loss: 1.2345832586288452, testing loss: 1.7572680711746216\n",
      "Accuracy: 0.3902\n",
      "epoch: 1000, training loss: 1.2978367805480957, testing loss: 1.6567463874816895\n",
      "Accuracy: 0.4413\n",
      "epoch: 1000, training loss: 1.1916356086730957, testing loss: 1.8307785987854004\n",
      "Accuracy: 0.4473\n",
      "epoch: 1000, training loss: 1.1984491348266602, testing loss: 1.6735293865203857\n",
      "Accuracy: 0.4313\n",
      "epoch: 1000, training loss: 1.2668037414550781, testing loss: 1.732926607131958\n",
      "Accuracy: 0.4416\n",
      "epoch: 1000, training loss: 1.1840029954910278, testing loss: 1.7127788066864014\n",
      "Accuracy: 0.4469\n",
      "epoch: 1000, training loss: 1.243671178817749, testing loss: 1.648693323135376\n",
      "Accuracy: 0.4307\n",
      "epoch: 1000, training loss: 1.1800389289855957, testing loss: 1.723547339439392\n",
      "Accuracy: 0.3902\n",
      "epoch: 1000, training loss: 1.2797187566757202, testing loss: 1.7886608839035034\n",
      "Accuracy: 0.4209\n",
      "epoch: 1000, training loss: 1.1967464685440063, testing loss: 1.6746933460235596\n",
      "Accuracy: 0.4569\n",
      "epoch: 1000, training loss: 1.2753403186798096, testing loss: 1.7774271965026855\n",
      "Accuracy: 0.4209\n",
      "epoch: 1000, training loss: 1.227603793144226, testing loss: 1.7444148063659668\n",
      "Accuracy: 0.4598\n",
      "epoch: 1000, training loss: 1.2631442546844482, testing loss: 1.8041719198226929\n",
      "Accuracy: 0.3927\n",
      "epoch: 1000, training loss: 1.2523146867752075, testing loss: 1.8327795267105103\n",
      "Accuracy: 0.4007\n",
      "epoch: 1000, training loss: 1.2379180192947388, testing loss: 1.697595238685608\n",
      "Accuracy: 0.4485\n",
      "epoch: 1000, training loss: 1.2139270305633545, testing loss: 1.8162057399749756\n",
      "Accuracy: 0.4020\n",
      "epoch: 1000, training loss: 1.1935389041900635, testing loss: 1.7926759719848633\n",
      "Accuracy: 0.4237\n",
      "epoch: 1000, training loss: 1.253267765045166, testing loss: 1.8822680711746216\n",
      "Accuracy: 0.4391\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "gcn_model.train()\n",
    "for epoch in range(1000):\n",
    "    for subgraph in loader:\n",
    "        subgraph.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = gcn_model(subgraph)\n",
    "        loss = F.nll_loss(out[subgraph.train_mask], subgraph.y[subgraph.train_mask])\n",
    "    \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            t_loss = F.nll_loss(out[subgraph.test_mask], subgraph.y[subgraph.test_mask])\n",
    "            print(f'epoch: {epoch+1}, training loss: {loss.item()}, testing loss: {t_loss.item()}')\n",
    "            \n",
    "            pred = gcn_model(subgraph).argmax(dim=1)\n",
    "            correct = (pred[subgraph.test_mask] == subgraph.y[subgraph.test_mask]).sum()\n",
    "            acc = int(correct) / int(subgraph.test_mask.sum())\n",
    "            print(f'Accuracy: {acc:.4f}')\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e99d0971-efb1-4926-9792-29bb2bf4568c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, training loss: 1.6761001348495483, testing loss: 1.6795542240142822\n",
      "Accuracy: 0.4032\n",
      "epoch: 20, training loss: 1.6251262426376343, testing loss: 1.633974552154541\n",
      "Accuracy: 0.4384\n",
      "epoch: 30, training loss: 1.6103239059448242, testing loss: 1.621830940246582\n",
      "Accuracy: 0.4635\n",
      "epoch: 40, training loss: 1.5913920402526855, testing loss: 1.6060259342193604\n",
      "Accuracy: 0.4686\n",
      "epoch: 50, training loss: 1.5840343236923218, testing loss: 1.5941628217697144\n",
      "Accuracy: 0.4717\n",
      "epoch: 60, training loss: 1.5681936740875244, testing loss: 1.5830105543136597\n",
      "Accuracy: 0.4740\n",
      "epoch: 70, training loss: 1.563368320465088, testing loss: 1.5739176273345947\n",
      "Accuracy: 0.4814\n",
      "epoch: 80, training loss: 1.5536601543426514, testing loss: 1.5654451847076416\n",
      "Accuracy: 0.4786\n",
      "epoch: 90, training loss: 1.5425270795822144, testing loss: 1.5558490753173828\n",
      "Accuracy: 0.4764\n",
      "epoch: 100, training loss: 1.5299633741378784, testing loss: 1.5451009273529053\n",
      "Accuracy: 0.4794\n",
      "epoch: 110, training loss: 1.5203063488006592, testing loss: 1.5341131687164307\n",
      "Accuracy: 0.4801\n",
      "epoch: 120, training loss: 1.5145741701126099, testing loss: 1.5251275300979614\n",
      "Accuracy: 0.4815\n",
      "epoch: 130, training loss: 1.5095938444137573, testing loss: 1.5219677686691284\n",
      "Accuracy: 0.4850\n",
      "epoch: 140, training loss: 1.5004267692565918, testing loss: 1.5106009244918823\n",
      "Accuracy: 0.4834\n",
      "epoch: 150, training loss: 1.495066523551941, testing loss: 1.5038082599639893\n",
      "Accuracy: 0.4836\n",
      "epoch: 160, training loss: 1.4898864030838013, testing loss: 1.5013798475265503\n",
      "Accuracy: 0.4846\n",
      "epoch: 170, training loss: 1.494756817817688, testing loss: 1.5048004388809204\n",
      "Accuracy: 0.4769\n",
      "epoch: 180, training loss: 1.4839180707931519, testing loss: 1.4929022789001465\n",
      "Accuracy: 0.4841\n",
      "epoch: 190, training loss: 1.478198528289795, testing loss: 1.4889899492263794\n",
      "Accuracy: 0.4861\n",
      "epoch: 200, training loss: 1.4727894067764282, testing loss: 1.482703447341919\n",
      "Accuracy: 0.4857\n",
      "epoch: 210, training loss: 1.4691715240478516, testing loss: 1.4798791408538818\n",
      "Accuracy: 0.4882\n",
      "epoch: 220, training loss: 1.4641780853271484, testing loss: 1.4735230207443237\n",
      "Accuracy: 0.4886\n",
      "epoch: 230, training loss: 1.4627628326416016, testing loss: 1.4747048616409302\n",
      "Accuracy: 0.4909\n",
      "epoch: 240, training loss: 1.4595907926559448, testing loss: 1.4685527086257935\n",
      "Accuracy: 0.4919\n",
      "epoch: 250, training loss: 1.4582977294921875, testing loss: 1.4677115678787231\n",
      "Accuracy: 0.4919\n",
      "epoch: 260, training loss: 1.4520869255065918, testing loss: 1.465006947517395\n",
      "Accuracy: 0.4922\n",
      "epoch: 270, training loss: 1.4542999267578125, testing loss: 1.46424400806427\n",
      "Accuracy: 0.4955\n",
      "epoch: 280, training loss: 1.451404094696045, testing loss: 1.4601502418518066\n",
      "Accuracy: 0.4912\n",
      "epoch: 290, training loss: 1.4508750438690186, testing loss: 1.4636764526367188\n",
      "Accuracy: 0.4851\n",
      "epoch: 300, training loss: 1.444478988647461, testing loss: 1.4545398950576782\n",
      "Accuracy: 0.4942\n",
      "epoch: 310, training loss: 1.4409141540527344, testing loss: 1.4522291421890259\n",
      "Accuracy: 0.4945\n",
      "epoch: 320, training loss: 1.439888596534729, testing loss: 1.4511308670043945\n",
      "Accuracy: 0.4961\n",
      "epoch: 330, training loss: 1.4391306638717651, testing loss: 1.4490317106246948\n",
      "Accuracy: 0.4920\n",
      "epoch: 340, training loss: 1.4371994733810425, testing loss: 1.44782292842865\n",
      "Accuracy: 0.4963\n",
      "epoch: 350, training loss: 1.4346870183944702, testing loss: 1.448689579963684\n",
      "Accuracy: 0.4962\n",
      "epoch: 360, training loss: 1.4346646070480347, testing loss: 1.4448453187942505\n",
      "Accuracy: 0.4960\n",
      "epoch: 370, training loss: 1.4302830696105957, testing loss: 1.4425281286239624\n",
      "Accuracy: 0.4966\n",
      "epoch: 380, training loss: 1.4291884899139404, testing loss: 1.439342737197876\n",
      "Accuracy: 0.4989\n",
      "epoch: 390, training loss: 1.4242583513259888, testing loss: 1.4382476806640625\n",
      "Accuracy: 0.4978\n",
      "epoch: 400, training loss: 1.4230304956436157, testing loss: 1.4363162517547607\n",
      "Accuracy: 0.4991\n",
      "epoch: 410, training loss: 1.4251289367675781, testing loss: 1.436909794807434\n",
      "Accuracy: 0.5000\n",
      "epoch: 420, training loss: 1.4227218627929688, testing loss: 1.4356769323349\n",
      "Accuracy: 0.4986\n",
      "epoch: 430, training loss: 1.42097806930542, testing loss: 1.4319279193878174\n",
      "Accuracy: 0.5004\n",
      "epoch: 440, training loss: 1.4239226579666138, testing loss: 1.4387354850769043\n",
      "Accuracy: 0.5027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projappl/project_2005600/frozen_pretrain/baselines.py:18\u001b[0m, in \u001b[0;36mGCN_Classification.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     16\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[1;32m     17\u001b[0m n_nodes \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg_conv_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py:244\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    241\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m                     \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:480\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         aggr_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 480\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maggr_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    483\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:604\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Tensor, index: Tensor,\n\u001b[1;32m    592\u001b[0m               ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    593\u001b[0m               dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03m    :math:`\\bigoplus_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggr_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch_geometric/experimental.py:115\u001b[0m, in \u001b[0;36mdisable_dynamic_shapes.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_experimental_mode_enabled(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisable_dynamic_shapes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m required_arg \u001b[38;5;129;01min\u001b[39;00m required_args:\n\u001b[1;32m    118\u001b[0m         index \u001b[38;5;241m=\u001b[39m required_args_pos[required_arg]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py:125\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch_geometric/nn/aggr/basic.py:22\u001b[0m, in \u001b[0;36mSumAggregation.forward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m             ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m             dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py:176\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m segment(x, ptr, reduce\u001b[38;5;241m=\u001b[39mreduce)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:70\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     69\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     73\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "gcn_model.train()\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    out = gcn_model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        t_loss = F.nll_loss(out[data.test_mask], data.y[data.test_mask])\n",
    "        print(f'epoch: {epoch+1}, training loss: {loss.item()}, testing loss: {t_loss.item()}')\n",
    "        \n",
    "        pred = gcn_model(data).argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "        acc = int(correct) / int(data.test_mask.sum())\n",
    "        print(f'Accuracy: {acc:.4f}')\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32ec6041-f4f5-41a6-850e-72a00361acb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph_GPT_Classification(\n",
       "  (g_conv_1): GCNConv(500, 768)\n",
       "  (transformer_layers): ModuleList(\n",
       "    (0-5): 6 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (g_conv_2): GCNConv(768, 7)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initilize model\n",
    "gpt_model = GPT2Model.from_pretrained('distilgpt2')\n",
    "graph_gpt_model = Graph_GPT_Classification(gpt_model,\n",
    "                                           dataset.num_node_features, \n",
    "                                           128,\n",
    "                                           dataset.num_classes)\n",
    "\n",
    "graph_gpt_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcf0a276-2b0b-42b5-94bc-ac9e11b5ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GraphSAINTNodeSampler(\n",
    "    dataset[0],\n",
    "    batch_size = 256, \n",
    "    num_steps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436d46a5-5241-42a8-b1a8-78733dbea344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 50, training loss: 6.843184947967529, testing loss: 6.887197971343994\n",
      "Accuracy: 0.3115\n",
      "epoch: 1, step: 100, training loss: 3.9646103382110596, testing loss: 4.012615203857422\n",
      "Accuracy: 0.2881\n",
      "epoch: 1, step: 150, training loss: 3.2658944129943848, testing loss: 3.02982497215271\n",
      "Accuracy: 0.2239\n",
      "epoch: 1, step: 200, training loss: 2.4180073738098145, testing loss: 4.494039058685303\n",
      "Accuracy: 0.2833\n",
      "epoch: 1, step: 250, training loss: 1.8815360069274902, testing loss: 2.364426374435425\n",
      "Accuracy: 0.3478\n",
      "epoch: 1, step: 300, training loss: 2.1509695053100586, testing loss: 1.7727957963943481\n",
      "Accuracy: 0.3026\n",
      "epoch: 1, step: 350, training loss: 2.1587321758270264, testing loss: 2.1124653816223145\n",
      "Accuracy: 0.3750\n",
      "epoch: 1, step: 400, training loss: 1.8465936183929443, testing loss: 2.1011505126953125\n",
      "Accuracy: 0.3115\n",
      "epoch: 1, step: 450, training loss: 1.6380594968795776, testing loss: 2.1301674842834473\n",
      "Accuracy: 0.3269\n",
      "epoch: 1, step: 500, training loss: 2.0073163509368896, testing loss: 1.823908805847168\n",
      "Accuracy: 0.3051\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(graph_gpt_model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(10):\n",
    "    step = 1\n",
    "    for subgraph in loader:\n",
    "        step += 1\n",
    "        subgraph = subgraph.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = graph_gpt_model(subgraph)\n",
    "        loss = F.nll_loss(out[subgraph.train_mask], subgraph.y[subgraph.train_mask])\n",
    "\n",
    "        if (step + 1) % 50 == 0:\n",
    "            # print(f'step: {step+1}, training loss: {loss.item()}')\n",
    "            t_loss = F.nll_loss(out[subgraph.test_mask], subgraph.y[subgraph.test_mask])\n",
    "            print(f'epoch: {epoch + 1}, step: {step+1}, training loss: {loss.item()}, testing loss: {t_loss.item()}')\n",
    "\n",
    "        if (step + 1) % 50 == 0:\n",
    "            pred = graph_gpt_model(subgraph).argmax(dim=1)\n",
    "            correct = (pred[subgraph.test_mask] == subgraph.y[subgraph.test_mask]).sum()\n",
    "            acc = int(correct) / int(subgraph.test_mask.sum())\n",
    "            print(f'Accuracy: {acc:.4f}')\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0018ec2d-b64d-4a72-a341-9a0a7e8ba587",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacty of 31.74 GiB of which 235.38 MiB is free. Including non-PyTorch memory, this process has 31.51 GiB memory in use. Of the allocated memory 29.75 GiB is allocated by PyTorch, and 879.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m      5\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 6\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mgraph_gpt_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/projappl/project_2005600/frozen_pretrain/graph_gpt_classification.py:32\u001b[0m, in \u001b[0;36mGraph_GPT_Classification.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview([n_nodes, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_layers)):\n\u001b[0;32m---> 32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview([n_nodes, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:427\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    426\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 427\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    429\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:354\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m--> 354\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m    356\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/pytorch_utils.py:107\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    106\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacty of 31.74 GiB of which 235.38 MiB is free. Including non-PyTorch memory, this process has 31.51 GiB memory in use. Of the allocated memory 29.75 GiB is allocated by PyTorch, and 879.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "graph_gpt_model.train()\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    out = graph_gpt_model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        t_loss = F.nll_loss(out[data.test_mask], data.y[data.test_mask])\n",
    "        print(f'epoch: {epoch+1}, training loss: {loss.item()}, testing loss: {t_loss.item()}')\n",
    "        \n",
    "        pred = graph_gpt_model(data).argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "        acc = int(correct) / int(data.test_mask.sum())\n",
    "        print(f'Accuracy: {acc:.4f}')\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
